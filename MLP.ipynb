{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('my_df_train.pickle')\n",
    "df_test = pd.read_pickle('my_df_test.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year_month</th>\n",
       "      <th>inj_vol</th>\n",
       "      <th>pp</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2017.83</td>\n",
       "      <td>[0.24230300000000002, 0.364577, 0.69597, 0.760...</td>\n",
       "      <td>[5.260705, 5.240016000000001, 11.603881, 10.67...</td>\n",
       "      <td>0.255919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2017.92</td>\n",
       "      <td>[0.364577, 0.69597, 0.7606510000000001, 0.8123...</td>\n",
       "      <td>[5.240016000000001, 11.603881, 10.673083, 12.5...</td>\n",
       "      <td>0.255919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>[0.69597, 0.7606510000000001, 0.812368, 0.8489...</td>\n",
       "      <td>[11.603881, 10.673083, 12.5484706, 9.9422368, ...</td>\n",
       "      <td>0.255919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2018.08</td>\n",
       "      <td>[0.7606510000000001, 0.812368, 0.8489909999999...</td>\n",
       "      <td>[10.673083, 12.5484706, 9.9422368, 14.05150280...</td>\n",
       "      <td>0.255919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.8</td>\n",
       "      <td>36.4</td>\n",
       "      <td>2018.17</td>\n",
       "      <td>[0.812368, 0.8489909999999999, 0.793254, 0.850...</td>\n",
       "      <td>[12.5484706, 9.9422368, 14.051502800000002, 13...</td>\n",
       "      <td>0.255919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>96.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>2018.33</td>\n",
       "      <td>[2.56893, 2.43225, 2.54391, 2.66482, 2.3774599...</td>\n",
       "      <td>[14.80995, 15.22363, 15.03058, 16.80253, 15.88...</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>96.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>2018.42</td>\n",
       "      <td>[2.43225, 2.54391, 2.66482, 2.3774599999999997...</td>\n",
       "      <td>[15.22363, 15.03058, 16.80253, 15.885525999999...</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>96.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>2018.50</td>\n",
       "      <td>[2.54391, 2.66482, 2.3774599999999997, 2.60024...</td>\n",
       "      <td>[15.03058, 16.80253, 15.885525999999999, 16.25...</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>96.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>2018.58</td>\n",
       "      <td>[2.66482, 2.3774599999999997, 2.60024000000000...</td>\n",
       "      <td>[16.80253, 15.885525999999999, 16.25784, 12.70...</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>96.8</td>\n",
       "      <td>35.4</td>\n",
       "      <td>2018.67</td>\n",
       "      <td>[2.3774599999999997, 2.6002400000000003, 2.529...</td>\n",
       "      <td>[15.885525999999999, 16.25784, 12.707046000000...</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>809 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lat   lon  year_month                                            inj_vol  \\\n",
       "0   96.8  36.4     2017.83  [0.24230300000000002, 0.364577, 0.69597, 0.760...   \n",
       "1   96.8  36.4     2017.92  [0.364577, 0.69597, 0.7606510000000001, 0.8123...   \n",
       "2   96.8  36.4     2018.00  [0.69597, 0.7606510000000001, 0.812368, 0.8489...   \n",
       "3   96.8  36.4     2018.08  [0.7606510000000001, 0.812368, 0.8489909999999...   \n",
       "4   96.8  36.4     2018.17  [0.812368, 0.8489909999999999, 0.793254, 0.850...   \n",
       "..   ...   ...         ...                                                ...   \n",
       "14  96.8  35.4     2018.33  [2.56893, 2.43225, 2.54391, 2.66482, 2.3774599...   \n",
       "15  96.8  35.4     2018.42  [2.43225, 2.54391, 2.66482, 2.3774599999999997...   \n",
       "16  96.8  35.4     2018.50  [2.54391, 2.66482, 2.3774599999999997, 2.60024...   \n",
       "17  96.8  35.4     2018.58  [2.66482, 2.3774599999999997, 2.60024000000000...   \n",
       "18  96.8  35.4     2018.67  [2.3774599999999997, 2.6002400000000003, 2.529...   \n",
       "\n",
       "                                                   pp    lambda  \n",
       "0   [5.260705, 5.240016000000001, 11.603881, 10.67...  0.255919  \n",
       "1   [5.240016000000001, 11.603881, 10.673083, 12.5...  0.255919  \n",
       "2   [11.603881, 10.673083, 12.5484706, 9.9422368, ...  0.255919  \n",
       "3   [10.673083, 12.5484706, 9.9422368, 14.05150280...  0.255919  \n",
       "4   [12.5484706, 9.9422368, 14.051502800000002, 13...  0.255919  \n",
       "..                                                ...       ...  \n",
       "14  [14.80995, 15.22363, 15.03058, 16.80253, 15.88...  0.080100  \n",
       "15  [15.22363, 15.03058, 16.80253, 15.885525999999...  0.080100  \n",
       "16  [15.03058, 16.80253, 15.885525999999999, 16.25...  0.080100  \n",
       "17  [16.80253, 15.885525999999999, 16.25784, 12.70...  0.080100  \n",
       "18  [15.885525999999999, 16.25784, 12.707046000000...  0.080100  \n",
       "\n",
       "[809 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2814, 24) (2814,)\n",
      "(809, 24) (809,)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "inj_vol = np.stack(df_train['inj_vol'].values)\n",
    "pp = np.stack(df_train['pp'].values)\n",
    "\n",
    "# dim: 2814 x 24\n",
    "x_train = np.hstack([inj_vol, pp])\n",
    "y_train = df_train['lambda'].values\n",
    "\n",
    "# test\n",
    "inj_vol = np.stack(df_test['inj_vol'].values)\n",
    "pp = np.stack(df_test['pp'].values)\n",
    "\n",
    "# dim: _ x 24\n",
    "x_test = np.hstack([inj_vol, pp])\n",
    "y_test = df_test['lambda'].values\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        # create layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train test data to torch tensors\n",
    "X_train = torch.from_numpy(x_train.astype('float32'))\n",
    "Y_train = torch.from_numpy(np.expand_dims(y_train, axis=-1).astype('float32'))\n",
    "X_test = torch.from_numpy(x_test.astype('float32'))\n",
    "Y_test = torch.from_numpy(np.expand_dims(y_test, axis=-1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2814, 24]) torch.Size([2814, 1]) torch.float32 torch.float32\n",
      "torch.Size([809, 24]) torch.Size([809, 1]) torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_train.dtype, Y_train.dtype)\n",
    "print(X_test.shape, Y_test.shape, X_test.dtype, Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(24, 32)\n",
    "loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 1.2543, test loss 6.6307\n",
      "Epoch 1: train loss: 1.1132, test loss 6.6091\n",
      "Epoch 2: train loss: 1.0013, test loss 6.6097\n",
      "Epoch 3: train loss: 0.9124, test loss 6.6249\n",
      "Epoch 4: train loss: 0.8410, test loss 6.6496\n",
      "Epoch 5: train loss: 0.7837, test loss 6.6789\n",
      "Epoch 6: train loss: 0.7379, test loss 6.7100\n",
      "Epoch 7: train loss: 0.7010, test loss 6.7416\n",
      "Epoch 8: train loss: 0.6712, test loss 6.7722\n",
      "Epoch 9: train loss: 0.6468, test loss 6.8013\n",
      "Epoch 10: train loss: 0.6269, test loss 6.8280\n",
      "Epoch 11: train loss: 0.6105, test loss 6.8517\n",
      "Epoch 12: train loss: 0.5969, test loss 6.8722\n",
      "Epoch 13: train loss: 0.5855, test loss 6.8894\n",
      "Epoch 14: train loss: 0.5759, test loss 6.9034\n",
      "Epoch 15: train loss: 0.5678, test loss 6.9143\n",
      "Epoch 16: train loss: 0.5608, test loss 6.9222\n",
      "Epoch 17: train loss: 0.5547, test loss 6.9275\n",
      "Epoch 18: train loss: 0.5493, test loss 6.9303\n",
      "Epoch 19: train loss: 0.5445, test loss 6.9307\n",
      "Epoch 20: train loss: 0.5401, test loss 6.9290\n",
      "Epoch 21: train loss: 0.5362, test loss 6.9254\n",
      "Epoch 22: train loss: 0.5326, test loss 6.9200\n",
      "Epoch 23: train loss: 0.5292, test loss 6.9132\n",
      "Epoch 24: train loss: 0.5260, test loss 6.9049\n",
      "Epoch 25: train loss: 0.5230, test loss 6.8952\n",
      "Epoch 26: train loss: 0.5201, test loss 6.8844\n",
      "Epoch 27: train loss: 0.5174, test loss 6.8726\n",
      "Epoch 28: train loss: 0.5148, test loss 6.8598\n",
      "Epoch 29: train loss: 0.5122, test loss 6.8462\n",
      "Epoch 30: train loss: 0.5097, test loss 6.8319\n",
      "Epoch 31: train loss: 0.5073, test loss 6.8169\n",
      "Epoch 32: train loss: 0.5049, test loss 6.8015\n",
      "Epoch 33: train loss: 0.5026, test loss 6.7856\n",
      "Epoch 34: train loss: 0.5004, test loss 6.7693\n",
      "Epoch 35: train loss: 0.4981, test loss 6.7527\n",
      "Epoch 36: train loss: 0.4959, test loss 6.7357\n",
      "Epoch 37: train loss: 0.4938, test loss 6.7186\n",
      "Epoch 38: train loss: 0.4916, test loss 6.7013\n",
      "Epoch 39: train loss: 0.4895, test loss 6.6840\n",
      "Epoch 40: train loss: 0.4875, test loss 6.6665\n",
      "Epoch 41: train loss: 0.4854, test loss 6.6489\n",
      "Epoch 42: train loss: 0.4834, test loss 6.6312\n",
      "Epoch 43: train loss: 0.4814, test loss 6.6135\n",
      "Epoch 44: train loss: 0.4794, test loss 6.5957\n",
      "Epoch 45: train loss: 0.4774, test loss 6.5779\n",
      "Epoch 46: train loss: 0.4755, test loss 6.5602\n",
      "Epoch 47: train loss: 0.4736, test loss 6.5426\n",
      "Epoch 48: train loss: 0.4717, test loss 6.5250\n",
      "Epoch 49: train loss: 0.4699, test loss 6.5075\n",
      "Epoch 50: train loss: 0.4680, test loss 6.4901\n",
      "Epoch 51: train loss: 0.4662, test loss 6.4727\n",
      "Epoch 52: train loss: 0.4644, test loss 6.4553\n",
      "Epoch 53: train loss: 0.4626, test loss 6.4381\n",
      "Epoch 54: train loss: 0.4608, test loss 6.4209\n",
      "Epoch 55: train loss: 0.4590, test loss 6.4038\n",
      "Epoch 56: train loss: 0.4573, test loss 6.3868\n",
      "Epoch 57: train loss: 0.4556, test loss 6.3700\n",
      "Epoch 58: train loss: 0.4539, test loss 6.3534\n",
      "Epoch 59: train loss: 0.4522, test loss 6.3370\n",
      "Epoch 60: train loss: 0.4506, test loss 6.3206\n",
      "Epoch 61: train loss: 0.4489, test loss 6.3044\n",
      "Epoch 62: train loss: 0.4473, test loss 6.2883\n",
      "Epoch 63: train loss: 0.4457, test loss 6.2723\n",
      "Epoch 64: train loss: 0.4441, test loss 6.2564\n",
      "Epoch 65: train loss: 0.4426, test loss 6.2406\n",
      "Epoch 66: train loss: 0.4410, test loss 6.2250\n",
      "Epoch 67: train loss: 0.4395, test loss 6.2095\n",
      "Epoch 68: train loss: 0.4380, test loss 6.1940\n",
      "Epoch 69: train loss: 0.4365, test loss 6.1787\n",
      "Epoch 70: train loss: 0.4350, test loss 6.1635\n",
      "Epoch 71: train loss: 0.4335, test loss 6.1484\n",
      "Epoch 72: train loss: 0.4320, test loss 6.1334\n",
      "Epoch 73: train loss: 0.4306, test loss 6.1187\n",
      "Epoch 74: train loss: 0.4292, test loss 6.1041\n",
      "Epoch 75: train loss: 0.4278, test loss 6.0897\n",
      "Epoch 76: train loss: 0.4264, test loss 6.0753\n",
      "Epoch 77: train loss: 0.4250, test loss 6.0611\n",
      "Epoch 78: train loss: 0.4236, test loss 6.0469\n",
      "Epoch 79: train loss: 0.4223, test loss 6.0329\n",
      "Epoch 80: train loss: 0.4209, test loss 6.0190\n",
      "Epoch 81: train loss: 0.4196, test loss 6.0052\n",
      "Epoch 82: train loss: 0.4183, test loss 5.9915\n",
      "Epoch 83: train loss: 0.4170, test loss 5.9779\n",
      "Epoch 84: train loss: 0.4157, test loss 5.9644\n",
      "Epoch 85: train loss: 0.4144, test loss 5.9511\n",
      "Epoch 86: train loss: 0.4132, test loss 5.9378\n",
      "Epoch 87: train loss: 0.4119, test loss 5.9246\n",
      "Epoch 88: train loss: 0.4107, test loss 5.9116\n",
      "Epoch 89: train loss: 0.4095, test loss 5.8988\n",
      "Epoch 90: train loss: 0.4083, test loss 5.8860\n",
      "Epoch 91: train loss: 0.4071, test loss 5.8734\n",
      "Epoch 92: train loss: 0.4059, test loss 5.8608\n",
      "Epoch 93: train loss: 0.4047, test loss 5.8484\n",
      "Epoch 94: train loss: 0.4035, test loss 5.8360\n",
      "Epoch 95: train loss: 0.4024, test loss 5.8237\n",
      "Epoch 96: train loss: 0.4012, test loss 5.8115\n",
      "Epoch 97: train loss: 0.4001, test loss 5.7993\n",
      "Epoch 98: train loss: 0.3989, test loss 5.7873\n",
      "Epoch 99: train loss: 0.3978, test loss 5.7753\n",
      "Epoch 100: train loss: 0.3967, test loss 5.7634\n",
      "Epoch 101: train loss: 0.3956, test loss 5.7517\n",
      "Epoch 102: train loss: 0.3945, test loss 5.7400\n",
      "Epoch 103: train loss: 0.3934, test loss 5.7284\n",
      "Epoch 104: train loss: 0.3923, test loss 5.7169\n",
      "Epoch 105: train loss: 0.3913, test loss 5.7055\n",
      "Epoch 106: train loss: 0.3902, test loss 5.6942\n",
      "Epoch 107: train loss: 0.3892, test loss 5.6830\n",
      "Epoch 108: train loss: 0.3882, test loss 5.6719\n",
      "Epoch 109: train loss: 0.3871, test loss 5.6609\n",
      "Epoch 110: train loss: 0.3861, test loss 5.6500\n",
      "Epoch 111: train loss: 0.3851, test loss 5.6392\n",
      "Epoch 112: train loss: 0.3841, test loss 5.6285\n",
      "Epoch 113: train loss: 0.3831, test loss 5.6179\n",
      "Epoch 114: train loss: 0.3821, test loss 5.6073\n",
      "Epoch 115: train loss: 0.3812, test loss 5.5968\n",
      "Epoch 116: train loss: 0.3802, test loss 5.5863\n",
      "Epoch 117: train loss: 0.3793, test loss 5.5758\n",
      "Epoch 118: train loss: 0.3783, test loss 5.5655\n",
      "Epoch 119: train loss: 0.3774, test loss 5.5553\n",
      "Epoch 120: train loss: 0.3764, test loss 5.5451\n",
      "Epoch 121: train loss: 0.3755, test loss 5.5350\n",
      "Epoch 122: train loss: 0.3746, test loss 5.5250\n",
      "Epoch 123: train loss: 0.3737, test loss 5.5151\n",
      "Epoch 124: train loss: 0.3728, test loss 5.5052\n",
      "Epoch 125: train loss: 0.3719, test loss 5.4953\n",
      "Epoch 126: train loss: 0.3710, test loss 5.4855\n",
      "Epoch 127: train loss: 0.3702, test loss 5.4757\n",
      "Epoch 128: train loss: 0.3693, test loss 5.4660\n",
      "Epoch 129: train loss: 0.3684, test loss 5.4564\n",
      "Epoch 130: train loss: 0.3676, test loss 5.4469\n",
      "Epoch 131: train loss: 0.3667, test loss 5.4374\n",
      "Epoch 132: train loss: 0.3659, test loss 5.4280\n",
      "Epoch 133: train loss: 0.3651, test loss 5.4187\n",
      "Epoch 134: train loss: 0.3643, test loss 5.4095\n",
      "Epoch 135: train loss: 0.3634, test loss 5.4003\n",
      "Epoch 136: train loss: 0.3626, test loss 5.3911\n",
      "Epoch 137: train loss: 0.3618, test loss 5.3821\n",
      "Epoch 138: train loss: 0.3611, test loss 5.3731\n",
      "Epoch 139: train loss: 0.3603, test loss 5.3642\n",
      "Epoch 140: train loss: 0.3595, test loss 5.3553\n",
      "Epoch 141: train loss: 0.3587, test loss 5.3465\n",
      "Epoch 142: train loss: 0.3580, test loss 5.3377\n",
      "Epoch 143: train loss: 0.3572, test loss 5.3290\n",
      "Epoch 144: train loss: 0.3565, test loss 5.3203\n",
      "Epoch 145: train loss: 0.3557, test loss 5.3117\n",
      "Epoch 146: train loss: 0.3550, test loss 5.3032\n",
      "Epoch 147: train loss: 0.3543, test loss 5.2947\n",
      "Epoch 148: train loss: 0.3535, test loss 5.2863\n",
      "Epoch 149: train loss: 0.3528, test loss 5.2779\n",
      "Epoch 150: train loss: 0.3521, test loss 5.2696\n",
      "Epoch 151: train loss: 0.3514, test loss 5.2613\n",
      "Epoch 152: train loss: 0.3507, test loss 5.2531\n",
      "Epoch 153: train loss: 0.3500, test loss 5.2450\n",
      "Epoch 154: train loss: 0.3493, test loss 5.2369\n",
      "Epoch 155: train loss: 0.3487, test loss 5.2288\n",
      "Epoch 156: train loss: 0.3480, test loss 5.2208\n",
      "Epoch 157: train loss: 0.3473, test loss 5.2128\n",
      "Epoch 158: train loss: 0.3467, test loss 5.2049\n",
      "Epoch 159: train loss: 0.3460, test loss 5.1970\n",
      "Epoch 160: train loss: 0.3453, test loss 5.1892\n",
      "Epoch 161: train loss: 0.3447, test loss 5.1814\n",
      "Epoch 162: train loss: 0.3441, test loss 5.1736\n",
      "Epoch 163: train loss: 0.3434, test loss 5.1659\n",
      "Epoch 164: train loss: 0.3428, test loss 5.1583\n",
      "Epoch 165: train loss: 0.3422, test loss 5.1507\n",
      "Epoch 166: train loss: 0.3415, test loss 5.1431\n",
      "Epoch 167: train loss: 0.3409, test loss 5.1356\n",
      "Epoch 168: train loss: 0.3403, test loss 5.1282\n",
      "Epoch 169: train loss: 0.3397, test loss 5.1208\n",
      "Epoch 170: train loss: 0.3391, test loss 5.1135\n",
      "Epoch 171: train loss: 0.3385, test loss 5.1062\n",
      "Epoch 172: train loss: 0.3379, test loss 5.0989\n",
      "Epoch 173: train loss: 0.3373, test loss 5.0917\n",
      "Epoch 174: train loss: 0.3368, test loss 5.0846\n",
      "Epoch 175: train loss: 0.3362, test loss 5.0774\n",
      "Epoch 176: train loss: 0.3356, test loss 5.0704\n",
      "Epoch 177: train loss: 0.3351, test loss 5.0633\n",
      "Epoch 178: train loss: 0.3345, test loss 5.0563\n",
      "Epoch 179: train loss: 0.3339, test loss 5.0492\n",
      "Epoch 180: train loss: 0.3334, test loss 5.0422\n",
      "Epoch 181: train loss: 0.3328, test loss 5.0353\n",
      "Epoch 182: train loss: 0.3323, test loss 5.0284\n",
      "Epoch 183: train loss: 0.3317, test loss 5.0215\n",
      "Epoch 184: train loss: 0.3312, test loss 5.0147\n",
      "Epoch 185: train loss: 0.3307, test loss 5.0079\n",
      "Epoch 186: train loss: 0.3301, test loss 5.0012\n",
      "Epoch 187: train loss: 0.3296, test loss 4.9945\n",
      "Epoch 188: train loss: 0.3291, test loss 4.9878\n",
      "Epoch 189: train loss: 0.3286, test loss 4.9812\n",
      "Epoch 190: train loss: 0.3281, test loss 4.9746\n",
      "Epoch 191: train loss: 0.3276, test loss 4.9681\n",
      "Epoch 192: train loss: 0.3271, test loss 4.9616\n",
      "Epoch 193: train loss: 0.3266, test loss 4.9551\n",
      "Epoch 194: train loss: 0.3261, test loss 4.9487\n",
      "Epoch 195: train loss: 0.3256, test loss 4.9423\n",
      "Epoch 196: train loss: 0.3251, test loss 4.9359\n",
      "Epoch 197: train loss: 0.3246, test loss 4.9295\n",
      "Epoch 198: train loss: 0.3241, test loss 4.9232\n",
      "Epoch 199: train loss: 0.3237, test loss 4.9170\n",
      "Epoch 200: train loss: 0.3232, test loss 4.9107\n",
      "Epoch 201: train loss: 0.3227, test loss 4.9045\n",
      "Epoch 202: train loss: 0.3223, test loss 4.8983\n",
      "Epoch 203: train loss: 0.3218, test loss 4.8922\n",
      "Epoch 204: train loss: 0.3213, test loss 4.8861\n",
      "Epoch 205: train loss: 0.3209, test loss 4.8800\n",
      "Epoch 206: train loss: 0.3204, test loss 4.8740\n",
      "Epoch 207: train loss: 0.3200, test loss 4.8680\n",
      "Epoch 208: train loss: 0.3195, test loss 4.8620\n",
      "Epoch 209: train loss: 0.3191, test loss 4.8561\n",
      "Epoch 210: train loss: 0.3187, test loss 4.8502\n",
      "Epoch 211: train loss: 0.3182, test loss 4.8443\n",
      "Epoch 212: train loss: 0.3178, test loss 4.8385\n",
      "Epoch 213: train loss: 0.3174, test loss 4.8327\n",
      "Epoch 214: train loss: 0.3169, test loss 4.8269\n",
      "Epoch 215: train loss: 0.3165, test loss 4.8212\n",
      "Epoch 216: train loss: 0.3161, test loss 4.8155\n",
      "Epoch 217: train loss: 0.3157, test loss 4.8098\n",
      "Epoch 218: train loss: 0.3153, test loss 4.8041\n",
      "Epoch 219: train loss: 0.3149, test loss 4.7984\n",
      "Epoch 220: train loss: 0.3145, test loss 4.7928\n",
      "Epoch 221: train loss: 0.3141, test loss 4.7871\n",
      "Epoch 222: train loss: 0.3137, test loss 4.7815\n",
      "Epoch 223: train loss: 0.3133, test loss 4.7759\n",
      "Epoch 224: train loss: 0.3129, test loss 4.7704\n",
      "Epoch 225: train loss: 0.3125, test loss 4.7648\n",
      "Epoch 226: train loss: 0.3121, test loss 4.7593\n",
      "Epoch 227: train loss: 0.3117, test loss 4.7539\n",
      "Epoch 228: train loss: 0.3113, test loss 4.7485\n",
      "Epoch 229: train loss: 0.3109, test loss 4.7431\n",
      "Epoch 230: train loss: 0.3106, test loss 4.7377\n",
      "Epoch 231: train loss: 0.3102, test loss 4.7324\n",
      "Epoch 232: train loss: 0.3098, test loss 4.7271\n",
      "Epoch 233: train loss: 0.3095, test loss 4.7218\n",
      "Epoch 234: train loss: 0.3091, test loss 4.7166\n",
      "Epoch 235: train loss: 0.3087, test loss 4.7113\n",
      "Epoch 236: train loss: 0.3084, test loss 4.7061\n",
      "Epoch 237: train loss: 0.3080, test loss 4.7010\n",
      "Epoch 238: train loss: 0.3077, test loss 4.6959\n",
      "Epoch 239: train loss: 0.3073, test loss 4.6907\n",
      "Epoch 240: train loss: 0.3070, test loss 4.6856\n",
      "Epoch 241: train loss: 0.3066, test loss 4.6805\n",
      "Epoch 242: train loss: 0.3063, test loss 4.6755\n",
      "Epoch 243: train loss: 0.3059, test loss 4.6704\n",
      "Epoch 244: train loss: 0.3056, test loss 4.6654\n",
      "Epoch 245: train loss: 0.3052, test loss 4.6604\n",
      "Epoch 246: train loss: 0.3049, test loss 4.6554\n",
      "Epoch 247: train loss: 0.3046, test loss 4.6505\n",
      "Epoch 248: train loss: 0.3042, test loss 4.6456\n",
      "Epoch 249: train loss: 0.3039, test loss 4.6407\n",
      "Epoch 250: train loss: 0.3036, test loss 4.6358\n",
      "Epoch 251: train loss: 0.3032, test loss 4.6309\n",
      "Epoch 252: train loss: 0.3029, test loss 4.6261\n",
      "Epoch 253: train loss: 0.3026, test loss 4.6212\n",
      "Epoch 254: train loss: 0.3023, test loss 4.6164\n",
      "Epoch 255: train loss: 0.3020, test loss 4.6116\n",
      "Epoch 256: train loss: 0.3017, test loss 4.6068\n",
      "Epoch 257: train loss: 0.3013, test loss 4.6020\n",
      "Epoch 258: train loss: 0.3010, test loss 4.5972\n",
      "Epoch 259: train loss: 0.3007, test loss 4.5924\n",
      "Epoch 260: train loss: 0.3004, test loss 4.5877\n",
      "Epoch 261: train loss: 0.3001, test loss 4.5830\n",
      "Epoch 262: train loss: 0.2998, test loss 4.5783\n",
      "Epoch 263: train loss: 0.2995, test loss 4.5736\n",
      "Epoch 264: train loss: 0.2992, test loss 4.5690\n",
      "Epoch 265: train loss: 0.2989, test loss 4.5643\n",
      "Epoch 266: train loss: 0.2986, test loss 4.5597\n",
      "Epoch 267: train loss: 0.2983, test loss 4.5551\n",
      "Epoch 268: train loss: 0.2980, test loss 4.5506\n",
      "Epoch 269: train loss: 0.2978, test loss 4.5460\n",
      "Epoch 270: train loss: 0.2975, test loss 4.5415\n",
      "Epoch 271: train loss: 0.2972, test loss 4.5370\n",
      "Epoch 272: train loss: 0.2969, test loss 4.5325\n",
      "Epoch 273: train loss: 0.2966, test loss 4.5281\n",
      "Epoch 274: train loss: 0.2964, test loss 4.5236\n",
      "Epoch 275: train loss: 0.2961, test loss 4.5192\n",
      "Epoch 276: train loss: 0.2958, test loss 4.5149\n",
      "Epoch 277: train loss: 0.2955, test loss 4.5105\n",
      "Epoch 278: train loss: 0.2953, test loss 4.5061\n",
      "Epoch 279: train loss: 0.2950, test loss 4.5018\n",
      "Epoch 280: train loss: 0.2948, test loss 4.4976\n",
      "Epoch 281: train loss: 0.2945, test loss 4.4933\n",
      "Epoch 282: train loss: 0.2942, test loss 4.4891\n",
      "Epoch 283: train loss: 0.2940, test loss 4.4848\n",
      "Epoch 284: train loss: 0.2937, test loss 4.4806\n",
      "Epoch 285: train loss: 0.2935, test loss 4.4764\n",
      "Epoch 286: train loss: 0.2932, test loss 4.4722\n",
      "Epoch 287: train loss: 0.2930, test loss 4.4681\n",
      "Epoch 288: train loss: 0.2927, test loss 4.4639\n",
      "Epoch 289: train loss: 0.2925, test loss 4.4598\n",
      "Epoch 290: train loss: 0.2922, test loss 4.4557\n",
      "Epoch 291: train loss: 0.2920, test loss 4.4516\n",
      "Epoch 292: train loss: 0.2918, test loss 4.4476\n",
      "Epoch 293: train loss: 0.2915, test loss 4.4435\n",
      "Epoch 294: train loss: 0.2913, test loss 4.4394\n",
      "Epoch 295: train loss: 0.2910, test loss 4.4354\n",
      "Epoch 296: train loss: 0.2908, test loss 4.4313\n",
      "Epoch 297: train loss: 0.2906, test loss 4.4273\n",
      "Epoch 298: train loss: 0.2903, test loss 4.4233\n",
      "Epoch 299: train loss: 0.2901, test loss 4.4193\n",
      "Epoch 300: train loss: 0.2899, test loss 4.4153\n",
      "Epoch 301: train loss: 0.2897, test loss 4.4113\n",
      "Epoch 302: train loss: 0.2894, test loss 4.4074\n",
      "Epoch 303: train loss: 0.2892, test loss 4.4034\n",
      "Epoch 304: train loss: 0.2890, test loss 4.3995\n",
      "Epoch 305: train loss: 0.2888, test loss 4.3956\n",
      "Epoch 306: train loss: 0.2886, test loss 4.3917\n",
      "Epoch 307: train loss: 0.2883, test loss 4.3878\n",
      "Epoch 308: train loss: 0.2881, test loss 4.3840\n",
      "Epoch 309: train loss: 0.2879, test loss 4.3801\n",
      "Epoch 310: train loss: 0.2877, test loss 4.3763\n",
      "Epoch 311: train loss: 0.2875, test loss 4.3725\n",
      "Epoch 312: train loss: 0.2873, test loss 4.3686\n",
      "Epoch 313: train loss: 0.2871, test loss 4.3648\n",
      "Epoch 314: train loss: 0.2869, test loss 4.3610\n",
      "Epoch 315: train loss: 0.2867, test loss 4.3572\n",
      "Epoch 316: train loss: 0.2864, test loss 4.3534\n",
      "Epoch 317: train loss: 0.2862, test loss 4.3496\n",
      "Epoch 318: train loss: 0.2860, test loss 4.3458\n",
      "Epoch 319: train loss: 0.2858, test loss 4.3421\n",
      "Epoch 320: train loss: 0.2856, test loss 4.3383\n",
      "Epoch 321: train loss: 0.2854, test loss 4.3345\n",
      "Epoch 322: train loss: 0.2852, test loss 4.3308\n",
      "Epoch 323: train loss: 0.2850, test loss 4.3270\n",
      "Epoch 324: train loss: 0.2848, test loss 4.3233\n",
      "Epoch 325: train loss: 0.2847, test loss 4.3196\n",
      "Epoch 326: train loss: 0.2845, test loss 4.3159\n",
      "Epoch 327: train loss: 0.2843, test loss 4.3121\n",
      "Epoch 328: train loss: 0.2841, test loss 4.3085\n",
      "Epoch 329: train loss: 0.2839, test loss 4.3048\n",
      "Epoch 330: train loss: 0.2837, test loss 4.3011\n",
      "Epoch 331: train loss: 0.2835, test loss 4.2974\n",
      "Epoch 332: train loss: 0.2833, test loss 4.2938\n",
      "Epoch 333: train loss: 0.2831, test loss 4.2902\n",
      "Epoch 334: train loss: 0.2829, test loss 4.2865\n",
      "Epoch 335: train loss: 0.2828, test loss 4.2829\n",
      "Epoch 336: train loss: 0.2826, test loss 4.2794\n",
      "Epoch 337: train loss: 0.2824, test loss 4.2758\n",
      "Epoch 338: train loss: 0.2822, test loss 4.2722\n",
      "Epoch 339: train loss: 0.2820, test loss 4.2687\n",
      "Epoch 340: train loss: 0.2819, test loss 4.2651\n",
      "Epoch 341: train loss: 0.2817, test loss 4.2616\n",
      "Epoch 342: train loss: 0.2815, test loss 4.2581\n",
      "Epoch 343: train loss: 0.2813, test loss 4.2546\n",
      "Epoch 344: train loss: 0.2811, test loss 4.2511\n",
      "Epoch 345: train loss: 0.2810, test loss 4.2476\n",
      "Epoch 346: train loss: 0.2808, test loss 4.2441\n",
      "Epoch 347: train loss: 0.2806, test loss 4.2407\n",
      "Epoch 348: train loss: 0.2805, test loss 4.2372\n",
      "Epoch 349: train loss: 0.2803, test loss 4.2338\n",
      "Epoch 350: train loss: 0.2801, test loss 4.2303\n",
      "Epoch 351: train loss: 0.2799, test loss 4.2269\n",
      "Epoch 352: train loss: 0.2798, test loss 4.2235\n",
      "Epoch 353: train loss: 0.2796, test loss 4.2201\n",
      "Epoch 354: train loss: 0.2794, test loss 4.2167\n",
      "Epoch 355: train loss: 0.2793, test loss 4.2132\n",
      "Epoch 356: train loss: 0.2791, test loss 4.2098\n",
      "Epoch 357: train loss: 0.2790, test loss 4.2064\n",
      "Epoch 358: train loss: 0.2788, test loss 4.2030\n",
      "Epoch 359: train loss: 0.2786, test loss 4.1996\n",
      "Epoch 360: train loss: 0.2785, test loss 4.1962\n",
      "Epoch 361: train loss: 0.2783, test loss 4.1929\n",
      "Epoch 362: train loss: 0.2781, test loss 4.1895\n",
      "Epoch 363: train loss: 0.2780, test loss 4.1862\n",
      "Epoch 364: train loss: 0.2778, test loss 4.1829\n",
      "Epoch 365: train loss: 0.2777, test loss 4.1796\n",
      "Epoch 366: train loss: 0.2775, test loss 4.1764\n",
      "Epoch 367: train loss: 0.2774, test loss 4.1731\n",
      "Epoch 368: train loss: 0.2772, test loss 4.1698\n",
      "Epoch 369: train loss: 0.2771, test loss 4.1666\n",
      "Epoch 370: train loss: 0.2769, test loss 4.1634\n",
      "Epoch 371: train loss: 0.2767, test loss 4.1601\n",
      "Epoch 372: train loss: 0.2766, test loss 4.1569\n",
      "Epoch 373: train loss: 0.2764, test loss 4.1537\n",
      "Epoch 374: train loss: 0.2763, test loss 4.1505\n",
      "Epoch 375: train loss: 0.2761, test loss 4.1473\n",
      "Epoch 376: train loss: 0.2760, test loss 4.1442\n",
      "Epoch 377: train loss: 0.2758, test loss 4.1410\n",
      "Epoch 378: train loss: 0.2757, test loss 4.1378\n",
      "Epoch 379: train loss: 0.2756, test loss 4.1347\n",
      "Epoch 380: train loss: 0.2754, test loss 4.1315\n",
      "Epoch 381: train loss: 0.2753, test loss 4.1284\n",
      "Epoch 382: train loss: 0.2751, test loss 4.1253\n",
      "Epoch 383: train loss: 0.2750, test loss 4.1221\n",
      "Epoch 384: train loss: 0.2748, test loss 4.1190\n",
      "Epoch 385: train loss: 0.2747, test loss 4.1159\n",
      "Epoch 386: train loss: 0.2746, test loss 4.1128\n",
      "Epoch 387: train loss: 0.2744, test loss 4.1097\n",
      "Epoch 388: train loss: 0.2743, test loss 4.1066\n",
      "Epoch 389: train loss: 0.2741, test loss 4.1035\n",
      "Epoch 390: train loss: 0.2740, test loss 4.1004\n",
      "Epoch 391: train loss: 0.2739, test loss 4.0973\n",
      "Epoch 392: train loss: 0.2737, test loss 4.0941\n",
      "Epoch 393: train loss: 0.2736, test loss 4.0910\n",
      "Epoch 394: train loss: 0.2734, test loss 4.0879\n",
      "Epoch 395: train loss: 0.2733, test loss 4.0848\n",
      "Epoch 396: train loss: 0.2732, test loss 4.0818\n",
      "Epoch 397: train loss: 0.2730, test loss 4.0786\n",
      "Epoch 398: train loss: 0.2729, test loss 4.0755\n",
      "Epoch 399: train loss: 0.2728, test loss 4.0725\n",
      "Epoch 400: train loss: 0.2726, test loss 4.0694\n",
      "Epoch 401: train loss: 0.2725, test loss 4.0663\n",
      "Epoch 402: train loss: 0.2724, test loss 4.0633\n",
      "Epoch 403: train loss: 0.2722, test loss 4.0603\n",
      "Epoch 404: train loss: 0.2721, test loss 4.0572\n",
      "Epoch 405: train loss: 0.2720, test loss 4.0542\n",
      "Epoch 406: train loss: 0.2718, test loss 4.0512\n",
      "Epoch 407: train loss: 0.2717, test loss 4.0482\n",
      "Epoch 408: train loss: 0.2716, test loss 4.0453\n",
      "Epoch 409: train loss: 0.2715, test loss 4.0423\n",
      "Epoch 410: train loss: 0.2713, test loss 4.0393\n",
      "Epoch 411: train loss: 0.2712, test loss 4.0364\n",
      "Epoch 412: train loss: 0.2711, test loss 4.0334\n",
      "Epoch 413: train loss: 0.2710, test loss 4.0305\n",
      "Epoch 414: train loss: 0.2708, test loss 4.0276\n",
      "Epoch 415: train loss: 0.2707, test loss 4.0247\n",
      "Epoch 416: train loss: 0.2706, test loss 4.0218\n",
      "Epoch 417: train loss: 0.2705, test loss 4.0189\n",
      "Epoch 418: train loss: 0.2704, test loss 4.0160\n",
      "Epoch 419: train loss: 0.2702, test loss 4.0132\n",
      "Epoch 420: train loss: 0.2701, test loss 4.0103\n",
      "Epoch 421: train loss: 0.2700, test loss 4.0075\n",
      "Epoch 422: train loss: 0.2699, test loss 4.0047\n",
      "Epoch 423: train loss: 0.2698, test loss 4.0018\n",
      "Epoch 424: train loss: 0.2696, test loss 3.9990\n",
      "Epoch 425: train loss: 0.2695, test loss 3.9961\n",
      "Epoch 426: train loss: 0.2694, test loss 3.9933\n",
      "Epoch 427: train loss: 0.2693, test loss 3.9905\n",
      "Epoch 428: train loss: 0.2692, test loss 3.9877\n",
      "Epoch 429: train loss: 0.2691, test loss 3.9849\n",
      "Epoch 430: train loss: 0.2690, test loss 3.9822\n",
      "Epoch 431: train loss: 0.2688, test loss 3.9794\n",
      "Epoch 432: train loss: 0.2687, test loss 3.9766\n",
      "Epoch 433: train loss: 0.2686, test loss 3.9739\n",
      "Epoch 434: train loss: 0.2685, test loss 3.9711\n",
      "Epoch 435: train loss: 0.2684, test loss 3.9684\n",
      "Epoch 436: train loss: 0.2683, test loss 3.9657\n",
      "Epoch 437: train loss: 0.2682, test loss 3.9630\n",
      "Epoch 438: train loss: 0.2681, test loss 3.9603\n",
      "Epoch 439: train loss: 0.2679, test loss 3.9576\n",
      "Epoch 440: train loss: 0.2678, test loss 3.9549\n",
      "Epoch 441: train loss: 0.2677, test loss 3.9522\n",
      "Epoch 442: train loss: 0.2676, test loss 3.9494\n",
      "Epoch 443: train loss: 0.2675, test loss 3.9467\n",
      "Epoch 444: train loss: 0.2674, test loss 3.9440\n",
      "Epoch 445: train loss: 0.2673, test loss 3.9413\n",
      "Epoch 446: train loss: 0.2672, test loss 3.9387\n",
      "Epoch 447: train loss: 0.2671, test loss 3.9360\n",
      "Epoch 448: train loss: 0.2670, test loss 3.9333\n",
      "Epoch 449: train loss: 0.2669, test loss 3.9307\n",
      "Epoch 450: train loss: 0.2668, test loss 3.9281\n",
      "Epoch 451: train loss: 0.2667, test loss 3.9255\n",
      "Epoch 452: train loss: 0.2666, test loss 3.9229\n",
      "Epoch 453: train loss: 0.2665, test loss 3.9203\n",
      "Epoch 454: train loss: 0.2664, test loss 3.9177\n",
      "Epoch 455: train loss: 0.2663, test loss 3.9151\n",
      "Epoch 456: train loss: 0.2662, test loss 3.9126\n",
      "Epoch 457: train loss: 0.2661, test loss 3.9100\n",
      "Epoch 458: train loss: 0.2660, test loss 3.9074\n",
      "Epoch 459: train loss: 0.2659, test loss 3.9048\n",
      "Epoch 460: train loss: 0.2658, test loss 3.9023\n",
      "Epoch 461: train loss: 0.2657, test loss 3.8997\n",
      "Epoch 462: train loss: 0.2656, test loss 3.8972\n",
      "Epoch 463: train loss: 0.2655, test loss 3.8946\n",
      "Epoch 464: train loss: 0.2654, test loss 3.8920\n",
      "Epoch 465: train loss: 0.2653, test loss 3.8895\n",
      "Epoch 466: train loss: 0.2652, test loss 3.8869\n",
      "Epoch 467: train loss: 0.2651, test loss 3.8844\n",
      "Epoch 468: train loss: 0.2650, test loss 3.8818\n",
      "Epoch 469: train loss: 0.2649, test loss 3.8793\n",
      "Epoch 470: train loss: 0.2648, test loss 3.8767\n",
      "Epoch 471: train loss: 0.2647, test loss 3.8742\n",
      "Epoch 472: train loss: 0.2646, test loss 3.8717\n",
      "Epoch 473: train loss: 0.2645, test loss 3.8692\n",
      "Epoch 474: train loss: 0.2644, test loss 3.8667\n",
      "Epoch 475: train loss: 0.2643, test loss 3.8642\n",
      "Epoch 476: train loss: 0.2642, test loss 3.8616\n",
      "Epoch 477: train loss: 0.2641, test loss 3.8591\n",
      "Epoch 478: train loss: 0.2640, test loss 3.8566\n",
      "Epoch 479: train loss: 0.2639, test loss 3.8541\n",
      "Epoch 480: train loss: 0.2638, test loss 3.8516\n",
      "Epoch 481: train loss: 0.2637, test loss 3.8491\n",
      "Epoch 482: train loss: 0.2636, test loss 3.8465\n",
      "Epoch 483: train loss: 0.2635, test loss 3.8440\n",
      "Epoch 484: train loss: 0.2634, test loss 3.8414\n",
      "Epoch 485: train loss: 0.2633, test loss 3.8389\n",
      "Epoch 486: train loss: 0.2632, test loss 3.8364\n",
      "Epoch 487: train loss: 0.2631, test loss 3.8339\n",
      "Epoch 488: train loss: 0.2630, test loss 3.8314\n",
      "Epoch 489: train loss: 0.2630, test loss 3.8289\n",
      "Epoch 490: train loss: 0.2629, test loss 3.8264\n",
      "Epoch 491: train loss: 0.2628, test loss 3.8239\n",
      "Epoch 492: train loss: 0.2627, test loss 3.8215\n",
      "Epoch 493: train loss: 0.2626, test loss 3.8190\n",
      "Epoch 494: train loss: 0.2625, test loss 3.8166\n",
      "Epoch 495: train loss: 0.2624, test loss 3.8142\n",
      "Epoch 496: train loss: 0.2623, test loss 3.8118\n",
      "Epoch 497: train loss: 0.2622, test loss 3.8093\n",
      "Epoch 498: train loss: 0.2621, test loss 3.8069\n",
      "Epoch 499: train loss: 0.2620, test loss 3.8045\n",
      "Epoch 500: train loss: 0.2619, test loss 3.8021\n",
      "Epoch 501: train loss: 0.2619, test loss 3.7997\n",
      "Epoch 502: train loss: 0.2618, test loss 3.7973\n",
      "Epoch 503: train loss: 0.2617, test loss 3.7949\n",
      "Epoch 504: train loss: 0.2616, test loss 3.7925\n",
      "Epoch 505: train loss: 0.2615, test loss 3.7901\n",
      "Epoch 506: train loss: 0.2614, test loss 3.7877\n",
      "Epoch 507: train loss: 0.2613, test loss 3.7854\n",
      "Epoch 508: train loss: 0.2612, test loss 3.7830\n",
      "Epoch 509: train loss: 0.2612, test loss 3.7806\n",
      "Epoch 510: train loss: 0.2611, test loss 3.7782\n",
      "Epoch 511: train loss: 0.2610, test loss 3.7759\n",
      "Epoch 512: train loss: 0.2609, test loss 3.7735\n",
      "Epoch 513: train loss: 0.2608, test loss 3.7712\n",
      "Epoch 514: train loss: 0.2607, test loss 3.7688\n",
      "Epoch 515: train loss: 0.2606, test loss 3.7664\n",
      "Epoch 516: train loss: 0.2605, test loss 3.7641\n",
      "Epoch 517: train loss: 0.2605, test loss 3.7617\n",
      "Epoch 518: train loss: 0.2604, test loss 3.7593\n",
      "Epoch 519: train loss: 0.2603, test loss 3.7570\n",
      "Epoch 520: train loss: 0.2602, test loss 3.7547\n",
      "Epoch 521: train loss: 0.2601, test loss 3.7523\n",
      "Epoch 522: train loss: 0.2600, test loss 3.7500\n",
      "Epoch 523: train loss: 0.2599, test loss 3.7477\n",
      "Epoch 524: train loss: 0.2598, test loss 3.7453\n",
      "Epoch 525: train loss: 0.2598, test loss 3.7430\n",
      "Epoch 526: train loss: 0.2597, test loss 3.7407\n",
      "Epoch 527: train loss: 0.2596, test loss 3.7384\n",
      "Epoch 528: train loss: 0.2595, test loss 3.7361\n",
      "Epoch 529: train loss: 0.2594, test loss 3.7338\n",
      "Epoch 530: train loss: 0.2593, test loss 3.7315\n",
      "Epoch 531: train loss: 0.2592, test loss 3.7292\n",
      "Epoch 532: train loss: 0.2592, test loss 3.7269\n",
      "Epoch 533: train loss: 0.2591, test loss 3.7246\n",
      "Epoch 534: train loss: 0.2590, test loss 3.7224\n",
      "Epoch 535: train loss: 0.2589, test loss 3.7201\n",
      "Epoch 536: train loss: 0.2588, test loss 3.7178\n",
      "Epoch 537: train loss: 0.2587, test loss 3.7155\n",
      "Epoch 538: train loss: 0.2587, test loss 3.7132\n",
      "Epoch 539: train loss: 0.2586, test loss 3.7110\n",
      "Epoch 540: train loss: 0.2585, test loss 3.7087\n",
      "Epoch 541: train loss: 0.2584, test loss 3.7064\n",
      "Epoch 542: train loss: 0.2583, test loss 3.7042\n",
      "Epoch 543: train loss: 0.2583, test loss 3.7019\n",
      "Epoch 544: train loss: 0.2582, test loss 3.6997\n",
      "Epoch 545: train loss: 0.2581, test loss 3.6975\n",
      "Epoch 546: train loss: 0.2580, test loss 3.6952\n",
      "Epoch 547: train loss: 0.2579, test loss 3.6930\n",
      "Epoch 548: train loss: 0.2578, test loss 3.6908\n",
      "Epoch 549: train loss: 0.2578, test loss 3.6886\n",
      "Epoch 550: train loss: 0.2577, test loss 3.6864\n",
      "Epoch 551: train loss: 0.2576, test loss 3.6841\n",
      "Epoch 552: train loss: 0.2575, test loss 3.6819\n",
      "Epoch 553: train loss: 0.2574, test loss 3.6797\n",
      "Epoch 554: train loss: 0.2574, test loss 3.6775\n",
      "Epoch 555: train loss: 0.2573, test loss 3.6753\n",
      "Epoch 556: train loss: 0.2572, test loss 3.6730\n",
      "Epoch 557: train loss: 0.2571, test loss 3.6708\n",
      "Epoch 558: train loss: 0.2570, test loss 3.6686\n",
      "Epoch 559: train loss: 0.2570, test loss 3.6664\n",
      "Epoch 560: train loss: 0.2569, test loss 3.6642\n",
      "Epoch 561: train loss: 0.2568, test loss 3.6620\n",
      "Epoch 562: train loss: 0.2567, test loss 3.6599\n",
      "Epoch 563: train loss: 0.2566, test loss 3.6577\n",
      "Epoch 564: train loss: 0.2566, test loss 3.6555\n",
      "Epoch 565: train loss: 0.2565, test loss 3.6534\n",
      "Epoch 566: train loss: 0.2564, test loss 3.6512\n",
      "Epoch 567: train loss: 0.2563, test loss 3.6491\n",
      "Epoch 568: train loss: 0.2563, test loss 3.6470\n",
      "Epoch 569: train loss: 0.2562, test loss 3.6448\n",
      "Epoch 570: train loss: 0.2561, test loss 3.6427\n",
      "Epoch 571: train loss: 0.2560, test loss 3.6405\n",
      "Epoch 572: train loss: 0.2560, test loss 3.6383\n",
      "Epoch 573: train loss: 0.2559, test loss 3.6361\n",
      "Epoch 574: train loss: 0.2558, test loss 3.6339\n",
      "Epoch 575: train loss: 0.2557, test loss 3.6317\n",
      "Epoch 576: train loss: 0.2556, test loss 3.6295\n",
      "Epoch 577: train loss: 0.2556, test loss 3.6274\n",
      "Epoch 578: train loss: 0.2555, test loss 3.6252\n",
      "Epoch 579: train loss: 0.2554, test loss 3.6230\n",
      "Epoch 580: train loss: 0.2553, test loss 3.6208\n",
      "Epoch 581: train loss: 0.2552, test loss 3.6186\n",
      "Epoch 582: train loss: 0.2552, test loss 3.6165\n",
      "Epoch 583: train loss: 0.2551, test loss 3.6143\n",
      "Epoch 584: train loss: 0.2550, test loss 3.6122\n",
      "Epoch 585: train loss: 0.2549, test loss 3.6100\n",
      "Epoch 586: train loss: 0.2548, test loss 3.6079\n",
      "Epoch 587: train loss: 0.2548, test loss 3.6058\n",
      "Epoch 588: train loss: 0.2547, test loss 3.6036\n",
      "Epoch 589: train loss: 0.2546, test loss 3.6015\n",
      "Epoch 590: train loss: 0.2545, test loss 3.5994\n",
      "Epoch 591: train loss: 0.2545, test loss 3.5973\n",
      "Epoch 592: train loss: 0.2544, test loss 3.5952\n",
      "Epoch 593: train loss: 0.2543, test loss 3.5931\n",
      "Epoch 594: train loss: 0.2542, test loss 3.5910\n",
      "Epoch 595: train loss: 0.2542, test loss 3.5889\n",
      "Epoch 596: train loss: 0.2541, test loss 3.5868\n",
      "Epoch 597: train loss: 0.2540, test loss 3.5847\n",
      "Epoch 598: train loss: 0.2539, test loss 3.5826\n",
      "Epoch 599: train loss: 0.2539, test loss 3.5806\n",
      "Epoch 600: train loss: 0.2538, test loss 3.5784\n",
      "Epoch 601: train loss: 0.2537, test loss 3.5763\n",
      "Epoch 602: train loss: 0.2536, test loss 3.5742\n",
      "Epoch 603: train loss: 0.2535, test loss 3.5721\n",
      "Epoch 604: train loss: 0.2535, test loss 3.5700\n",
      "Epoch 605: train loss: 0.2534, test loss 3.5679\n",
      "Epoch 606: train loss: 0.2533, test loss 3.5658\n",
      "Epoch 607: train loss: 0.2532, test loss 3.5638\n",
      "Epoch 608: train loss: 0.2532, test loss 3.5617\n",
      "Epoch 609: train loss: 0.2531, test loss 3.5596\n",
      "Epoch 610: train loss: 0.2530, test loss 3.5575\n",
      "Epoch 611: train loss: 0.2529, test loss 3.5554\n",
      "Epoch 612: train loss: 0.2529, test loss 3.5534\n",
      "Epoch 613: train loss: 0.2528, test loss 3.5513\n",
      "Epoch 614: train loss: 0.2527, test loss 3.5492\n",
      "Epoch 615: train loss: 0.2526, test loss 3.5471\n",
      "Epoch 616: train loss: 0.2526, test loss 3.5451\n",
      "Epoch 617: train loss: 0.2525, test loss 3.5430\n",
      "Epoch 618: train loss: 0.2524, test loss 3.5409\n",
      "Epoch 619: train loss: 0.2524, test loss 3.5389\n",
      "Epoch 620: train loss: 0.2523, test loss 3.5369\n",
      "Epoch 621: train loss: 0.2522, test loss 3.5348\n",
      "Epoch 622: train loss: 0.2521, test loss 3.5328\n",
      "Epoch 623: train loss: 0.2521, test loss 3.5307\n",
      "Epoch 624: train loss: 0.2520, test loss 3.5287\n",
      "Epoch 625: train loss: 0.2519, test loss 3.5266\n",
      "Epoch 626: train loss: 0.2518, test loss 3.5245\n",
      "Epoch 627: train loss: 0.2518, test loss 3.5225\n",
      "Epoch 628: train loss: 0.2517, test loss 3.5204\n",
      "Epoch 629: train loss: 0.2516, test loss 3.5184\n",
      "Epoch 630: train loss: 0.2515, test loss 3.5163\n",
      "Epoch 631: train loss: 0.2515, test loss 3.5143\n",
      "Epoch 632: train loss: 0.2514, test loss 3.5123\n",
      "Epoch 633: train loss: 0.2513, test loss 3.5103\n",
      "Epoch 634: train loss: 0.2512, test loss 3.5083\n",
      "Epoch 635: train loss: 0.2512, test loss 3.5062\n",
      "Epoch 636: train loss: 0.2511, test loss 3.5042\n",
      "Epoch 637: train loss: 0.2510, test loss 3.5022\n",
      "Epoch 638: train loss: 0.2509, test loss 3.5001\n",
      "Epoch 639: train loss: 0.2509, test loss 3.4981\n",
      "Epoch 640: train loss: 0.2508, test loss 3.4961\n",
      "Epoch 641: train loss: 0.2507, test loss 3.4941\n",
      "Epoch 642: train loss: 0.2506, test loss 3.4921\n",
      "Epoch 643: train loss: 0.2506, test loss 3.4901\n",
      "Epoch 644: train loss: 0.2505, test loss 3.4881\n",
      "Epoch 645: train loss: 0.2504, test loss 3.4862\n",
      "Epoch 646: train loss: 0.2503, test loss 3.4842\n",
      "Epoch 647: train loss: 0.2503, test loss 3.4822\n",
      "Epoch 648: train loss: 0.2502, test loss 3.4802\n",
      "Epoch 649: train loss: 0.2501, test loss 3.4782\n",
      "Epoch 650: train loss: 0.2501, test loss 3.4762\n",
      "Epoch 651: train loss: 0.2500, test loss 3.4742\n",
      "Epoch 652: train loss: 0.2499, test loss 3.4723\n",
      "Epoch 653: train loss: 0.2498, test loss 3.4703\n",
      "Epoch 654: train loss: 0.2498, test loss 3.4683\n",
      "Epoch 655: train loss: 0.2497, test loss 3.4664\n",
      "Epoch 656: train loss: 0.2496, test loss 3.4644\n",
      "Epoch 657: train loss: 0.2496, test loss 3.4624\n",
      "Epoch 658: train loss: 0.2495, test loss 3.4605\n",
      "Epoch 659: train loss: 0.2494, test loss 3.4585\n",
      "Epoch 660: train loss: 0.2493, test loss 3.4566\n",
      "Epoch 661: train loss: 0.2493, test loss 3.4546\n",
      "Epoch 662: train loss: 0.2492, test loss 3.4527\n",
      "Epoch 663: train loss: 0.2491, test loss 3.4507\n",
      "Epoch 664: train loss: 0.2491, test loss 3.4488\n",
      "Epoch 665: train loss: 0.2490, test loss 3.4468\n",
      "Epoch 666: train loss: 0.2489, test loss 3.4448\n",
      "Epoch 667: train loss: 0.2489, test loss 3.4428\n",
      "Epoch 668: train loss: 0.2488, test loss 3.4408\n",
      "Epoch 669: train loss: 0.2487, test loss 3.4389\n",
      "Epoch 670: train loss: 0.2486, test loss 3.4369\n",
      "Epoch 671: train loss: 0.2486, test loss 3.4349\n",
      "Epoch 672: train loss: 0.2485, test loss 3.4329\n",
      "Epoch 673: train loss: 0.2484, test loss 3.4309\n",
      "Epoch 674: train loss: 0.2483, test loss 3.4289\n",
      "Epoch 675: train loss: 0.2483, test loss 3.4269\n",
      "Epoch 676: train loss: 0.2482, test loss 3.4249\n",
      "Epoch 677: train loss: 0.2481, test loss 3.4230\n",
      "Epoch 678: train loss: 0.2481, test loss 3.4210\n",
      "Epoch 679: train loss: 0.2480, test loss 3.4191\n",
      "Epoch 680: train loss: 0.2479, test loss 3.4171\n",
      "Epoch 681: train loss: 0.2479, test loss 3.4152\n",
      "Epoch 682: train loss: 0.2478, test loss 3.4133\n",
      "Epoch 683: train loss: 0.2477, test loss 3.4113\n",
      "Epoch 684: train loss: 0.2476, test loss 3.4094\n",
      "Epoch 685: train loss: 0.2476, test loss 3.4075\n",
      "Epoch 686: train loss: 0.2475, test loss 3.4055\n",
      "Epoch 687: train loss: 0.2474, test loss 3.4036\n",
      "Epoch 688: train loss: 0.2474, test loss 3.4017\n",
      "Epoch 689: train loss: 0.2473, test loss 3.3998\n",
      "Epoch 690: train loss: 0.2472, test loss 3.3979\n",
      "Epoch 691: train loss: 0.2472, test loss 3.3960\n",
      "Epoch 692: train loss: 0.2471, test loss 3.3942\n",
      "Epoch 693: train loss: 0.2470, test loss 3.3922\n",
      "Epoch 694: train loss: 0.2470, test loss 3.3903\n",
      "Epoch 695: train loss: 0.2469, test loss 3.3884\n",
      "Epoch 696: train loss: 0.2468, test loss 3.3865\n",
      "Epoch 697: train loss: 0.2468, test loss 3.3847\n",
      "Epoch 698: train loss: 0.2467, test loss 3.3828\n",
      "Epoch 699: train loss: 0.2466, test loss 3.3809\n",
      "Epoch 700: train loss: 0.2466, test loss 3.3790\n",
      "Epoch 701: train loss: 0.2465, test loss 3.3771\n",
      "Epoch 702: train loss: 0.2464, test loss 3.3752\n",
      "Epoch 703: train loss: 0.2464, test loss 3.3733\n",
      "Epoch 704: train loss: 0.2463, test loss 3.3714\n",
      "Epoch 705: train loss: 0.2462, test loss 3.3695\n",
      "Epoch 706: train loss: 0.2462, test loss 3.3676\n",
      "Epoch 707: train loss: 0.2461, test loss 3.3657\n",
      "Epoch 708: train loss: 0.2460, test loss 3.3638\n",
      "Epoch 709: train loss: 0.2460, test loss 3.3619\n",
      "Epoch 710: train loss: 0.2459, test loss 3.3600\n",
      "Epoch 711: train loss: 0.2458, test loss 3.3581\n",
      "Epoch 712: train loss: 0.2458, test loss 3.3562\n",
      "Epoch 713: train loss: 0.2457, test loss 3.3543\n",
      "Epoch 714: train loss: 0.2456, test loss 3.3524\n",
      "Epoch 715: train loss: 0.2456, test loss 3.3506\n",
      "Epoch 716: train loss: 0.2455, test loss 3.3487\n",
      "Epoch 717: train loss: 0.2454, test loss 3.3468\n",
      "Epoch 718: train loss: 0.2454, test loss 3.3450\n",
      "Epoch 719: train loss: 0.2453, test loss 3.3431\n",
      "Epoch 720: train loss: 0.2452, test loss 3.3412\n",
      "Epoch 721: train loss: 0.2452, test loss 3.3394\n",
      "Epoch 722: train loss: 0.2451, test loss 3.3375\n",
      "Epoch 723: train loss: 0.2450, test loss 3.3356\n",
      "Epoch 724: train loss: 0.2450, test loss 3.3338\n",
      "Epoch 725: train loss: 0.2449, test loss 3.3319\n",
      "Epoch 726: train loss: 0.2448, test loss 3.3301\n",
      "Epoch 727: train loss: 0.2448, test loss 3.3282\n",
      "Epoch 728: train loss: 0.2447, test loss 3.3263\n",
      "Epoch 729: train loss: 0.2446, test loss 3.3245\n",
      "Epoch 730: train loss: 0.2446, test loss 3.3226\n",
      "Epoch 731: train loss: 0.2445, test loss 3.3207\n",
      "Epoch 732: train loss: 0.2444, test loss 3.3188\n",
      "Epoch 733: train loss: 0.2444, test loss 3.3170\n",
      "Epoch 734: train loss: 0.2443, test loss 3.3151\n",
      "Epoch 735: train loss: 0.2442, test loss 3.3132\n",
      "Epoch 736: train loss: 0.2442, test loss 3.3113\n",
      "Epoch 737: train loss: 0.2441, test loss 3.3095\n",
      "Epoch 738: train loss: 0.2440, test loss 3.3076\n",
      "Epoch 739: train loss: 0.2440, test loss 3.3057\n",
      "Epoch 740: train loss: 0.2439, test loss 3.3039\n",
      "Epoch 741: train loss: 0.2438, test loss 3.3020\n",
      "Epoch 742: train loss: 0.2438, test loss 3.3001\n",
      "Epoch 743: train loss: 0.2437, test loss 3.2982\n",
      "Epoch 744: train loss: 0.2436, test loss 3.2963\n",
      "Epoch 745: train loss: 0.2436, test loss 3.2945\n",
      "Epoch 746: train loss: 0.2435, test loss 3.2926\n",
      "Epoch 747: train loss: 0.2435, test loss 3.2907\n",
      "Epoch 748: train loss: 0.2434, test loss 3.2888\n",
      "Epoch 749: train loss: 0.2433, test loss 3.2870\n",
      "Epoch 750: train loss: 0.2433, test loss 3.2851\n",
      "Epoch 751: train loss: 0.2432, test loss 3.2832\n",
      "Epoch 752: train loss: 0.2431, test loss 3.2813\n",
      "Epoch 753: train loss: 0.2431, test loss 3.2795\n",
      "Epoch 754: train loss: 0.2430, test loss 3.2776\n",
      "Epoch 755: train loss: 0.2429, test loss 3.2757\n",
      "Epoch 756: train loss: 0.2429, test loss 3.2739\n",
      "Epoch 757: train loss: 0.2428, test loss 3.2720\n",
      "Epoch 758: train loss: 0.2427, test loss 3.2702\n",
      "Epoch 759: train loss: 0.2427, test loss 3.2683\n",
      "Epoch 760: train loss: 0.2426, test loss 3.2664\n",
      "Epoch 761: train loss: 0.2425, test loss 3.2646\n",
      "Epoch 762: train loss: 0.2425, test loss 3.2627\n",
      "Epoch 763: train loss: 0.2424, test loss 3.2609\n",
      "Epoch 764: train loss: 0.2423, test loss 3.2590\n",
      "Epoch 765: train loss: 0.2423, test loss 3.2572\n",
      "Epoch 766: train loss: 0.2422, test loss 3.2553\n",
      "Epoch 767: train loss: 0.2421, test loss 3.2534\n",
      "Epoch 768: train loss: 0.2421, test loss 3.2515\n",
      "Epoch 769: train loss: 0.2420, test loss 3.2496\n",
      "Epoch 770: train loss: 0.2419, test loss 3.2477\n",
      "Epoch 771: train loss: 0.2419, test loss 3.2458\n",
      "Epoch 772: train loss: 0.2418, test loss 3.2440\n",
      "Epoch 773: train loss: 0.2417, test loss 3.2421\n",
      "Epoch 774: train loss: 0.2416, test loss 3.2402\n",
      "Epoch 775: train loss: 0.2416, test loss 3.2384\n",
      "Epoch 776: train loss: 0.2415, test loss 3.2365\n",
      "Epoch 777: train loss: 0.2414, test loss 3.2347\n",
      "Epoch 778: train loss: 0.2414, test loss 3.2328\n",
      "Epoch 779: train loss: 0.2413, test loss 3.2309\n",
      "Epoch 780: train loss: 0.2412, test loss 3.2290\n",
      "Epoch 781: train loss: 0.2412, test loss 3.2271\n",
      "Epoch 782: train loss: 0.2411, test loss 3.2253\n",
      "Epoch 783: train loss: 0.2410, test loss 3.2234\n",
      "Epoch 784: train loss: 0.2410, test loss 3.2215\n",
      "Epoch 785: train loss: 0.2409, test loss 3.2196\n",
      "Epoch 786: train loss: 0.2408, test loss 3.2178\n",
      "Epoch 787: train loss: 0.2407, test loss 3.2159\n",
      "Epoch 788: train loss: 0.2407, test loss 3.2140\n",
      "Epoch 789: train loss: 0.2406, test loss 3.2122\n",
      "Epoch 790: train loss: 0.2405, test loss 3.2103\n",
      "Epoch 791: train loss: 0.2405, test loss 3.2085\n",
      "Epoch 792: train loss: 0.2404, test loss 3.2066\n",
      "Epoch 793: train loss: 0.2403, test loss 3.2048\n",
      "Epoch 794: train loss: 0.2403, test loss 3.2029\n",
      "Epoch 795: train loss: 0.2402, test loss 3.2011\n",
      "Epoch 796: train loss: 0.2401, test loss 3.1993\n",
      "Epoch 797: train loss: 0.2401, test loss 3.1974\n",
      "Epoch 798: train loss: 0.2400, test loss 3.1956\n",
      "Epoch 799: train loss: 0.2399, test loss 3.1938\n",
      "Epoch 800: train loss: 0.2399, test loss 3.1919\n",
      "Epoch 801: train loss: 0.2398, test loss 3.1901\n",
      "Epoch 802: train loss: 0.2397, test loss 3.1883\n",
      "Epoch 803: train loss: 0.2397, test loss 3.1865\n",
      "Epoch 804: train loss: 0.2396, test loss 3.1846\n",
      "Epoch 805: train loss: 0.2395, test loss 3.1829\n",
      "Epoch 806: train loss: 0.2395, test loss 3.1811\n",
      "Epoch 807: train loss: 0.2394, test loss 3.1793\n",
      "Epoch 808: train loss: 0.2393, test loss 3.1775\n",
      "Epoch 809: train loss: 0.2393, test loss 3.1758\n",
      "Epoch 810: train loss: 0.2392, test loss 3.1740\n",
      "Epoch 811: train loss: 0.2392, test loss 3.1722\n",
      "Epoch 812: train loss: 0.2391, test loss 3.1704\n",
      "Epoch 813: train loss: 0.2390, test loss 3.1686\n",
      "Epoch 814: train loss: 0.2390, test loss 3.1668\n",
      "Epoch 815: train loss: 0.2389, test loss 3.1650\n",
      "Epoch 816: train loss: 0.2388, test loss 3.1633\n",
      "Epoch 817: train loss: 0.2388, test loss 3.1615\n",
      "Epoch 818: train loss: 0.2387, test loss 3.1597\n",
      "Epoch 819: train loss: 0.2387, test loss 3.1579\n",
      "Epoch 820: train loss: 0.2386, test loss 3.1561\n",
      "Epoch 821: train loss: 0.2385, test loss 3.1544\n",
      "Epoch 822: train loss: 0.2385, test loss 3.1526\n",
      "Epoch 823: train loss: 0.2384, test loss 3.1508\n",
      "Epoch 824: train loss: 0.2383, test loss 3.1490\n",
      "Epoch 825: train loss: 0.2383, test loss 3.1473\n",
      "Epoch 826: train loss: 0.2382, test loss 3.1455\n",
      "Epoch 827: train loss: 0.2382, test loss 3.1437\n",
      "Epoch 828: train loss: 0.2381, test loss 3.1420\n",
      "Epoch 829: train loss: 0.2380, test loss 3.1402\n",
      "Epoch 830: train loss: 0.2380, test loss 3.1385\n",
      "Epoch 831: train loss: 0.2379, test loss 3.1367\n",
      "Epoch 832: train loss: 0.2379, test loss 3.1350\n",
      "Epoch 833: train loss: 0.2378, test loss 3.1333\n",
      "Epoch 834: train loss: 0.2377, test loss 3.1315\n",
      "Epoch 835: train loss: 0.2377, test loss 3.1298\n",
      "Epoch 836: train loss: 0.2376, test loss 3.1281\n",
      "Epoch 837: train loss: 0.2376, test loss 3.1264\n",
      "Epoch 838: train loss: 0.2375, test loss 3.1246\n",
      "Epoch 839: train loss: 0.2374, test loss 3.1229\n",
      "Epoch 840: train loss: 0.2374, test loss 3.1212\n",
      "Epoch 841: train loss: 0.2373, test loss 3.1195\n",
      "Epoch 842: train loss: 0.2373, test loss 3.1178\n",
      "Epoch 843: train loss: 0.2372, test loss 3.1162\n",
      "Epoch 844: train loss: 0.2372, test loss 3.1145\n",
      "Epoch 845: train loss: 0.2371, test loss 3.1128\n",
      "Epoch 846: train loss: 0.2370, test loss 3.1111\n",
      "Epoch 847: train loss: 0.2370, test loss 3.1094\n",
      "Epoch 848: train loss: 0.2369, test loss 3.1077\n",
      "Epoch 849: train loss: 0.2369, test loss 3.1060\n",
      "Epoch 850: train loss: 0.2368, test loss 3.1043\n",
      "Epoch 851: train loss: 0.2368, test loss 3.1027\n",
      "Epoch 852: train loss: 0.2367, test loss 3.1010\n",
      "Epoch 853: train loss: 0.2367, test loss 3.0993\n",
      "Epoch 854: train loss: 0.2366, test loss 3.0976\n",
      "Epoch 855: train loss: 0.2365, test loss 3.0959\n",
      "Epoch 856: train loss: 0.2365, test loss 3.0942\n",
      "Epoch 857: train loss: 0.2364, test loss 3.0926\n",
      "Epoch 858: train loss: 0.2364, test loss 3.0909\n",
      "Epoch 859: train loss: 0.2363, test loss 3.0893\n",
      "Epoch 860: train loss: 0.2363, test loss 3.0876\n",
      "Epoch 861: train loss: 0.2362, test loss 3.0860\n",
      "Epoch 862: train loss: 0.2362, test loss 3.0843\n",
      "Epoch 863: train loss: 0.2361, test loss 3.0827\n",
      "Epoch 864: train loss: 0.2361, test loss 3.0810\n",
      "Epoch 865: train loss: 0.2360, test loss 3.0794\n",
      "Epoch 866: train loss: 0.2360, test loss 3.0778\n",
      "Epoch 867: train loss: 0.2359, test loss 3.0761\n",
      "Epoch 868: train loss: 0.2359, test loss 3.0745\n",
      "Epoch 869: train loss: 0.2358, test loss 3.0729\n",
      "Epoch 870: train loss: 0.2357, test loss 3.0712\n",
      "Epoch 871: train loss: 0.2357, test loss 3.0696\n",
      "Epoch 872: train loss: 0.2356, test loss 3.0680\n",
      "Epoch 873: train loss: 0.2356, test loss 3.0664\n",
      "Epoch 874: train loss: 0.2355, test loss 3.0648\n",
      "Epoch 875: train loss: 0.2355, test loss 3.0632\n",
      "Epoch 876: train loss: 0.2354, test loss 3.0615\n",
      "Epoch 877: train loss: 0.2354, test loss 3.0599\n",
      "Epoch 878: train loss: 0.2353, test loss 3.0583\n",
      "Epoch 879: train loss: 0.2353, test loss 3.0567\n",
      "Epoch 880: train loss: 0.2352, test loss 3.0551\n",
      "Epoch 881: train loss: 0.2352, test loss 3.0535\n",
      "Epoch 882: train loss: 0.2351, test loss 3.0519\n",
      "Epoch 883: train loss: 0.2351, test loss 3.0503\n",
      "Epoch 884: train loss: 0.2350, test loss 3.0487\n",
      "Epoch 885: train loss: 0.2350, test loss 3.0471\n",
      "Epoch 886: train loss: 0.2349, test loss 3.0455\n",
      "Epoch 887: train loss: 0.2349, test loss 3.0439\n",
      "Epoch 888: train loss: 0.2348, test loss 3.0423\n",
      "Epoch 889: train loss: 0.2348, test loss 3.0407\n",
      "Epoch 890: train loss: 0.2347, test loss 3.0391\n",
      "Epoch 891: train loss: 0.2347, test loss 3.0376\n",
      "Epoch 892: train loss: 0.2347, test loss 3.0360\n",
      "Epoch 893: train loss: 0.2346, test loss 3.0344\n",
      "Epoch 894: train loss: 0.2346, test loss 3.0329\n",
      "Epoch 895: train loss: 0.2345, test loss 3.0313\n",
      "Epoch 896: train loss: 0.2345, test loss 3.0298\n",
      "Epoch 897: train loss: 0.2344, test loss 3.0282\n",
      "Epoch 898: train loss: 0.2344, test loss 3.0266\n",
      "Epoch 899: train loss: 0.2343, test loss 3.0251\n",
      "Epoch 900: train loss: 0.2343, test loss 3.0235\n",
      "Epoch 901: train loss: 0.2342, test loss 3.0220\n",
      "Epoch 902: train loss: 0.2342, test loss 3.0204\n",
      "Epoch 903: train loss: 0.2341, test loss 3.0188\n",
      "Epoch 904: train loss: 0.2341, test loss 3.0173\n",
      "Epoch 905: train loss: 0.2340, test loss 3.0158\n",
      "Epoch 906: train loss: 0.2340, test loss 3.0142\n",
      "Epoch 907: train loss: 0.2340, test loss 3.0127\n",
      "Epoch 908: train loss: 0.2339, test loss 3.0111\n",
      "Epoch 909: train loss: 0.2339, test loss 3.0096\n",
      "Epoch 910: train loss: 0.2338, test loss 3.0081\n",
      "Epoch 911: train loss: 0.2338, test loss 3.0065\n",
      "Epoch 912: train loss: 0.2337, test loss 3.0050\n",
      "Epoch 913: train loss: 0.2337, test loss 3.0035\n",
      "Epoch 914: train loss: 0.2336, test loss 3.0020\n",
      "Epoch 915: train loss: 0.2336, test loss 3.0005\n",
      "Epoch 916: train loss: 0.2336, test loss 2.9990\n",
      "Epoch 917: train loss: 0.2335, test loss 2.9974\n",
      "Epoch 918: train loss: 0.2335, test loss 2.9959\n",
      "Epoch 919: train loss: 0.2334, test loss 2.9945\n",
      "Epoch 920: train loss: 0.2334, test loss 2.9930\n",
      "Epoch 921: train loss: 0.2333, test loss 2.9915\n",
      "Epoch 922: train loss: 0.2333, test loss 2.9900\n",
      "Epoch 923: train loss: 0.2333, test loss 2.9885\n",
      "Epoch 924: train loss: 0.2332, test loss 2.9870\n",
      "Epoch 925: train loss: 0.2332, test loss 2.9855\n",
      "Epoch 926: train loss: 0.2331, test loss 2.9840\n",
      "Epoch 927: train loss: 0.2331, test loss 2.9826\n",
      "Epoch 928: train loss: 0.2330, test loss 2.9811\n",
      "Epoch 929: train loss: 0.2330, test loss 2.9796\n",
      "Epoch 930: train loss: 0.2330, test loss 2.9781\n",
      "Epoch 931: train loss: 0.2329, test loss 2.9766\n",
      "Epoch 932: train loss: 0.2329, test loss 2.9752\n",
      "Epoch 933: train loss: 0.2328, test loss 2.9737\n",
      "Epoch 934: train loss: 0.2328, test loss 2.9722\n",
      "Epoch 935: train loss: 0.2328, test loss 2.9708\n",
      "Epoch 936: train loss: 0.2327, test loss 2.9693\n",
      "Epoch 937: train loss: 0.2327, test loss 2.9678\n",
      "Epoch 938: train loss: 0.2326, test loss 2.9664\n",
      "Epoch 939: train loss: 0.2326, test loss 2.9649\n",
      "Epoch 940: train loss: 0.2326, test loss 2.9634\n",
      "Epoch 941: train loss: 0.2325, test loss 2.9620\n",
      "Epoch 942: train loss: 0.2325, test loss 2.9605\n",
      "Epoch 943: train loss: 0.2324, test loss 2.9590\n",
      "Epoch 944: train loss: 0.2324, test loss 2.9576\n",
      "Epoch 945: train loss: 0.2324, test loss 2.9561\n",
      "Epoch 946: train loss: 0.2323, test loss 2.9546\n",
      "Epoch 947: train loss: 0.2323, test loss 2.9532\n",
      "Epoch 948: train loss: 0.2322, test loss 2.9517\n",
      "Epoch 949: train loss: 0.2322, test loss 2.9502\n",
      "Epoch 950: train loss: 0.2321, test loss 2.9487\n",
      "Epoch 951: train loss: 0.2321, test loss 2.9472\n",
      "Epoch 952: train loss: 0.2321, test loss 2.9457\n",
      "Epoch 953: train loss: 0.2320, test loss 2.9443\n",
      "Epoch 954: train loss: 0.2320, test loss 2.9428\n",
      "Epoch 955: train loss: 0.2319, test loss 2.9414\n",
      "Epoch 956: train loss: 0.2319, test loss 2.9399\n",
      "Epoch 957: train loss: 0.2319, test loss 2.9384\n",
      "Epoch 958: train loss: 0.2318, test loss 2.9370\n",
      "Epoch 959: train loss: 0.2318, test loss 2.9355\n",
      "Epoch 960: train loss: 0.2317, test loss 2.9341\n",
      "Epoch 961: train loss: 0.2317, test loss 2.9327\n",
      "Epoch 962: train loss: 0.2317, test loss 2.9312\n",
      "Epoch 963: train loss: 0.2316, test loss 2.9298\n",
      "Epoch 964: train loss: 0.2316, test loss 2.9283\n",
      "Epoch 965: train loss: 0.2316, test loss 2.9269\n",
      "Epoch 966: train loss: 0.2315, test loss 2.9255\n",
      "Epoch 967: train loss: 0.2315, test loss 2.9241\n",
      "Epoch 968: train loss: 0.2314, test loss 2.9226\n",
      "Epoch 969: train loss: 0.2314, test loss 2.9212\n",
      "Epoch 970: train loss: 0.2314, test loss 2.9198\n",
      "Epoch 971: train loss: 0.2313, test loss 2.9184\n",
      "Epoch 972: train loss: 0.2313, test loss 2.9171\n",
      "Epoch 973: train loss: 0.2313, test loss 2.9157\n",
      "Epoch 974: train loss: 0.2312, test loss 2.9143\n",
      "Epoch 975: train loss: 0.2312, test loss 2.9129\n",
      "Epoch 976: train loss: 0.2311, test loss 2.9115\n",
      "Epoch 977: train loss: 0.2311, test loss 2.9102\n",
      "Epoch 978: train loss: 0.2311, test loss 2.9088\n",
      "Epoch 979: train loss: 0.2310, test loss 2.9074\n",
      "Epoch 980: train loss: 0.2310, test loss 2.9060\n",
      "Epoch 981: train loss: 0.2310, test loss 2.9047\n",
      "Epoch 982: train loss: 0.2309, test loss 2.9033\n",
      "Epoch 983: train loss: 0.2309, test loss 2.9019\n",
      "Epoch 984: train loss: 0.2309, test loss 2.9006\n",
      "Epoch 985: train loss: 0.2308, test loss 2.8992\n",
      "Epoch 986: train loss: 0.2308, test loss 2.8979\n",
      "Epoch 987: train loss: 0.2307, test loss 2.8965\n",
      "Epoch 988: train loss: 0.2307, test loss 2.8952\n",
      "Epoch 989: train loss: 0.2307, test loss 2.8938\n",
      "Epoch 990: train loss: 0.2306, test loss 2.8924\n",
      "Epoch 991: train loss: 0.2306, test loss 2.8911\n",
      "Epoch 992: train loss: 0.2306, test loss 2.8897\n",
      "Epoch 993: train loss: 0.2305, test loss 2.8884\n",
      "Epoch 994: train loss: 0.2305, test loss 2.8870\n",
      "Epoch 995: train loss: 0.2305, test loss 2.8857\n",
      "Epoch 996: train loss: 0.2304, test loss 2.8843\n",
      "Epoch 997: train loss: 0.2304, test loss 2.8830\n",
      "Epoch 998: train loss: 0.2304, test loss 2.8816\n",
      "Epoch 999: train loss: 0.2303, test loss 2.8803\n",
      "Epoch 1000: train loss: 0.2303, test loss 2.8790\n",
      "Epoch 1001: train loss: 0.2303, test loss 2.8776\n",
      "Epoch 1002: train loss: 0.2302, test loss 2.8763\n",
      "Epoch 1003: train loss: 0.2302, test loss 2.8750\n",
      "Epoch 1004: train loss: 0.2302, test loss 2.8737\n",
      "Epoch 1005: train loss: 0.2301, test loss 2.8724\n",
      "Epoch 1006: train loss: 0.2301, test loss 2.8711\n",
      "Epoch 1007: train loss: 0.2301, test loss 2.8699\n",
      "Epoch 1008: train loss: 0.2300, test loss 2.8686\n",
      "Epoch 1009: train loss: 0.2300, test loss 2.8673\n",
      "Epoch 1010: train loss: 0.2300, test loss 2.8660\n",
      "Epoch 1011: train loss: 0.2299, test loss 2.8647\n",
      "Epoch 1012: train loss: 0.2299, test loss 2.8635\n",
      "Epoch 1013: train loss: 0.2299, test loss 2.8622\n",
      "Epoch 1014: train loss: 0.2298, test loss 2.8609\n",
      "Epoch 1015: train loss: 0.2298, test loss 2.8596\n",
      "Epoch 1016: train loss: 0.2298, test loss 2.8583\n",
      "Epoch 1017: train loss: 0.2298, test loss 2.8571\n",
      "Epoch 1018: train loss: 0.2297, test loss 2.8558\n",
      "Epoch 1019: train loss: 0.2297, test loss 2.8545\n",
      "Epoch 1020: train loss: 0.2297, test loss 2.8533\n",
      "Epoch 1021: train loss: 0.2296, test loss 2.8520\n",
      "Epoch 1022: train loss: 0.2296, test loss 2.8507\n",
      "Epoch 1023: train loss: 0.2296, test loss 2.8495\n",
      "Epoch 1024: train loss: 0.2295, test loss 2.8482\n",
      "Epoch 1025: train loss: 0.2295, test loss 2.8469\n",
      "Epoch 1026: train loss: 0.2295, test loss 2.8457\n",
      "Epoch 1027: train loss: 0.2294, test loss 2.8444\n",
      "Epoch 1028: train loss: 0.2294, test loss 2.8431\n",
      "Epoch 1029: train loss: 0.2294, test loss 2.8419\n",
      "Epoch 1030: train loss: 0.2293, test loss 2.8406\n",
      "Epoch 1031: train loss: 0.2293, test loss 2.8393\n",
      "Epoch 1032: train loss: 0.2293, test loss 2.8381\n",
      "Epoch 1033: train loss: 0.2293, test loss 2.8368\n",
      "Epoch 1034: train loss: 0.2292, test loss 2.8356\n",
      "Epoch 1035: train loss: 0.2292, test loss 2.8343\n",
      "Epoch 1036: train loss: 0.2292, test loss 2.8330\n",
      "Epoch 1037: train loss: 0.2291, test loss 2.8318\n",
      "Epoch 1038: train loss: 0.2291, test loss 2.8305\n",
      "Epoch 1039: train loss: 0.2291, test loss 2.8292\n",
      "Epoch 1040: train loss: 0.2290, test loss 2.8280\n",
      "Epoch 1041: train loss: 0.2290, test loss 2.8267\n",
      "Epoch 1042: train loss: 0.2290, test loss 2.8254\n",
      "Epoch 1043: train loss: 0.2289, test loss 2.8242\n",
      "Epoch 1044: train loss: 0.2289, test loss 2.8229\n",
      "Epoch 1045: train loss: 0.2289, test loss 2.8216\n",
      "Epoch 1046: train loss: 0.2288, test loss 2.8204\n",
      "Epoch 1047: train loss: 0.2288, test loss 2.8191\n",
      "Epoch 1048: train loss: 0.2288, test loss 2.8178\n",
      "Epoch 1049: train loss: 0.2288, test loss 2.8166\n",
      "Epoch 1050: train loss: 0.2287, test loss 2.8153\n",
      "Epoch 1051: train loss: 0.2287, test loss 2.8140\n",
      "Epoch 1052: train loss: 0.2287, test loss 2.8128\n",
      "Epoch 1053: train loss: 0.2286, test loss 2.8115\n",
      "Epoch 1054: train loss: 0.2286, test loss 2.8102\n",
      "Epoch 1055: train loss: 0.2286, test loss 2.8090\n",
      "Epoch 1056: train loss: 0.2285, test loss 2.8077\n",
      "Epoch 1057: train loss: 0.2285, test loss 2.8065\n",
      "Epoch 1058: train loss: 0.2285, test loss 2.8052\n",
      "Epoch 1059: train loss: 0.2284, test loss 2.8039\n",
      "Epoch 1060: train loss: 0.2284, test loss 2.8027\n",
      "Epoch 1061: train loss: 0.2284, test loss 2.8014\n",
      "Epoch 1062: train loss: 0.2284, test loss 2.8002\n",
      "Epoch 1063: train loss: 0.2283, test loss 2.7989\n",
      "Epoch 1064: train loss: 0.2283, test loss 2.7977\n",
      "Epoch 1065: train loss: 0.2283, test loss 2.7964\n",
      "Epoch 1066: train loss: 0.2282, test loss 2.7952\n",
      "Epoch 1067: train loss: 0.2282, test loss 2.7939\n",
      "Epoch 1068: train loss: 0.2282, test loss 2.7927\n",
      "Epoch 1069: train loss: 0.2281, test loss 2.7914\n",
      "Epoch 1070: train loss: 0.2281, test loss 2.7902\n",
      "Epoch 1071: train loss: 0.2281, test loss 2.7889\n",
      "Epoch 1072: train loss: 0.2280, test loss 2.7877\n",
      "Epoch 1073: train loss: 0.2280, test loss 2.7864\n",
      "Epoch 1074: train loss: 0.2280, test loss 2.7852\n",
      "Epoch 1075: train loss: 0.2280, test loss 2.7840\n",
      "Epoch 1076: train loss: 0.2279, test loss 2.7827\n",
      "Epoch 1077: train loss: 0.2279, test loss 2.7815\n",
      "Epoch 1078: train loss: 0.2279, test loss 2.7803\n",
      "Epoch 1079: train loss: 0.2278, test loss 2.7791\n",
      "Epoch 1080: train loss: 0.2278, test loss 2.7778\n",
      "Epoch 1081: train loss: 0.2278, test loss 2.7766\n",
      "Epoch 1082: train loss: 0.2277, test loss 2.7754\n",
      "Epoch 1083: train loss: 0.2277, test loss 2.7742\n",
      "Epoch 1084: train loss: 0.2277, test loss 2.7730\n",
      "Epoch 1085: train loss: 0.2277, test loss 2.7718\n",
      "Epoch 1086: train loss: 0.2276, test loss 2.7705\n",
      "Epoch 1087: train loss: 0.2276, test loss 2.7693\n",
      "Epoch 1088: train loss: 0.2276, test loss 2.7681\n",
      "Epoch 1089: train loss: 0.2275, test loss 2.7669\n",
      "Epoch 1090: train loss: 0.2275, test loss 2.7657\n",
      "Epoch 1091: train loss: 0.2275, test loss 2.7645\n",
      "Epoch 1092: train loss: 0.2274, test loss 2.7633\n",
      "Epoch 1093: train loss: 0.2274, test loss 2.7620\n",
      "Epoch 1094: train loss: 0.2274, test loss 2.7608\n",
      "Epoch 1095: train loss: 0.2274, test loss 2.7596\n",
      "Epoch 1096: train loss: 0.2273, test loss 2.7584\n",
      "Epoch 1097: train loss: 0.2273, test loss 2.7572\n",
      "Epoch 1098: train loss: 0.2273, test loss 2.7560\n",
      "Epoch 1099: train loss: 0.2272, test loss 2.7548\n",
      "Epoch 1100: train loss: 0.2272, test loss 2.7536\n",
      "Epoch 1101: train loss: 0.2272, test loss 2.7524\n",
      "Epoch 1102: train loss: 0.2272, test loss 2.7512\n",
      "Epoch 1103: train loss: 0.2271, test loss 2.7500\n",
      "Epoch 1104: train loss: 0.2271, test loss 2.7488\n",
      "Epoch 1105: train loss: 0.2271, test loss 2.7476\n",
      "Epoch 1106: train loss: 0.2270, test loss 2.7464\n",
      "Epoch 1107: train loss: 0.2270, test loss 2.7452\n",
      "Epoch 1108: train loss: 0.2270, test loss 2.7440\n",
      "Epoch 1109: train loss: 0.2269, test loss 2.7428\n",
      "Epoch 1110: train loss: 0.2269, test loss 2.7416\n",
      "Epoch 1111: train loss: 0.2269, test loss 2.7404\n",
      "Epoch 1112: train loss: 0.2269, test loss 2.7392\n",
      "Epoch 1113: train loss: 0.2268, test loss 2.7380\n",
      "Epoch 1114: train loss: 0.2268, test loss 2.7369\n",
      "Epoch 1115: train loss: 0.2268, test loss 2.7357\n",
      "Epoch 1116: train loss: 0.2267, test loss 2.7345\n",
      "Epoch 1117: train loss: 0.2267, test loss 2.7333\n",
      "Epoch 1118: train loss: 0.2267, test loss 2.7321\n",
      "Epoch 1119: train loss: 0.2267, test loss 2.7309\n",
      "Epoch 1120: train loss: 0.2266, test loss 2.7297\n",
      "Epoch 1121: train loss: 0.2266, test loss 2.7286\n",
      "Epoch 1122: train loss: 0.2266, test loss 2.7274\n",
      "Epoch 1123: train loss: 0.2265, test loss 2.7262\n",
      "Epoch 1124: train loss: 0.2265, test loss 2.7251\n",
      "Epoch 1125: train loss: 0.2265, test loss 2.7239\n",
      "Epoch 1126: train loss: 0.2265, test loss 2.7227\n",
      "Epoch 1127: train loss: 0.2264, test loss 2.7216\n",
      "Epoch 1128: train loss: 0.2264, test loss 2.7204\n",
      "Epoch 1129: train loss: 0.2264, test loss 2.7192\n",
      "Epoch 1130: train loss: 0.2263, test loss 2.7181\n",
      "Epoch 1131: train loss: 0.2263, test loss 2.7169\n",
      "Epoch 1132: train loss: 0.2263, test loss 2.7158\n",
      "Epoch 1133: train loss: 0.2263, test loss 2.7146\n",
      "Epoch 1134: train loss: 0.2262, test loss 2.7135\n",
      "Epoch 1135: train loss: 0.2262, test loss 2.7123\n",
      "Epoch 1136: train loss: 0.2262, test loss 2.7112\n",
      "Epoch 1137: train loss: 0.2261, test loss 2.7100\n",
      "Epoch 1138: train loss: 0.2261, test loss 2.7089\n",
      "Epoch 1139: train loss: 0.2261, test loss 2.7077\n",
      "Epoch 1140: train loss: 0.2261, test loss 2.7066\n",
      "Epoch 1141: train loss: 0.2260, test loss 2.7054\n",
      "Epoch 1142: train loss: 0.2260, test loss 2.7043\n",
      "Epoch 1143: train loss: 0.2260, test loss 2.7031\n",
      "Epoch 1144: train loss: 0.2260, test loss 2.7020\n",
      "Epoch 1145: train loss: 0.2259, test loss 2.7009\n",
      "Epoch 1146: train loss: 0.2259, test loss 2.6997\n",
      "Epoch 1147: train loss: 0.2259, test loss 2.6986\n",
      "Epoch 1148: train loss: 0.2258, test loss 2.6975\n",
      "Epoch 1149: train loss: 0.2258, test loss 2.6963\n",
      "Epoch 1150: train loss: 0.2258, test loss 2.6952\n",
      "Epoch 1151: train loss: 0.2258, test loss 2.6941\n",
      "Epoch 1152: train loss: 0.2257, test loss 2.6929\n",
      "Epoch 1153: train loss: 0.2257, test loss 2.6918\n",
      "Epoch 1154: train loss: 0.2257, test loss 2.6907\n",
      "Epoch 1155: train loss: 0.2257, test loss 2.6895\n",
      "Epoch 1156: train loss: 0.2256, test loss 2.6884\n",
      "Epoch 1157: train loss: 0.2256, test loss 2.6873\n",
      "Epoch 1158: train loss: 0.2256, test loss 2.6861\n",
      "Epoch 1159: train loss: 0.2255, test loss 2.6850\n",
      "Epoch 1160: train loss: 0.2255, test loss 2.6839\n",
      "Epoch 1161: train loss: 0.2255, test loss 2.6827\n",
      "Epoch 1162: train loss: 0.2255, test loss 2.6816\n",
      "Epoch 1163: train loss: 0.2254, test loss 2.6804\n",
      "Epoch 1164: train loss: 0.2254, test loss 2.6793\n",
      "Epoch 1165: train loss: 0.2254, test loss 2.6782\n",
      "Epoch 1166: train loss: 0.2254, test loss 2.6770\n",
      "Epoch 1167: train loss: 0.2253, test loss 2.6759\n",
      "Epoch 1168: train loss: 0.2253, test loss 2.6747\n",
      "Epoch 1169: train loss: 0.2253, test loss 2.6736\n",
      "Epoch 1170: train loss: 0.2252, test loss 2.6724\n",
      "Epoch 1171: train loss: 0.2252, test loss 2.6713\n",
      "Epoch 1172: train loss: 0.2252, test loss 2.6702\n",
      "Epoch 1173: train loss: 0.2252, test loss 2.6690\n",
      "Epoch 1174: train loss: 0.2251, test loss 2.6679\n",
      "Epoch 1175: train loss: 0.2251, test loss 2.6668\n",
      "Epoch 1176: train loss: 0.2251, test loss 2.6656\n",
      "Epoch 1177: train loss: 0.2251, test loss 2.6645\n",
      "Epoch 1178: train loss: 0.2250, test loss 2.6634\n",
      "Epoch 1179: train loss: 0.2250, test loss 2.6622\n",
      "Epoch 1180: train loss: 0.2250, test loss 2.6611\n",
      "Epoch 1181: train loss: 0.2250, test loss 2.6600\n",
      "Epoch 1182: train loss: 0.2249, test loss 2.6589\n",
      "Epoch 1183: train loss: 0.2249, test loss 2.6577\n",
      "Epoch 1184: train loss: 0.2249, test loss 2.6566\n",
      "Epoch 1185: train loss: 0.2248, test loss 2.6555\n",
      "Epoch 1186: train loss: 0.2248, test loss 2.6544\n",
      "Epoch 1187: train loss: 0.2248, test loss 2.6533\n",
      "Epoch 1188: train loss: 0.2248, test loss 2.6522\n",
      "Epoch 1189: train loss: 0.2247, test loss 2.6510\n",
      "Epoch 1190: train loss: 0.2247, test loss 2.6499\n",
      "Epoch 1191: train loss: 0.2247, test loss 2.6488\n",
      "Epoch 1192: train loss: 0.2247, test loss 2.6477\n",
      "Epoch 1193: train loss: 0.2246, test loss 2.6466\n",
      "Epoch 1194: train loss: 0.2246, test loss 2.6455\n",
      "Epoch 1195: train loss: 0.2246, test loss 2.6443\n",
      "Epoch 1196: train loss: 0.2246, test loss 2.6432\n",
      "Epoch 1197: train loss: 0.2245, test loss 2.6421\n",
      "Epoch 1198: train loss: 0.2245, test loss 2.6410\n",
      "Epoch 1199: train loss: 0.2245, test loss 2.6399\n",
      "Epoch 1200: train loss: 0.2244, test loss 2.6388\n",
      "Epoch 1201: train loss: 0.2244, test loss 2.6376\n",
      "Epoch 1202: train loss: 0.2244, test loss 2.6365\n",
      "Epoch 1203: train loss: 0.2244, test loss 2.6354\n",
      "Epoch 1204: train loss: 0.2243, test loss 2.6343\n",
      "Epoch 1205: train loss: 0.2243, test loss 2.6332\n",
      "Epoch 1206: train loss: 0.2243, test loss 2.6321\n",
      "Epoch 1207: train loss: 0.2243, test loss 2.6310\n",
      "Epoch 1208: train loss: 0.2242, test loss 2.6299\n",
      "Epoch 1209: train loss: 0.2242, test loss 2.6288\n",
      "Epoch 1210: train loss: 0.2242, test loss 2.6277\n",
      "Epoch 1211: train loss: 0.2242, test loss 2.6266\n",
      "Epoch 1212: train loss: 0.2241, test loss 2.6255\n",
      "Epoch 1213: train loss: 0.2241, test loss 2.6244\n",
      "Epoch 1214: train loss: 0.2241, test loss 2.6233\n",
      "Epoch 1215: train loss: 0.2241, test loss 2.6222\n",
      "Epoch 1216: train loss: 0.2240, test loss 2.6211\n",
      "Epoch 1217: train loss: 0.2240, test loss 2.6200\n",
      "Epoch 1218: train loss: 0.2240, test loss 2.6190\n",
      "Epoch 1219: train loss: 0.2239, test loss 2.6179\n",
      "Epoch 1220: train loss: 0.2239, test loss 2.6168\n",
      "Epoch 1221: train loss: 0.2239, test loss 2.6157\n",
      "Epoch 1222: train loss: 0.2239, test loss 2.6146\n",
      "Epoch 1223: train loss: 0.2238, test loss 2.6136\n",
      "Epoch 1224: train loss: 0.2238, test loss 2.6125\n",
      "Epoch 1225: train loss: 0.2238, test loss 2.6114\n",
      "Epoch 1226: train loss: 0.2238, test loss 2.6103\n",
      "Epoch 1227: train loss: 0.2237, test loss 2.6092\n",
      "Epoch 1228: train loss: 0.2237, test loss 2.6082\n",
      "Epoch 1229: train loss: 0.2237, test loss 2.6071\n",
      "Epoch 1230: train loss: 0.2237, test loss 2.6061\n",
      "Epoch 1231: train loss: 0.2236, test loss 2.6050\n",
      "Epoch 1232: train loss: 0.2236, test loss 2.6039\n",
      "Epoch 1233: train loss: 0.2236, test loss 2.6029\n",
      "Epoch 1234: train loss: 0.2236, test loss 2.6018\n",
      "Epoch 1235: train loss: 0.2235, test loss 2.6008\n",
      "Epoch 1236: train loss: 0.2235, test loss 2.5997\n",
      "Epoch 1237: train loss: 0.2235, test loss 2.5986\n",
      "Epoch 1238: train loss: 0.2235, test loss 2.5976\n",
      "Epoch 1239: train loss: 0.2234, test loss 2.5965\n",
      "Epoch 1240: train loss: 0.2234, test loss 2.5955\n",
      "Epoch 1241: train loss: 0.2234, test loss 2.5944\n",
      "Epoch 1242: train loss: 0.2234, test loss 2.5933\n",
      "Epoch 1243: train loss: 0.2233, test loss 2.5923\n",
      "Epoch 1244: train loss: 0.2233, test loss 2.5912\n",
      "Epoch 1245: train loss: 0.2233, test loss 2.5902\n",
      "Epoch 1246: train loss: 0.2233, test loss 2.5891\n",
      "Epoch 1247: train loss: 0.2232, test loss 2.5880\n",
      "Epoch 1248: train loss: 0.2232, test loss 2.5870\n",
      "Epoch 1249: train loss: 0.2232, test loss 2.5859\n",
      "Epoch 1250: train loss: 0.2232, test loss 2.5848\n",
      "Epoch 1251: train loss: 0.2231, test loss 2.5838\n",
      "Epoch 1252: train loss: 0.2231, test loss 2.5827\n",
      "Epoch 1253: train loss: 0.2231, test loss 2.5816\n",
      "Epoch 1254: train loss: 0.2231, test loss 2.5806\n",
      "Epoch 1255: train loss: 0.2230, test loss 2.5795\n",
      "Epoch 1256: train loss: 0.2230, test loss 2.5785\n",
      "Epoch 1257: train loss: 0.2230, test loss 2.5774\n",
      "Epoch 1258: train loss: 0.2230, test loss 2.5764\n",
      "Epoch 1259: train loss: 0.2229, test loss 2.5753\n",
      "Epoch 1260: train loss: 0.2229, test loss 2.5743\n",
      "Epoch 1261: train loss: 0.2229, test loss 2.5732\n",
      "Epoch 1262: train loss: 0.2229, test loss 2.5722\n",
      "Epoch 1263: train loss: 0.2228, test loss 2.5711\n",
      "Epoch 1264: train loss: 0.2228, test loss 2.5701\n",
      "Epoch 1265: train loss: 0.2228, test loss 2.5690\n",
      "Epoch 1266: train loss: 0.2228, test loss 2.5680\n",
      "Epoch 1267: train loss: 0.2227, test loss 2.5669\n",
      "Epoch 1268: train loss: 0.2227, test loss 2.5659\n",
      "Epoch 1269: train loss: 0.2227, test loss 2.5648\n",
      "Epoch 1270: train loss: 0.2227, test loss 2.5638\n",
      "Epoch 1271: train loss: 0.2226, test loss 2.5628\n",
      "Epoch 1272: train loss: 0.2226, test loss 2.5617\n",
      "Epoch 1273: train loss: 0.2226, test loss 2.5607\n",
      "Epoch 1274: train loss: 0.2226, test loss 2.5597\n",
      "Epoch 1275: train loss: 0.2225, test loss 2.5587\n",
      "Epoch 1276: train loss: 0.2225, test loss 2.5576\n",
      "Epoch 1277: train loss: 0.2225, test loss 2.5566\n",
      "Epoch 1278: train loss: 0.2225, test loss 2.5556\n",
      "Epoch 1279: train loss: 0.2224, test loss 2.5546\n",
      "Epoch 1280: train loss: 0.2224, test loss 2.5535\n",
      "Epoch 1281: train loss: 0.2224, test loss 2.5525\n",
      "Epoch 1282: train loss: 0.2224, test loss 2.5515\n",
      "Epoch 1283: train loss: 0.2224, test loss 2.5504\n",
      "Epoch 1284: train loss: 0.2223, test loss 2.5494\n",
      "Epoch 1285: train loss: 0.2223, test loss 2.5484\n",
      "Epoch 1286: train loss: 0.2223, test loss 2.5473\n",
      "Epoch 1287: train loss: 0.2223, test loss 2.5463\n",
      "Epoch 1288: train loss: 0.2222, test loss 2.5453\n",
      "Epoch 1289: train loss: 0.2222, test loss 2.5443\n",
      "Epoch 1290: train loss: 0.2222, test loss 2.5432\n",
      "Epoch 1291: train loss: 0.2222, test loss 2.5422\n",
      "Epoch 1292: train loss: 0.2221, test loss 2.5412\n",
      "Epoch 1293: train loss: 0.2221, test loss 2.5402\n",
      "Epoch 1294: train loss: 0.2221, test loss 2.5392\n",
      "Epoch 1295: train loss: 0.2221, test loss 2.5381\n",
      "Epoch 1296: train loss: 0.2220, test loss 2.5371\n",
      "Epoch 1297: train loss: 0.2220, test loss 2.5361\n",
      "Epoch 1298: train loss: 0.2220, test loss 2.5351\n",
      "Epoch 1299: train loss: 0.2220, test loss 2.5341\n",
      "Epoch 1300: train loss: 0.2219, test loss 2.5330\n",
      "Epoch 1301: train loss: 0.2219, test loss 2.5320\n",
      "Epoch 1302: train loss: 0.2219, test loss 2.5310\n",
      "Epoch 1303: train loss: 0.2219, test loss 2.5300\n",
      "Epoch 1304: train loss: 0.2218, test loss 2.5290\n",
      "Epoch 1305: train loss: 0.2218, test loss 2.5279\n",
      "Epoch 1306: train loss: 0.2218, test loss 2.5269\n",
      "Epoch 1307: train loss: 0.2218, test loss 2.5259\n",
      "Epoch 1308: train loss: 0.2217, test loss 2.5249\n",
      "Epoch 1309: train loss: 0.2217, test loss 2.5239\n",
      "Epoch 1310: train loss: 0.2217, test loss 2.5229\n",
      "Epoch 1311: train loss: 0.2217, test loss 2.5218\n",
      "Epoch 1312: train loss: 0.2217, test loss 2.5208\n",
      "Epoch 1313: train loss: 0.2216, test loss 2.5198\n",
      "Epoch 1314: train loss: 0.2216, test loss 2.5188\n",
      "Epoch 1315: train loss: 0.2216, test loss 2.5178\n",
      "Epoch 1316: train loss: 0.2216, test loss 2.5168\n",
      "Epoch 1317: train loss: 0.2215, test loss 2.5158\n",
      "Epoch 1318: train loss: 0.2215, test loss 2.5148\n",
      "Epoch 1319: train loss: 0.2215, test loss 2.5138\n",
      "Epoch 1320: train loss: 0.2215, test loss 2.5128\n",
      "Epoch 1321: train loss: 0.2214, test loss 2.5118\n",
      "Epoch 1322: train loss: 0.2214, test loss 2.5109\n",
      "Epoch 1323: train loss: 0.2214, test loss 2.5099\n",
      "Epoch 1324: train loss: 0.2214, test loss 2.5089\n",
      "Epoch 1325: train loss: 0.2213, test loss 2.5079\n",
      "Epoch 1326: train loss: 0.2213, test loss 2.5069\n",
      "Epoch 1327: train loss: 0.2213, test loss 2.5059\n",
      "Epoch 1328: train loss: 0.2213, test loss 2.5049\n",
      "Epoch 1329: train loss: 0.2212, test loss 2.5039\n",
      "Epoch 1330: train loss: 0.2212, test loss 2.5029\n",
      "Epoch 1331: train loss: 0.2212, test loss 2.5019\n",
      "Epoch 1332: train loss: 0.2212, test loss 2.5009\n",
      "Epoch 1333: train loss: 0.2212, test loss 2.4999\n",
      "Epoch 1334: train loss: 0.2211, test loss 2.4989\n",
      "Epoch 1335: train loss: 0.2211, test loss 2.4979\n",
      "Epoch 1336: train loss: 0.2211, test loss 2.4969\n",
      "Epoch 1337: train loss: 0.2211, test loss 2.4959\n",
      "Epoch 1338: train loss: 0.2210, test loss 2.4949\n",
      "Epoch 1339: train loss: 0.2210, test loss 2.4939\n",
      "Epoch 1340: train loss: 0.2210, test loss 2.4929\n",
      "Epoch 1341: train loss: 0.2210, test loss 2.4919\n",
      "Epoch 1342: train loss: 0.2209, test loss 2.4909\n",
      "Epoch 1343: train loss: 0.2209, test loss 2.4899\n",
      "Epoch 1344: train loss: 0.2209, test loss 2.4889\n",
      "Epoch 1345: train loss: 0.2209, test loss 2.4879\n",
      "Epoch 1346: train loss: 0.2208, test loss 2.4869\n",
      "Epoch 1347: train loss: 0.2208, test loss 2.4859\n",
      "Epoch 1348: train loss: 0.2208, test loss 2.4849\n",
      "Epoch 1349: train loss: 0.2208, test loss 2.4839\n",
      "Epoch 1350: train loss: 0.2207, test loss 2.4830\n",
      "Epoch 1351: train loss: 0.2207, test loss 2.4820\n",
      "Epoch 1352: train loss: 0.2207, test loss 2.4810\n",
      "Epoch 1353: train loss: 0.2207, test loss 2.4800\n",
      "Epoch 1354: train loss: 0.2207, test loss 2.4790\n",
      "Epoch 1355: train loss: 0.2206, test loss 2.4780\n",
      "Epoch 1356: train loss: 0.2206, test loss 2.4770\n",
      "Epoch 1357: train loss: 0.2206, test loss 2.4760\n",
      "Epoch 1358: train loss: 0.2206, test loss 2.4750\n",
      "Epoch 1359: train loss: 0.2205, test loss 2.4740\n",
      "Epoch 1360: train loss: 0.2205, test loss 2.4730\n",
      "Epoch 1361: train loss: 0.2205, test loss 2.4720\n",
      "Epoch 1362: train loss: 0.2205, test loss 2.4710\n",
      "Epoch 1363: train loss: 0.2204, test loss 2.4700\n",
      "Epoch 1364: train loss: 0.2204, test loss 2.4691\n",
      "Epoch 1365: train loss: 0.2204, test loss 2.4681\n",
      "Epoch 1366: train loss: 0.2204, test loss 2.4671\n",
      "Epoch 1367: train loss: 0.2203, test loss 2.4661\n",
      "Epoch 1368: train loss: 0.2203, test loss 2.4651\n",
      "Epoch 1369: train loss: 0.2203, test loss 2.4641\n",
      "Epoch 1370: train loss: 0.2203, test loss 2.4631\n",
      "Epoch 1371: train loss: 0.2203, test loss 2.4622\n",
      "Epoch 1372: train loss: 0.2202, test loss 2.4612\n",
      "Epoch 1373: train loss: 0.2202, test loss 2.4602\n",
      "Epoch 1374: train loss: 0.2202, test loss 2.4592\n",
      "Epoch 1375: train loss: 0.2202, test loss 2.4582\n",
      "Epoch 1376: train loss: 0.2201, test loss 2.4573\n",
      "Epoch 1377: train loss: 0.2201, test loss 2.4563\n",
      "Epoch 1378: train loss: 0.2201, test loss 2.4553\n",
      "Epoch 1379: train loss: 0.2201, test loss 2.4543\n",
      "Epoch 1380: train loss: 0.2200, test loss 2.4533\n",
      "Epoch 1381: train loss: 0.2200, test loss 2.4524\n",
      "Epoch 1382: train loss: 0.2200, test loss 2.4514\n",
      "Epoch 1383: train loss: 0.2200, test loss 2.4504\n",
      "Epoch 1384: train loss: 0.2199, test loss 2.4494\n",
      "Epoch 1385: train loss: 0.2199, test loss 2.4485\n",
      "Epoch 1386: train loss: 0.2199, test loss 2.4475\n",
      "Epoch 1387: train loss: 0.2199, test loss 2.4465\n",
      "Epoch 1388: train loss: 0.2198, test loss 2.4455\n",
      "Epoch 1389: train loss: 0.2198, test loss 2.4446\n",
      "Epoch 1390: train loss: 0.2198, test loss 2.4436\n",
      "Epoch 1391: train loss: 0.2198, test loss 2.4426\n",
      "Epoch 1392: train loss: 0.2198, test loss 2.4416\n",
      "Epoch 1393: train loss: 0.2197, test loss 2.4407\n",
      "Epoch 1394: train loss: 0.2197, test loss 2.4397\n",
      "Epoch 1395: train loss: 0.2197, test loss 2.4387\n",
      "Epoch 1396: train loss: 0.2197, test loss 2.4378\n",
      "Epoch 1397: train loss: 0.2196, test loss 2.4368\n",
      "Epoch 1398: train loss: 0.2196, test loss 2.4358\n",
      "Epoch 1399: train loss: 0.2196, test loss 2.4348\n",
      "Epoch 1400: train loss: 0.2196, test loss 2.4339\n",
      "Epoch 1401: train loss: 0.2195, test loss 2.4329\n",
      "Epoch 1402: train loss: 0.2195, test loss 2.4319\n",
      "Epoch 1403: train loss: 0.2195, test loss 2.4309\n",
      "Epoch 1404: train loss: 0.2195, test loss 2.4300\n",
      "Epoch 1405: train loss: 0.2194, test loss 2.4290\n",
      "Epoch 1406: train loss: 0.2194, test loss 2.4281\n",
      "Epoch 1407: train loss: 0.2194, test loss 2.4272\n",
      "Epoch 1408: train loss: 0.2194, test loss 2.4262\n",
      "Epoch 1409: train loss: 0.2194, test loss 2.4253\n",
      "Epoch 1410: train loss: 0.2193, test loss 2.4243\n",
      "Epoch 1411: train loss: 0.2193, test loss 2.4234\n",
      "Epoch 1412: train loss: 0.2193, test loss 2.4224\n",
      "Epoch 1413: train loss: 0.2193, test loss 2.4215\n",
      "Epoch 1414: train loss: 0.2192, test loss 2.4205\n",
      "Epoch 1415: train loss: 0.2192, test loss 2.4196\n",
      "Epoch 1416: train loss: 0.2192, test loss 2.4186\n",
      "Epoch 1417: train loss: 0.2192, test loss 2.4177\n",
      "Epoch 1418: train loss: 0.2191, test loss 2.4168\n",
      "Epoch 1419: train loss: 0.2191, test loss 2.4158\n",
      "Epoch 1420: train loss: 0.2191, test loss 2.4149\n",
      "Epoch 1421: train loss: 0.2191, test loss 2.4139\n",
      "Epoch 1422: train loss: 0.2191, test loss 2.4130\n",
      "Epoch 1423: train loss: 0.2190, test loss 2.4121\n",
      "Epoch 1424: train loss: 0.2190, test loss 2.4111\n",
      "Epoch 1425: train loss: 0.2190, test loss 2.4102\n",
      "Epoch 1426: train loss: 0.2190, test loss 2.4093\n",
      "Epoch 1427: train loss: 0.2189, test loss 2.4083\n",
      "Epoch 1428: train loss: 0.2189, test loss 2.4074\n",
      "Epoch 1429: train loss: 0.2189, test loss 2.4065\n",
      "Epoch 1430: train loss: 0.2189, test loss 2.4055\n",
      "Epoch 1431: train loss: 0.2189, test loss 2.4046\n",
      "Epoch 1432: train loss: 0.2188, test loss 2.4037\n",
      "Epoch 1433: train loss: 0.2188, test loss 2.4028\n",
      "Epoch 1434: train loss: 0.2188, test loss 2.4018\n",
      "Epoch 1435: train loss: 0.2188, test loss 2.4009\n",
      "Epoch 1436: train loss: 0.2187, test loss 2.4000\n",
      "Epoch 1437: train loss: 0.2187, test loss 2.3991\n",
      "Epoch 1438: train loss: 0.2187, test loss 2.3982\n",
      "Epoch 1439: train loss: 0.2187, test loss 2.3972\n",
      "Epoch 1440: train loss: 0.2187, test loss 2.3963\n",
      "Epoch 1441: train loss: 0.2186, test loss 2.3954\n",
      "Epoch 1442: train loss: 0.2186, test loss 2.3945\n",
      "Epoch 1443: train loss: 0.2186, test loss 2.3936\n",
      "Epoch 1444: train loss: 0.2186, test loss 2.3927\n",
      "Epoch 1445: train loss: 0.2185, test loss 2.3917\n",
      "Epoch 1446: train loss: 0.2185, test loss 2.3908\n",
      "Epoch 1447: train loss: 0.2185, test loss 2.3899\n",
      "Epoch 1448: train loss: 0.2185, test loss 2.3890\n",
      "Epoch 1449: train loss: 0.2185, test loss 2.3881\n",
      "Epoch 1450: train loss: 0.2184, test loss 2.3872\n",
      "Epoch 1451: train loss: 0.2184, test loss 2.3863\n",
      "Epoch 1452: train loss: 0.2184, test loss 2.3854\n",
      "Epoch 1453: train loss: 0.2184, test loss 2.3844\n",
      "Epoch 1454: train loss: 0.2183, test loss 2.3835\n",
      "Epoch 1455: train loss: 0.2183, test loss 2.3826\n",
      "Epoch 1456: train loss: 0.2183, test loss 2.3817\n",
      "Epoch 1457: train loss: 0.2183, test loss 2.3808\n",
      "Epoch 1458: train loss: 0.2183, test loss 2.3799\n",
      "Epoch 1459: train loss: 0.2182, test loss 2.3789\n",
      "Epoch 1460: train loss: 0.2182, test loss 2.3780\n",
      "Epoch 1461: train loss: 0.2182, test loss 2.3771\n",
      "Epoch 1462: train loss: 0.2182, test loss 2.3762\n",
      "Epoch 1463: train loss: 0.2181, test loss 2.3753\n",
      "Epoch 1464: train loss: 0.2181, test loss 2.3744\n",
      "Epoch 1465: train loss: 0.2181, test loss 2.3735\n",
      "Epoch 1466: train loss: 0.2181, test loss 2.3726\n",
      "Epoch 1467: train loss: 0.2181, test loss 2.3717\n",
      "Epoch 1468: train loss: 0.2180, test loss 2.3708\n",
      "Epoch 1469: train loss: 0.2180, test loss 2.3699\n",
      "Epoch 1470: train loss: 0.2180, test loss 2.3690\n",
      "Epoch 1471: train loss: 0.2180, test loss 2.3680\n",
      "Epoch 1472: train loss: 0.2179, test loss 2.3671\n",
      "Epoch 1473: train loss: 0.2179, test loss 2.3662\n",
      "Epoch 1474: train loss: 0.2179, test loss 2.3653\n",
      "Epoch 1475: train loss: 0.2179, test loss 2.3644\n",
      "Epoch 1476: train loss: 0.2179, test loss 2.3635\n",
      "Epoch 1477: train loss: 0.2178, test loss 2.3626\n",
      "Epoch 1478: train loss: 0.2178, test loss 2.3617\n",
      "Epoch 1479: train loss: 0.2178, test loss 2.3608\n",
      "Epoch 1480: train loss: 0.2178, test loss 2.3599\n",
      "Epoch 1481: train loss: 0.2178, test loss 2.3591\n",
      "Epoch 1482: train loss: 0.2177, test loss 2.3582\n",
      "Epoch 1483: train loss: 0.2177, test loss 2.3573\n",
      "Epoch 1484: train loss: 0.2177, test loss 2.3564\n",
      "Epoch 1485: train loss: 0.2177, test loss 2.3555\n",
      "Epoch 1486: train loss: 0.2176, test loss 2.3546\n",
      "Epoch 1487: train loss: 0.2176, test loss 2.3537\n",
      "Epoch 1488: train loss: 0.2176, test loss 2.3528\n",
      "Epoch 1489: train loss: 0.2176, test loss 2.3519\n",
      "Epoch 1490: train loss: 0.2176, test loss 2.3510\n",
      "Epoch 1491: train loss: 0.2175, test loss 2.3501\n",
      "Epoch 1492: train loss: 0.2175, test loss 2.3492\n",
      "Epoch 1493: train loss: 0.2175, test loss 2.3484\n",
      "Epoch 1494: train loss: 0.2175, test loss 2.3475\n",
      "Epoch 1495: train loss: 0.2175, test loss 2.3466\n",
      "Epoch 1496: train loss: 0.2174, test loss 2.3457\n",
      "Epoch 1497: train loss: 0.2174, test loss 2.3449\n",
      "Epoch 1498: train loss: 0.2174, test loss 2.3440\n",
      "Epoch 1499: train loss: 0.2174, test loss 2.3431\n",
      "Epoch 1500: train loss: 0.2174, test loss 2.3422\n",
      "Epoch 1501: train loss: 0.2173, test loss 2.3414\n",
      "Epoch 1502: train loss: 0.2173, test loss 2.3405\n",
      "Epoch 1503: train loss: 0.2173, test loss 2.3396\n",
      "Epoch 1504: train loss: 0.2173, test loss 2.3388\n",
      "Epoch 1505: train loss: 0.2172, test loss 2.3379\n",
      "Epoch 1506: train loss: 0.2172, test loss 2.3370\n",
      "Epoch 1507: train loss: 0.2172, test loss 2.3362\n",
      "Epoch 1508: train loss: 0.2172, test loss 2.3353\n",
      "Epoch 1509: train loss: 0.2172, test loss 2.3344\n",
      "Epoch 1510: train loss: 0.2171, test loss 2.3336\n",
      "Epoch 1511: train loss: 0.2171, test loss 2.3327\n",
      "Epoch 1512: train loss: 0.2171, test loss 2.3318\n",
      "Epoch 1513: train loss: 0.2171, test loss 2.3310\n",
      "Epoch 1514: train loss: 0.2171, test loss 2.3301\n",
      "Epoch 1515: train loss: 0.2170, test loss 2.3293\n",
      "Epoch 1516: train loss: 0.2170, test loss 2.3284\n",
      "Epoch 1517: train loss: 0.2170, test loss 2.3275\n",
      "Epoch 1518: train loss: 0.2170, test loss 2.3267\n",
      "Epoch 1519: train loss: 0.2170, test loss 2.3258\n",
      "Epoch 1520: train loss: 0.2169, test loss 2.3250\n",
      "Epoch 1521: train loss: 0.2169, test loss 2.3241\n",
      "Epoch 1522: train loss: 0.2169, test loss 2.3232\n",
      "Epoch 1523: train loss: 0.2169, test loss 2.3224\n",
      "Epoch 1524: train loss: 0.2169, test loss 2.3215\n",
      "Epoch 1525: train loss: 0.2168, test loss 2.3207\n",
      "Epoch 1526: train loss: 0.2168, test loss 2.3198\n",
      "Epoch 1527: train loss: 0.2168, test loss 2.3190\n",
      "Epoch 1528: train loss: 0.2168, test loss 2.3181\n",
      "Epoch 1529: train loss: 0.2168, test loss 2.3173\n",
      "Epoch 1530: train loss: 0.2167, test loss 2.3164\n",
      "Epoch 1531: train loss: 0.2167, test loss 2.3156\n",
      "Epoch 1532: train loss: 0.2167, test loss 2.3148\n",
      "Epoch 1533: train loss: 0.2167, test loss 2.3139\n",
      "Epoch 1534: train loss: 0.2166, test loss 2.3131\n",
      "Epoch 1535: train loss: 0.2166, test loss 2.3122\n",
      "Epoch 1536: train loss: 0.2166, test loss 2.3114\n",
      "Epoch 1537: train loss: 0.2166, test loss 2.3105\n",
      "Epoch 1538: train loss: 0.2166, test loss 2.3097\n",
      "Epoch 1539: train loss: 0.2165, test loss 2.3088\n",
      "Epoch 1540: train loss: 0.2165, test loss 2.3080\n",
      "Epoch 1541: train loss: 0.2165, test loss 2.3072\n",
      "Epoch 1542: train loss: 0.2165, test loss 2.3063\n",
      "Epoch 1543: train loss: 0.2165, test loss 2.3055\n",
      "Epoch 1544: train loss: 0.2164, test loss 2.3046\n",
      "Epoch 1545: train loss: 0.2164, test loss 2.3038\n",
      "Epoch 1546: train loss: 0.2164, test loss 2.3030\n",
      "Epoch 1547: train loss: 0.2164, test loss 2.3021\n",
      "Epoch 1548: train loss: 0.2164, test loss 2.3013\n",
      "Epoch 1549: train loss: 0.2163, test loss 2.3004\n",
      "Epoch 1550: train loss: 0.2163, test loss 2.2996\n",
      "Epoch 1551: train loss: 0.2163, test loss 2.2988\n",
      "Epoch 1552: train loss: 0.2163, test loss 2.2979\n",
      "Epoch 1553: train loss: 0.2163, test loss 2.2971\n",
      "Epoch 1554: train loss: 0.2162, test loss 2.2963\n",
      "Epoch 1555: train loss: 0.2162, test loss 2.2954\n",
      "Epoch 1556: train loss: 0.2162, test loss 2.2946\n",
      "Epoch 1557: train loss: 0.2162, test loss 2.2937\n",
      "Epoch 1558: train loss: 0.2162, test loss 2.2929\n",
      "Epoch 1559: train loss: 0.2161, test loss 2.2921\n",
      "Epoch 1560: train loss: 0.2161, test loss 2.2912\n",
      "Epoch 1561: train loss: 0.2161, test loss 2.2904\n",
      "Epoch 1562: train loss: 0.2161, test loss 2.2896\n",
      "Epoch 1563: train loss: 0.2161, test loss 2.2887\n",
      "Epoch 1564: train loss: 0.2160, test loss 2.2879\n",
      "Epoch 1565: train loss: 0.2160, test loss 2.2871\n",
      "Epoch 1566: train loss: 0.2160, test loss 2.2862\n",
      "Epoch 1567: train loss: 0.2160, test loss 2.2854\n",
      "Epoch 1568: train loss: 0.2160, test loss 2.2846\n",
      "Epoch 1569: train loss: 0.2159, test loss 2.2837\n",
      "Epoch 1570: train loss: 0.2159, test loss 2.2829\n",
      "Epoch 1571: train loss: 0.2159, test loss 2.2821\n",
      "Epoch 1572: train loss: 0.2159, test loss 2.2813\n",
      "Epoch 1573: train loss: 0.2159, test loss 2.2804\n",
      "Epoch 1574: train loss: 0.2158, test loss 2.2796\n",
      "Epoch 1575: train loss: 0.2158, test loss 2.2788\n",
      "Epoch 1576: train loss: 0.2158, test loss 2.2780\n",
      "Epoch 1577: train loss: 0.2158, test loss 2.2771\n",
      "Epoch 1578: train loss: 0.2158, test loss 2.2763\n",
      "Epoch 1579: train loss: 0.2157, test loss 2.2755\n",
      "Epoch 1580: train loss: 0.2157, test loss 2.2747\n",
      "Epoch 1581: train loss: 0.2157, test loss 2.2739\n",
      "Epoch 1582: train loss: 0.2157, test loss 2.2730\n",
      "Epoch 1583: train loss: 0.2157, test loss 2.2722\n",
      "Epoch 1584: train loss: 0.2156, test loss 2.2714\n",
      "Epoch 1585: train loss: 0.2156, test loss 2.2706\n",
      "Epoch 1586: train loss: 0.2156, test loss 2.2698\n",
      "Epoch 1587: train loss: 0.2156, test loss 2.2690\n",
      "Epoch 1588: train loss: 0.2156, test loss 2.2682\n",
      "Epoch 1589: train loss: 0.2155, test loss 2.2673\n",
      "Epoch 1590: train loss: 0.2155, test loss 2.2665\n",
      "Epoch 1591: train loss: 0.2155, test loss 2.2657\n",
      "Epoch 1592: train loss: 0.2155, test loss 2.2649\n",
      "Epoch 1593: train loss: 0.2155, test loss 2.2641\n",
      "Epoch 1594: train loss: 0.2154, test loss 2.2633\n",
      "Epoch 1595: train loss: 0.2154, test loss 2.2625\n",
      "Epoch 1596: train loss: 0.2154, test loss 2.2617\n",
      "Epoch 1597: train loss: 0.2154, test loss 2.2609\n",
      "Epoch 1598: train loss: 0.2154, test loss 2.2601\n",
      "Epoch 1599: train loss: 0.2153, test loss 2.2593\n",
      "Epoch 1600: train loss: 0.2153, test loss 2.2585\n",
      "Epoch 1601: train loss: 0.2153, test loss 2.2577\n",
      "Epoch 1602: train loss: 0.2153, test loss 2.2569\n",
      "Epoch 1603: train loss: 0.2153, test loss 2.2561\n",
      "Epoch 1604: train loss: 0.2152, test loss 2.2553\n",
      "Epoch 1605: train loss: 0.2152, test loss 2.2545\n",
      "Epoch 1606: train loss: 0.2152, test loss 2.2537\n",
      "Epoch 1607: train loss: 0.2152, test loss 2.2529\n",
      "Epoch 1608: train loss: 0.2152, test loss 2.2521\n",
      "Epoch 1609: train loss: 0.2152, test loss 2.2513\n",
      "Epoch 1610: train loss: 0.2151, test loss 2.2505\n",
      "Epoch 1611: train loss: 0.2151, test loss 2.2497\n",
      "Epoch 1612: train loss: 0.2151, test loss 2.2489\n",
      "Epoch 1613: train loss: 0.2151, test loss 2.2481\n",
      "Epoch 1614: train loss: 0.2151, test loss 2.2473\n",
      "Epoch 1615: train loss: 0.2150, test loss 2.2465\n",
      "Epoch 1616: train loss: 0.2150, test loss 2.2457\n",
      "Epoch 1617: train loss: 0.2150, test loss 2.2449\n",
      "Epoch 1618: train loss: 0.2150, test loss 2.2442\n",
      "Epoch 1619: train loss: 0.2150, test loss 2.2434\n",
      "Epoch 1620: train loss: 0.2149, test loss 2.2426\n",
      "Epoch 1621: train loss: 0.2149, test loss 2.2418\n",
      "Epoch 1622: train loss: 0.2149, test loss 2.2410\n",
      "Epoch 1623: train loss: 0.2149, test loss 2.2402\n",
      "Epoch 1624: train loss: 0.2149, test loss 2.2394\n",
      "Epoch 1625: train loss: 0.2148, test loss 2.2386\n",
      "Epoch 1626: train loss: 0.2148, test loss 2.2378\n",
      "Epoch 1627: train loss: 0.2148, test loss 2.2371\n",
      "Epoch 1628: train loss: 0.2148, test loss 2.2363\n",
      "Epoch 1629: train loss: 0.2148, test loss 2.2355\n",
      "Epoch 1630: train loss: 0.2148, test loss 2.2347\n",
      "Epoch 1631: train loss: 0.2147, test loss 2.2339\n",
      "Epoch 1632: train loss: 0.2147, test loss 2.2331\n",
      "Epoch 1633: train loss: 0.2147, test loss 2.2324\n",
      "Epoch 1634: train loss: 0.2147, test loss 2.2316\n",
      "Epoch 1635: train loss: 0.2147, test loss 2.2308\n",
      "Epoch 1636: train loss: 0.2146, test loss 2.2300\n",
      "Epoch 1637: train loss: 0.2146, test loss 2.2293\n",
      "Epoch 1638: train loss: 0.2146, test loss 2.2285\n",
      "Epoch 1639: train loss: 0.2146, test loss 2.2277\n",
      "Epoch 1640: train loss: 0.2146, test loss 2.2270\n",
      "Epoch 1641: train loss: 0.2146, test loss 2.2262\n",
      "Epoch 1642: train loss: 0.2145, test loss 2.2254\n",
      "Epoch 1643: train loss: 0.2145, test loss 2.2247\n",
      "Epoch 1644: train loss: 0.2145, test loss 2.2239\n",
      "Epoch 1645: train loss: 0.2145, test loss 2.2231\n",
      "Epoch 1646: train loss: 0.2145, test loss 2.2224\n",
      "Epoch 1647: train loss: 0.2144, test loss 2.2216\n",
      "Epoch 1648: train loss: 0.2144, test loss 2.2208\n",
      "Epoch 1649: train loss: 0.2144, test loss 2.2201\n",
      "Epoch 1650: train loss: 0.2144, test loss 2.2193\n",
      "Epoch 1651: train loss: 0.2144, test loss 2.2186\n",
      "Epoch 1652: train loss: 0.2143, test loss 2.2178\n",
      "Epoch 1653: train loss: 0.2143, test loss 2.2170\n",
      "Epoch 1654: train loss: 0.2143, test loss 2.2163\n",
      "Epoch 1655: train loss: 0.2143, test loss 2.2155\n",
      "Epoch 1656: train loss: 0.2143, test loss 2.2148\n",
      "Epoch 1657: train loss: 0.2143, test loss 2.2140\n",
      "Epoch 1658: train loss: 0.2142, test loss 2.2133\n",
      "Epoch 1659: train loss: 0.2142, test loss 2.2125\n",
      "Epoch 1660: train loss: 0.2142, test loss 2.2117\n",
      "Epoch 1661: train loss: 0.2142, test loss 2.2110\n",
      "Epoch 1662: train loss: 0.2142, test loss 2.2102\n",
      "Epoch 1663: train loss: 0.2141, test loss 2.2095\n",
      "Epoch 1664: train loss: 0.2141, test loss 2.2087\n",
      "Epoch 1665: train loss: 0.2141, test loss 2.2080\n",
      "Epoch 1666: train loss: 0.2141, test loss 2.2072\n",
      "Epoch 1667: train loss: 0.2141, test loss 2.2065\n",
      "Epoch 1668: train loss: 0.2141, test loss 2.2058\n",
      "Epoch 1669: train loss: 0.2140, test loss 2.2050\n",
      "Epoch 1670: train loss: 0.2140, test loss 2.2043\n",
      "Epoch 1671: train loss: 0.2140, test loss 2.2035\n",
      "Epoch 1672: train loss: 0.2140, test loss 2.2028\n",
      "Epoch 1673: train loss: 0.2140, test loss 2.2020\n",
      "Epoch 1674: train loss: 0.2140, test loss 2.2013\n",
      "Epoch 1675: train loss: 0.2139, test loss 2.2005\n",
      "Epoch 1676: train loss: 0.2139, test loss 2.1998\n",
      "Epoch 1677: train loss: 0.2139, test loss 2.1991\n",
      "Epoch 1678: train loss: 0.2139, test loss 2.1983\n",
      "Epoch 1679: train loss: 0.2139, test loss 2.1976\n",
      "Epoch 1680: train loss: 0.2138, test loss 2.1968\n",
      "Epoch 1681: train loss: 0.2138, test loss 2.1961\n",
      "Epoch 1682: train loss: 0.2138, test loss 2.1953\n",
      "Epoch 1683: train loss: 0.2138, test loss 2.1946\n",
      "Epoch 1684: train loss: 0.2138, test loss 2.1939\n",
      "Epoch 1685: train loss: 0.2138, test loss 2.1931\n",
      "Epoch 1686: train loss: 0.2137, test loss 2.1924\n",
      "Epoch 1687: train loss: 0.2137, test loss 2.1916\n",
      "Epoch 1688: train loss: 0.2137, test loss 2.1909\n",
      "Epoch 1689: train loss: 0.2137, test loss 2.1901\n",
      "Epoch 1690: train loss: 0.2137, test loss 2.1894\n",
      "Epoch 1691: train loss: 0.2136, test loss 2.1886\n",
      "Epoch 1692: train loss: 0.2136, test loss 2.1879\n",
      "Epoch 1693: train loss: 0.2136, test loss 2.1872\n",
      "Epoch 1694: train loss: 0.2136, test loss 2.1864\n",
      "Epoch 1695: train loss: 0.2136, test loss 2.1857\n",
      "Epoch 1696: train loss: 0.2136, test loss 2.1849\n",
      "Epoch 1697: train loss: 0.2135, test loss 2.1842\n",
      "Epoch 1698: train loss: 0.2135, test loss 2.1834\n",
      "Epoch 1699: train loss: 0.2135, test loss 2.1827\n",
      "Epoch 1700: train loss: 0.2135, test loss 2.1819\n",
      "Epoch 1701: train loss: 0.2135, test loss 2.1812\n",
      "Epoch 1702: train loss: 0.2135, test loss 2.1805\n",
      "Epoch 1703: train loss: 0.2134, test loss 2.1797\n",
      "Epoch 1704: train loss: 0.2134, test loss 2.1790\n",
      "Epoch 1705: train loss: 0.2134, test loss 2.1782\n",
      "Epoch 1706: train loss: 0.2134, test loss 2.1775\n",
      "Epoch 1707: train loss: 0.2134, test loss 2.1767\n",
      "Epoch 1708: train loss: 0.2133, test loss 2.1760\n",
      "Epoch 1709: train loss: 0.2133, test loss 2.1753\n",
      "Epoch 1710: train loss: 0.2133, test loss 2.1745\n",
      "Epoch 1711: train loss: 0.2133, test loss 2.1738\n",
      "Epoch 1712: train loss: 0.2133, test loss 2.1731\n",
      "Epoch 1713: train loss: 0.2133, test loss 2.1723\n",
      "Epoch 1714: train loss: 0.2132, test loss 2.1716\n",
      "Epoch 1715: train loss: 0.2132, test loss 2.1708\n",
      "Epoch 1716: train loss: 0.2132, test loss 2.1701\n",
      "Epoch 1717: train loss: 0.2132, test loss 2.1694\n",
      "Epoch 1718: train loss: 0.2132, test loss 2.1686\n",
      "Epoch 1719: train loss: 0.2131, test loss 2.1679\n",
      "Epoch 1720: train loss: 0.2131, test loss 2.1672\n",
      "Epoch 1721: train loss: 0.2131, test loss 2.1664\n",
      "Epoch 1722: train loss: 0.2131, test loss 2.1657\n",
      "Epoch 1723: train loss: 0.2131, test loss 2.1650\n",
      "Epoch 1724: train loss: 0.2131, test loss 2.1642\n",
      "Epoch 1725: train loss: 0.2130, test loss 2.1635\n",
      "Epoch 1726: train loss: 0.2130, test loss 2.1628\n",
      "Epoch 1727: train loss: 0.2130, test loss 2.1620\n",
      "Epoch 1728: train loss: 0.2130, test loss 2.1613\n",
      "Epoch 1729: train loss: 0.2130, test loss 2.1606\n",
      "Epoch 1730: train loss: 0.2129, test loss 2.1599\n",
      "Epoch 1731: train loss: 0.2129, test loss 2.1591\n",
      "Epoch 1732: train loss: 0.2129, test loss 2.1584\n",
      "Epoch 1733: train loss: 0.2129, test loss 2.1576\n",
      "Epoch 1734: train loss: 0.2129, test loss 2.1569\n",
      "Epoch 1735: train loss: 0.2129, test loss 2.1562\n",
      "Epoch 1736: train loss: 0.2128, test loss 2.1554\n",
      "Epoch 1737: train loss: 0.2128, test loss 2.1547\n",
      "Epoch 1738: train loss: 0.2128, test loss 2.1540\n",
      "Epoch 1739: train loss: 0.2128, test loss 2.1532\n",
      "Epoch 1740: train loss: 0.2128, test loss 2.1525\n",
      "Epoch 1741: train loss: 0.2128, test loss 2.1518\n",
      "Epoch 1742: train loss: 0.2127, test loss 2.1510\n",
      "Epoch 1743: train loss: 0.2127, test loss 2.1503\n",
      "Epoch 1744: train loss: 0.2127, test loss 2.1496\n",
      "Epoch 1745: train loss: 0.2127, test loss 2.1488\n",
      "Epoch 1746: train loss: 0.2127, test loss 2.1481\n",
      "Epoch 1747: train loss: 0.2126, test loss 2.1473\n",
      "Epoch 1748: train loss: 0.2126, test loss 2.1466\n",
      "Epoch 1749: train loss: 0.2126, test loss 2.1459\n",
      "Epoch 1750: train loss: 0.2126, test loss 2.1452\n",
      "Epoch 1751: train loss: 0.2126, test loss 2.1444\n",
      "Epoch 1752: train loss: 0.2126, test loss 2.1437\n",
      "Epoch 1753: train loss: 0.2125, test loss 2.1430\n",
      "Epoch 1754: train loss: 0.2125, test loss 2.1422\n",
      "Epoch 1755: train loss: 0.2125, test loss 2.1415\n",
      "Epoch 1756: train loss: 0.2125, test loss 2.1408\n",
      "Epoch 1757: train loss: 0.2125, test loss 2.1400\n",
      "Epoch 1758: train loss: 0.2125, test loss 2.1393\n",
      "Epoch 1759: train loss: 0.2124, test loss 2.1386\n",
      "Epoch 1760: train loss: 0.2124, test loss 2.1378\n",
      "Epoch 1761: train loss: 0.2124, test loss 2.1371\n",
      "Epoch 1762: train loss: 0.2124, test loss 2.1364\n",
      "Epoch 1763: train loss: 0.2124, test loss 2.1356\n",
      "Epoch 1764: train loss: 0.2123, test loss 2.1349\n",
      "Epoch 1765: train loss: 0.2123, test loss 2.1341\n",
      "Epoch 1766: train loss: 0.2123, test loss 2.1334\n",
      "Epoch 1767: train loss: 0.2123, test loss 2.1327\n",
      "Epoch 1768: train loss: 0.2123, test loss 2.1319\n",
      "Epoch 1769: train loss: 0.2123, test loss 2.1312\n",
      "Epoch 1770: train loss: 0.2122, test loss 2.1305\n",
      "Epoch 1771: train loss: 0.2122, test loss 2.1297\n",
      "Epoch 1772: train loss: 0.2122, test loss 2.1290\n",
      "Epoch 1773: train loss: 0.2122, test loss 2.1283\n",
      "Epoch 1774: train loss: 0.2122, test loss 2.1275\n",
      "Epoch 1775: train loss: 0.2121, test loss 2.1268\n",
      "Epoch 1776: train loss: 0.2121, test loss 2.1261\n",
      "Epoch 1777: train loss: 0.2121, test loss 2.1253\n",
      "Epoch 1778: train loss: 0.2121, test loss 2.1246\n",
      "Epoch 1779: train loss: 0.2121, test loss 2.1239\n",
      "Epoch 1780: train loss: 0.2121, test loss 2.1231\n",
      "Epoch 1781: train loss: 0.2120, test loss 2.1224\n",
      "Epoch 1782: train loss: 0.2120, test loss 2.1217\n",
      "Epoch 1783: train loss: 0.2120, test loss 2.1209\n",
      "Epoch 1784: train loss: 0.2120, test loss 2.1202\n",
      "Epoch 1785: train loss: 0.2120, test loss 2.1195\n",
      "Epoch 1786: train loss: 0.2119, test loss 2.1187\n",
      "Epoch 1787: train loss: 0.2119, test loss 2.1180\n",
      "Epoch 1788: train loss: 0.2119, test loss 2.1173\n",
      "Epoch 1789: train loss: 0.2119, test loss 2.1166\n",
      "Epoch 1790: train loss: 0.2119, test loss 2.1158\n",
      "Epoch 1791: train loss: 0.2119, test loss 2.1151\n",
      "Epoch 1792: train loss: 0.2118, test loss 2.1144\n",
      "Epoch 1793: train loss: 0.2118, test loss 2.1136\n",
      "Epoch 1794: train loss: 0.2118, test loss 2.1129\n",
      "Epoch 1795: train loss: 0.2118, test loss 2.1122\n",
      "Epoch 1796: train loss: 0.2118, test loss 2.1115\n",
      "Epoch 1797: train loss: 0.2118, test loss 2.1108\n",
      "Epoch 1798: train loss: 0.2117, test loss 2.1101\n",
      "Epoch 1799: train loss: 0.2117, test loss 2.1094\n",
      "Epoch 1800: train loss: 0.2117, test loss 2.1087\n",
      "Epoch 1801: train loss: 0.2117, test loss 2.1079\n",
      "Epoch 1802: train loss: 0.2117, test loss 2.1072\n",
      "Epoch 1803: train loss: 0.2117, test loss 2.1065\n",
      "Epoch 1804: train loss: 0.2116, test loss 2.1058\n",
      "Epoch 1805: train loss: 0.2116, test loss 2.1051\n",
      "Epoch 1806: train loss: 0.2116, test loss 2.1044\n",
      "Epoch 1807: train loss: 0.2116, test loss 2.1037\n",
      "Epoch 1808: train loss: 0.2116, test loss 2.1030\n",
      "Epoch 1809: train loss: 0.2115, test loss 2.1023\n",
      "Epoch 1810: train loss: 0.2115, test loss 2.1016\n",
      "Epoch 1811: train loss: 0.2115, test loss 2.1009\n",
      "Epoch 1812: train loss: 0.2115, test loss 2.1002\n",
      "Epoch 1813: train loss: 0.2115, test loss 2.0995\n",
      "Epoch 1814: train loss: 0.2115, test loss 2.0988\n",
      "Epoch 1815: train loss: 0.2114, test loss 2.0981\n",
      "Epoch 1816: train loss: 0.2114, test loss 2.0974\n",
      "Epoch 1817: train loss: 0.2114, test loss 2.0967\n",
      "Epoch 1818: train loss: 0.2114, test loss 2.0960\n",
      "Epoch 1819: train loss: 0.2114, test loss 2.0953\n",
      "Epoch 1820: train loss: 0.2114, test loss 2.0946\n",
      "Epoch 1821: train loss: 0.2113, test loss 2.0939\n",
      "Epoch 1822: train loss: 0.2113, test loss 2.0932\n",
      "Epoch 1823: train loss: 0.2113, test loss 2.0925\n",
      "Epoch 1824: train loss: 0.2113, test loss 2.0918\n",
      "Epoch 1825: train loss: 0.2113, test loss 2.0911\n",
      "Epoch 1826: train loss: 0.2113, test loss 2.0903\n",
      "Epoch 1827: train loss: 0.2112, test loss 2.0896\n",
      "Epoch 1828: train loss: 0.2112, test loss 2.0889\n",
      "Epoch 1829: train loss: 0.2112, test loss 2.0882\n",
      "Epoch 1830: train loss: 0.2112, test loss 2.0875\n",
      "Epoch 1831: train loss: 0.2112, test loss 2.0868\n",
      "Epoch 1832: train loss: 0.2112, test loss 2.0861\n",
      "Epoch 1833: train loss: 0.2111, test loss 2.0854\n",
      "Epoch 1834: train loss: 0.2111, test loss 2.0846\n",
      "Epoch 1835: train loss: 0.2111, test loss 2.0839\n",
      "Epoch 1836: train loss: 0.2111, test loss 2.0832\n",
      "Epoch 1837: train loss: 0.2111, test loss 2.0825\n",
      "Epoch 1838: train loss: 0.2110, test loss 2.0818\n",
      "Epoch 1839: train loss: 0.2110, test loss 2.0811\n",
      "Epoch 1840: train loss: 0.2110, test loss 2.0804\n",
      "Epoch 1841: train loss: 0.2110, test loss 2.0797\n",
      "Epoch 1842: train loss: 0.2110, test loss 2.0790\n",
      "Epoch 1843: train loss: 0.2110, test loss 2.0783\n",
      "Epoch 1844: train loss: 0.2109, test loss 2.0776\n",
      "Epoch 1845: train loss: 0.2109, test loss 2.0769\n",
      "Epoch 1846: train loss: 0.2109, test loss 2.0762\n",
      "Epoch 1847: train loss: 0.2109, test loss 2.0755\n",
      "Epoch 1848: train loss: 0.2109, test loss 2.0747\n",
      "Epoch 1849: train loss: 0.2109, test loss 2.0740\n",
      "Epoch 1850: train loss: 0.2108, test loss 2.0733\n",
      "Epoch 1851: train loss: 0.2108, test loss 2.0727\n",
      "Epoch 1852: train loss: 0.2108, test loss 2.0720\n",
      "Epoch 1853: train loss: 0.2108, test loss 2.0713\n",
      "Epoch 1854: train loss: 0.2108, test loss 2.0706\n",
      "Epoch 1855: train loss: 0.2108, test loss 2.0699\n",
      "Epoch 1856: train loss: 0.2107, test loss 2.0692\n",
      "Epoch 1857: train loss: 0.2107, test loss 2.0685\n",
      "Epoch 1858: train loss: 0.2107, test loss 2.0678\n",
      "Epoch 1859: train loss: 0.2107, test loss 2.0671\n",
      "Epoch 1860: train loss: 0.2107, test loss 2.0664\n",
      "Epoch 1861: train loss: 0.2107, test loss 2.0658\n",
      "Epoch 1862: train loss: 0.2106, test loss 2.0651\n",
      "Epoch 1863: train loss: 0.2106, test loss 2.0644\n",
      "Epoch 1864: train loss: 0.2106, test loss 2.0637\n",
      "Epoch 1865: train loss: 0.2106, test loss 2.0630\n",
      "Epoch 1866: train loss: 0.2106, test loss 2.0623\n",
      "Epoch 1867: train loss: 0.2106, test loss 2.0617\n",
      "Epoch 1868: train loss: 0.2105, test loss 2.0610\n",
      "Epoch 1869: train loss: 0.2105, test loss 2.0603\n",
      "Epoch 1870: train loss: 0.2105, test loss 2.0596\n",
      "Epoch 1871: train loss: 0.2105, test loss 2.0589\n",
      "Epoch 1872: train loss: 0.2105, test loss 2.0582\n",
      "Epoch 1873: train loss: 0.2105, test loss 2.0576\n",
      "Epoch 1874: train loss: 0.2104, test loss 2.0569\n",
      "Epoch 1875: train loss: 0.2104, test loss 2.0562\n",
      "Epoch 1876: train loss: 0.2104, test loss 2.0555\n",
      "Epoch 1877: train loss: 0.2104, test loss 2.0548\n",
      "Epoch 1878: train loss: 0.2104, test loss 2.0541\n",
      "Epoch 1879: train loss: 0.2104, test loss 2.0535\n",
      "Epoch 1880: train loss: 0.2103, test loss 2.0528\n",
      "Epoch 1881: train loss: 0.2103, test loss 2.0521\n",
      "Epoch 1882: train loss: 0.2103, test loss 2.0515\n",
      "Epoch 1883: train loss: 0.2103, test loss 2.0508\n",
      "Epoch 1884: train loss: 0.2103, test loss 2.0501\n",
      "Epoch 1885: train loss: 0.2103, test loss 2.0495\n",
      "Epoch 1886: train loss: 0.2102, test loss 2.0488\n",
      "Epoch 1887: train loss: 0.2102, test loss 2.0482\n",
      "Epoch 1888: train loss: 0.2102, test loss 2.0475\n",
      "Epoch 1889: train loss: 0.2102, test loss 2.0468\n",
      "Epoch 1890: train loss: 0.2102, test loss 2.0462\n",
      "Epoch 1891: train loss: 0.2102, test loss 2.0455\n",
      "Epoch 1892: train loss: 0.2101, test loss 2.0449\n",
      "Epoch 1893: train loss: 0.2101, test loss 2.0442\n",
      "Epoch 1894: train loss: 0.2101, test loss 2.0436\n",
      "Epoch 1895: train loss: 0.2101, test loss 2.0429\n",
      "Epoch 1896: train loss: 0.2101, test loss 2.0422\n",
      "Epoch 1897: train loss: 0.2101, test loss 2.0416\n",
      "Epoch 1898: train loss: 0.2100, test loss 2.0409\n",
      "Epoch 1899: train loss: 0.2100, test loss 2.0403\n",
      "Epoch 1900: train loss: 0.2100, test loss 2.0397\n",
      "Epoch 1901: train loss: 0.2100, test loss 2.0390\n",
      "Epoch 1902: train loss: 0.2100, test loss 2.0384\n",
      "Epoch 1903: train loss: 0.2100, test loss 2.0377\n",
      "Epoch 1904: train loss: 0.2100, test loss 2.0371\n",
      "Epoch 1905: train loss: 0.2099, test loss 2.0364\n",
      "Epoch 1906: train loss: 0.2099, test loss 2.0358\n",
      "Epoch 1907: train loss: 0.2099, test loss 2.0351\n",
      "Epoch 1908: train loss: 0.2099, test loss 2.0345\n",
      "Epoch 1909: train loss: 0.2099, test loss 2.0338\n",
      "Epoch 1910: train loss: 0.2099, test loss 2.0332\n",
      "Epoch 1911: train loss: 0.2098, test loss 2.0326\n",
      "Epoch 1912: train loss: 0.2098, test loss 2.0319\n",
      "Epoch 1913: train loss: 0.2098, test loss 2.0313\n",
      "Epoch 1914: train loss: 0.2098, test loss 2.0306\n",
      "Epoch 1915: train loss: 0.2098, test loss 2.0300\n",
      "Epoch 1916: train loss: 0.2098, test loss 2.0293\n",
      "Epoch 1917: train loss: 0.2097, test loss 2.0287\n",
      "Epoch 1918: train loss: 0.2097, test loss 2.0281\n",
      "Epoch 1919: train loss: 0.2097, test loss 2.0274\n",
      "Epoch 1920: train loss: 0.2097, test loss 2.0268\n",
      "Epoch 1921: train loss: 0.2097, test loss 2.0262\n",
      "Epoch 1922: train loss: 0.2097, test loss 2.0255\n",
      "Epoch 1923: train loss: 0.2096, test loss 2.0249\n",
      "Epoch 1924: train loss: 0.2096, test loss 2.0243\n",
      "Epoch 1925: train loss: 0.2096, test loss 2.0236\n",
      "Epoch 1926: train loss: 0.2096, test loss 2.0230\n",
      "Epoch 1927: train loss: 0.2096, test loss 2.0224\n",
      "Epoch 1928: train loss: 0.2096, test loss 2.0218\n",
      "Epoch 1929: train loss: 0.2096, test loss 2.0211\n",
      "Epoch 1930: train loss: 0.2095, test loss 2.0205\n",
      "Epoch 1931: train loss: 0.2095, test loss 2.0199\n",
      "Epoch 1932: train loss: 0.2095, test loss 2.0192\n",
      "Epoch 1933: train loss: 0.2095, test loss 2.0186\n",
      "Epoch 1934: train loss: 0.2095, test loss 2.0180\n",
      "Epoch 1935: train loss: 0.2095, test loss 2.0174\n",
      "Epoch 1936: train loss: 0.2094, test loss 2.0168\n",
      "Epoch 1937: train loss: 0.2094, test loss 2.0161\n",
      "Epoch 1938: train loss: 0.2094, test loss 2.0155\n",
      "Epoch 1939: train loss: 0.2094, test loss 2.0149\n",
      "Epoch 1940: train loss: 0.2094, test loss 2.0143\n",
      "Epoch 1941: train loss: 0.2094, test loss 2.0137\n",
      "Epoch 1942: train loss: 0.2093, test loss 2.0130\n",
      "Epoch 1943: train loss: 0.2093, test loss 2.0124\n",
      "Epoch 1944: train loss: 0.2093, test loss 2.0118\n",
      "Epoch 1945: train loss: 0.2093, test loss 2.0112\n",
      "Epoch 1946: train loss: 0.2093, test loss 2.0106\n",
      "Epoch 1947: train loss: 0.2093, test loss 2.0100\n",
      "Epoch 1948: train loss: 0.2093, test loss 2.0094\n",
      "Epoch 1949: train loss: 0.2092, test loss 2.0088\n",
      "Epoch 1950: train loss: 0.2092, test loss 2.0082\n",
      "Epoch 1951: train loss: 0.2092, test loss 2.0076\n",
      "Epoch 1952: train loss: 0.2092, test loss 2.0070\n",
      "Epoch 1953: train loss: 0.2092, test loss 2.0064\n",
      "Epoch 1954: train loss: 0.2092, test loss 2.0058\n",
      "Epoch 1955: train loss: 0.2092, test loss 2.0052\n",
      "Epoch 1956: train loss: 0.2091, test loss 2.0046\n",
      "Epoch 1957: train loss: 0.2091, test loss 2.0040\n",
      "Epoch 1958: train loss: 0.2091, test loss 2.0034\n",
      "Epoch 1959: train loss: 0.2091, test loss 2.0028\n",
      "Epoch 1960: train loss: 0.2091, test loss 2.0022\n",
      "Epoch 1961: train loss: 0.2091, test loss 2.0016\n",
      "Epoch 1962: train loss: 0.2090, test loss 2.0011\n",
      "Epoch 1963: train loss: 0.2090, test loss 2.0005\n",
      "Epoch 1964: train loss: 0.2090, test loss 1.9999\n",
      "Epoch 1965: train loss: 0.2090, test loss 1.9993\n",
      "Epoch 1966: train loss: 0.2090, test loss 1.9987\n",
      "Epoch 1967: train loss: 0.2090, test loss 1.9981\n",
      "Epoch 1968: train loss: 0.2090, test loss 1.9975\n",
      "Epoch 1969: train loss: 0.2089, test loss 1.9970\n",
      "Epoch 1970: train loss: 0.2089, test loss 1.9964\n",
      "Epoch 1971: train loss: 0.2089, test loss 1.9958\n",
      "Epoch 1972: train loss: 0.2089, test loss 1.9952\n",
      "Epoch 1973: train loss: 0.2089, test loss 1.9946\n",
      "Epoch 1974: train loss: 0.2089, test loss 1.9940\n",
      "Epoch 1975: train loss: 0.2089, test loss 1.9934\n",
      "Epoch 1976: train loss: 0.2088, test loss 1.9929\n",
      "Epoch 1977: train loss: 0.2088, test loss 1.9923\n",
      "Epoch 1978: train loss: 0.2088, test loss 1.9917\n",
      "Epoch 1979: train loss: 0.2088, test loss 1.9911\n",
      "Epoch 1980: train loss: 0.2088, test loss 1.9905\n",
      "Epoch 1981: train loss: 0.2088, test loss 1.9899\n",
      "Epoch 1982: train loss: 0.2087, test loss 1.9894\n",
      "Epoch 1983: train loss: 0.2087, test loss 1.9888\n",
      "Epoch 1984: train loss: 0.2087, test loss 1.9882\n",
      "Epoch 1985: train loss: 0.2087, test loss 1.9876\n",
      "Epoch 1986: train loss: 0.2087, test loss 1.9870\n",
      "Epoch 1987: train loss: 0.2087, test loss 1.9864\n",
      "Epoch 1988: train loss: 0.2087, test loss 1.9859\n",
      "Epoch 1989: train loss: 0.2086, test loss 1.9853\n",
      "Epoch 1990: train loss: 0.2086, test loss 1.9847\n",
      "Epoch 1991: train loss: 0.2086, test loss 1.9841\n",
      "Epoch 1992: train loss: 0.2086, test loss 1.9835\n",
      "Epoch 1993: train loss: 0.2086, test loss 1.9829\n",
      "Epoch 1994: train loss: 0.2086, test loss 1.9824\n",
      "Epoch 1995: train loss: 0.2086, test loss 1.9818\n",
      "Epoch 1996: train loss: 0.2085, test loss 1.9812\n",
      "Epoch 1997: train loss: 0.2085, test loss 1.9806\n",
      "Epoch 1998: train loss: 0.2085, test loss 1.9800\n",
      "Epoch 1999: train loss: 0.2085, test loss 1.9795\n",
      "Epoch 2000: train loss: 0.2085, test loss 1.9789\n",
      "Epoch 2001: train loss: 0.2085, test loss 1.9783\n",
      "Epoch 2002: train loss: 0.2085, test loss 1.9777\n",
      "Epoch 2003: train loss: 0.2084, test loss 1.9771\n",
      "Epoch 2004: train loss: 0.2084, test loss 1.9766\n",
      "Epoch 2005: train loss: 0.2084, test loss 1.9760\n",
      "Epoch 2006: train loss: 0.2084, test loss 1.9754\n",
      "Epoch 2007: train loss: 0.2084, test loss 1.9748\n",
      "Epoch 2008: train loss: 0.2084, test loss 1.9743\n",
      "Epoch 2009: train loss: 0.2084, test loss 1.9737\n",
      "Epoch 2010: train loss: 0.2083, test loss 1.9731\n",
      "Epoch 2011: train loss: 0.2083, test loss 1.9725\n",
      "Epoch 2012: train loss: 0.2083, test loss 1.9720\n",
      "Epoch 2013: train loss: 0.2083, test loss 1.9714\n",
      "Epoch 2014: train loss: 0.2083, test loss 1.9708\n",
      "Epoch 2015: train loss: 0.2083, test loss 1.9703\n",
      "Epoch 2016: train loss: 0.2083, test loss 1.9697\n",
      "Epoch 2017: train loss: 0.2082, test loss 1.9691\n",
      "Epoch 2018: train loss: 0.2082, test loss 1.9685\n",
      "Epoch 2019: train loss: 0.2082, test loss 1.9680\n",
      "Epoch 2020: train loss: 0.2082, test loss 1.9674\n",
      "Epoch 2021: train loss: 0.2082, test loss 1.9668\n",
      "Epoch 2022: train loss: 0.2082, test loss 1.9663\n",
      "Epoch 2023: train loss: 0.2081, test loss 1.9657\n",
      "Epoch 2024: train loss: 0.2081, test loss 1.9651\n",
      "Epoch 2025: train loss: 0.2081, test loss 1.9646\n",
      "Epoch 2026: train loss: 0.2081, test loss 1.9640\n",
      "Epoch 2027: train loss: 0.2081, test loss 1.9635\n",
      "Epoch 2028: train loss: 0.2081, test loss 1.9629\n",
      "Epoch 2029: train loss: 0.2081, test loss 1.9624\n",
      "Epoch 2030: train loss: 0.2080, test loss 1.9618\n",
      "Epoch 2031: train loss: 0.2080, test loss 1.9612\n",
      "Epoch 2032: train loss: 0.2080, test loss 1.9607\n",
      "Epoch 2033: train loss: 0.2080, test loss 1.9601\n",
      "Epoch 2034: train loss: 0.2080, test loss 1.9596\n",
      "Epoch 2035: train loss: 0.2080, test loss 1.9590\n",
      "Epoch 2036: train loss: 0.2080, test loss 1.9585\n",
      "Epoch 2037: train loss: 0.2079, test loss 1.9579\n",
      "Epoch 2038: train loss: 0.2079, test loss 1.9574\n",
      "Epoch 2039: train loss: 0.2079, test loss 1.9568\n",
      "Epoch 2040: train loss: 0.2079, test loss 1.9563\n",
      "Epoch 2041: train loss: 0.2079, test loss 1.9557\n",
      "Epoch 2042: train loss: 0.2079, test loss 1.9551\n",
      "Epoch 2043: train loss: 0.2079, test loss 1.9546\n",
      "Epoch 2044: train loss: 0.2078, test loss 1.9540\n",
      "Epoch 2045: train loss: 0.2078, test loss 1.9535\n",
      "Epoch 2046: train loss: 0.2078, test loss 1.9529\n",
      "Epoch 2047: train loss: 0.2078, test loss 1.9524\n",
      "Epoch 2048: train loss: 0.2078, test loss 1.9518\n",
      "Epoch 2049: train loss: 0.2078, test loss 1.9513\n",
      "Epoch 2050: train loss: 0.2078, test loss 1.9507\n",
      "Epoch 2051: train loss: 0.2077, test loss 1.9502\n",
      "Epoch 2052: train loss: 0.2077, test loss 1.9496\n",
      "Epoch 2053: train loss: 0.2077, test loss 1.9491\n",
      "Epoch 2054: train loss: 0.2077, test loss 1.9485\n",
      "Epoch 2055: train loss: 0.2077, test loss 1.9480\n",
      "Epoch 2056: train loss: 0.2077, test loss 1.9474\n",
      "Epoch 2057: train loss: 0.2077, test loss 1.9469\n",
      "Epoch 2058: train loss: 0.2077, test loss 1.9463\n",
      "Epoch 2059: train loss: 0.2076, test loss 1.9458\n",
      "Epoch 2060: train loss: 0.2076, test loss 1.9453\n",
      "Epoch 2061: train loss: 0.2076, test loss 1.9447\n",
      "Epoch 2062: train loss: 0.2076, test loss 1.9442\n",
      "Epoch 2063: train loss: 0.2076, test loss 1.9436\n",
      "Epoch 2064: train loss: 0.2076, test loss 1.9431\n",
      "Epoch 2065: train loss: 0.2076, test loss 1.9425\n",
      "Epoch 2066: train loss: 0.2075, test loss 1.9420\n",
      "Epoch 2067: train loss: 0.2075, test loss 1.9414\n",
      "Epoch 2068: train loss: 0.2075, test loss 1.9409\n",
      "Epoch 2069: train loss: 0.2075, test loss 1.9404\n",
      "Epoch 2070: train loss: 0.2075, test loss 1.9398\n",
      "Epoch 2071: train loss: 0.2075, test loss 1.9393\n",
      "Epoch 2072: train loss: 0.2075, test loss 1.9387\n",
      "Epoch 2073: train loss: 0.2074, test loss 1.9382\n",
      "Epoch 2074: train loss: 0.2074, test loss 1.9377\n",
      "Epoch 2075: train loss: 0.2074, test loss 1.9371\n",
      "Epoch 2076: train loss: 0.2074, test loss 1.9366\n",
      "Epoch 2077: train loss: 0.2074, test loss 1.9360\n",
      "Epoch 2078: train loss: 0.2074, test loss 1.9355\n",
      "Epoch 2079: train loss: 0.2074, test loss 1.9350\n",
      "Epoch 2080: train loss: 0.2073, test loss 1.9344\n",
      "Epoch 2081: train loss: 0.2073, test loss 1.9339\n",
      "Epoch 2082: train loss: 0.2073, test loss 1.9333\n",
      "Epoch 2083: train loss: 0.2073, test loss 1.9328\n",
      "Epoch 2084: train loss: 0.2073, test loss 1.9323\n",
      "Epoch 2085: train loss: 0.2073, test loss 1.9317\n",
      "Epoch 2086: train loss: 0.2073, test loss 1.9312\n",
      "Epoch 2087: train loss: 0.2072, test loss 1.9307\n",
      "Epoch 2088: train loss: 0.2072, test loss 1.9301\n",
      "Epoch 2089: train loss: 0.2072, test loss 1.9296\n",
      "Epoch 2090: train loss: 0.2072, test loss 1.9291\n",
      "Epoch 2091: train loss: 0.2072, test loss 1.9285\n",
      "Epoch 2092: train loss: 0.2072, test loss 1.9280\n",
      "Epoch 2093: train loss: 0.2072, test loss 1.9275\n",
      "Epoch 2094: train loss: 0.2071, test loss 1.9269\n",
      "Epoch 2095: train loss: 0.2071, test loss 1.9264\n",
      "Epoch 2096: train loss: 0.2071, test loss 1.9259\n",
      "Epoch 2097: train loss: 0.2071, test loss 1.9253\n",
      "Epoch 2098: train loss: 0.2071, test loss 1.9248\n",
      "Epoch 2099: train loss: 0.2071, test loss 1.9243\n",
      "Epoch 2100: train loss: 0.2071, test loss 1.9238\n",
      "Epoch 2101: train loss: 0.2070, test loss 1.9232\n",
      "Epoch 2102: train loss: 0.2070, test loss 1.9227\n",
      "Epoch 2103: train loss: 0.2070, test loss 1.9222\n",
      "Epoch 2104: train loss: 0.2070, test loss 1.9216\n",
      "Epoch 2105: train loss: 0.2070, test loss 1.9211\n",
      "Epoch 2106: train loss: 0.2070, test loss 1.9206\n",
      "Epoch 2107: train loss: 0.2070, test loss 1.9201\n",
      "Epoch 2108: train loss: 0.2069, test loss 1.9195\n",
      "Epoch 2109: train loss: 0.2069, test loss 1.9190\n",
      "Epoch 2110: train loss: 0.2069, test loss 1.9185\n",
      "Epoch 2111: train loss: 0.2069, test loss 1.9179\n",
      "Epoch 2112: train loss: 0.2069, test loss 1.9174\n",
      "Epoch 2113: train loss: 0.2069, test loss 1.9169\n",
      "Epoch 2114: train loss: 0.2069, test loss 1.9163\n",
      "Epoch 2115: train loss: 0.2068, test loss 1.9158\n",
      "Epoch 2116: train loss: 0.2068, test loss 1.9153\n",
      "Epoch 2117: train loss: 0.2068, test loss 1.9148\n",
      "Epoch 2118: train loss: 0.2068, test loss 1.9142\n",
      "Epoch 2119: train loss: 0.2068, test loss 1.9137\n",
      "Epoch 2120: train loss: 0.2068, test loss 1.9132\n",
      "Epoch 2121: train loss: 0.2068, test loss 1.9126\n",
      "Epoch 2122: train loss: 0.2067, test loss 1.9121\n",
      "Epoch 2123: train loss: 0.2067, test loss 1.9116\n",
      "Epoch 2124: train loss: 0.2067, test loss 1.9111\n",
      "Epoch 2125: train loss: 0.2067, test loss 1.9105\n",
      "Epoch 2126: train loss: 0.2067, test loss 1.9100\n",
      "Epoch 2127: train loss: 0.2067, test loss 1.9095\n",
      "Epoch 2128: train loss: 0.2067, test loss 1.9090\n",
      "Epoch 2129: train loss: 0.2067, test loss 1.9084\n",
      "Epoch 2130: train loss: 0.2066, test loss 1.9079\n",
      "Epoch 2131: train loss: 0.2066, test loss 1.9074\n",
      "Epoch 2132: train loss: 0.2066, test loss 1.9069\n",
      "Epoch 2133: train loss: 0.2066, test loss 1.9064\n",
      "Epoch 2134: train loss: 0.2066, test loss 1.9058\n",
      "Epoch 2135: train loss: 0.2066, test loss 1.9053\n",
      "Epoch 2136: train loss: 0.2066, test loss 1.9048\n",
      "Epoch 2137: train loss: 0.2065, test loss 1.9043\n",
      "Epoch 2138: train loss: 0.2065, test loss 1.9037\n",
      "Epoch 2139: train loss: 0.2065, test loss 1.9032\n",
      "Epoch 2140: train loss: 0.2065, test loss 1.9027\n",
      "Epoch 2141: train loss: 0.2065, test loss 1.9022\n",
      "Epoch 2142: train loss: 0.2065, test loss 1.9017\n",
      "Epoch 2143: train loss: 0.2065, test loss 1.9011\n",
      "Epoch 2144: train loss: 0.2064, test loss 1.9006\n",
      "Epoch 2145: train loss: 0.2064, test loss 1.9001\n",
      "Epoch 2146: train loss: 0.2064, test loss 1.8996\n",
      "Epoch 2147: train loss: 0.2064, test loss 1.8991\n",
      "Epoch 2148: train loss: 0.2064, test loss 1.8985\n",
      "Epoch 2149: train loss: 0.2064, test loss 1.8980\n",
      "Epoch 2150: train loss: 0.2064, test loss 1.8975\n",
      "Epoch 2151: train loss: 0.2063, test loss 1.8969\n",
      "Epoch 2152: train loss: 0.2063, test loss 1.8964\n",
      "Epoch 2153: train loss: 0.2063, test loss 1.8959\n",
      "Epoch 2154: train loss: 0.2063, test loss 1.8954\n",
      "Epoch 2155: train loss: 0.2063, test loss 1.8949\n",
      "Epoch 2156: train loss: 0.2063, test loss 1.8943\n",
      "Epoch 2157: train loss: 0.2063, test loss 1.8938\n",
      "Epoch 2158: train loss: 0.2063, test loss 1.8933\n",
      "Epoch 2159: train loss: 0.2062, test loss 1.8928\n",
      "Epoch 2160: train loss: 0.2062, test loss 1.8923\n",
      "Epoch 2161: train loss: 0.2062, test loss 1.8918\n",
      "Epoch 2162: train loss: 0.2062, test loss 1.8913\n",
      "Epoch 2163: train loss: 0.2062, test loss 1.8908\n",
      "Epoch 2164: train loss: 0.2062, test loss 1.8903\n",
      "Epoch 2165: train loss: 0.2062, test loss 1.8898\n",
      "Epoch 2166: train loss: 0.2061, test loss 1.8893\n",
      "Epoch 2167: train loss: 0.2061, test loss 1.8887\n",
      "Epoch 2168: train loss: 0.2061, test loss 1.8882\n",
      "Epoch 2169: train loss: 0.2061, test loss 1.8877\n",
      "Epoch 2170: train loss: 0.2061, test loss 1.8872\n",
      "Epoch 2171: train loss: 0.2061, test loss 1.8867\n",
      "Epoch 2172: train loss: 0.2061, test loss 1.8862\n",
      "Epoch 2173: train loss: 0.2061, test loss 1.8857\n",
      "Epoch 2174: train loss: 0.2060, test loss 1.8852\n",
      "Epoch 2175: train loss: 0.2060, test loss 1.8847\n",
      "Epoch 2176: train loss: 0.2060, test loss 1.8842\n",
      "Epoch 2177: train loss: 0.2060, test loss 1.8837\n",
      "Epoch 2178: train loss: 0.2060, test loss 1.8832\n",
      "Epoch 2179: train loss: 0.2060, test loss 1.8827\n",
      "Epoch 2180: train loss: 0.2060, test loss 1.8822\n",
      "Epoch 2181: train loss: 0.2059, test loss 1.8817\n",
      "Epoch 2182: train loss: 0.2059, test loss 1.8812\n",
      "Epoch 2183: train loss: 0.2059, test loss 1.8807\n",
      "Epoch 2184: train loss: 0.2059, test loss 1.8802\n",
      "Epoch 2185: train loss: 0.2059, test loss 1.8797\n",
      "Epoch 2186: train loss: 0.2059, test loss 1.8792\n",
      "Epoch 2187: train loss: 0.2059, test loss 1.8787\n",
      "Epoch 2188: train loss: 0.2058, test loss 1.8782\n",
      "Epoch 2189: train loss: 0.2058, test loss 1.8777\n",
      "Epoch 2190: train loss: 0.2058, test loss 1.8772\n",
      "Epoch 2191: train loss: 0.2058, test loss 1.8767\n",
      "Epoch 2192: train loss: 0.2058, test loss 1.8762\n",
      "Epoch 2193: train loss: 0.2058, test loss 1.8757\n",
      "Epoch 2194: train loss: 0.2058, test loss 1.8752\n",
      "Epoch 2195: train loss: 0.2058, test loss 1.8747\n",
      "Epoch 2196: train loss: 0.2057, test loss 1.8742\n",
      "Epoch 2197: train loss: 0.2057, test loss 1.8737\n",
      "Epoch 2198: train loss: 0.2057, test loss 1.8732\n",
      "Epoch 2199: train loss: 0.2057, test loss 1.8727\n",
      "Epoch 2200: train loss: 0.2057, test loss 1.8722\n",
      "Epoch 2201: train loss: 0.2057, test loss 1.8717\n",
      "Epoch 2202: train loss: 0.2057, test loss 1.8712\n",
      "Epoch 2203: train loss: 0.2056, test loss 1.8707\n",
      "Epoch 2204: train loss: 0.2056, test loss 1.8702\n",
      "Epoch 2205: train loss: 0.2056, test loss 1.8697\n",
      "Epoch 2206: train loss: 0.2056, test loss 1.8692\n",
      "Epoch 2207: train loss: 0.2056, test loss 1.8687\n",
      "Epoch 2208: train loss: 0.2056, test loss 1.8682\n",
      "Epoch 2209: train loss: 0.2056, test loss 1.8677\n",
      "Epoch 2210: train loss: 0.2056, test loss 1.8672\n",
      "Epoch 2211: train loss: 0.2055, test loss 1.8667\n",
      "Epoch 2212: train loss: 0.2055, test loss 1.8662\n",
      "Epoch 2213: train loss: 0.2055, test loss 1.8657\n",
      "Epoch 2214: train loss: 0.2055, test loss 1.8652\n",
      "Epoch 2215: train loss: 0.2055, test loss 1.8648\n",
      "Epoch 2216: train loss: 0.2055, test loss 1.8643\n",
      "Epoch 2217: train loss: 0.2055, test loss 1.8638\n",
      "Epoch 2218: train loss: 0.2055, test loss 1.8633\n",
      "Epoch 2219: train loss: 0.2054, test loss 1.8628\n",
      "Epoch 2220: train loss: 0.2054, test loss 1.8623\n",
      "Epoch 2221: train loss: 0.2054, test loss 1.8618\n",
      "Epoch 2222: train loss: 0.2054, test loss 1.8613\n",
      "Epoch 2223: train loss: 0.2054, test loss 1.8608\n",
      "Epoch 2224: train loss: 0.2054, test loss 1.8603\n",
      "Epoch 2225: train loss: 0.2054, test loss 1.8599\n",
      "Epoch 2226: train loss: 0.2053, test loss 1.8594\n",
      "Epoch 2227: train loss: 0.2053, test loss 1.8589\n",
      "Epoch 2228: train loss: 0.2053, test loss 1.8584\n",
      "Epoch 2229: train loss: 0.2053, test loss 1.8579\n",
      "Epoch 2230: train loss: 0.2053, test loss 1.8574\n",
      "Epoch 2231: train loss: 0.2053, test loss 1.8569\n",
      "Epoch 2232: train loss: 0.2053, test loss 1.8565\n",
      "Epoch 2233: train loss: 0.2053, test loss 1.8560\n",
      "Epoch 2234: train loss: 0.2052, test loss 1.8555\n",
      "Epoch 2235: train loss: 0.2052, test loss 1.8550\n",
      "Epoch 2236: train loss: 0.2052, test loss 1.8545\n",
      "Epoch 2237: train loss: 0.2052, test loss 1.8540\n",
      "Epoch 2238: train loss: 0.2052, test loss 1.8536\n",
      "Epoch 2239: train loss: 0.2052, test loss 1.8531\n",
      "Epoch 2240: train loss: 0.2052, test loss 1.8526\n",
      "Epoch 2241: train loss: 0.2052, test loss 1.8521\n",
      "Epoch 2242: train loss: 0.2051, test loss 1.8516\n",
      "Epoch 2243: train loss: 0.2051, test loss 1.8511\n",
      "Epoch 2244: train loss: 0.2051, test loss 1.8507\n",
      "Epoch 2245: train loss: 0.2051, test loss 1.8502\n",
      "Epoch 2246: train loss: 0.2051, test loss 1.8497\n",
      "Epoch 2247: train loss: 0.2051, test loss 1.8492\n",
      "Epoch 2248: train loss: 0.2051, test loss 1.8487\n",
      "Epoch 2249: train loss: 0.2051, test loss 1.8483\n",
      "Epoch 2250: train loss: 0.2050, test loss 1.8478\n",
      "Epoch 2251: train loss: 0.2050, test loss 1.8473\n",
      "Epoch 2252: train loss: 0.2050, test loss 1.8468\n",
      "Epoch 2253: train loss: 0.2050, test loss 1.8463\n",
      "Epoch 2254: train loss: 0.2050, test loss 1.8459\n",
      "Epoch 2255: train loss: 0.2050, test loss 1.8454\n",
      "Epoch 2256: train loss: 0.2050, test loss 1.8449\n",
      "Epoch 2257: train loss: 0.2050, test loss 1.8444\n",
      "Epoch 2258: train loss: 0.2049, test loss 1.8439\n",
      "Epoch 2259: train loss: 0.2049, test loss 1.8435\n",
      "Epoch 2260: train loss: 0.2049, test loss 1.8430\n",
      "Epoch 2261: train loss: 0.2049, test loss 1.8425\n",
      "Epoch 2262: train loss: 0.2049, test loss 1.8420\n",
      "Epoch 2263: train loss: 0.2049, test loss 1.8415\n",
      "Epoch 2264: train loss: 0.2049, test loss 1.8410\n",
      "Epoch 2265: train loss: 0.2049, test loss 1.8406\n",
      "Epoch 2266: train loss: 0.2048, test loss 1.8401\n",
      "Epoch 2267: train loss: 0.2048, test loss 1.8396\n",
      "Epoch 2268: train loss: 0.2048, test loss 1.8391\n",
      "Epoch 2269: train loss: 0.2048, test loss 1.8387\n",
      "Epoch 2270: train loss: 0.2048, test loss 1.8382\n",
      "Epoch 2271: train loss: 0.2048, test loss 1.8377\n",
      "Epoch 2272: train loss: 0.2048, test loss 1.8372\n",
      "Epoch 2273: train loss: 0.2048, test loss 1.8368\n",
      "Epoch 2274: train loss: 0.2047, test loss 1.8363\n",
      "Epoch 2275: train loss: 0.2047, test loss 1.8358\n",
      "Epoch 2276: train loss: 0.2047, test loss 1.8354\n",
      "Epoch 2277: train loss: 0.2047, test loss 1.8349\n",
      "Epoch 2278: train loss: 0.2047, test loss 1.8344\n",
      "Epoch 2279: train loss: 0.2047, test loss 1.8340\n",
      "Epoch 2280: train loss: 0.2047, test loss 1.8335\n",
      "Epoch 2281: train loss: 0.2047, test loss 1.8331\n",
      "Epoch 2282: train loss: 0.2046, test loss 1.8326\n",
      "Epoch 2283: train loss: 0.2046, test loss 1.8321\n",
      "Epoch 2284: train loss: 0.2046, test loss 1.8317\n",
      "Epoch 2285: train loss: 0.2046, test loss 1.8312\n",
      "Epoch 2286: train loss: 0.2046, test loss 1.8307\n",
      "Epoch 2287: train loss: 0.2046, test loss 1.8303\n",
      "Epoch 2288: train loss: 0.2046, test loss 1.8298\n",
      "Epoch 2289: train loss: 0.2046, test loss 1.8293\n",
      "Epoch 2290: train loss: 0.2045, test loss 1.8289\n",
      "Epoch 2291: train loss: 0.2045, test loss 1.8284\n",
      "Epoch 2292: train loss: 0.2045, test loss 1.8279\n",
      "Epoch 2293: train loss: 0.2045, test loss 1.8275\n",
      "Epoch 2294: train loss: 0.2045, test loss 1.8270\n",
      "Epoch 2295: train loss: 0.2045, test loss 1.8265\n",
      "Epoch 2296: train loss: 0.2045, test loss 1.8261\n",
      "Epoch 2297: train loss: 0.2045, test loss 1.8256\n",
      "Epoch 2298: train loss: 0.2044, test loss 1.8251\n",
      "Epoch 2299: train loss: 0.2044, test loss 1.8247\n",
      "Epoch 2300: train loss: 0.2044, test loss 1.8242\n",
      "Epoch 2301: train loss: 0.2044, test loss 1.8237\n",
      "Epoch 2302: train loss: 0.2044, test loss 1.8233\n",
      "Epoch 2303: train loss: 0.2044, test loss 1.8228\n",
      "Epoch 2304: train loss: 0.2044, test loss 1.8223\n",
      "Epoch 2305: train loss: 0.2044, test loss 1.8219\n",
      "Epoch 2306: train loss: 0.2043, test loss 1.8214\n",
      "Epoch 2307: train loss: 0.2043, test loss 1.8209\n",
      "Epoch 2308: train loss: 0.2043, test loss 1.8205\n",
      "Epoch 2309: train loss: 0.2043, test loss 1.8200\n",
      "Epoch 2310: train loss: 0.2043, test loss 1.8196\n",
      "Epoch 2311: train loss: 0.2043, test loss 1.8191\n",
      "Epoch 2312: train loss: 0.2043, test loss 1.8186\n",
      "Epoch 2313: train loss: 0.2043, test loss 1.8182\n",
      "Epoch 2314: train loss: 0.2043, test loss 1.8177\n",
      "Epoch 2315: train loss: 0.2042, test loss 1.8173\n",
      "Epoch 2316: train loss: 0.2042, test loss 1.8168\n",
      "Epoch 2317: train loss: 0.2042, test loss 1.8163\n",
      "Epoch 2318: train loss: 0.2042, test loss 1.8159\n",
      "Epoch 2319: train loss: 0.2042, test loss 1.8154\n",
      "Epoch 2320: train loss: 0.2042, test loss 1.8150\n",
      "Epoch 2321: train loss: 0.2042, test loss 1.8145\n",
      "Epoch 2322: train loss: 0.2042, test loss 1.8140\n",
      "Epoch 2323: train loss: 0.2041, test loss 1.8136\n",
      "Epoch 2324: train loss: 0.2041, test loss 1.8131\n",
      "Epoch 2325: train loss: 0.2041, test loss 1.8126\n",
      "Epoch 2326: train loss: 0.2041, test loss 1.8122\n",
      "Epoch 2327: train loss: 0.2041, test loss 1.8117\n",
      "Epoch 2328: train loss: 0.2041, test loss 1.8112\n",
      "Epoch 2329: train loss: 0.2041, test loss 1.8108\n",
      "Epoch 2330: train loss: 0.2041, test loss 1.8103\n",
      "Epoch 2331: train loss: 0.2041, test loss 1.8099\n",
      "Epoch 2332: train loss: 0.2040, test loss 1.8094\n",
      "Epoch 2333: train loss: 0.2040, test loss 1.8089\n",
      "Epoch 2334: train loss: 0.2040, test loss 1.8085\n",
      "Epoch 2335: train loss: 0.2040, test loss 1.8080\n",
      "Epoch 2336: train loss: 0.2040, test loss 1.8076\n",
      "Epoch 2337: train loss: 0.2040, test loss 1.8071\n",
      "Epoch 2338: train loss: 0.2040, test loss 1.8067\n",
      "Epoch 2339: train loss: 0.2040, test loss 1.8062\n",
      "Epoch 2340: train loss: 0.2040, test loss 1.8057\n",
      "Epoch 2341: train loss: 0.2039, test loss 1.8053\n",
      "Epoch 2342: train loss: 0.2039, test loss 1.8048\n",
      "Epoch 2343: train loss: 0.2039, test loss 1.8044\n",
      "Epoch 2344: train loss: 0.2039, test loss 1.8039\n",
      "Epoch 2345: train loss: 0.2039, test loss 1.8035\n",
      "Epoch 2346: train loss: 0.2039, test loss 1.8031\n",
      "Epoch 2347: train loss: 0.2039, test loss 1.8026\n",
      "Epoch 2348: train loss: 0.2039, test loss 1.8022\n",
      "Epoch 2349: train loss: 0.2039, test loss 1.8017\n",
      "Epoch 2350: train loss: 0.2038, test loss 1.8013\n",
      "Epoch 2351: train loss: 0.2038, test loss 1.8008\n",
      "Epoch 2352: train loss: 0.2038, test loss 1.8004\n",
      "Epoch 2353: train loss: 0.2038, test loss 1.7999\n",
      "Epoch 2354: train loss: 0.2038, test loss 1.7995\n",
      "Epoch 2355: train loss: 0.2038, test loss 1.7991\n",
      "Epoch 2356: train loss: 0.2038, test loss 1.7986\n",
      "Epoch 2357: train loss: 0.2038, test loss 1.7982\n",
      "Epoch 2358: train loss: 0.2038, test loss 1.7977\n",
      "Epoch 2359: train loss: 0.2037, test loss 1.7973\n",
      "Epoch 2360: train loss: 0.2037, test loss 1.7969\n",
      "Epoch 2361: train loss: 0.2037, test loss 1.7964\n",
      "Epoch 2362: train loss: 0.2037, test loss 1.7960\n",
      "Epoch 2363: train loss: 0.2037, test loss 1.7955\n",
      "Epoch 2364: train loss: 0.2037, test loss 1.7951\n",
      "Epoch 2365: train loss: 0.2037, test loss 1.7947\n",
      "Epoch 2366: train loss: 0.2037, test loss 1.7942\n",
      "Epoch 2367: train loss: 0.2037, test loss 1.7938\n",
      "Epoch 2368: train loss: 0.2036, test loss 1.7934\n",
      "Epoch 2369: train loss: 0.2036, test loss 1.7929\n",
      "Epoch 2370: train loss: 0.2036, test loss 1.7925\n",
      "Epoch 2371: train loss: 0.2036, test loss 1.7920\n",
      "Epoch 2372: train loss: 0.2036, test loss 1.7916\n",
      "Epoch 2373: train loss: 0.2036, test loss 1.7911\n",
      "Epoch 2374: train loss: 0.2036, test loss 1.7907\n",
      "Epoch 2375: train loss: 0.2036, test loss 1.7903\n",
      "Epoch 2376: train loss: 0.2036, test loss 1.7898\n",
      "Epoch 2377: train loss: 0.2035, test loss 1.7894\n",
      "Epoch 2378: train loss: 0.2035, test loss 1.7889\n",
      "Epoch 2379: train loss: 0.2035, test loss 1.7885\n",
      "Epoch 2380: train loss: 0.2035, test loss 1.7881\n",
      "Epoch 2381: train loss: 0.2035, test loss 1.7876\n",
      "Epoch 2382: train loss: 0.2035, test loss 1.7872\n",
      "Epoch 2383: train loss: 0.2035, test loss 1.7867\n",
      "Epoch 2384: train loss: 0.2035, test loss 1.7863\n",
      "Epoch 2385: train loss: 0.2035, test loss 1.7859\n",
      "Epoch 2386: train loss: 0.2034, test loss 1.7854\n",
      "Epoch 2387: train loss: 0.2034, test loss 1.7850\n",
      "Epoch 2388: train loss: 0.2034, test loss 1.7845\n",
      "Epoch 2389: train loss: 0.2034, test loss 1.7841\n",
      "Epoch 2390: train loss: 0.2034, test loss 1.7837\n",
      "Epoch 2391: train loss: 0.2034, test loss 1.7832\n",
      "Epoch 2392: train loss: 0.2034, test loss 1.7828\n",
      "Epoch 2393: train loss: 0.2034, test loss 1.7823\n",
      "Epoch 2394: train loss: 0.2034, test loss 1.7819\n",
      "Epoch 2395: train loss: 0.2033, test loss 1.7815\n",
      "Epoch 2396: train loss: 0.2033, test loss 1.7810\n",
      "Epoch 2397: train loss: 0.2033, test loss 1.7806\n",
      "Epoch 2398: train loss: 0.2033, test loss 1.7802\n",
      "Epoch 2399: train loss: 0.2033, test loss 1.7797\n",
      "Epoch 2400: train loss: 0.2033, test loss 1.7793\n",
      "Epoch 2401: train loss: 0.2033, test loss 1.7788\n",
      "Epoch 2402: train loss: 0.2033, test loss 1.7784\n",
      "Epoch 2403: train loss: 0.2033, test loss 1.7780\n",
      "Epoch 2404: train loss: 0.2032, test loss 1.7775\n",
      "Epoch 2405: train loss: 0.2032, test loss 1.7771\n",
      "Epoch 2406: train loss: 0.2032, test loss 1.7767\n",
      "Epoch 2407: train loss: 0.2032, test loss 1.7762\n",
      "Epoch 2408: train loss: 0.2032, test loss 1.7758\n",
      "Epoch 2409: train loss: 0.2032, test loss 1.7754\n",
      "Epoch 2410: train loss: 0.2032, test loss 1.7749\n",
      "Epoch 2411: train loss: 0.2032, test loss 1.7745\n",
      "Epoch 2412: train loss: 0.2032, test loss 1.7740\n",
      "Epoch 2413: train loss: 0.2031, test loss 1.7736\n",
      "Epoch 2414: train loss: 0.2031, test loss 1.7732\n",
      "Epoch 2415: train loss: 0.2031, test loss 1.7727\n",
      "Epoch 2416: train loss: 0.2031, test loss 1.7723\n",
      "Epoch 2417: train loss: 0.2031, test loss 1.7719\n",
      "Epoch 2418: train loss: 0.2031, test loss 1.7714\n",
      "Epoch 2419: train loss: 0.2031, test loss 1.7710\n",
      "Epoch 2420: train loss: 0.2031, test loss 1.7705\n",
      "Epoch 2421: train loss: 0.2031, test loss 1.7701\n",
      "Epoch 2422: train loss: 0.2030, test loss 1.7697\n",
      "Epoch 2423: train loss: 0.2030, test loss 1.7692\n",
      "Epoch 2424: train loss: 0.2030, test loss 1.7688\n",
      "Epoch 2425: train loss: 0.2030, test loss 1.7684\n",
      "Epoch 2426: train loss: 0.2030, test loss 1.7679\n",
      "Epoch 2427: train loss: 0.2030, test loss 1.7675\n",
      "Epoch 2428: train loss: 0.2030, test loss 1.7671\n",
      "Epoch 2429: train loss: 0.2030, test loss 1.7667\n",
      "Epoch 2430: train loss: 0.2030, test loss 1.7662\n",
      "Epoch 2431: train loss: 0.2030, test loss 1.7658\n",
      "Epoch 2432: train loss: 0.2029, test loss 1.7654\n",
      "Epoch 2433: train loss: 0.2029, test loss 1.7649\n",
      "Epoch 2434: train loss: 0.2029, test loss 1.7645\n",
      "Epoch 2435: train loss: 0.2029, test loss 1.7641\n",
      "Epoch 2436: train loss: 0.2029, test loss 1.7637\n",
      "Epoch 2437: train loss: 0.2029, test loss 1.7632\n",
      "Epoch 2438: train loss: 0.2029, test loss 1.7628\n",
      "Epoch 2439: train loss: 0.2029, test loss 1.7624\n",
      "Epoch 2440: train loss: 0.2029, test loss 1.7620\n",
      "Epoch 2441: train loss: 0.2028, test loss 1.7615\n",
      "Epoch 2442: train loss: 0.2028, test loss 1.7611\n",
      "Epoch 2443: train loss: 0.2028, test loss 1.7607\n",
      "Epoch 2444: train loss: 0.2028, test loss 1.7603\n",
      "Epoch 2445: train loss: 0.2028, test loss 1.7598\n",
      "Epoch 2446: train loss: 0.2028, test loss 1.7594\n",
      "Epoch 2447: train loss: 0.2028, test loss 1.7590\n",
      "Epoch 2448: train loss: 0.2028, test loss 1.7586\n",
      "Epoch 2449: train loss: 0.2028, test loss 1.7581\n",
      "Epoch 2450: train loss: 0.2028, test loss 1.7577\n",
      "Epoch 2451: train loss: 0.2027, test loss 1.7573\n",
      "Epoch 2452: train loss: 0.2027, test loss 1.7569\n",
      "Epoch 2453: train loss: 0.2027, test loss 1.7564\n",
      "Epoch 2454: train loss: 0.2027, test loss 1.7560\n",
      "Epoch 2455: train loss: 0.2027, test loss 1.7556\n",
      "Epoch 2456: train loss: 0.2027, test loss 1.7552\n",
      "Epoch 2457: train loss: 0.2027, test loss 1.7547\n",
      "Epoch 2458: train loss: 0.2027, test loss 1.7543\n",
      "Epoch 2459: train loss: 0.2027, test loss 1.7539\n",
      "Epoch 2460: train loss: 0.2026, test loss 1.7535\n",
      "Epoch 2461: train loss: 0.2026, test loss 1.7531\n",
      "Epoch 2462: train loss: 0.2026, test loss 1.7526\n",
      "Epoch 2463: train loss: 0.2026, test loss 1.7522\n",
      "Epoch 2464: train loss: 0.2026, test loss 1.7518\n",
      "Epoch 2465: train loss: 0.2026, test loss 1.7514\n",
      "Epoch 2466: train loss: 0.2026, test loss 1.7509\n",
      "Epoch 2467: train loss: 0.2026, test loss 1.7505\n",
      "Epoch 2468: train loss: 0.2026, test loss 1.7501\n",
      "Epoch 2469: train loss: 0.2026, test loss 1.7497\n",
      "Epoch 2470: train loss: 0.2025, test loss 1.7493\n",
      "Epoch 2471: train loss: 0.2025, test loss 1.7488\n",
      "Epoch 2472: train loss: 0.2025, test loss 1.7484\n",
      "Epoch 2473: train loss: 0.2025, test loss 1.7480\n",
      "Epoch 2474: train loss: 0.2025, test loss 1.7476\n",
      "Epoch 2475: train loss: 0.2025, test loss 1.7472\n",
      "Epoch 2476: train loss: 0.2025, test loss 1.7468\n",
      "Epoch 2477: train loss: 0.2025, test loss 1.7463\n",
      "Epoch 2478: train loss: 0.2025, test loss 1.7459\n",
      "Epoch 2479: train loss: 0.2025, test loss 1.7455\n",
      "Epoch 2480: train loss: 0.2024, test loss 1.7451\n",
      "Epoch 2481: train loss: 0.2024, test loss 1.7447\n",
      "Epoch 2482: train loss: 0.2024, test loss 1.7443\n",
      "Epoch 2483: train loss: 0.2024, test loss 1.7438\n",
      "Epoch 2484: train loss: 0.2024, test loss 1.7434\n",
      "Epoch 2485: train loss: 0.2024, test loss 1.7430\n",
      "Epoch 2486: train loss: 0.2024, test loss 1.7426\n",
      "Epoch 2487: train loss: 0.2024, test loss 1.7422\n",
      "Epoch 2488: train loss: 0.2024, test loss 1.7418\n",
      "Epoch 2489: train loss: 0.2023, test loss 1.7413\n",
      "Epoch 2490: train loss: 0.2023, test loss 1.7409\n",
      "Epoch 2491: train loss: 0.2023, test loss 1.7405\n",
      "Epoch 2492: train loss: 0.2023, test loss 1.7401\n",
      "Epoch 2493: train loss: 0.2023, test loss 1.7397\n",
      "Epoch 2494: train loss: 0.2023, test loss 1.7393\n",
      "Epoch 2495: train loss: 0.2023, test loss 1.7389\n",
      "Epoch 2496: train loss: 0.2023, test loss 1.7385\n",
      "Epoch 2497: train loss: 0.2023, test loss 1.7380\n",
      "Epoch 2498: train loss: 0.2023, test loss 1.7376\n",
      "Epoch 2499: train loss: 0.2022, test loss 1.7372\n",
      "Epoch 2500: train loss: 0.2022, test loss 1.7368\n",
      "Epoch 2501: train loss: 0.2022, test loss 1.7364\n",
      "Epoch 2502: train loss: 0.2022, test loss 1.7360\n",
      "Epoch 2503: train loss: 0.2022, test loss 1.7356\n",
      "Epoch 2504: train loss: 0.2022, test loss 1.7351\n",
      "Epoch 2505: train loss: 0.2022, test loss 1.7347\n",
      "Epoch 2506: train loss: 0.2022, test loss 1.7343\n",
      "Epoch 2507: train loss: 0.2022, test loss 1.7339\n",
      "Epoch 2508: train loss: 0.2022, test loss 1.7335\n",
      "Epoch 2509: train loss: 0.2021, test loss 1.7331\n",
      "Epoch 2510: train loss: 0.2021, test loss 1.7327\n",
      "Epoch 2511: train loss: 0.2021, test loss 1.7323\n",
      "Epoch 2512: train loss: 0.2021, test loss 1.7319\n",
      "Epoch 2513: train loss: 0.2021, test loss 1.7315\n",
      "Epoch 2514: train loss: 0.2021, test loss 1.7310\n",
      "Epoch 2515: train loss: 0.2021, test loss 1.7306\n",
      "Epoch 2516: train loss: 0.2021, test loss 1.7302\n",
      "Epoch 2517: train loss: 0.2021, test loss 1.7298\n",
      "Epoch 2518: train loss: 0.2021, test loss 1.7294\n",
      "Epoch 2519: train loss: 0.2020, test loss 1.7290\n",
      "Epoch 2520: train loss: 0.2020, test loss 1.7286\n",
      "Epoch 2521: train loss: 0.2020, test loss 1.7282\n",
      "Epoch 2522: train loss: 0.2020, test loss 1.7278\n",
      "Epoch 2523: train loss: 0.2020, test loss 1.7274\n",
      "Epoch 2524: train loss: 0.2020, test loss 1.7270\n",
      "Epoch 2525: train loss: 0.2020, test loss 1.7266\n",
      "Epoch 2526: train loss: 0.2020, test loss 1.7262\n",
      "Epoch 2527: train loss: 0.2020, test loss 1.7258\n",
      "Epoch 2528: train loss: 0.2020, test loss 1.7254\n",
      "Epoch 2529: train loss: 0.2019, test loss 1.7250\n",
      "Epoch 2530: train loss: 0.2019, test loss 1.7246\n",
      "Epoch 2531: train loss: 0.2019, test loss 1.7242\n",
      "Epoch 2532: train loss: 0.2019, test loss 1.7238\n",
      "Epoch 2533: train loss: 0.2019, test loss 1.7234\n",
      "Epoch 2534: train loss: 0.2019, test loss 1.7230\n",
      "Epoch 2535: train loss: 0.2019, test loss 1.7226\n",
      "Epoch 2536: train loss: 0.2019, test loss 1.7222\n",
      "Epoch 2537: train loss: 0.2019, test loss 1.7218\n",
      "Epoch 2538: train loss: 0.2019, test loss 1.7214\n",
      "Epoch 2539: train loss: 0.2018, test loss 1.7210\n",
      "Epoch 2540: train loss: 0.2018, test loss 1.7206\n",
      "Epoch 2541: train loss: 0.2018, test loss 1.7202\n",
      "Epoch 2542: train loss: 0.2018, test loss 1.7198\n",
      "Epoch 2543: train loss: 0.2018, test loss 1.7194\n",
      "Epoch 2544: train loss: 0.2018, test loss 1.7190\n",
      "Epoch 2545: train loss: 0.2018, test loss 1.7186\n",
      "Epoch 2546: train loss: 0.2018, test loss 1.7182\n",
      "Epoch 2547: train loss: 0.2018, test loss 1.7178\n",
      "Epoch 2548: train loss: 0.2018, test loss 1.7174\n",
      "Epoch 2549: train loss: 0.2017, test loss 1.7170\n",
      "Epoch 2550: train loss: 0.2017, test loss 1.7167\n",
      "Epoch 2551: train loss: 0.2017, test loss 1.7163\n",
      "Epoch 2552: train loss: 0.2017, test loss 1.7159\n",
      "Epoch 2553: train loss: 0.2017, test loss 1.7155\n",
      "Epoch 2554: train loss: 0.2017, test loss 1.7151\n",
      "Epoch 2555: train loss: 0.2017, test loss 1.7147\n",
      "Epoch 2556: train loss: 0.2017, test loss 1.7143\n",
      "Epoch 2557: train loss: 0.2017, test loss 1.7139\n",
      "Epoch 2558: train loss: 0.2017, test loss 1.7135\n",
      "Epoch 2559: train loss: 0.2017, test loss 1.7131\n",
      "Epoch 2560: train loss: 0.2016, test loss 1.7127\n",
      "Epoch 2561: train loss: 0.2016, test loss 1.7124\n",
      "Epoch 2562: train loss: 0.2016, test loss 1.7120\n",
      "Epoch 2563: train loss: 0.2016, test loss 1.7116\n",
      "Epoch 2564: train loss: 0.2016, test loss 1.7112\n",
      "Epoch 2565: train loss: 0.2016, test loss 1.7108\n",
      "Epoch 2566: train loss: 0.2016, test loss 1.7104\n",
      "Epoch 2567: train loss: 0.2016, test loss 1.7100\n",
      "Epoch 2568: train loss: 0.2016, test loss 1.7096\n",
      "Epoch 2569: train loss: 0.2016, test loss 1.7092\n",
      "Epoch 2570: train loss: 0.2015, test loss 1.7089\n",
      "Epoch 2571: train loss: 0.2015, test loss 1.7085\n",
      "Epoch 2572: train loss: 0.2015, test loss 1.7081\n",
      "Epoch 2573: train loss: 0.2015, test loss 1.7077\n",
      "Epoch 2574: train loss: 0.2015, test loss 1.7073\n",
      "Epoch 2575: train loss: 0.2015, test loss 1.7069\n",
      "Epoch 2576: train loss: 0.2015, test loss 1.7065\n",
      "Epoch 2577: train loss: 0.2015, test loss 1.7061\n",
      "Epoch 2578: train loss: 0.2015, test loss 1.7057\n",
      "Epoch 2579: train loss: 0.2015, test loss 1.7054\n",
      "Epoch 2580: train loss: 0.2014, test loss 1.7050\n",
      "Epoch 2581: train loss: 0.2014, test loss 1.7046\n",
      "Epoch 2582: train loss: 0.2014, test loss 1.7042\n",
      "Epoch 2583: train loss: 0.2014, test loss 1.7038\n",
      "Epoch 2584: train loss: 0.2014, test loss 1.7034\n",
      "Epoch 2585: train loss: 0.2014, test loss 1.7030\n",
      "Epoch 2586: train loss: 0.2014, test loss 1.7027\n",
      "Epoch 2587: train loss: 0.2014, test loss 1.7023\n",
      "Epoch 2588: train loss: 0.2014, test loss 1.7019\n",
      "Epoch 2589: train loss: 0.2014, test loss 1.7015\n",
      "Epoch 2590: train loss: 0.2013, test loss 1.7011\n",
      "Epoch 2591: train loss: 0.2013, test loss 1.7007\n",
      "Epoch 2592: train loss: 0.2013, test loss 1.7003\n",
      "Epoch 2593: train loss: 0.2013, test loss 1.7000\n",
      "Epoch 2594: train loss: 0.2013, test loss 1.6996\n",
      "Epoch 2595: train loss: 0.2013, test loss 1.6992\n",
      "Epoch 2596: train loss: 0.2013, test loss 1.6988\n",
      "Epoch 2597: train loss: 0.2013, test loss 1.6984\n",
      "Epoch 2598: train loss: 0.2013, test loss 1.6980\n",
      "Epoch 2599: train loss: 0.2013, test loss 1.6977\n",
      "Epoch 2600: train loss: 0.2013, test loss 1.6973\n",
      "Epoch 2601: train loss: 0.2012, test loss 1.6969\n",
      "Epoch 2602: train loss: 0.2012, test loss 1.6965\n",
      "Epoch 2603: train loss: 0.2012, test loss 1.6961\n",
      "Epoch 2604: train loss: 0.2012, test loss 1.6957\n",
      "Epoch 2605: train loss: 0.2012, test loss 1.6954\n",
      "Epoch 2606: train loss: 0.2012, test loss 1.6950\n",
      "Epoch 2607: train loss: 0.2012, test loss 1.6946\n",
      "Epoch 2608: train loss: 0.2012, test loss 1.6942\n",
      "Epoch 2609: train loss: 0.2012, test loss 1.6938\n",
      "Epoch 2610: train loss: 0.2012, test loss 1.6934\n",
      "Epoch 2611: train loss: 0.2011, test loss 1.6931\n",
      "Epoch 2612: train loss: 0.2011, test loss 1.6927\n",
      "Epoch 2613: train loss: 0.2011, test loss 1.6923\n",
      "Epoch 2614: train loss: 0.2011, test loss 1.6919\n",
      "Epoch 2615: train loss: 0.2011, test loss 1.6915\n",
      "Epoch 2616: train loss: 0.2011, test loss 1.6912\n",
      "Epoch 2617: train loss: 0.2011, test loss 1.6908\n",
      "Epoch 2618: train loss: 0.2011, test loss 1.6904\n",
      "Epoch 2619: train loss: 0.2011, test loss 1.6900\n",
      "Epoch 2620: train loss: 0.2011, test loss 1.6897\n",
      "Epoch 2621: train loss: 0.2011, test loss 1.6893\n",
      "Epoch 2622: train loss: 0.2010, test loss 1.6889\n",
      "Epoch 2623: train loss: 0.2010, test loss 1.6885\n",
      "Epoch 2624: train loss: 0.2010, test loss 1.6881\n",
      "Epoch 2625: train loss: 0.2010, test loss 1.6878\n",
      "Epoch 2626: train loss: 0.2010, test loss 1.6874\n",
      "Epoch 2627: train loss: 0.2010, test loss 1.6870\n",
      "Epoch 2628: train loss: 0.2010, test loss 1.6866\n",
      "Epoch 2629: train loss: 0.2010, test loss 1.6862\n",
      "Epoch 2630: train loss: 0.2010, test loss 1.6859\n",
      "Epoch 2631: train loss: 0.2010, test loss 1.6855\n",
      "Epoch 2632: train loss: 0.2009, test loss 1.6851\n",
      "Epoch 2633: train loss: 0.2009, test loss 1.6847\n",
      "Epoch 2634: train loss: 0.2009, test loss 1.6844\n",
      "Epoch 2635: train loss: 0.2009, test loss 1.6840\n",
      "Epoch 2636: train loss: 0.2009, test loss 1.6836\n",
      "Epoch 2637: train loss: 0.2009, test loss 1.6832\n",
      "Epoch 2638: train loss: 0.2009, test loss 1.6829\n",
      "Epoch 2639: train loss: 0.2009, test loss 1.6825\n",
      "Epoch 2640: train loss: 0.2009, test loss 1.6821\n",
      "Epoch 2641: train loss: 0.2009, test loss 1.6817\n",
      "Epoch 2642: train loss: 0.2009, test loss 1.6814\n",
      "Epoch 2643: train loss: 0.2008, test loss 1.6810\n",
      "Epoch 2644: train loss: 0.2008, test loss 1.6806\n",
      "Epoch 2645: train loss: 0.2008, test loss 1.6802\n",
      "Epoch 2646: train loss: 0.2008, test loss 1.6799\n",
      "Epoch 2647: train loss: 0.2008, test loss 1.6795\n",
      "Epoch 2648: train loss: 0.2008, test loss 1.6791\n",
      "Epoch 2649: train loss: 0.2008, test loss 1.6787\n",
      "Epoch 2650: train loss: 0.2008, test loss 1.6784\n",
      "Epoch 2651: train loss: 0.2008, test loss 1.6780\n",
      "Epoch 2652: train loss: 0.2008, test loss 1.6776\n",
      "Epoch 2653: train loss: 0.2007, test loss 1.6772\n",
      "Epoch 2654: train loss: 0.2007, test loss 1.6769\n",
      "Epoch 2655: train loss: 0.2007, test loss 1.6765\n",
      "Epoch 2656: train loss: 0.2007, test loss 1.6761\n",
      "Epoch 2657: train loss: 0.2007, test loss 1.6757\n",
      "Epoch 2658: train loss: 0.2007, test loss 1.6754\n",
      "Epoch 2659: train loss: 0.2007, test loss 1.6750\n",
      "Epoch 2660: train loss: 0.2007, test loss 1.6746\n",
      "Epoch 2661: train loss: 0.2007, test loss 1.6742\n",
      "Epoch 2662: train loss: 0.2007, test loss 1.6739\n",
      "Epoch 2663: train loss: 0.2007, test loss 1.6735\n",
      "Epoch 2664: train loss: 0.2006, test loss 1.6731\n",
      "Epoch 2665: train loss: 0.2006, test loss 1.6727\n",
      "Epoch 2666: train loss: 0.2006, test loss 1.6724\n",
      "Epoch 2667: train loss: 0.2006, test loss 1.6720\n",
      "Epoch 2668: train loss: 0.2006, test loss 1.6716\n",
      "Epoch 2669: train loss: 0.2006, test loss 1.6713\n",
      "Epoch 2670: train loss: 0.2006, test loss 1.6709\n",
      "Epoch 2671: train loss: 0.2006, test loss 1.6705\n",
      "Epoch 2672: train loss: 0.2006, test loss 1.6701\n",
      "Epoch 2673: train loss: 0.2006, test loss 1.6698\n",
      "Epoch 2674: train loss: 0.2006, test loss 1.6694\n",
      "Epoch 2675: train loss: 0.2005, test loss 1.6690\n",
      "Epoch 2676: train loss: 0.2005, test loss 1.6687\n",
      "Epoch 2677: train loss: 0.2005, test loss 1.6683\n",
      "Epoch 2678: train loss: 0.2005, test loss 1.6679\n",
      "Epoch 2679: train loss: 0.2005, test loss 1.6676\n",
      "Epoch 2680: train loss: 0.2005, test loss 1.6672\n",
      "Epoch 2681: train loss: 0.2005, test loss 1.6668\n",
      "Epoch 2682: train loss: 0.2005, test loss 1.6665\n",
      "Epoch 2683: train loss: 0.2005, test loss 1.6661\n",
      "Epoch 2684: train loss: 0.2005, test loss 1.6657\n",
      "Epoch 2685: train loss: 0.2005, test loss 1.6654\n",
      "Epoch 2686: train loss: 0.2004, test loss 1.6650\n",
      "Epoch 2687: train loss: 0.2004, test loss 1.6646\n",
      "Epoch 2688: train loss: 0.2004, test loss 1.6643\n",
      "Epoch 2689: train loss: 0.2004, test loss 1.6639\n",
      "Epoch 2690: train loss: 0.2004, test loss 1.6635\n",
      "Epoch 2691: train loss: 0.2004, test loss 1.6631\n",
      "Epoch 2692: train loss: 0.2004, test loss 1.6628\n",
      "Epoch 2693: train loss: 0.2004, test loss 1.6624\n",
      "Epoch 2694: train loss: 0.2004, test loss 1.6620\n",
      "Epoch 2695: train loss: 0.2004, test loss 1.6617\n",
      "Epoch 2696: train loss: 0.2004, test loss 1.6613\n",
      "Epoch 2697: train loss: 0.2003, test loss 1.6609\n",
      "Epoch 2698: train loss: 0.2003, test loss 1.6606\n",
      "Epoch 2699: train loss: 0.2003, test loss 1.6602\n",
      "Epoch 2700: train loss: 0.2003, test loss 1.6598\n",
      "Epoch 2701: train loss: 0.2003, test loss 1.6595\n",
      "Epoch 2702: train loss: 0.2003, test loss 1.6591\n",
      "Epoch 2703: train loss: 0.2003, test loss 1.6588\n",
      "Epoch 2704: train loss: 0.2003, test loss 1.6584\n",
      "Epoch 2705: train loss: 0.2003, test loss 1.6580\n",
      "Epoch 2706: train loss: 0.2003, test loss 1.6577\n",
      "Epoch 2707: train loss: 0.2003, test loss 1.6573\n",
      "Epoch 2708: train loss: 0.2002, test loss 1.6569\n",
      "Epoch 2709: train loss: 0.2002, test loss 1.6566\n",
      "Epoch 2710: train loss: 0.2002, test loss 1.6562\n",
      "Epoch 2711: train loss: 0.2002, test loss 1.6558\n",
      "Epoch 2712: train loss: 0.2002, test loss 1.6555\n",
      "Epoch 2713: train loss: 0.2002, test loss 1.6551\n",
      "Epoch 2714: train loss: 0.2002, test loss 1.6548\n",
      "Epoch 2715: train loss: 0.2002, test loss 1.6544\n",
      "Epoch 2716: train loss: 0.2002, test loss 1.6540\n",
      "Epoch 2717: train loss: 0.2002, test loss 1.6537\n",
      "Epoch 2718: train loss: 0.2002, test loss 1.6533\n",
      "Epoch 2719: train loss: 0.2001, test loss 1.6530\n",
      "Epoch 2720: train loss: 0.2001, test loss 1.6526\n",
      "Epoch 2721: train loss: 0.2001, test loss 1.6522\n",
      "Epoch 2722: train loss: 0.2001, test loss 1.6519\n",
      "Epoch 2723: train loss: 0.2001, test loss 1.6515\n",
      "Epoch 2724: train loss: 0.2001, test loss 1.6512\n",
      "Epoch 2725: train loss: 0.2001, test loss 1.6508\n",
      "Epoch 2726: train loss: 0.2001, test loss 1.6504\n",
      "Epoch 2727: train loss: 0.2001, test loss 1.6501\n",
      "Epoch 2728: train loss: 0.2001, test loss 1.6497\n",
      "Epoch 2729: train loss: 0.2001, test loss 1.6494\n",
      "Epoch 2730: train loss: 0.2000, test loss 1.6490\n",
      "Epoch 2731: train loss: 0.2000, test loss 1.6487\n",
      "Epoch 2732: train loss: 0.2000, test loss 1.6483\n",
      "Epoch 2733: train loss: 0.2000, test loss 1.6480\n",
      "Epoch 2734: train loss: 0.2000, test loss 1.6476\n",
      "Epoch 2735: train loss: 0.2000, test loss 1.6472\n",
      "Epoch 2736: train loss: 0.2000, test loss 1.6469\n",
      "Epoch 2737: train loss: 0.2000, test loss 1.6465\n",
      "Epoch 2738: train loss: 0.2000, test loss 1.6462\n",
      "Epoch 2739: train loss: 0.2000, test loss 1.6458\n",
      "Epoch 2740: train loss: 0.2000, test loss 1.6455\n",
      "Epoch 2741: train loss: 0.1999, test loss 1.6451\n",
      "Epoch 2742: train loss: 0.1999, test loss 1.6448\n",
      "Epoch 2743: train loss: 0.1999, test loss 1.6444\n",
      "Epoch 2744: train loss: 0.1999, test loss 1.6441\n",
      "Epoch 2745: train loss: 0.1999, test loss 1.6437\n",
      "Epoch 2746: train loss: 0.1999, test loss 1.6434\n",
      "Epoch 2747: train loss: 0.1999, test loss 1.6430\n",
      "Epoch 2748: train loss: 0.1999, test loss 1.6427\n",
      "Epoch 2749: train loss: 0.1999, test loss 1.6423\n",
      "Epoch 2750: train loss: 0.1999, test loss 1.6419\n",
      "Epoch 2751: train loss: 0.1999, test loss 1.6416\n",
      "Epoch 2752: train loss: 0.1999, test loss 1.6413\n",
      "Epoch 2753: train loss: 0.1998, test loss 1.6409\n",
      "Epoch 2754: train loss: 0.1998, test loss 1.6406\n",
      "Epoch 2755: train loss: 0.1998, test loss 1.6402\n",
      "Epoch 2756: train loss: 0.1998, test loss 1.6399\n",
      "Epoch 2757: train loss: 0.1998, test loss 1.6395\n",
      "Epoch 2758: train loss: 0.1998, test loss 1.6392\n",
      "Epoch 2759: train loss: 0.1998, test loss 1.6388\n",
      "Epoch 2760: train loss: 0.1998, test loss 1.6385\n",
      "Epoch 2761: train loss: 0.1998, test loss 1.6381\n",
      "Epoch 2762: train loss: 0.1998, test loss 1.6378\n",
      "Epoch 2763: train loss: 0.1998, test loss 1.6374\n",
      "Epoch 2764: train loss: 0.1997, test loss 1.6371\n",
      "Epoch 2765: train loss: 0.1997, test loss 1.6367\n",
      "Epoch 2766: train loss: 0.1997, test loss 1.6364\n",
      "Epoch 2767: train loss: 0.1997, test loss 1.6360\n",
      "Epoch 2768: train loss: 0.1997, test loss 1.6357\n",
      "Epoch 2769: train loss: 0.1997, test loss 1.6353\n",
      "Epoch 2770: train loss: 0.1997, test loss 1.6350\n",
      "Epoch 2771: train loss: 0.1997, test loss 1.6346\n",
      "Epoch 2772: train loss: 0.1997, test loss 1.6343\n",
      "Epoch 2773: train loss: 0.1997, test loss 1.6340\n",
      "Epoch 2774: train loss: 0.1997, test loss 1.6336\n",
      "Epoch 2775: train loss: 0.1997, test loss 1.6333\n",
      "Epoch 2776: train loss: 0.1996, test loss 1.6329\n",
      "Epoch 2777: train loss: 0.1996, test loss 1.6326\n",
      "Epoch 2778: train loss: 0.1996, test loss 1.6322\n",
      "Epoch 2779: train loss: 0.1996, test loss 1.6319\n",
      "Epoch 2780: train loss: 0.1996, test loss 1.6316\n",
      "Epoch 2781: train loss: 0.1996, test loss 1.6312\n",
      "Epoch 2782: train loss: 0.1996, test loss 1.6309\n",
      "Epoch 2783: train loss: 0.1996, test loss 1.6305\n",
      "Epoch 2784: train loss: 0.1996, test loss 1.6302\n",
      "Epoch 2785: train loss: 0.1996, test loss 1.6299\n",
      "Epoch 2786: train loss: 0.1996, test loss 1.6295\n",
      "Epoch 2787: train loss: 0.1996, test loss 1.6292\n",
      "Epoch 2788: train loss: 0.1995, test loss 1.6289\n",
      "Epoch 2789: train loss: 0.1995, test loss 1.6285\n",
      "Epoch 2790: train loss: 0.1995, test loss 1.6282\n",
      "Epoch 2791: train loss: 0.1995, test loss 1.6278\n",
      "Epoch 2792: train loss: 0.1995, test loss 1.6275\n",
      "Epoch 2793: train loss: 0.1995, test loss 1.6272\n",
      "Epoch 2794: train loss: 0.1995, test loss 1.6268\n",
      "Epoch 2795: train loss: 0.1995, test loss 1.6265\n",
      "Epoch 2796: train loss: 0.1995, test loss 1.6261\n",
      "Epoch 2797: train loss: 0.1995, test loss 1.6258\n",
      "Epoch 2798: train loss: 0.1995, test loss 1.6255\n",
      "Epoch 2799: train loss: 0.1994, test loss 1.6251\n",
      "Epoch 2800: train loss: 0.1994, test loss 1.6248\n",
      "Epoch 2801: train loss: 0.1994, test loss 1.6244\n",
      "Epoch 2802: train loss: 0.1994, test loss 1.6241\n",
      "Epoch 2803: train loss: 0.1994, test loss 1.6238\n",
      "Epoch 2804: train loss: 0.1994, test loss 1.6234\n",
      "Epoch 2805: train loss: 0.1994, test loss 1.6231\n",
      "Epoch 2806: train loss: 0.1994, test loss 1.6227\n",
      "Epoch 2807: train loss: 0.1994, test loss 1.6224\n",
      "Epoch 2808: train loss: 0.1994, test loss 1.6220\n",
      "Epoch 2809: train loss: 0.1994, test loss 1.6217\n",
      "Epoch 2810: train loss: 0.1994, test loss 1.6214\n",
      "Epoch 2811: train loss: 0.1993, test loss 1.6211\n",
      "Epoch 2812: train loss: 0.1993, test loss 1.6207\n",
      "Epoch 2813: train loss: 0.1993, test loss 1.6204\n",
      "Epoch 2814: train loss: 0.1993, test loss 1.6201\n",
      "Epoch 2815: train loss: 0.1993, test loss 1.6197\n",
      "Epoch 2816: train loss: 0.1993, test loss 1.6194\n",
      "Epoch 2817: train loss: 0.1993, test loss 1.6191\n",
      "Epoch 2818: train loss: 0.1993, test loss 1.6188\n",
      "Epoch 2819: train loss: 0.1993, test loss 1.6184\n",
      "Epoch 2820: train loss: 0.1993, test loss 1.6181\n",
      "Epoch 2821: train loss: 0.1993, test loss 1.6178\n",
      "Epoch 2822: train loss: 0.1992, test loss 1.6174\n",
      "Epoch 2823: train loss: 0.1992, test loss 1.6171\n",
      "Epoch 2824: train loss: 0.1992, test loss 1.6168\n",
      "Epoch 2825: train loss: 0.1992, test loss 1.6165\n",
      "Epoch 2826: train loss: 0.1992, test loss 1.6161\n",
      "Epoch 2827: train loss: 0.1992, test loss 1.6158\n",
      "Epoch 2828: train loss: 0.1992, test loss 1.6155\n",
      "Epoch 2829: train loss: 0.1992, test loss 1.6151\n",
      "Epoch 2830: train loss: 0.1992, test loss 1.6148\n",
      "Epoch 2831: train loss: 0.1992, test loss 1.6145\n",
      "Epoch 2832: train loss: 0.1992, test loss 1.6142\n",
      "Epoch 2833: train loss: 0.1992, test loss 1.6138\n",
      "Epoch 2834: train loss: 0.1991, test loss 1.6135\n",
      "Epoch 2835: train loss: 0.1991, test loss 1.6132\n",
      "Epoch 2836: train loss: 0.1991, test loss 1.6129\n",
      "Epoch 2837: train loss: 0.1991, test loss 1.6125\n",
      "Epoch 2838: train loss: 0.1991, test loss 1.6122\n",
      "Epoch 2839: train loss: 0.1991, test loss 1.6119\n",
      "Epoch 2840: train loss: 0.1991, test loss 1.6115\n",
      "Epoch 2841: train loss: 0.1991, test loss 1.6112\n",
      "Epoch 2842: train loss: 0.1991, test loss 1.6109\n",
      "Epoch 2843: train loss: 0.1991, test loss 1.6106\n",
      "Epoch 2844: train loss: 0.1991, test loss 1.6102\n",
      "Epoch 2845: train loss: 0.1991, test loss 1.6099\n",
      "Epoch 2846: train loss: 0.1990, test loss 1.6096\n",
      "Epoch 2847: train loss: 0.1990, test loss 1.6093\n",
      "Epoch 2848: train loss: 0.1990, test loss 1.6089\n",
      "Epoch 2849: train loss: 0.1990, test loss 1.6086\n",
      "Epoch 2850: train loss: 0.1990, test loss 1.6083\n",
      "Epoch 2851: train loss: 0.1990, test loss 1.6080\n",
      "Epoch 2852: train loss: 0.1990, test loss 1.6076\n",
      "Epoch 2853: train loss: 0.1990, test loss 1.6073\n",
      "Epoch 2854: train loss: 0.1990, test loss 1.6070\n",
      "Epoch 2855: train loss: 0.1990, test loss 1.6067\n",
      "Epoch 2856: train loss: 0.1990, test loss 1.6063\n",
      "Epoch 2857: train loss: 0.1990, test loss 1.6060\n",
      "Epoch 2858: train loss: 0.1989, test loss 1.6057\n",
      "Epoch 2859: train loss: 0.1989, test loss 1.6054\n",
      "Epoch 2860: train loss: 0.1989, test loss 1.6051\n",
      "Epoch 2861: train loss: 0.1989, test loss 1.6047\n",
      "Epoch 2862: train loss: 0.1989, test loss 1.6044\n",
      "Epoch 2863: train loss: 0.1989, test loss 1.6041\n",
      "Epoch 2864: train loss: 0.1989, test loss 1.6038\n",
      "Epoch 2865: train loss: 0.1989, test loss 1.6034\n",
      "Epoch 2866: train loss: 0.1989, test loss 1.6031\n",
      "Epoch 2867: train loss: 0.1989, test loss 1.6028\n",
      "Epoch 2868: train loss: 0.1989, test loss 1.6025\n",
      "Epoch 2869: train loss: 0.1989, test loss 1.6022\n",
      "Epoch 2870: train loss: 0.1988, test loss 1.6018\n",
      "Epoch 2871: train loss: 0.1988, test loss 1.6015\n",
      "Epoch 2872: train loss: 0.1988, test loss 1.6012\n",
      "Epoch 2873: train loss: 0.1988, test loss 1.6009\n",
      "Epoch 2874: train loss: 0.1988, test loss 1.6006\n",
      "Epoch 2875: train loss: 0.1988, test loss 1.6002\n",
      "Epoch 2876: train loss: 0.1988, test loss 1.5999\n",
      "Epoch 2877: train loss: 0.1988, test loss 1.5996\n",
      "Epoch 2878: train loss: 0.1988, test loss 1.5993\n",
      "Epoch 2879: train loss: 0.1988, test loss 1.5990\n",
      "Epoch 2880: train loss: 0.1988, test loss 1.5986\n",
      "Epoch 2881: train loss: 0.1988, test loss 1.5983\n",
      "Epoch 2882: train loss: 0.1987, test loss 1.5980\n",
      "Epoch 2883: train loss: 0.1987, test loss 1.5977\n",
      "Epoch 2884: train loss: 0.1987, test loss 1.5974\n",
      "Epoch 2885: train loss: 0.1987, test loss 1.5971\n",
      "Epoch 2886: train loss: 0.1987, test loss 1.5967\n",
      "Epoch 2887: train loss: 0.1987, test loss 1.5964\n",
      "Epoch 2888: train loss: 0.1987, test loss 1.5961\n",
      "Epoch 2889: train loss: 0.1987, test loss 1.5958\n",
      "Epoch 2890: train loss: 0.1987, test loss 1.5955\n",
      "Epoch 2891: train loss: 0.1987, test loss 1.5951\n",
      "Epoch 2892: train loss: 0.1987, test loss 1.5948\n",
      "Epoch 2893: train loss: 0.1987, test loss 1.5945\n",
      "Epoch 2894: train loss: 0.1986, test loss 1.5942\n",
      "Epoch 2895: train loss: 0.1986, test loss 1.5939\n",
      "Epoch 2896: train loss: 0.1986, test loss 1.5935\n",
      "Epoch 2897: train loss: 0.1986, test loss 1.5932\n",
      "Epoch 2898: train loss: 0.1986, test loss 1.5929\n",
      "Epoch 2899: train loss: 0.1986, test loss 1.5926\n",
      "Epoch 2900: train loss: 0.1986, test loss 1.5923\n",
      "Epoch 2901: train loss: 0.1986, test loss 1.5919\n",
      "Epoch 2902: train loss: 0.1986, test loss 1.5916\n",
      "Epoch 2903: train loss: 0.1986, test loss 1.5913\n",
      "Epoch 2904: train loss: 0.1986, test loss 1.5910\n",
      "Epoch 2905: train loss: 0.1986, test loss 1.5907\n",
      "Epoch 2906: train loss: 0.1985, test loss 1.5903\n",
      "Epoch 2907: train loss: 0.1985, test loss 1.5900\n",
      "Epoch 2908: train loss: 0.1985, test loss 1.5897\n",
      "Epoch 2909: train loss: 0.1985, test loss 1.5894\n",
      "Epoch 2910: train loss: 0.1985, test loss 1.5891\n",
      "Epoch 2911: train loss: 0.1985, test loss 1.5888\n",
      "Epoch 2912: train loss: 0.1985, test loss 1.5884\n",
      "Epoch 2913: train loss: 0.1985, test loss 1.5881\n",
      "Epoch 2914: train loss: 0.1985, test loss 1.5878\n",
      "Epoch 2915: train loss: 0.1985, test loss 1.5875\n",
      "Epoch 2916: train loss: 0.1985, test loss 1.5872\n",
      "Epoch 2917: train loss: 0.1985, test loss 1.5869\n",
      "Epoch 2918: train loss: 0.1985, test loss 1.5865\n",
      "Epoch 2919: train loss: 0.1984, test loss 1.5862\n",
      "Epoch 2920: train loss: 0.1984, test loss 1.5859\n",
      "Epoch 2921: train loss: 0.1984, test loss 1.5856\n",
      "Epoch 2922: train loss: 0.1984, test loss 1.5853\n",
      "Epoch 2923: train loss: 0.1984, test loss 1.5850\n",
      "Epoch 2924: train loss: 0.1984, test loss 1.5846\n",
      "Epoch 2925: train loss: 0.1984, test loss 1.5843\n",
      "Epoch 2926: train loss: 0.1984, test loss 1.5840\n",
      "Epoch 2927: train loss: 0.1984, test loss 1.5837\n",
      "Epoch 2928: train loss: 0.1984, test loss 1.5834\n",
      "Epoch 2929: train loss: 0.1984, test loss 1.5831\n",
      "Epoch 2930: train loss: 0.1984, test loss 1.5828\n",
      "Epoch 2931: train loss: 0.1984, test loss 1.5824\n",
      "Epoch 2932: train loss: 0.1983, test loss 1.5821\n",
      "Epoch 2933: train loss: 0.1983, test loss 1.5818\n",
      "Epoch 2934: train loss: 0.1983, test loss 1.5815\n",
      "Epoch 2935: train loss: 0.1983, test loss 1.5812\n",
      "Epoch 2936: train loss: 0.1983, test loss 1.5809\n",
      "Epoch 2937: train loss: 0.1983, test loss 1.5806\n",
      "Epoch 2938: train loss: 0.1983, test loss 1.5803\n",
      "Epoch 2939: train loss: 0.1983, test loss 1.5800\n",
      "Epoch 2940: train loss: 0.1983, test loss 1.5796\n",
      "Epoch 2941: train loss: 0.1983, test loss 1.5793\n",
      "Epoch 2942: train loss: 0.1983, test loss 1.5790\n",
      "Epoch 2943: train loss: 0.1983, test loss 1.5787\n",
      "Epoch 2944: train loss: 0.1982, test loss 1.5784\n",
      "Epoch 2945: train loss: 0.1982, test loss 1.5781\n",
      "Epoch 2946: train loss: 0.1982, test loss 1.5778\n",
      "Epoch 2947: train loss: 0.1982, test loss 1.5775\n",
      "Epoch 2948: train loss: 0.1982, test loss 1.5772\n",
      "Epoch 2949: train loss: 0.1982, test loss 1.5769\n",
      "Epoch 2950: train loss: 0.1982, test loss 1.5766\n",
      "Epoch 2951: train loss: 0.1982, test loss 1.5762\n",
      "Epoch 2952: train loss: 0.1982, test loss 1.5759\n",
      "Epoch 2953: train loss: 0.1982, test loss 1.5756\n",
      "Epoch 2954: train loss: 0.1982, test loss 1.5753\n",
      "Epoch 2955: train loss: 0.1982, test loss 1.5750\n",
      "Epoch 2956: train loss: 0.1982, test loss 1.5747\n",
      "Epoch 2957: train loss: 0.1981, test loss 1.5744\n",
      "Epoch 2958: train loss: 0.1981, test loss 1.5741\n",
      "Epoch 2959: train loss: 0.1981, test loss 1.5738\n",
      "Epoch 2960: train loss: 0.1981, test loss 1.5735\n",
      "Epoch 2961: train loss: 0.1981, test loss 1.5732\n",
      "Epoch 2962: train loss: 0.1981, test loss 1.5729\n",
      "Epoch 2963: train loss: 0.1981, test loss 1.5726\n",
      "Epoch 2964: train loss: 0.1981, test loss 1.5723\n",
      "Epoch 2965: train loss: 0.1981, test loss 1.5720\n",
      "Epoch 2966: train loss: 0.1981, test loss 1.5717\n",
      "Epoch 2967: train loss: 0.1981, test loss 1.5714\n",
      "Epoch 2968: train loss: 0.1981, test loss 1.5710\n",
      "Epoch 2969: train loss: 0.1981, test loss 1.5707\n",
      "Epoch 2970: train loss: 0.1980, test loss 1.5704\n",
      "Epoch 2971: train loss: 0.1980, test loss 1.5701\n",
      "Epoch 2972: train loss: 0.1980, test loss 1.5698\n",
      "Epoch 2973: train loss: 0.1980, test loss 1.5695\n",
      "Epoch 2974: train loss: 0.1980, test loss 1.5692\n",
      "Epoch 2975: train loss: 0.1980, test loss 1.5689\n",
      "Epoch 2976: train loss: 0.1980, test loss 1.5686\n",
      "Epoch 2977: train loss: 0.1980, test loss 1.5683\n",
      "Epoch 2978: train loss: 0.1980, test loss 1.5680\n",
      "Epoch 2979: train loss: 0.1980, test loss 1.5677\n",
      "Epoch 2980: train loss: 0.1980, test loss 1.5674\n",
      "Epoch 2981: train loss: 0.1980, test loss 1.5671\n",
      "Epoch 2982: train loss: 0.1980, test loss 1.5668\n",
      "Epoch 2983: train loss: 0.1979, test loss 1.5665\n",
      "Epoch 2984: train loss: 0.1979, test loss 1.5662\n",
      "Epoch 2985: train loss: 0.1979, test loss 1.5659\n",
      "Epoch 2986: train loss: 0.1979, test loss 1.5656\n",
      "Epoch 2987: train loss: 0.1979, test loss 1.5653\n",
      "Epoch 2988: train loss: 0.1979, test loss 1.5650\n",
      "Epoch 2989: train loss: 0.1979, test loss 1.5647\n",
      "Epoch 2990: train loss: 0.1979, test loss 1.5644\n",
      "Epoch 2991: train loss: 0.1979, test loss 1.5641\n",
      "Epoch 2992: train loss: 0.1979, test loss 1.5638\n",
      "Epoch 2993: train loss: 0.1979, test loss 1.5635\n",
      "Epoch 2994: train loss: 0.1979, test loss 1.5632\n",
      "Epoch 2995: train loss: 0.1979, test loss 1.5630\n",
      "Epoch 2996: train loss: 0.1978, test loss 1.5627\n",
      "Epoch 2997: train loss: 0.1978, test loss 1.5624\n",
      "Epoch 2998: train loss: 0.1978, test loss 1.5621\n",
      "Epoch 2999: train loss: 0.1978, test loss 1.5618\n",
      "Epoch 3000: train loss: 0.1978, test loss 1.5615\n",
      "Epoch 3001: train loss: 0.1978, test loss 1.5612\n",
      "Epoch 3002: train loss: 0.1978, test loss 1.5609\n",
      "Epoch 3003: train loss: 0.1978, test loss 1.5606\n",
      "Epoch 3004: train loss: 0.1978, test loss 1.5603\n",
      "Epoch 3005: train loss: 0.1978, test loss 1.5600\n",
      "Epoch 3006: train loss: 0.1978, test loss 1.5597\n",
      "Epoch 3007: train loss: 0.1978, test loss 1.5594\n",
      "Epoch 3008: train loss: 0.1978, test loss 1.5591\n",
      "Epoch 3009: train loss: 0.1978, test loss 1.5588\n",
      "Epoch 3010: train loss: 0.1977, test loss 1.5585\n",
      "Epoch 3011: train loss: 0.1977, test loss 1.5582\n",
      "Epoch 3012: train loss: 0.1977, test loss 1.5579\n",
      "Epoch 3013: train loss: 0.1977, test loss 1.5576\n",
      "Epoch 3014: train loss: 0.1977, test loss 1.5573\n",
      "Epoch 3015: train loss: 0.1977, test loss 1.5571\n",
      "Epoch 3016: train loss: 0.1977, test loss 1.5568\n",
      "Epoch 3017: train loss: 0.1977, test loss 1.5565\n",
      "Epoch 3018: train loss: 0.1977, test loss 1.5562\n",
      "Epoch 3019: train loss: 0.1977, test loss 1.5559\n",
      "Epoch 3020: train loss: 0.1977, test loss 1.5556\n",
      "Epoch 3021: train loss: 0.1977, test loss 1.5553\n",
      "Epoch 3022: train loss: 0.1977, test loss 1.5550\n",
      "Epoch 3023: train loss: 0.1976, test loss 1.5547\n",
      "Epoch 3024: train loss: 0.1976, test loss 1.5544\n",
      "Epoch 3025: train loss: 0.1976, test loss 1.5541\n",
      "Epoch 3026: train loss: 0.1976, test loss 1.5538\n",
      "Epoch 3027: train loss: 0.1976, test loss 1.5535\n",
      "Epoch 3028: train loss: 0.1976, test loss 1.5533\n",
      "Epoch 3029: train loss: 0.1976, test loss 1.5530\n",
      "Epoch 3030: train loss: 0.1976, test loss 1.5527\n",
      "Epoch 3031: train loss: 0.1976, test loss 1.5524\n",
      "Epoch 3032: train loss: 0.1976, test loss 1.5521\n",
      "Epoch 3033: train loss: 0.1976, test loss 1.5518\n",
      "Epoch 3034: train loss: 0.1976, test loss 1.5515\n",
      "Epoch 3035: train loss: 0.1976, test loss 1.5512\n",
      "Epoch 3036: train loss: 0.1976, test loss 1.5509\n",
      "Epoch 3037: train loss: 0.1975, test loss 1.5507\n",
      "Epoch 3038: train loss: 0.1975, test loss 1.5504\n",
      "Epoch 3039: train loss: 0.1975, test loss 1.5501\n",
      "Epoch 3040: train loss: 0.1975, test loss 1.5498\n",
      "Epoch 3041: train loss: 0.1975, test loss 1.5495\n",
      "Epoch 3042: train loss: 0.1975, test loss 1.5492\n",
      "Epoch 3043: train loss: 0.1975, test loss 1.5489\n",
      "Epoch 3044: train loss: 0.1975, test loss 1.5486\n",
      "Epoch 3045: train loss: 0.1975, test loss 1.5483\n",
      "Epoch 3046: train loss: 0.1975, test loss 1.5481\n",
      "Epoch 3047: train loss: 0.1975, test loss 1.5478\n",
      "Epoch 3048: train loss: 0.1975, test loss 1.5475\n",
      "Epoch 3049: train loss: 0.1975, test loss 1.5472\n",
      "Epoch 3050: train loss: 0.1974, test loss 1.5469\n",
      "Epoch 3051: train loss: 0.1974, test loss 1.5466\n",
      "Epoch 3052: train loss: 0.1974, test loss 1.5463\n",
      "Epoch 3053: train loss: 0.1974, test loss 1.5460\n",
      "Epoch 3054: train loss: 0.1974, test loss 1.5458\n",
      "Epoch 3055: train loss: 0.1974, test loss 1.5455\n",
      "Epoch 3056: train loss: 0.1974, test loss 1.5452\n",
      "Epoch 3057: train loss: 0.1974, test loss 1.5449\n",
      "Epoch 3058: train loss: 0.1974, test loss 1.5446\n",
      "Epoch 3059: train loss: 0.1974, test loss 1.5443\n",
      "Epoch 3060: train loss: 0.1974, test loss 1.5440\n",
      "Epoch 3061: train loss: 0.1974, test loss 1.5438\n",
      "Epoch 3062: train loss: 0.1974, test loss 1.5435\n",
      "Epoch 3063: train loss: 0.1974, test loss 1.5432\n",
      "Epoch 3064: train loss: 0.1973, test loss 1.5429\n",
      "Epoch 3065: train loss: 0.1973, test loss 1.5426\n",
      "Epoch 3066: train loss: 0.1973, test loss 1.5423\n",
      "Epoch 3067: train loss: 0.1973, test loss 1.5420\n",
      "Epoch 3068: train loss: 0.1973, test loss 1.5418\n",
      "Epoch 3069: train loss: 0.1973, test loss 1.5415\n",
      "Epoch 3070: train loss: 0.1973, test loss 1.5412\n",
      "Epoch 3071: train loss: 0.1973, test loss 1.5409\n",
      "Epoch 3072: train loss: 0.1973, test loss 1.5406\n",
      "Epoch 3073: train loss: 0.1973, test loss 1.5403\n",
      "Epoch 3074: train loss: 0.1973, test loss 1.5401\n",
      "Epoch 3075: train loss: 0.1973, test loss 1.5398\n",
      "Epoch 3076: train loss: 0.1973, test loss 1.5395\n",
      "Epoch 3077: train loss: 0.1973, test loss 1.5392\n",
      "Epoch 3078: train loss: 0.1972, test loss 1.5389\n",
      "Epoch 3079: train loss: 0.1972, test loss 1.5386\n",
      "Epoch 3080: train loss: 0.1972, test loss 1.5384\n",
      "Epoch 3081: train loss: 0.1972, test loss 1.5381\n",
      "Epoch 3082: train loss: 0.1972, test loss 1.5378\n",
      "Epoch 3083: train loss: 0.1972, test loss 1.5375\n",
      "Epoch 3084: train loss: 0.1972, test loss 1.5372\n",
      "Epoch 3085: train loss: 0.1972, test loss 1.5369\n",
      "Epoch 3086: train loss: 0.1972, test loss 1.5367\n",
      "Epoch 3087: train loss: 0.1972, test loss 1.5364\n",
      "Epoch 3088: train loss: 0.1972, test loss 1.5361\n",
      "Epoch 3089: train loss: 0.1972, test loss 1.5358\n",
      "Epoch 3090: train loss: 0.1972, test loss 1.5355\n",
      "Epoch 3091: train loss: 0.1972, test loss 1.5353\n",
      "Epoch 3092: train loss: 0.1971, test loss 1.5350\n",
      "Epoch 3093: train loss: 0.1971, test loss 1.5347\n",
      "Epoch 3094: train loss: 0.1971, test loss 1.5344\n",
      "Epoch 3095: train loss: 0.1971, test loss 1.5341\n",
      "Epoch 3096: train loss: 0.1971, test loss 1.5338\n",
      "Epoch 3097: train loss: 0.1971, test loss 1.5336\n",
      "Epoch 3098: train loss: 0.1971, test loss 1.5333\n",
      "Epoch 3099: train loss: 0.1971, test loss 1.5330\n",
      "Epoch 3100: train loss: 0.1971, test loss 1.5327\n",
      "Epoch 3101: train loss: 0.1971, test loss 1.5324\n",
      "Epoch 3102: train loss: 0.1971, test loss 1.5322\n",
      "Epoch 3103: train loss: 0.1971, test loss 1.5319\n",
      "Epoch 3104: train loss: 0.1971, test loss 1.5316\n",
      "Epoch 3105: train loss: 0.1971, test loss 1.5313\n",
      "Epoch 3106: train loss: 0.1970, test loss 1.5311\n",
      "Epoch 3107: train loss: 0.1970, test loss 1.5308\n",
      "Epoch 3108: train loss: 0.1970, test loss 1.5305\n",
      "Epoch 3109: train loss: 0.1970, test loss 1.5302\n",
      "Epoch 3110: train loss: 0.1970, test loss 1.5299\n",
      "Epoch 3111: train loss: 0.1970, test loss 1.5297\n",
      "Epoch 3112: train loss: 0.1970, test loss 1.5294\n",
      "Epoch 3113: train loss: 0.1970, test loss 1.5291\n",
      "Epoch 3114: train loss: 0.1970, test loss 1.5288\n",
      "Epoch 3115: train loss: 0.1970, test loss 1.5285\n",
      "Epoch 3116: train loss: 0.1970, test loss 1.5283\n",
      "Epoch 3117: train loss: 0.1970, test loss 1.5280\n",
      "Epoch 3118: train loss: 0.1970, test loss 1.5277\n",
      "Epoch 3119: train loss: 0.1970, test loss 1.5274\n",
      "Epoch 3120: train loss: 0.1970, test loss 1.5272\n",
      "Epoch 3121: train loss: 0.1969, test loss 1.5269\n",
      "Epoch 3122: train loss: 0.1969, test loss 1.5266\n",
      "Epoch 3123: train loss: 0.1969, test loss 1.5263\n",
      "Epoch 3124: train loss: 0.1969, test loss 1.5261\n",
      "Epoch 3125: train loss: 0.1969, test loss 1.5258\n",
      "Epoch 3126: train loss: 0.1969, test loss 1.5255\n",
      "Epoch 3127: train loss: 0.1969, test loss 1.5252\n",
      "Epoch 3128: train loss: 0.1969, test loss 1.5250\n",
      "Epoch 3129: train loss: 0.1969, test loss 1.5247\n",
      "Epoch 3130: train loss: 0.1969, test loss 1.5244\n",
      "Epoch 3131: train loss: 0.1969, test loss 1.5242\n",
      "Epoch 3132: train loss: 0.1969, test loss 1.5239\n",
      "Epoch 3133: train loss: 0.1969, test loss 1.5236\n",
      "Epoch 3134: train loss: 0.1969, test loss 1.5233\n",
      "Epoch 3135: train loss: 0.1968, test loss 1.5231\n",
      "Epoch 3136: train loss: 0.1968, test loss 1.5228\n",
      "Epoch 3137: train loss: 0.1968, test loss 1.5225\n",
      "Epoch 3138: train loss: 0.1968, test loss 1.5223\n",
      "Epoch 3139: train loss: 0.1968, test loss 1.5220\n",
      "Epoch 3140: train loss: 0.1968, test loss 1.5217\n",
      "Epoch 3141: train loss: 0.1968, test loss 1.5214\n",
      "Epoch 3142: train loss: 0.1968, test loss 1.5212\n",
      "Epoch 3143: train loss: 0.1968, test loss 1.5209\n",
      "Epoch 3144: train loss: 0.1968, test loss 1.5206\n",
      "Epoch 3145: train loss: 0.1968, test loss 1.5203\n",
      "Epoch 3146: train loss: 0.1968, test loss 1.5201\n",
      "Epoch 3147: train loss: 0.1968, test loss 1.5198\n",
      "Epoch 3148: train loss: 0.1968, test loss 1.5195\n",
      "Epoch 3149: train loss: 0.1968, test loss 1.5193\n",
      "Epoch 3150: train loss: 0.1967, test loss 1.5190\n",
      "Epoch 3151: train loss: 0.1967, test loss 1.5187\n",
      "Epoch 3152: train loss: 0.1967, test loss 1.5185\n",
      "Epoch 3153: train loss: 0.1967, test loss 1.5182\n",
      "Epoch 3154: train loss: 0.1967, test loss 1.5179\n",
      "Epoch 3155: train loss: 0.1967, test loss 1.5176\n",
      "Epoch 3156: train loss: 0.1967, test loss 1.5174\n",
      "Epoch 3157: train loss: 0.1967, test loss 1.5171\n",
      "Epoch 3158: train loss: 0.1967, test loss 1.5168\n",
      "Epoch 3159: train loss: 0.1967, test loss 1.5166\n",
      "Epoch 3160: train loss: 0.1967, test loss 1.5163\n",
      "Epoch 3161: train loss: 0.1967, test loss 1.5160\n",
      "Epoch 3162: train loss: 0.1967, test loss 1.5158\n",
      "Epoch 3163: train loss: 0.1967, test loss 1.5155\n",
      "Epoch 3164: train loss: 0.1967, test loss 1.5152\n",
      "Epoch 3165: train loss: 0.1966, test loss 1.5149\n",
      "Epoch 3166: train loss: 0.1966, test loss 1.5147\n",
      "Epoch 3167: train loss: 0.1966, test loss 1.5144\n",
      "Epoch 3168: train loss: 0.1966, test loss 1.5141\n",
      "Epoch 3169: train loss: 0.1966, test loss 1.5139\n",
      "Epoch 3170: train loss: 0.1966, test loss 1.5136\n",
      "Epoch 3171: train loss: 0.1966, test loss 1.5133\n",
      "Epoch 3172: train loss: 0.1966, test loss 1.5131\n",
      "Epoch 3173: train loss: 0.1966, test loss 1.5128\n",
      "Epoch 3174: train loss: 0.1966, test loss 1.5125\n",
      "Epoch 3175: train loss: 0.1966, test loss 1.5123\n",
      "Epoch 3176: train loss: 0.1966, test loss 1.5120\n",
      "Epoch 3177: train loss: 0.1966, test loss 1.5117\n",
      "Epoch 3178: train loss: 0.1966, test loss 1.5115\n",
      "Epoch 3179: train loss: 0.1966, test loss 1.5112\n",
      "Epoch 3180: train loss: 0.1965, test loss 1.5109\n",
      "Epoch 3181: train loss: 0.1965, test loss 1.5107\n",
      "Epoch 3182: train loss: 0.1965, test loss 1.5104\n",
      "Epoch 3183: train loss: 0.1965, test loss 1.5101\n",
      "Epoch 3184: train loss: 0.1965, test loss 1.5099\n",
      "Epoch 3185: train loss: 0.1965, test loss 1.5096\n",
      "Epoch 3186: train loss: 0.1965, test loss 1.5093\n",
      "Epoch 3187: train loss: 0.1965, test loss 1.5091\n",
      "Epoch 3188: train loss: 0.1965, test loss 1.5088\n",
      "Epoch 3189: train loss: 0.1965, test loss 1.5085\n",
      "Epoch 3190: train loss: 0.1965, test loss 1.5083\n",
      "Epoch 3191: train loss: 0.1965, test loss 1.5080\n",
      "Epoch 3192: train loss: 0.1965, test loss 1.5078\n",
      "Epoch 3193: train loss: 0.1965, test loss 1.5075\n",
      "Epoch 3194: train loss: 0.1965, test loss 1.5072\n",
      "Epoch 3195: train loss: 0.1964, test loss 1.5070\n",
      "Epoch 3196: train loss: 0.1964, test loss 1.5067\n",
      "Epoch 3197: train loss: 0.1964, test loss 1.5064\n",
      "Epoch 3198: train loss: 0.1964, test loss 1.5062\n",
      "Epoch 3199: train loss: 0.1964, test loss 1.5059\n",
      "Epoch 3200: train loss: 0.1964, test loss 1.5056\n",
      "Epoch 3201: train loss: 0.1964, test loss 1.5054\n",
      "Epoch 3202: train loss: 0.1964, test loss 1.5051\n",
      "Epoch 3203: train loss: 0.1964, test loss 1.5049\n",
      "Epoch 3204: train loss: 0.1964, test loss 1.5046\n",
      "Epoch 3205: train loss: 0.1964, test loss 1.5043\n",
      "Epoch 3206: train loss: 0.1964, test loss 1.5041\n",
      "Epoch 3207: train loss: 0.1964, test loss 1.5038\n",
      "Epoch 3208: train loss: 0.1964, test loss 1.5036\n",
      "Epoch 3209: train loss: 0.1964, test loss 1.5033\n",
      "Epoch 3210: train loss: 0.1963, test loss 1.5030\n",
      "Epoch 3211: train loss: 0.1963, test loss 1.5028\n",
      "Epoch 3212: train loss: 0.1963, test loss 1.5025\n",
      "Epoch 3213: train loss: 0.1963, test loss 1.5022\n",
      "Epoch 3214: train loss: 0.1963, test loss 1.5020\n",
      "Epoch 3215: train loss: 0.1963, test loss 1.5017\n",
      "Epoch 3216: train loss: 0.1963, test loss 1.5015\n",
      "Epoch 3217: train loss: 0.1963, test loss 1.5012\n",
      "Epoch 3218: train loss: 0.1963, test loss 1.5009\n",
      "Epoch 3219: train loss: 0.1963, test loss 1.5007\n",
      "Epoch 3220: train loss: 0.1963, test loss 1.5004\n",
      "Epoch 3221: train loss: 0.1963, test loss 1.5002\n",
      "Epoch 3222: train loss: 0.1963, test loss 1.4999\n",
      "Epoch 3223: train loss: 0.1963, test loss 1.4996\n",
      "Epoch 3224: train loss: 0.1963, test loss 1.4994\n",
      "Epoch 3225: train loss: 0.1962, test loss 1.4991\n",
      "Epoch 3226: train loss: 0.1962, test loss 1.4988\n",
      "Epoch 3227: train loss: 0.1962, test loss 1.4986\n",
      "Epoch 3228: train loss: 0.1962, test loss 1.4983\n",
      "Epoch 3229: train loss: 0.1962, test loss 1.4981\n",
      "Epoch 3230: train loss: 0.1962, test loss 1.4978\n",
      "Epoch 3231: train loss: 0.1962, test loss 1.4975\n",
      "Epoch 3232: train loss: 0.1962, test loss 1.4973\n",
      "Epoch 3233: train loss: 0.1962, test loss 1.4970\n",
      "Epoch 3234: train loss: 0.1962, test loss 1.4968\n",
      "Epoch 3235: train loss: 0.1962, test loss 1.4965\n",
      "Epoch 3236: train loss: 0.1962, test loss 1.4962\n",
      "Epoch 3237: train loss: 0.1962, test loss 1.4960\n",
      "Epoch 3238: train loss: 0.1962, test loss 1.4957\n",
      "Epoch 3239: train loss: 0.1962, test loss 1.4955\n",
      "Epoch 3240: train loss: 0.1962, test loss 1.4952\n",
      "Epoch 3241: train loss: 0.1961, test loss 1.4949\n",
      "Epoch 3242: train loss: 0.1961, test loss 1.4947\n",
      "Epoch 3243: train loss: 0.1961, test loss 1.4944\n",
      "Epoch 3244: train loss: 0.1961, test loss 1.4942\n",
      "Epoch 3245: train loss: 0.1961, test loss 1.4939\n",
      "Epoch 3246: train loss: 0.1961, test loss 1.4936\n",
      "Epoch 3247: train loss: 0.1961, test loss 1.4934\n",
      "Epoch 3248: train loss: 0.1961, test loss 1.4931\n",
      "Epoch 3249: train loss: 0.1961, test loss 1.4929\n",
      "Epoch 3250: train loss: 0.1961, test loss 1.4926\n",
      "Epoch 3251: train loss: 0.1961, test loss 1.4923\n",
      "Epoch 3252: train loss: 0.1961, test loss 1.4921\n",
      "Epoch 3253: train loss: 0.1961, test loss 1.4918\n",
      "Epoch 3254: train loss: 0.1961, test loss 1.4916\n",
      "Epoch 3255: train loss: 0.1961, test loss 1.4913\n",
      "Epoch 3256: train loss: 0.1960, test loss 1.4910\n",
      "Epoch 3257: train loss: 0.1960, test loss 1.4908\n",
      "Epoch 3258: train loss: 0.1960, test loss 1.4905\n",
      "Epoch 3259: train loss: 0.1960, test loss 1.4903\n",
      "Epoch 3260: train loss: 0.1960, test loss 1.4900\n",
      "Epoch 3261: train loss: 0.1960, test loss 1.4898\n",
      "Epoch 3262: train loss: 0.1960, test loss 1.4895\n",
      "Epoch 3263: train loss: 0.1960, test loss 1.4893\n",
      "Epoch 3264: train loss: 0.1960, test loss 1.4890\n",
      "Epoch 3265: train loss: 0.1960, test loss 1.4888\n",
      "Epoch 3266: train loss: 0.1960, test loss 1.4885\n",
      "Epoch 3267: train loss: 0.1960, test loss 1.4883\n",
      "Epoch 3268: train loss: 0.1960, test loss 1.4880\n",
      "Epoch 3269: train loss: 0.1960, test loss 1.4878\n",
      "Epoch 3270: train loss: 0.1960, test loss 1.4875\n",
      "Epoch 3271: train loss: 0.1960, test loss 1.4873\n",
      "Epoch 3272: train loss: 0.1959, test loss 1.4871\n",
      "Epoch 3273: train loss: 0.1959, test loss 1.4868\n",
      "Epoch 3274: train loss: 0.1959, test loss 1.4866\n",
      "Epoch 3275: train loss: 0.1959, test loss 1.4863\n",
      "Epoch 3276: train loss: 0.1959, test loss 1.4860\n",
      "Epoch 3277: train loss: 0.1959, test loss 1.4858\n",
      "Epoch 3278: train loss: 0.1959, test loss 1.4855\n",
      "Epoch 3279: train loss: 0.1959, test loss 1.4853\n",
      "Epoch 3280: train loss: 0.1959, test loss 1.4850\n",
      "Epoch 3281: train loss: 0.1959, test loss 1.4848\n",
      "Epoch 3282: train loss: 0.1959, test loss 1.4845\n",
      "Epoch 3283: train loss: 0.1959, test loss 1.4843\n",
      "Epoch 3284: train loss: 0.1959, test loss 1.4840\n",
      "Epoch 3285: train loss: 0.1959, test loss 1.4838\n",
      "Epoch 3286: train loss: 0.1959, test loss 1.4835\n",
      "Epoch 3287: train loss: 0.1959, test loss 1.4833\n",
      "Epoch 3288: train loss: 0.1958, test loss 1.4830\n",
      "Epoch 3289: train loss: 0.1958, test loss 1.4828\n",
      "Epoch 3290: train loss: 0.1958, test loss 1.4825\n",
      "Epoch 3291: train loss: 0.1958, test loss 1.4823\n",
      "Epoch 3292: train loss: 0.1958, test loss 1.4820\n",
      "Epoch 3293: train loss: 0.1958, test loss 1.4818\n",
      "Epoch 3294: train loss: 0.1958, test loss 1.4815\n",
      "Epoch 3295: train loss: 0.1958, test loss 1.4813\n",
      "Epoch 3296: train loss: 0.1958, test loss 1.4810\n",
      "Epoch 3297: train loss: 0.1958, test loss 1.4808\n",
      "Epoch 3298: train loss: 0.1958, test loss 1.4805\n",
      "Epoch 3299: train loss: 0.1958, test loss 1.4803\n",
      "Epoch 3300: train loss: 0.1958, test loss 1.4800\n",
      "Epoch 3301: train loss: 0.1958, test loss 1.4798\n",
      "Epoch 3302: train loss: 0.1958, test loss 1.4796\n",
      "Epoch 3303: train loss: 0.1958, test loss 1.4793\n",
      "Epoch 3304: train loss: 0.1957, test loss 1.4791\n",
      "Epoch 3305: train loss: 0.1957, test loss 1.4788\n",
      "Epoch 3306: train loss: 0.1957, test loss 1.4786\n",
      "Epoch 3307: train loss: 0.1957, test loss 1.4783\n",
      "Epoch 3308: train loss: 0.1957, test loss 1.4781\n",
      "Epoch 3309: train loss: 0.1957, test loss 1.4778\n",
      "Epoch 3310: train loss: 0.1957, test loss 1.4776\n",
      "Epoch 3311: train loss: 0.1957, test loss 1.4773\n",
      "Epoch 3312: train loss: 0.1957, test loss 1.4771\n",
      "Epoch 3313: train loss: 0.1957, test loss 1.4768\n",
      "Epoch 3314: train loss: 0.1957, test loss 1.4766\n",
      "Epoch 3315: train loss: 0.1957, test loss 1.4763\n",
      "Epoch 3316: train loss: 0.1957, test loss 1.4761\n",
      "Epoch 3317: train loss: 0.1957, test loss 1.4759\n",
      "Epoch 3318: train loss: 0.1957, test loss 1.4756\n",
      "Epoch 3319: train loss: 0.1957, test loss 1.4754\n",
      "Epoch 3320: train loss: 0.1956, test loss 1.4751\n",
      "Epoch 3321: train loss: 0.1956, test loss 1.4749\n",
      "Epoch 3322: train loss: 0.1956, test loss 1.4746\n",
      "Epoch 3323: train loss: 0.1956, test loss 1.4744\n",
      "Epoch 3324: train loss: 0.1956, test loss 1.4741\n",
      "Epoch 3325: train loss: 0.1956, test loss 1.4739\n",
      "Epoch 3326: train loss: 0.1956, test loss 1.4736\n",
      "Epoch 3327: train loss: 0.1956, test loss 1.4734\n",
      "Epoch 3328: train loss: 0.1956, test loss 1.4732\n",
      "Epoch 3329: train loss: 0.1956, test loss 1.4729\n",
      "Epoch 3330: train loss: 0.1956, test loss 1.4727\n",
      "Epoch 3331: train loss: 0.1956, test loss 1.4724\n",
      "Epoch 3332: train loss: 0.1956, test loss 1.4722\n",
      "Epoch 3333: train loss: 0.1956, test loss 1.4719\n",
      "Epoch 3334: train loss: 0.1956, test loss 1.4717\n",
      "Epoch 3335: train loss: 0.1956, test loss 1.4715\n",
      "Epoch 3336: train loss: 0.1955, test loss 1.4712\n",
      "Epoch 3337: train loss: 0.1955, test loss 1.4710\n",
      "Epoch 3338: train loss: 0.1955, test loss 1.4707\n",
      "Epoch 3339: train loss: 0.1955, test loss 1.4705\n",
      "Epoch 3340: train loss: 0.1955, test loss 1.4702\n",
      "Epoch 3341: train loss: 0.1955, test loss 1.4700\n",
      "Epoch 3342: train loss: 0.1955, test loss 1.4698\n",
      "Epoch 3343: train loss: 0.1955, test loss 1.4695\n",
      "Epoch 3344: train loss: 0.1955, test loss 1.4693\n",
      "Epoch 3345: train loss: 0.1955, test loss 1.4690\n",
      "Epoch 3346: train loss: 0.1955, test loss 1.4688\n",
      "Epoch 3347: train loss: 0.1955, test loss 1.4685\n",
      "Epoch 3348: train loss: 0.1955, test loss 1.4683\n",
      "Epoch 3349: train loss: 0.1955, test loss 1.4681\n",
      "Epoch 3350: train loss: 0.1955, test loss 1.4678\n",
      "Epoch 3351: train loss: 0.1955, test loss 1.4676\n",
      "Epoch 3352: train loss: 0.1954, test loss 1.4673\n",
      "Epoch 3353: train loss: 0.1954, test loss 1.4671\n",
      "Epoch 3354: train loss: 0.1954, test loss 1.4668\n",
      "Epoch 3355: train loss: 0.1954, test loss 1.4666\n",
      "Epoch 3356: train loss: 0.1954, test loss 1.4664\n",
      "Epoch 3357: train loss: 0.1954, test loss 1.4661\n",
      "Epoch 3358: train loss: 0.1954, test loss 1.4659\n",
      "Epoch 3359: train loss: 0.1954, test loss 1.4657\n",
      "Epoch 3360: train loss: 0.1954, test loss 1.4654\n",
      "Epoch 3361: train loss: 0.1954, test loss 1.4652\n",
      "Epoch 3362: train loss: 0.1954, test loss 1.4650\n",
      "Epoch 3363: train loss: 0.1954, test loss 1.4647\n",
      "Epoch 3364: train loss: 0.1954, test loss 1.4645\n",
      "Epoch 3365: train loss: 0.1954, test loss 1.4642\n",
      "Epoch 3366: train loss: 0.1954, test loss 1.4640\n",
      "Epoch 3367: train loss: 0.1954, test loss 1.4638\n",
      "Epoch 3368: train loss: 0.1954, test loss 1.4635\n",
      "Epoch 3369: train loss: 0.1953, test loss 1.4633\n",
      "Epoch 3370: train loss: 0.1953, test loss 1.4631\n",
      "Epoch 3371: train loss: 0.1953, test loss 1.4628\n",
      "Epoch 3372: train loss: 0.1953, test loss 1.4626\n",
      "Epoch 3373: train loss: 0.1953, test loss 1.4624\n",
      "Epoch 3374: train loss: 0.1953, test loss 1.4621\n",
      "Epoch 3375: train loss: 0.1953, test loss 1.4619\n",
      "Epoch 3376: train loss: 0.1953, test loss 1.4616\n",
      "Epoch 3377: train loss: 0.1953, test loss 1.4614\n",
      "Epoch 3378: train loss: 0.1953, test loss 1.4611\n",
      "Epoch 3379: train loss: 0.1953, test loss 1.4609\n",
      "Epoch 3380: train loss: 0.1953, test loss 1.4607\n",
      "Epoch 3381: train loss: 0.1953, test loss 1.4604\n",
      "Epoch 3382: train loss: 0.1953, test loss 1.4602\n",
      "Epoch 3383: train loss: 0.1953, test loss 1.4599\n",
      "Epoch 3384: train loss: 0.1953, test loss 1.4597\n",
      "Epoch 3385: train loss: 0.1953, test loss 1.4594\n",
      "Epoch 3386: train loss: 0.1952, test loss 1.4592\n",
      "Epoch 3387: train loss: 0.1952, test loss 1.4589\n",
      "Epoch 3388: train loss: 0.1952, test loss 1.4587\n",
      "Epoch 3389: train loss: 0.1952, test loss 1.4585\n",
      "Epoch 3390: train loss: 0.1952, test loss 1.4582\n",
      "Epoch 3391: train loss: 0.1952, test loss 1.4580\n",
      "Epoch 3392: train loss: 0.1952, test loss 1.4577\n",
      "Epoch 3393: train loss: 0.1952, test loss 1.4575\n",
      "Epoch 3394: train loss: 0.1952, test loss 1.4572\n",
      "Epoch 3395: train loss: 0.1952, test loss 1.4570\n",
      "Epoch 3396: train loss: 0.1952, test loss 1.4567\n",
      "Epoch 3397: train loss: 0.1952, test loss 1.4565\n",
      "Epoch 3398: train loss: 0.1952, test loss 1.4563\n",
      "Epoch 3399: train loss: 0.1952, test loss 1.4560\n",
      "Epoch 3400: train loss: 0.1952, test loss 1.4558\n",
      "Epoch 3401: train loss: 0.1952, test loss 1.4555\n",
      "Epoch 3402: train loss: 0.1951, test loss 1.4553\n",
      "Epoch 3403: train loss: 0.1951, test loss 1.4550\n",
      "Epoch 3404: train loss: 0.1951, test loss 1.4548\n",
      "Epoch 3405: train loss: 0.1951, test loss 1.4545\n",
      "Epoch 3406: train loss: 0.1951, test loss 1.4543\n",
      "Epoch 3407: train loss: 0.1951, test loss 1.4541\n",
      "Epoch 3408: train loss: 0.1951, test loss 1.4538\n",
      "Epoch 3409: train loss: 0.1951, test loss 1.4536\n",
      "Epoch 3410: train loss: 0.1951, test loss 1.4533\n",
      "Epoch 3411: train loss: 0.1951, test loss 1.4531\n",
      "Epoch 3412: train loss: 0.1951, test loss 1.4529\n",
      "Epoch 3413: train loss: 0.1951, test loss 1.4526\n",
      "Epoch 3414: train loss: 0.1951, test loss 1.4524\n",
      "Epoch 3415: train loss: 0.1951, test loss 1.4521\n",
      "Epoch 3416: train loss: 0.1951, test loss 1.4519\n",
      "Epoch 3417: train loss: 0.1951, test loss 1.4516\n",
      "Epoch 3418: train loss: 0.1950, test loss 1.4514\n",
      "Epoch 3419: train loss: 0.1950, test loss 1.4512\n",
      "Epoch 3420: train loss: 0.1950, test loss 1.4509\n",
      "Epoch 3421: train loss: 0.1950, test loss 1.4507\n",
      "Epoch 3422: train loss: 0.1950, test loss 1.4504\n",
      "Epoch 3423: train loss: 0.1950, test loss 1.4502\n",
      "Epoch 3424: train loss: 0.1950, test loss 1.4500\n",
      "Epoch 3425: train loss: 0.1950, test loss 1.4497\n",
      "Epoch 3426: train loss: 0.1950, test loss 1.4495\n",
      "Epoch 3427: train loss: 0.1950, test loss 1.4493\n",
      "Epoch 3428: train loss: 0.1950, test loss 1.4490\n",
      "Epoch 3429: train loss: 0.1950, test loss 1.4488\n",
      "Epoch 3430: train loss: 0.1950, test loss 1.4485\n",
      "Epoch 3431: train loss: 0.1950, test loss 1.4483\n",
      "Epoch 3432: train loss: 0.1950, test loss 1.4481\n",
      "Epoch 3433: train loss: 0.1950, test loss 1.4478\n",
      "Epoch 3434: train loss: 0.1950, test loss 1.4476\n",
      "Epoch 3435: train loss: 0.1949, test loss 1.4474\n",
      "Epoch 3436: train loss: 0.1949, test loss 1.4471\n",
      "Epoch 3437: train loss: 0.1949, test loss 1.4469\n",
      "Epoch 3438: train loss: 0.1949, test loss 1.4467\n",
      "Epoch 3439: train loss: 0.1949, test loss 1.4464\n",
      "Epoch 3440: train loss: 0.1949, test loss 1.4462\n",
      "Epoch 3441: train loss: 0.1949, test loss 1.4459\n",
      "Epoch 3442: train loss: 0.1949, test loss 1.4457\n",
      "Epoch 3443: train loss: 0.1949, test loss 1.4455\n",
      "Epoch 3444: train loss: 0.1949, test loss 1.4452\n",
      "Epoch 3445: train loss: 0.1949, test loss 1.4450\n",
      "Epoch 3446: train loss: 0.1949, test loss 1.4448\n",
      "Epoch 3447: train loss: 0.1949, test loss 1.4445\n",
      "Epoch 3448: train loss: 0.1949, test loss 1.4443\n",
      "Epoch 3449: train loss: 0.1949, test loss 1.4441\n",
      "Epoch 3450: train loss: 0.1949, test loss 1.4438\n",
      "Epoch 3451: train loss: 0.1949, test loss 1.4436\n",
      "Epoch 3452: train loss: 0.1948, test loss 1.4434\n",
      "Epoch 3453: train loss: 0.1948, test loss 1.4431\n",
      "Epoch 3454: train loss: 0.1948, test loss 1.4429\n",
      "Epoch 3455: train loss: 0.1948, test loss 1.4427\n",
      "Epoch 3456: train loss: 0.1948, test loss 1.4424\n",
      "Epoch 3457: train loss: 0.1948, test loss 1.4422\n",
      "Epoch 3458: train loss: 0.1948, test loss 1.4420\n",
      "Epoch 3459: train loss: 0.1948, test loss 1.4417\n",
      "Epoch 3460: train loss: 0.1948, test loss 1.4415\n",
      "Epoch 3461: train loss: 0.1948, test loss 1.4413\n",
      "Epoch 3462: train loss: 0.1948, test loss 1.4410\n",
      "Epoch 3463: train loss: 0.1948, test loss 1.4408\n",
      "Epoch 3464: train loss: 0.1948, test loss 1.4406\n",
      "Epoch 3465: train loss: 0.1948, test loss 1.4403\n",
      "Epoch 3466: train loss: 0.1948, test loss 1.4401\n",
      "Epoch 3467: train loss: 0.1948, test loss 1.4399\n",
      "Epoch 3468: train loss: 0.1948, test loss 1.4397\n",
      "Epoch 3469: train loss: 0.1947, test loss 1.4394\n",
      "Epoch 3470: train loss: 0.1947, test loss 1.4392\n",
      "Epoch 3471: train loss: 0.1947, test loss 1.4390\n",
      "Epoch 3472: train loss: 0.1947, test loss 1.4387\n",
      "Epoch 3473: train loss: 0.1947, test loss 1.4385\n",
      "Epoch 3474: train loss: 0.1947, test loss 1.4383\n",
      "Epoch 3475: train loss: 0.1947, test loss 1.4380\n",
      "Epoch 3476: train loss: 0.1947, test loss 1.4378\n",
      "Epoch 3477: train loss: 0.1947, test loss 1.4376\n",
      "Epoch 3478: train loss: 0.1947, test loss 1.4374\n",
      "Epoch 3479: train loss: 0.1947, test loss 1.4371\n",
      "Epoch 3480: train loss: 0.1947, test loss 1.4369\n",
      "Epoch 3481: train loss: 0.1947, test loss 1.4367\n",
      "Epoch 3482: train loss: 0.1947, test loss 1.4364\n",
      "Epoch 3483: train loss: 0.1947, test loss 1.4362\n",
      "Epoch 3484: train loss: 0.1947, test loss 1.4360\n",
      "Epoch 3485: train loss: 0.1947, test loss 1.4358\n",
      "Epoch 3486: train loss: 0.1946, test loss 1.4355\n",
      "Epoch 3487: train loss: 0.1946, test loss 1.4353\n",
      "Epoch 3488: train loss: 0.1946, test loss 1.4351\n",
      "Epoch 3489: train loss: 0.1946, test loss 1.4348\n",
      "Epoch 3490: train loss: 0.1946, test loss 1.4346\n",
      "Epoch 3491: train loss: 0.1946, test loss 1.4344\n",
      "Epoch 3492: train loss: 0.1946, test loss 1.4342\n",
      "Epoch 3493: train loss: 0.1946, test loss 1.4339\n",
      "Epoch 3494: train loss: 0.1946, test loss 1.4337\n",
      "Epoch 3495: train loss: 0.1946, test loss 1.4335\n",
      "Epoch 3496: train loss: 0.1946, test loss 1.4333\n",
      "Epoch 3497: train loss: 0.1946, test loss 1.4330\n",
      "Epoch 3498: train loss: 0.1946, test loss 1.4328\n",
      "Epoch 3499: train loss: 0.1946, test loss 1.4326\n",
      "Epoch 3500: train loss: 0.1946, test loss 1.4323\n",
      "Epoch 3501: train loss: 0.1946, test loss 1.4321\n",
      "Epoch 3502: train loss: 0.1946, test loss 1.4319\n",
      "Epoch 3503: train loss: 0.1945, test loss 1.4317\n",
      "Epoch 3504: train loss: 0.1945, test loss 1.4314\n",
      "Epoch 3505: train loss: 0.1945, test loss 1.4312\n",
      "Epoch 3506: train loss: 0.1945, test loss 1.4310\n",
      "Epoch 3507: train loss: 0.1945, test loss 1.4308\n",
      "Epoch 3508: train loss: 0.1945, test loss 1.4305\n",
      "Epoch 3509: train loss: 0.1945, test loss 1.4303\n",
      "Epoch 3510: train loss: 0.1945, test loss 1.4301\n",
      "Epoch 3511: train loss: 0.1945, test loss 1.4299\n",
      "Epoch 3512: train loss: 0.1945, test loss 1.4296\n",
      "Epoch 3513: train loss: 0.1945, test loss 1.4294\n",
      "Epoch 3514: train loss: 0.1945, test loss 1.4292\n",
      "Epoch 3515: train loss: 0.1945, test loss 1.4289\n",
      "Epoch 3516: train loss: 0.1945, test loss 1.4287\n",
      "Epoch 3517: train loss: 0.1945, test loss 1.4285\n",
      "Epoch 3518: train loss: 0.1945, test loss 1.4283\n",
      "Epoch 3519: train loss: 0.1945, test loss 1.4280\n",
      "Epoch 3520: train loss: 0.1944, test loss 1.4278\n",
      "Epoch 3521: train loss: 0.1944, test loss 1.4276\n",
      "Epoch 3522: train loss: 0.1944, test loss 1.4274\n",
      "Epoch 3523: train loss: 0.1944, test loss 1.4271\n",
      "Epoch 3524: train loss: 0.1944, test loss 1.4269\n",
      "Epoch 3525: train loss: 0.1944, test loss 1.4267\n",
      "Epoch 3526: train loss: 0.1944, test loss 1.4265\n",
      "Epoch 3527: train loss: 0.1944, test loss 1.4262\n",
      "Epoch 3528: train loss: 0.1944, test loss 1.4260\n",
      "Epoch 3529: train loss: 0.1944, test loss 1.4258\n",
      "Epoch 3530: train loss: 0.1944, test loss 1.4255\n",
      "Epoch 3531: train loss: 0.1944, test loss 1.4253\n",
      "Epoch 3532: train loss: 0.1944, test loss 1.4251\n",
      "Epoch 3533: train loss: 0.1944, test loss 1.4249\n",
      "Epoch 3534: train loss: 0.1944, test loss 1.4246\n",
      "Epoch 3535: train loss: 0.1944, test loss 1.4244\n",
      "Epoch 3536: train loss: 0.1944, test loss 1.4242\n",
      "Epoch 3537: train loss: 0.1944, test loss 1.4240\n",
      "Epoch 3538: train loss: 0.1943, test loss 1.4237\n",
      "Epoch 3539: train loss: 0.1943, test loss 1.4235\n",
      "Epoch 3540: train loss: 0.1943, test loss 1.4233\n",
      "Epoch 3541: train loss: 0.1943, test loss 1.4231\n",
      "Epoch 3542: train loss: 0.1943, test loss 1.4228\n",
      "Epoch 3543: train loss: 0.1943, test loss 1.4226\n",
      "Epoch 3544: train loss: 0.1943, test loss 1.4224\n",
      "Epoch 3545: train loss: 0.1943, test loss 1.4222\n",
      "Epoch 3546: train loss: 0.1943, test loss 1.4220\n",
      "Epoch 3547: train loss: 0.1943, test loss 1.4217\n",
      "Epoch 3548: train loss: 0.1943, test loss 1.4215\n",
      "Epoch 3549: train loss: 0.1943, test loss 1.4213\n",
      "Epoch 3550: train loss: 0.1943, test loss 1.4211\n",
      "Epoch 3551: train loss: 0.1943, test loss 1.4208\n",
      "Epoch 3552: train loss: 0.1943, test loss 1.4206\n",
      "Epoch 3553: train loss: 0.1943, test loss 1.4204\n",
      "Epoch 3554: train loss: 0.1943, test loss 1.4202\n",
      "Epoch 3555: train loss: 0.1942, test loss 1.4199\n",
      "Epoch 3556: train loss: 0.1942, test loss 1.4197\n",
      "Epoch 3557: train loss: 0.1942, test loss 1.4195\n",
      "Epoch 3558: train loss: 0.1942, test loss 1.4193\n",
      "Epoch 3559: train loss: 0.1942, test loss 1.4191\n",
      "Epoch 3560: train loss: 0.1942, test loss 1.4188\n",
      "Epoch 3561: train loss: 0.1942, test loss 1.4186\n",
      "Epoch 3562: train loss: 0.1942, test loss 1.4184\n",
      "Epoch 3563: train loss: 0.1942, test loss 1.4182\n",
      "Epoch 3564: train loss: 0.1942, test loss 1.4179\n",
      "Epoch 3565: train loss: 0.1942, test loss 1.4177\n",
      "Epoch 3566: train loss: 0.1942, test loss 1.4175\n",
      "Epoch 3567: train loss: 0.1942, test loss 1.4173\n",
      "Epoch 3568: train loss: 0.1942, test loss 1.4171\n",
      "Epoch 3569: train loss: 0.1942, test loss 1.4168\n",
      "Epoch 3570: train loss: 0.1942, test loss 1.4166\n",
      "Epoch 3571: train loss: 0.1942, test loss 1.4164\n",
      "Epoch 3572: train loss: 0.1942, test loss 1.4162\n",
      "Epoch 3573: train loss: 0.1941, test loss 1.4160\n",
      "Epoch 3574: train loss: 0.1941, test loss 1.4157\n",
      "Epoch 3575: train loss: 0.1941, test loss 1.4155\n",
      "Epoch 3576: train loss: 0.1941, test loss 1.4153\n",
      "Epoch 3577: train loss: 0.1941, test loss 1.4151\n",
      "Epoch 3578: train loss: 0.1941, test loss 1.4149\n",
      "Epoch 3579: train loss: 0.1941, test loss 1.4146\n",
      "Epoch 3580: train loss: 0.1941, test loss 1.4144\n",
      "Epoch 3581: train loss: 0.1941, test loss 1.4142\n",
      "Epoch 3582: train loss: 0.1941, test loss 1.4140\n",
      "Epoch 3583: train loss: 0.1941, test loss 1.4138\n",
      "Epoch 3584: train loss: 0.1941, test loss 1.4135\n",
      "Epoch 3585: train loss: 0.1941, test loss 1.4133\n",
      "Epoch 3586: train loss: 0.1941, test loss 1.4131\n",
      "Epoch 3587: train loss: 0.1941, test loss 1.4129\n",
      "Epoch 3588: train loss: 0.1941, test loss 1.4127\n",
      "Epoch 3589: train loss: 0.1941, test loss 1.4124\n",
      "Epoch 3590: train loss: 0.1941, test loss 1.4122\n",
      "Epoch 3591: train loss: 0.1940, test loss 1.4120\n",
      "Epoch 3592: train loss: 0.1940, test loss 1.4118\n",
      "Epoch 3593: train loss: 0.1940, test loss 1.4116\n",
      "Epoch 3594: train loss: 0.1940, test loss 1.4113\n",
      "Epoch 3595: train loss: 0.1940, test loss 1.4111\n",
      "Epoch 3596: train loss: 0.1940, test loss 1.4109\n",
      "Epoch 3597: train loss: 0.1940, test loss 1.4107\n",
      "Epoch 3598: train loss: 0.1940, test loss 1.4105\n",
      "Epoch 3599: train loss: 0.1940, test loss 1.4103\n",
      "Epoch 3600: train loss: 0.1940, test loss 1.4100\n",
      "Epoch 3601: train loss: 0.1940, test loss 1.4098\n",
      "Epoch 3602: train loss: 0.1940, test loss 1.4096\n",
      "Epoch 3603: train loss: 0.1940, test loss 1.4094\n",
      "Epoch 3604: train loss: 0.1940, test loss 1.4092\n",
      "Epoch 3605: train loss: 0.1940, test loss 1.4089\n",
      "Epoch 3606: train loss: 0.1940, test loss 1.4087\n",
      "Epoch 3607: train loss: 0.1940, test loss 1.4085\n",
      "Epoch 3608: train loss: 0.1940, test loss 1.4083\n",
      "Epoch 3609: train loss: 0.1939, test loss 1.4081\n",
      "Epoch 3610: train loss: 0.1939, test loss 1.4079\n",
      "Epoch 3611: train loss: 0.1939, test loss 1.4077\n",
      "Epoch 3612: train loss: 0.1939, test loss 1.4074\n",
      "Epoch 3613: train loss: 0.1939, test loss 1.4072\n",
      "Epoch 3614: train loss: 0.1939, test loss 1.4070\n",
      "Epoch 3615: train loss: 0.1939, test loss 1.4068\n",
      "Epoch 3616: train loss: 0.1939, test loss 1.4066\n",
      "Epoch 3617: train loss: 0.1939, test loss 1.4064\n",
      "Epoch 3618: train loss: 0.1939, test loss 1.4061\n",
      "Epoch 3619: train loss: 0.1939, test loss 1.4059\n",
      "Epoch 3620: train loss: 0.1939, test loss 1.4057\n",
      "Epoch 3621: train loss: 0.1939, test loss 1.4055\n",
      "Epoch 3622: train loss: 0.1939, test loss 1.4053\n",
      "Epoch 3623: train loss: 0.1939, test loss 1.4051\n",
      "Epoch 3624: train loss: 0.1939, test loss 1.4048\n",
      "Epoch 3625: train loss: 0.1939, test loss 1.4046\n",
      "Epoch 3626: train loss: 0.1939, test loss 1.4044\n",
      "Epoch 3627: train loss: 0.1939, test loss 1.4042\n",
      "Epoch 3628: train loss: 0.1938, test loss 1.4040\n",
      "Epoch 3629: train loss: 0.1938, test loss 1.4038\n",
      "Epoch 3630: train loss: 0.1938, test loss 1.4036\n",
      "Epoch 3631: train loss: 0.1938, test loss 1.4034\n",
      "Epoch 3632: train loss: 0.1938, test loss 1.4031\n",
      "Epoch 3633: train loss: 0.1938, test loss 1.4029\n",
      "Epoch 3634: train loss: 0.1938, test loss 1.4027\n",
      "Epoch 3635: train loss: 0.1938, test loss 1.4025\n",
      "Epoch 3636: train loss: 0.1938, test loss 1.4023\n",
      "Epoch 3637: train loss: 0.1938, test loss 1.4021\n",
      "Epoch 3638: train loss: 0.1938, test loss 1.4019\n",
      "Epoch 3639: train loss: 0.1938, test loss 1.4017\n",
      "Epoch 3640: train loss: 0.1938, test loss 1.4014\n",
      "Epoch 3641: train loss: 0.1938, test loss 1.4012\n",
      "Epoch 3642: train loss: 0.1938, test loss 1.4010\n",
      "Epoch 3643: train loss: 0.1938, test loss 1.4008\n",
      "Epoch 3644: train loss: 0.1938, test loss 1.4006\n",
      "Epoch 3645: train loss: 0.1938, test loss 1.4004\n",
      "Epoch 3646: train loss: 0.1937, test loss 1.4002\n",
      "Epoch 3647: train loss: 0.1937, test loss 1.4000\n",
      "Epoch 3648: train loss: 0.1937, test loss 1.3997\n",
      "Epoch 3649: train loss: 0.1937, test loss 1.3995\n",
      "Epoch 3650: train loss: 0.1937, test loss 1.3993\n",
      "Epoch 3651: train loss: 0.1937, test loss 1.3991\n",
      "Epoch 3652: train loss: 0.1937, test loss 1.3989\n",
      "Epoch 3653: train loss: 0.1937, test loss 1.3987\n",
      "Epoch 3654: train loss: 0.1937, test loss 1.3985\n",
      "Epoch 3655: train loss: 0.1937, test loss 1.3983\n",
      "Epoch 3656: train loss: 0.1937, test loss 1.3981\n",
      "Epoch 3657: train loss: 0.1937, test loss 1.3978\n",
      "Epoch 3658: train loss: 0.1937, test loss 1.3976\n",
      "Epoch 3659: train loss: 0.1937, test loss 1.3974\n",
      "Epoch 3660: train loss: 0.1937, test loss 1.3972\n",
      "Epoch 3661: train loss: 0.1937, test loss 1.3970\n",
      "Epoch 3662: train loss: 0.1937, test loss 1.3968\n",
      "Epoch 3663: train loss: 0.1937, test loss 1.3966\n",
      "Epoch 3664: train loss: 0.1937, test loss 1.3964\n",
      "Epoch 3665: train loss: 0.1936, test loss 1.3962\n",
      "Epoch 3666: train loss: 0.1936, test loss 1.3960\n",
      "Epoch 3667: train loss: 0.1936, test loss 1.3957\n",
      "Epoch 3668: train loss: 0.1936, test loss 1.3955\n",
      "Epoch 3669: train loss: 0.1936, test loss 1.3953\n",
      "Epoch 3670: train loss: 0.1936, test loss 1.3951\n",
      "Epoch 3671: train loss: 0.1936, test loss 1.3949\n",
      "Epoch 3672: train loss: 0.1936, test loss 1.3947\n",
      "Epoch 3673: train loss: 0.1936, test loss 1.3945\n",
      "Epoch 3674: train loss: 0.1936, test loss 1.3943\n",
      "Epoch 3675: train loss: 0.1936, test loss 1.3941\n",
      "Epoch 3676: train loss: 0.1936, test loss 1.3939\n",
      "Epoch 3677: train loss: 0.1936, test loss 1.3937\n",
      "Epoch 3678: train loss: 0.1936, test loss 1.3934\n",
      "Epoch 3679: train loss: 0.1936, test loss 1.3932\n",
      "Epoch 3680: train loss: 0.1936, test loss 1.3930\n",
      "Epoch 3681: train loss: 0.1936, test loss 1.3928\n",
      "Epoch 3682: train loss: 0.1936, test loss 1.3926\n",
      "Epoch 3683: train loss: 0.1935, test loss 1.3924\n",
      "Epoch 3684: train loss: 0.1935, test loss 1.3922\n",
      "Epoch 3685: train loss: 0.1935, test loss 1.3920\n",
      "Epoch 3686: train loss: 0.1935, test loss 1.3918\n",
      "Epoch 3687: train loss: 0.1935, test loss 1.3916\n",
      "Epoch 3688: train loss: 0.1935, test loss 1.3914\n",
      "Epoch 3689: train loss: 0.1935, test loss 1.3912\n",
      "Epoch 3690: train loss: 0.1935, test loss 1.3910\n",
      "Epoch 3691: train loss: 0.1935, test loss 1.3908\n",
      "Epoch 3692: train loss: 0.1935, test loss 1.3906\n",
      "Epoch 3693: train loss: 0.1935, test loss 1.3904\n",
      "Epoch 3694: train loss: 0.1935, test loss 1.3902\n",
      "Epoch 3695: train loss: 0.1935, test loss 1.3899\n",
      "Epoch 3696: train loss: 0.1935, test loss 1.3897\n",
      "Epoch 3697: train loss: 0.1935, test loss 1.3895\n",
      "Epoch 3698: train loss: 0.1935, test loss 1.3893\n",
      "Epoch 3699: train loss: 0.1935, test loss 1.3891\n",
      "Epoch 3700: train loss: 0.1935, test loss 1.3889\n",
      "Epoch 3701: train loss: 0.1935, test loss 1.3887\n",
      "Epoch 3702: train loss: 0.1934, test loss 1.3885\n",
      "Epoch 3703: train loss: 0.1934, test loss 1.3883\n",
      "Epoch 3704: train loss: 0.1934, test loss 1.3881\n",
      "Epoch 3705: train loss: 0.1934, test loss 1.3879\n",
      "Epoch 3706: train loss: 0.1934, test loss 1.3877\n",
      "Epoch 3707: train loss: 0.1934, test loss 1.3875\n",
      "Epoch 3708: train loss: 0.1934, test loss 1.3873\n",
      "Epoch 3709: train loss: 0.1934, test loss 1.3871\n",
      "Epoch 3710: train loss: 0.1934, test loss 1.3869\n",
      "Epoch 3711: train loss: 0.1934, test loss 1.3867\n",
      "Epoch 3712: train loss: 0.1934, test loss 1.3865\n",
      "Epoch 3713: train loss: 0.1934, test loss 1.3863\n",
      "Epoch 3714: train loss: 0.1934, test loss 1.3861\n",
      "Epoch 3715: train loss: 0.1934, test loss 1.3859\n",
      "Epoch 3716: train loss: 0.1934, test loss 1.3856\n",
      "Epoch 3717: train loss: 0.1934, test loss 1.3854\n",
      "Epoch 3718: train loss: 0.1934, test loss 1.3852\n",
      "Epoch 3719: train loss: 0.1934, test loss 1.3850\n",
      "Epoch 3720: train loss: 0.1934, test loss 1.3848\n",
      "Epoch 3721: train loss: 0.1933, test loss 1.3846\n",
      "Epoch 3722: train loss: 0.1933, test loss 1.3844\n",
      "Epoch 3723: train loss: 0.1933, test loss 1.3842\n",
      "Epoch 3724: train loss: 0.1933, test loss 1.3840\n",
      "Epoch 3725: train loss: 0.1933, test loss 1.3838\n",
      "Epoch 3726: train loss: 0.1933, test loss 1.3836\n",
      "Epoch 3727: train loss: 0.1933, test loss 1.3834\n",
      "Epoch 3728: train loss: 0.1933, test loss 1.3832\n",
      "Epoch 3729: train loss: 0.1933, test loss 1.3830\n",
      "Epoch 3730: train loss: 0.1933, test loss 1.3828\n",
      "Epoch 3731: train loss: 0.1933, test loss 1.3826\n",
      "Epoch 3732: train loss: 0.1933, test loss 1.3824\n",
      "Epoch 3733: train loss: 0.1933, test loss 1.3822\n",
      "Epoch 3734: train loss: 0.1933, test loss 1.3820\n",
      "Epoch 3735: train loss: 0.1933, test loss 1.3818\n",
      "Epoch 3736: train loss: 0.1933, test loss 1.3816\n",
      "Epoch 3737: train loss: 0.1933, test loss 1.3814\n",
      "Epoch 3738: train loss: 0.1933, test loss 1.3812\n",
      "Epoch 3739: train loss: 0.1933, test loss 1.3810\n",
      "Epoch 3740: train loss: 0.1932, test loss 1.3808\n",
      "Epoch 3741: train loss: 0.1932, test loss 1.3806\n",
      "Epoch 3742: train loss: 0.1932, test loss 1.3804\n",
      "Epoch 3743: train loss: 0.1932, test loss 1.3802\n",
      "Epoch 3744: train loss: 0.1932, test loss 1.3800\n",
      "Epoch 3745: train loss: 0.1932, test loss 1.3798\n",
      "Epoch 3746: train loss: 0.1932, test loss 1.3796\n",
      "Epoch 3747: train loss: 0.1932, test loss 1.3794\n",
      "Epoch 3748: train loss: 0.1932, test loss 1.3792\n",
      "Epoch 3749: train loss: 0.1932, test loss 1.3789\n",
      "Epoch 3750: train loss: 0.1932, test loss 1.3787\n",
      "Epoch 3751: train loss: 0.1932, test loss 1.3785\n",
      "Epoch 3752: train loss: 0.1932, test loss 1.3783\n",
      "Epoch 3753: train loss: 0.1932, test loss 1.3781\n",
      "Epoch 3754: train loss: 0.1932, test loss 1.3779\n",
      "Epoch 3755: train loss: 0.1932, test loss 1.3777\n",
      "Epoch 3756: train loss: 0.1932, test loss 1.3775\n",
      "Epoch 3757: train loss: 0.1932, test loss 1.3773\n",
      "Epoch 3758: train loss: 0.1932, test loss 1.3771\n",
      "Epoch 3759: train loss: 0.1932, test loss 1.3769\n",
      "Epoch 3760: train loss: 0.1931, test loss 1.3767\n",
      "Epoch 3761: train loss: 0.1931, test loss 1.3765\n",
      "Epoch 3762: train loss: 0.1931, test loss 1.3763\n",
      "Epoch 3763: train loss: 0.1931, test loss 1.3761\n",
      "Epoch 3764: train loss: 0.1931, test loss 1.3759\n",
      "Epoch 3765: train loss: 0.1931, test loss 1.3757\n",
      "Epoch 3766: train loss: 0.1931, test loss 1.3755\n",
      "Epoch 3767: train loss: 0.1931, test loss 1.3753\n",
      "Epoch 3768: train loss: 0.1931, test loss 1.3751\n",
      "Epoch 3769: train loss: 0.1931, test loss 1.3749\n",
      "Epoch 3770: train loss: 0.1931, test loss 1.3747\n",
      "Epoch 3771: train loss: 0.1931, test loss 1.3745\n",
      "Epoch 3772: train loss: 0.1931, test loss 1.3743\n",
      "Epoch 3773: train loss: 0.1931, test loss 1.3741\n",
      "Epoch 3774: train loss: 0.1931, test loss 1.3739\n",
      "Epoch 3775: train loss: 0.1931, test loss 1.3737\n",
      "Epoch 3776: train loss: 0.1931, test loss 1.3735\n",
      "Epoch 3777: train loss: 0.1931, test loss 1.3733\n",
      "Epoch 3778: train loss: 0.1931, test loss 1.3731\n",
      "Epoch 3779: train loss: 0.1930, test loss 1.3729\n",
      "Epoch 3780: train loss: 0.1930, test loss 1.3727\n",
      "Epoch 3781: train loss: 0.1930, test loss 1.3725\n",
      "Epoch 3782: train loss: 0.1930, test loss 1.3723\n",
      "Epoch 3783: train loss: 0.1930, test loss 1.3721\n",
      "Epoch 3784: train loss: 0.1930, test loss 1.3719\n",
      "Epoch 3785: train loss: 0.1930, test loss 1.3717\n",
      "Epoch 3786: train loss: 0.1930, test loss 1.3715\n",
      "Epoch 3787: train loss: 0.1930, test loss 1.3713\n",
      "Epoch 3788: train loss: 0.1930, test loss 1.3711\n",
      "Epoch 3789: train loss: 0.1930, test loss 1.3709\n",
      "Epoch 3790: train loss: 0.1930, test loss 1.3707\n",
      "Epoch 3791: train loss: 0.1930, test loss 1.3705\n",
      "Epoch 3792: train loss: 0.1930, test loss 1.3704\n",
      "Epoch 3793: train loss: 0.1930, test loss 1.3702\n",
      "Epoch 3794: train loss: 0.1930, test loss 1.3700\n",
      "Epoch 3795: train loss: 0.1930, test loss 1.3698\n",
      "Epoch 3796: train loss: 0.1930, test loss 1.3696\n",
      "Epoch 3797: train loss: 0.1930, test loss 1.3694\n",
      "Epoch 3798: train loss: 0.1930, test loss 1.3692\n",
      "Epoch 3799: train loss: 0.1929, test loss 1.3690\n",
      "Epoch 3800: train loss: 0.1929, test loss 1.3688\n",
      "Epoch 3801: train loss: 0.1929, test loss 1.3686\n",
      "Epoch 3802: train loss: 0.1929, test loss 1.3684\n",
      "Epoch 3803: train loss: 0.1929, test loss 1.3682\n",
      "Epoch 3804: train loss: 0.1929, test loss 1.3680\n",
      "Epoch 3805: train loss: 0.1929, test loss 1.3678\n",
      "Epoch 3806: train loss: 0.1929, test loss 1.3676\n",
      "Epoch 3807: train loss: 0.1929, test loss 1.3674\n",
      "Epoch 3808: train loss: 0.1929, test loss 1.3673\n",
      "Epoch 3809: train loss: 0.1929, test loss 1.3671\n",
      "Epoch 3810: train loss: 0.1929, test loss 1.3669\n",
      "Epoch 3811: train loss: 0.1929, test loss 1.3667\n",
      "Epoch 3812: train loss: 0.1929, test loss 1.3665\n",
      "Epoch 3813: train loss: 0.1929, test loss 1.3663\n",
      "Epoch 3814: train loss: 0.1929, test loss 1.3661\n",
      "Epoch 3815: train loss: 0.1929, test loss 1.3659\n",
      "Epoch 3816: train loss: 0.1929, test loss 1.3657\n",
      "Epoch 3817: train loss: 0.1929, test loss 1.3655\n",
      "Epoch 3818: train loss: 0.1929, test loss 1.3653\n",
      "Epoch 3819: train loss: 0.1928, test loss 1.3651\n",
      "Epoch 3820: train loss: 0.1928, test loss 1.3650\n",
      "Epoch 3821: train loss: 0.1928, test loss 1.3648\n",
      "Epoch 3822: train loss: 0.1928, test loss 1.3646\n",
      "Epoch 3823: train loss: 0.1928, test loss 1.3644\n",
      "Epoch 3824: train loss: 0.1928, test loss 1.3642\n",
      "Epoch 3825: train loss: 0.1928, test loss 1.3640\n",
      "Epoch 3826: train loss: 0.1928, test loss 1.3638\n",
      "Epoch 3827: train loss: 0.1928, test loss 1.3636\n",
      "Epoch 3828: train loss: 0.1928, test loss 1.3634\n",
      "Epoch 3829: train loss: 0.1928, test loss 1.3632\n",
      "Epoch 3830: train loss: 0.1928, test loss 1.3630\n",
      "Epoch 3831: train loss: 0.1928, test loss 1.3628\n",
      "Epoch 3832: train loss: 0.1928, test loss 1.3627\n",
      "Epoch 3833: train loss: 0.1928, test loss 1.3625\n",
      "Epoch 3834: train loss: 0.1928, test loss 1.3623\n",
      "Epoch 3835: train loss: 0.1928, test loss 1.3621\n",
      "Epoch 3836: train loss: 0.1928, test loss 1.3619\n",
      "Epoch 3837: train loss: 0.1928, test loss 1.3617\n",
      "Epoch 3838: train loss: 0.1928, test loss 1.3615\n",
      "Epoch 3839: train loss: 0.1927, test loss 1.3613\n",
      "Epoch 3840: train loss: 0.1927, test loss 1.3611\n",
      "Epoch 3841: train loss: 0.1927, test loss 1.3609\n",
      "Epoch 3842: train loss: 0.1927, test loss 1.3608\n",
      "Epoch 3843: train loss: 0.1927, test loss 1.3606\n",
      "Epoch 3844: train loss: 0.1927, test loss 1.3604\n",
      "Epoch 3845: train loss: 0.1927, test loss 1.3602\n",
      "Epoch 3846: train loss: 0.1927, test loss 1.3600\n",
      "Epoch 3847: train loss: 0.1927, test loss 1.3598\n",
      "Epoch 3848: train loss: 0.1927, test loss 1.3596\n",
      "Epoch 3849: train loss: 0.1927, test loss 1.3594\n",
      "Epoch 3850: train loss: 0.1927, test loss 1.3592\n",
      "Epoch 3851: train loss: 0.1927, test loss 1.3591\n",
      "Epoch 3852: train loss: 0.1927, test loss 1.3589\n",
      "Epoch 3853: train loss: 0.1927, test loss 1.3587\n",
      "Epoch 3854: train loss: 0.1927, test loss 1.3585\n",
      "Epoch 3855: train loss: 0.1927, test loss 1.3583\n",
      "Epoch 3856: train loss: 0.1927, test loss 1.3581\n",
      "Epoch 3857: train loss: 0.1927, test loss 1.3579\n",
      "Epoch 3858: train loss: 0.1927, test loss 1.3577\n",
      "Epoch 3859: train loss: 0.1926, test loss 1.3575\n",
      "Epoch 3860: train loss: 0.1926, test loss 1.3573\n",
      "Epoch 3861: train loss: 0.1926, test loss 1.3572\n",
      "Epoch 3862: train loss: 0.1926, test loss 1.3570\n",
      "Epoch 3863: train loss: 0.1926, test loss 1.3568\n",
      "Epoch 3864: train loss: 0.1926, test loss 1.3566\n",
      "Epoch 3865: train loss: 0.1926, test loss 1.3564\n",
      "Epoch 3866: train loss: 0.1926, test loss 1.3562\n",
      "Epoch 3867: train loss: 0.1926, test loss 1.3560\n",
      "Epoch 3868: train loss: 0.1926, test loss 1.3558\n",
      "Epoch 3869: train loss: 0.1926, test loss 1.3556\n",
      "Epoch 3870: train loss: 0.1926, test loss 1.3555\n",
      "Epoch 3871: train loss: 0.1926, test loss 1.3553\n",
      "Epoch 3872: train loss: 0.1926, test loss 1.3551\n",
      "Epoch 3873: train loss: 0.1926, test loss 1.3549\n",
      "Epoch 3874: train loss: 0.1926, test loss 1.3547\n",
      "Epoch 3875: train loss: 0.1926, test loss 1.3545\n",
      "Epoch 3876: train loss: 0.1926, test loss 1.3543\n",
      "Epoch 3877: train loss: 0.1926, test loss 1.3541\n",
      "Epoch 3878: train loss: 0.1926, test loss 1.3540\n",
      "Epoch 3879: train loss: 0.1926, test loss 1.3538\n",
      "Epoch 3880: train loss: 0.1925, test loss 1.3536\n",
      "Epoch 3881: train loss: 0.1925, test loss 1.3534\n",
      "Epoch 3882: train loss: 0.1925, test loss 1.3532\n",
      "Epoch 3883: train loss: 0.1925, test loss 1.3530\n",
      "Epoch 3884: train loss: 0.1925, test loss 1.3528\n",
      "Epoch 3885: train loss: 0.1925, test loss 1.3526\n",
      "Epoch 3886: train loss: 0.1925, test loss 1.3525\n",
      "Epoch 3887: train loss: 0.1925, test loss 1.3523\n",
      "Epoch 3888: train loss: 0.1925, test loss 1.3521\n",
      "Epoch 3889: train loss: 0.1925, test loss 1.3519\n",
      "Epoch 3890: train loss: 0.1925, test loss 1.3517\n",
      "Epoch 3891: train loss: 0.1925, test loss 1.3515\n",
      "Epoch 3892: train loss: 0.1925, test loss 1.3513\n",
      "Epoch 3893: train loss: 0.1925, test loss 1.3512\n",
      "Epoch 3894: train loss: 0.1925, test loss 1.3510\n",
      "Epoch 3895: train loss: 0.1925, test loss 1.3508\n",
      "Epoch 3896: train loss: 0.1925, test loss 1.3506\n",
      "Epoch 3897: train loss: 0.1925, test loss 1.3504\n",
      "Epoch 3898: train loss: 0.1925, test loss 1.3502\n",
      "Epoch 3899: train loss: 0.1925, test loss 1.3500\n",
      "Epoch 3900: train loss: 0.1924, test loss 1.3499\n",
      "Epoch 3901: train loss: 0.1924, test loss 1.3497\n",
      "Epoch 3902: train loss: 0.1924, test loss 1.3495\n",
      "Epoch 3903: train loss: 0.1924, test loss 1.3493\n",
      "Epoch 3904: train loss: 0.1924, test loss 1.3491\n",
      "Epoch 3905: train loss: 0.1924, test loss 1.3489\n",
      "Epoch 3906: train loss: 0.1924, test loss 1.3487\n",
      "Epoch 3907: train loss: 0.1924, test loss 1.3486\n",
      "Epoch 3908: train loss: 0.1924, test loss 1.3484\n",
      "Epoch 3909: train loss: 0.1924, test loss 1.3482\n",
      "Epoch 3910: train loss: 0.1924, test loss 1.3480\n",
      "Epoch 3911: train loss: 0.1924, test loss 1.3478\n",
      "Epoch 3912: train loss: 0.1924, test loss 1.3476\n",
      "Epoch 3913: train loss: 0.1924, test loss 1.3474\n",
      "Epoch 3914: train loss: 0.1924, test loss 1.3473\n",
      "Epoch 3915: train loss: 0.1924, test loss 1.3471\n",
      "Epoch 3916: train loss: 0.1924, test loss 1.3469\n",
      "Epoch 3917: train loss: 0.1924, test loss 1.3467\n",
      "Epoch 3918: train loss: 0.1924, test loss 1.3465\n",
      "Epoch 3919: train loss: 0.1924, test loss 1.3463\n",
      "Epoch 3920: train loss: 0.1924, test loss 1.3461\n",
      "Epoch 3921: train loss: 0.1923, test loss 1.3460\n",
      "Epoch 3922: train loss: 0.1923, test loss 1.3458\n",
      "Epoch 3923: train loss: 0.1923, test loss 1.3456\n",
      "Epoch 3924: train loss: 0.1923, test loss 1.3454\n",
      "Epoch 3925: train loss: 0.1923, test loss 1.3452\n",
      "Epoch 3926: train loss: 0.1923, test loss 1.3450\n",
      "Epoch 3927: train loss: 0.1923, test loss 1.3448\n",
      "Epoch 3928: train loss: 0.1923, test loss 1.3447\n",
      "Epoch 3929: train loss: 0.1923, test loss 1.3445\n",
      "Epoch 3930: train loss: 0.1923, test loss 1.3443\n",
      "Epoch 3931: train loss: 0.1923, test loss 1.3441\n",
      "Epoch 3932: train loss: 0.1923, test loss 1.3439\n",
      "Epoch 3933: train loss: 0.1923, test loss 1.3437\n",
      "Epoch 3934: train loss: 0.1923, test loss 1.3436\n",
      "Epoch 3935: train loss: 0.1923, test loss 1.3434\n",
      "Epoch 3936: train loss: 0.1923, test loss 1.3432\n",
      "Epoch 3937: train loss: 0.1923, test loss 1.3430\n",
      "Epoch 3938: train loss: 0.1923, test loss 1.3428\n",
      "Epoch 3939: train loss: 0.1923, test loss 1.3427\n",
      "Epoch 3940: train loss: 0.1923, test loss 1.3425\n",
      "Epoch 3941: train loss: 0.1923, test loss 1.3423\n",
      "Epoch 3942: train loss: 0.1922, test loss 1.3421\n",
      "Epoch 3943: train loss: 0.1922, test loss 1.3419\n",
      "Epoch 3944: train loss: 0.1922, test loss 1.3417\n",
      "Epoch 3945: train loss: 0.1922, test loss 1.3416\n",
      "Epoch 3946: train loss: 0.1922, test loss 1.3414\n",
      "Epoch 3947: train loss: 0.1922, test loss 1.3412\n",
      "Epoch 3948: train loss: 0.1922, test loss 1.3410\n",
      "Epoch 3949: train loss: 0.1922, test loss 1.3408\n",
      "Epoch 3950: train loss: 0.1922, test loss 1.3406\n",
      "Epoch 3951: train loss: 0.1922, test loss 1.3405\n",
      "Epoch 3952: train loss: 0.1922, test loss 1.3403\n",
      "Epoch 3953: train loss: 0.1922, test loss 1.3401\n",
      "Epoch 3954: train loss: 0.1922, test loss 1.3399\n",
      "Epoch 3955: train loss: 0.1922, test loss 1.3397\n",
      "Epoch 3956: train loss: 0.1922, test loss 1.3396\n",
      "Epoch 3957: train loss: 0.1922, test loss 1.3394\n",
      "Epoch 3958: train loss: 0.1922, test loss 1.3392\n",
      "Epoch 3959: train loss: 0.1922, test loss 1.3390\n",
      "Epoch 3960: train loss: 0.1922, test loss 1.3388\n",
      "Epoch 3961: train loss: 0.1922, test loss 1.3387\n",
      "Epoch 3962: train loss: 0.1922, test loss 1.3385\n",
      "Epoch 3963: train loss: 0.1921, test loss 1.3383\n",
      "Epoch 3964: train loss: 0.1921, test loss 1.3381\n",
      "Epoch 3965: train loss: 0.1921, test loss 1.3379\n",
      "Epoch 3966: train loss: 0.1921, test loss 1.3378\n",
      "Epoch 3967: train loss: 0.1921, test loss 1.3376\n",
      "Epoch 3968: train loss: 0.1921, test loss 1.3374\n",
      "Epoch 3969: train loss: 0.1921, test loss 1.3372\n",
      "Epoch 3970: train loss: 0.1921, test loss 1.3370\n",
      "Epoch 3971: train loss: 0.1921, test loss 1.3369\n",
      "Epoch 3972: train loss: 0.1921, test loss 1.3367\n",
      "Epoch 3973: train loss: 0.1921, test loss 1.3365\n",
      "Epoch 3974: train loss: 0.1921, test loss 1.3363\n",
      "Epoch 3975: train loss: 0.1921, test loss 1.3361\n",
      "Epoch 3976: train loss: 0.1921, test loss 1.3360\n",
      "Epoch 3977: train loss: 0.1921, test loss 1.3358\n",
      "Epoch 3978: train loss: 0.1921, test loss 1.3356\n",
      "Epoch 3979: train loss: 0.1921, test loss 1.3354\n",
      "Epoch 3980: train loss: 0.1921, test loss 1.3352\n",
      "Epoch 3981: train loss: 0.1921, test loss 1.3351\n",
      "Epoch 3982: train loss: 0.1921, test loss 1.3349\n",
      "Epoch 3983: train loss: 0.1921, test loss 1.3347\n",
      "Epoch 3984: train loss: 0.1920, test loss 1.3345\n",
      "Epoch 3985: train loss: 0.1920, test loss 1.3343\n",
      "Epoch 3986: train loss: 0.1920, test loss 1.3342\n",
      "Epoch 3987: train loss: 0.1920, test loss 1.3340\n",
      "Epoch 3988: train loss: 0.1920, test loss 1.3338\n",
      "Epoch 3989: train loss: 0.1920, test loss 1.3336\n",
      "Epoch 3990: train loss: 0.1920, test loss 1.3334\n",
      "Epoch 3991: train loss: 0.1920, test loss 1.3333\n",
      "Epoch 3992: train loss: 0.1920, test loss 1.3331\n",
      "Epoch 3993: train loss: 0.1920, test loss 1.3329\n",
      "Epoch 3994: train loss: 0.1920, test loss 1.3327\n",
      "Epoch 3995: train loss: 0.1920, test loss 1.3326\n",
      "Epoch 3996: train loss: 0.1920, test loss 1.3324\n",
      "Epoch 3997: train loss: 0.1920, test loss 1.3322\n",
      "Epoch 3998: train loss: 0.1920, test loss 1.3320\n",
      "Epoch 3999: train loss: 0.1920, test loss 1.3318\n",
      "Epoch 4000: train loss: 0.1920, test loss 1.3317\n",
      "Epoch 4001: train loss: 0.1920, test loss 1.3315\n",
      "Epoch 4002: train loss: 0.1920, test loss 1.3313\n",
      "Epoch 4003: train loss: 0.1920, test loss 1.3311\n",
      "Epoch 4004: train loss: 0.1920, test loss 1.3310\n",
      "Epoch 4005: train loss: 0.1920, test loss 1.3308\n",
      "Epoch 4006: train loss: 0.1919, test loss 1.3306\n",
      "Epoch 4007: train loss: 0.1919, test loss 1.3304\n",
      "Epoch 4008: train loss: 0.1919, test loss 1.3303\n",
      "Epoch 4009: train loss: 0.1919, test loss 1.3301\n",
      "Epoch 4010: train loss: 0.1919, test loss 1.3299\n",
      "Epoch 4011: train loss: 0.1919, test loss 1.3297\n",
      "Epoch 4012: train loss: 0.1919, test loss 1.3296\n",
      "Epoch 4013: train loss: 0.1919, test loss 1.3294\n",
      "Epoch 4014: train loss: 0.1919, test loss 1.3292\n",
      "Epoch 4015: train loss: 0.1919, test loss 1.3290\n",
      "Epoch 4016: train loss: 0.1919, test loss 1.3289\n",
      "Epoch 4017: train loss: 0.1919, test loss 1.3287\n",
      "Epoch 4018: train loss: 0.1919, test loss 1.3285\n",
      "Epoch 4019: train loss: 0.1919, test loss 1.3283\n",
      "Epoch 4020: train loss: 0.1919, test loss 1.3282\n",
      "Epoch 4021: train loss: 0.1919, test loss 1.3280\n",
      "Epoch 4022: train loss: 0.1919, test loss 1.3278\n",
      "Epoch 4023: train loss: 0.1919, test loss 1.3276\n",
      "Epoch 4024: train loss: 0.1919, test loss 1.3275\n",
      "Epoch 4025: train loss: 0.1919, test loss 1.3273\n",
      "Epoch 4026: train loss: 0.1919, test loss 1.3271\n",
      "Epoch 4027: train loss: 0.1919, test loss 1.3269\n",
      "Epoch 4028: train loss: 0.1918, test loss 1.3268\n",
      "Epoch 4029: train loss: 0.1918, test loss 1.3266\n",
      "Epoch 4030: train loss: 0.1918, test loss 1.3264\n",
      "Epoch 4031: train loss: 0.1918, test loss 1.3263\n",
      "Epoch 4032: train loss: 0.1918, test loss 1.3261\n",
      "Epoch 4033: train loss: 0.1918, test loss 1.3259\n",
      "Epoch 4034: train loss: 0.1918, test loss 1.3257\n",
      "Epoch 4035: train loss: 0.1918, test loss 1.3256\n",
      "Epoch 4036: train loss: 0.1918, test loss 1.3254\n",
      "Epoch 4037: train loss: 0.1918, test loss 1.3252\n",
      "Epoch 4038: train loss: 0.1918, test loss 1.3250\n",
      "Epoch 4039: train loss: 0.1918, test loss 1.3249\n",
      "Epoch 4040: train loss: 0.1918, test loss 1.3247\n",
      "Epoch 4041: train loss: 0.1918, test loss 1.3245\n",
      "Epoch 4042: train loss: 0.1918, test loss 1.3244\n",
      "Epoch 4043: train loss: 0.1918, test loss 1.3242\n",
      "Epoch 4044: train loss: 0.1918, test loss 1.3240\n",
      "Epoch 4045: train loss: 0.1918, test loss 1.3238\n",
      "Epoch 4046: train loss: 0.1918, test loss 1.3237\n",
      "Epoch 4047: train loss: 0.1918, test loss 1.3235\n",
      "Epoch 4048: train loss: 0.1918, test loss 1.3233\n",
      "Epoch 4049: train loss: 0.1918, test loss 1.3232\n",
      "Epoch 4050: train loss: 0.1917, test loss 1.3230\n",
      "Epoch 4051: train loss: 0.1917, test loss 1.3228\n",
      "Epoch 4052: train loss: 0.1917, test loss 1.3226\n",
      "Epoch 4053: train loss: 0.1917, test loss 1.3225\n",
      "Epoch 4054: train loss: 0.1917, test loss 1.3223\n",
      "Epoch 4055: train loss: 0.1917, test loss 1.3221\n",
      "Epoch 4056: train loss: 0.1917, test loss 1.3220\n",
      "Epoch 4057: train loss: 0.1917, test loss 1.3218\n",
      "Epoch 4058: train loss: 0.1917, test loss 1.3216\n",
      "Epoch 4059: train loss: 0.1917, test loss 1.3214\n",
      "Epoch 4060: train loss: 0.1917, test loss 1.3213\n",
      "Epoch 4061: train loss: 0.1917, test loss 1.3211\n",
      "Epoch 4062: train loss: 0.1917, test loss 1.3209\n",
      "Epoch 4063: train loss: 0.1917, test loss 1.3208\n",
      "Epoch 4064: train loss: 0.1917, test loss 1.3206\n",
      "Epoch 4065: train loss: 0.1917, test loss 1.3204\n",
      "Epoch 4066: train loss: 0.1917, test loss 1.3202\n",
      "Epoch 4067: train loss: 0.1917, test loss 1.3201\n",
      "Epoch 4068: train loss: 0.1917, test loss 1.3199\n",
      "Epoch 4069: train loss: 0.1917, test loss 1.3197\n",
      "Epoch 4070: train loss: 0.1917, test loss 1.3196\n",
      "Epoch 4071: train loss: 0.1917, test loss 1.3194\n",
      "Epoch 4072: train loss: 0.1916, test loss 1.3192\n",
      "Epoch 4073: train loss: 0.1916, test loss 1.3191\n",
      "Epoch 4074: train loss: 0.1916, test loss 1.3189\n",
      "Epoch 4075: train loss: 0.1916, test loss 1.3187\n",
      "Epoch 4076: train loss: 0.1916, test loss 1.3185\n",
      "Epoch 4077: train loss: 0.1916, test loss 1.3184\n",
      "Epoch 4078: train loss: 0.1916, test loss 1.3182\n",
      "Epoch 4079: train loss: 0.1916, test loss 1.3180\n",
      "Epoch 4080: train loss: 0.1916, test loss 1.3179\n",
      "Epoch 4081: train loss: 0.1916, test loss 1.3177\n",
      "Epoch 4082: train loss: 0.1916, test loss 1.3175\n",
      "Epoch 4083: train loss: 0.1916, test loss 1.3174\n",
      "Epoch 4084: train loss: 0.1916, test loss 1.3172\n",
      "Epoch 4085: train loss: 0.1916, test loss 1.3170\n",
      "Epoch 4086: train loss: 0.1916, test loss 1.3168\n",
      "Epoch 4087: train loss: 0.1916, test loss 1.3167\n",
      "Epoch 4088: train loss: 0.1916, test loss 1.3165\n",
      "Epoch 4089: train loss: 0.1916, test loss 1.3163\n",
      "Epoch 4090: train loss: 0.1916, test loss 1.3162\n",
      "Epoch 4091: train loss: 0.1916, test loss 1.3160\n",
      "Epoch 4092: train loss: 0.1916, test loss 1.3158\n",
      "Epoch 4093: train loss: 0.1916, test loss 1.3157\n",
      "Epoch 4094: train loss: 0.1915, test loss 1.3155\n",
      "Epoch 4095: train loss: 0.1915, test loss 1.3153\n",
      "Epoch 4096: train loss: 0.1915, test loss 1.3152\n",
      "Epoch 4097: train loss: 0.1915, test loss 1.3150\n",
      "Epoch 4098: train loss: 0.1915, test loss 1.3148\n",
      "Epoch 4099: train loss: 0.1915, test loss 1.3147\n",
      "Epoch 4100: train loss: 0.1915, test loss 1.3145\n",
      "Epoch 4101: train loss: 0.1915, test loss 1.3143\n",
      "Epoch 4102: train loss: 0.1915, test loss 1.3141\n",
      "Epoch 4103: train loss: 0.1915, test loss 1.3140\n",
      "Epoch 4104: train loss: 0.1915, test loss 1.3138\n",
      "Epoch 4105: train loss: 0.1915, test loss 1.3136\n",
      "Epoch 4106: train loss: 0.1915, test loss 1.3135\n",
      "Epoch 4107: train loss: 0.1915, test loss 1.3133\n",
      "Epoch 4108: train loss: 0.1915, test loss 1.3131\n",
      "Epoch 4109: train loss: 0.1915, test loss 1.3130\n",
      "Epoch 4110: train loss: 0.1915, test loss 1.3128\n",
      "Epoch 4111: train loss: 0.1915, test loss 1.3126\n",
      "Epoch 4112: train loss: 0.1915, test loss 1.3125\n",
      "Epoch 4113: train loss: 0.1915, test loss 1.3123\n",
      "Epoch 4114: train loss: 0.1915, test loss 1.3121\n",
      "Epoch 4115: train loss: 0.1915, test loss 1.3120\n",
      "Epoch 4116: train loss: 0.1915, test loss 1.3118\n",
      "Epoch 4117: train loss: 0.1914, test loss 1.3116\n",
      "Epoch 4118: train loss: 0.1914, test loss 1.3115\n",
      "Epoch 4119: train loss: 0.1914, test loss 1.3113\n",
      "Epoch 4120: train loss: 0.1914, test loss 1.3111\n",
      "Epoch 4121: train loss: 0.1914, test loss 1.3110\n",
      "Epoch 4122: train loss: 0.1914, test loss 1.3108\n",
      "Epoch 4123: train loss: 0.1914, test loss 1.3106\n",
      "Epoch 4124: train loss: 0.1914, test loss 1.3104\n",
      "Epoch 4125: train loss: 0.1914, test loss 1.3103\n",
      "Epoch 4126: train loss: 0.1914, test loss 1.3101\n",
      "Epoch 4127: train loss: 0.1914, test loss 1.3099\n",
      "Epoch 4128: train loss: 0.1914, test loss 1.3098\n",
      "Epoch 4129: train loss: 0.1914, test loss 1.3096\n",
      "Epoch 4130: train loss: 0.1914, test loss 1.3094\n",
      "Epoch 4131: train loss: 0.1914, test loss 1.3093\n",
      "Epoch 4132: train loss: 0.1914, test loss 1.3091\n",
      "Epoch 4133: train loss: 0.1914, test loss 1.3089\n",
      "Epoch 4134: train loss: 0.1914, test loss 1.3088\n",
      "Epoch 4135: train loss: 0.1914, test loss 1.3086\n",
      "Epoch 4136: train loss: 0.1914, test loss 1.3084\n",
      "Epoch 4137: train loss: 0.1914, test loss 1.3083\n",
      "Epoch 4138: train loss: 0.1914, test loss 1.3081\n",
      "Epoch 4139: train loss: 0.1914, test loss 1.3079\n",
      "Epoch 4140: train loss: 0.1913, test loss 1.3078\n",
      "Epoch 4141: train loss: 0.1913, test loss 1.3076\n",
      "Epoch 4142: train loss: 0.1913, test loss 1.3074\n",
      "Epoch 4143: train loss: 0.1913, test loss 1.3073\n",
      "Epoch 4144: train loss: 0.1913, test loss 1.3071\n",
      "Epoch 4145: train loss: 0.1913, test loss 1.3069\n",
      "Epoch 4146: train loss: 0.1913, test loss 1.3068\n",
      "Epoch 4147: train loss: 0.1913, test loss 1.3066\n",
      "Epoch 4148: train loss: 0.1913, test loss 1.3064\n",
      "Epoch 4149: train loss: 0.1913, test loss 1.3063\n",
      "Epoch 4150: train loss: 0.1913, test loss 1.3061\n",
      "Epoch 4151: train loss: 0.1913, test loss 1.3059\n",
      "Epoch 4152: train loss: 0.1913, test loss 1.3058\n",
      "Epoch 4153: train loss: 0.1913, test loss 1.3056\n",
      "Epoch 4154: train loss: 0.1913, test loss 1.3054\n",
      "Epoch 4155: train loss: 0.1913, test loss 1.3053\n",
      "Epoch 4156: train loss: 0.1913, test loss 1.3051\n",
      "Epoch 4157: train loss: 0.1913, test loss 1.3049\n",
      "Epoch 4158: train loss: 0.1913, test loss 1.3048\n",
      "Epoch 4159: train loss: 0.1913, test loss 1.3046\n",
      "Epoch 4160: train loss: 0.1913, test loss 1.3044\n",
      "Epoch 4161: train loss: 0.1913, test loss 1.3043\n",
      "Epoch 4162: train loss: 0.1913, test loss 1.3041\n",
      "Epoch 4163: train loss: 0.1912, test loss 1.3039\n",
      "Epoch 4164: train loss: 0.1912, test loss 1.3038\n",
      "Epoch 4165: train loss: 0.1912, test loss 1.3036\n",
      "Epoch 4166: train loss: 0.1912, test loss 1.3034\n",
      "Epoch 4167: train loss: 0.1912, test loss 1.3033\n",
      "Epoch 4168: train loss: 0.1912, test loss 1.3031\n",
      "Epoch 4169: train loss: 0.1912, test loss 1.3029\n",
      "Epoch 4170: train loss: 0.1912, test loss 1.3028\n",
      "Epoch 4171: train loss: 0.1912, test loss 1.3026\n",
      "Epoch 4172: train loss: 0.1912, test loss 1.3024\n",
      "Epoch 4173: train loss: 0.1912, test loss 1.3023\n",
      "Epoch 4174: train loss: 0.1912, test loss 1.3021\n",
      "Epoch 4175: train loss: 0.1912, test loss 1.3019\n",
      "Epoch 4176: train loss: 0.1912, test loss 1.3018\n",
      "Epoch 4177: train loss: 0.1912, test loss 1.3016\n",
      "Epoch 4178: train loss: 0.1912, test loss 1.3014\n",
      "Epoch 4179: train loss: 0.1912, test loss 1.3013\n",
      "Epoch 4180: train loss: 0.1912, test loss 1.3011\n",
      "Epoch 4181: train loss: 0.1912, test loss 1.3009\n",
      "Epoch 4182: train loss: 0.1912, test loss 1.3008\n",
      "Epoch 4183: train loss: 0.1912, test loss 1.3006\n",
      "Epoch 4184: train loss: 0.1912, test loss 1.3004\n",
      "Epoch 4185: train loss: 0.1912, test loss 1.3003\n",
      "Epoch 4186: train loss: 0.1911, test loss 1.3001\n",
      "Epoch 4187: train loss: 0.1911, test loss 1.3000\n",
      "Epoch 4188: train loss: 0.1911, test loss 1.2998\n",
      "Epoch 4189: train loss: 0.1911, test loss 1.2996\n",
      "Epoch 4190: train loss: 0.1911, test loss 1.2995\n",
      "Epoch 4191: train loss: 0.1911, test loss 1.2993\n",
      "Epoch 4192: train loss: 0.1911, test loss 1.2991\n",
      "Epoch 4193: train loss: 0.1911, test loss 1.2990\n",
      "Epoch 4194: train loss: 0.1911, test loss 1.2988\n",
      "Epoch 4195: train loss: 0.1911, test loss 1.2986\n",
      "Epoch 4196: train loss: 0.1911, test loss 1.2985\n",
      "Epoch 4197: train loss: 0.1911, test loss 1.2983\n",
      "Epoch 4198: train loss: 0.1911, test loss 1.2981\n",
      "Epoch 4199: train loss: 0.1911, test loss 1.2980\n",
      "Epoch 4200: train loss: 0.1911, test loss 1.2978\n",
      "Epoch 4201: train loss: 0.1911, test loss 1.2977\n",
      "Epoch 4202: train loss: 0.1911, test loss 1.2975\n",
      "Epoch 4203: train loss: 0.1911, test loss 1.2973\n",
      "Epoch 4204: train loss: 0.1911, test loss 1.2972\n",
      "Epoch 4205: train loss: 0.1911, test loss 1.2970\n",
      "Epoch 4206: train loss: 0.1911, test loss 1.2968\n",
      "Epoch 4207: train loss: 0.1911, test loss 1.2967\n",
      "Epoch 4208: train loss: 0.1911, test loss 1.2965\n",
      "Epoch 4209: train loss: 0.1910, test loss 1.2963\n",
      "Epoch 4210: train loss: 0.1910, test loss 1.2962\n",
      "Epoch 4211: train loss: 0.1910, test loss 1.2960\n",
      "Epoch 4212: train loss: 0.1910, test loss 1.2959\n",
      "Epoch 4213: train loss: 0.1910, test loss 1.2957\n",
      "Epoch 4214: train loss: 0.1910, test loss 1.2955\n",
      "Epoch 4215: train loss: 0.1910, test loss 1.2954\n",
      "Epoch 4216: train loss: 0.1910, test loss 1.2952\n",
      "Epoch 4217: train loss: 0.1910, test loss 1.2950\n",
      "Epoch 4218: train loss: 0.1910, test loss 1.2949\n",
      "Epoch 4219: train loss: 0.1910, test loss 1.2947\n",
      "Epoch 4220: train loss: 0.1910, test loss 1.2946\n",
      "Epoch 4221: train loss: 0.1910, test loss 1.2944\n",
      "Epoch 4222: train loss: 0.1910, test loss 1.2942\n",
      "Epoch 4223: train loss: 0.1910, test loss 1.2941\n",
      "Epoch 4224: train loss: 0.1910, test loss 1.2939\n",
      "Epoch 4225: train loss: 0.1910, test loss 1.2938\n",
      "Epoch 4226: train loss: 0.1910, test loss 1.2936\n",
      "Epoch 4227: train loss: 0.1910, test loss 1.2934\n",
      "Epoch 4228: train loss: 0.1910, test loss 1.2933\n",
      "Epoch 4229: train loss: 0.1910, test loss 1.2931\n",
      "Epoch 4230: train loss: 0.1910, test loss 1.2929\n",
      "Epoch 4231: train loss: 0.1910, test loss 1.2928\n",
      "Epoch 4232: train loss: 0.1909, test loss 1.2926\n",
      "Epoch 4233: train loss: 0.1909, test loss 1.2925\n",
      "Epoch 4234: train loss: 0.1909, test loss 1.2923\n",
      "Epoch 4235: train loss: 0.1909, test loss 1.2921\n",
      "Epoch 4236: train loss: 0.1909, test loss 1.2920\n",
      "Epoch 4237: train loss: 0.1909, test loss 1.2918\n",
      "Epoch 4238: train loss: 0.1909, test loss 1.2917\n",
      "Epoch 4239: train loss: 0.1909, test loss 1.2915\n",
      "Epoch 4240: train loss: 0.1909, test loss 1.2913\n",
      "Epoch 4241: train loss: 0.1909, test loss 1.2912\n",
      "Epoch 4242: train loss: 0.1909, test loss 1.2910\n",
      "Epoch 4243: train loss: 0.1909, test loss 1.2909\n",
      "Epoch 4244: train loss: 0.1909, test loss 1.2907\n",
      "Epoch 4245: train loss: 0.1909, test loss 1.2905\n",
      "Epoch 4246: train loss: 0.1909, test loss 1.2904\n",
      "Epoch 4247: train loss: 0.1909, test loss 1.2902\n",
      "Epoch 4248: train loss: 0.1909, test loss 1.2901\n",
      "Epoch 4249: train loss: 0.1909, test loss 1.2899\n",
      "Epoch 4250: train loss: 0.1909, test loss 1.2897\n",
      "Epoch 4251: train loss: 0.1909, test loss 1.2896\n",
      "Epoch 4252: train loss: 0.1909, test loss 1.2894\n",
      "Epoch 4253: train loss: 0.1909, test loss 1.2893\n",
      "Epoch 4254: train loss: 0.1909, test loss 1.2891\n",
      "Epoch 4255: train loss: 0.1909, test loss 1.2889\n",
      "Epoch 4256: train loss: 0.1908, test loss 1.2888\n",
      "Epoch 4257: train loss: 0.1908, test loss 1.2886\n",
      "Epoch 4258: train loss: 0.1908, test loss 1.2885\n",
      "Epoch 4259: train loss: 0.1908, test loss 1.2883\n",
      "Epoch 4260: train loss: 0.1908, test loss 1.2881\n",
      "Epoch 4261: train loss: 0.1908, test loss 1.2880\n",
      "Epoch 4262: train loss: 0.1908, test loss 1.2878\n",
      "Epoch 4263: train loss: 0.1908, test loss 1.2877\n",
      "Epoch 4264: train loss: 0.1908, test loss 1.2875\n",
      "Epoch 4265: train loss: 0.1908, test loss 1.2873\n",
      "Epoch 4266: train loss: 0.1908, test loss 1.2872\n",
      "Epoch 4267: train loss: 0.1908, test loss 1.2870\n",
      "Epoch 4268: train loss: 0.1908, test loss 1.2869\n",
      "Epoch 4269: train loss: 0.1908, test loss 1.2867\n",
      "Epoch 4270: train loss: 0.1908, test loss 1.2865\n",
      "Epoch 4271: train loss: 0.1908, test loss 1.2864\n",
      "Epoch 4272: train loss: 0.1908, test loss 1.2862\n",
      "Epoch 4273: train loss: 0.1908, test loss 1.2861\n",
      "Epoch 4274: train loss: 0.1908, test loss 1.2859\n",
      "Epoch 4275: train loss: 0.1908, test loss 1.2857\n",
      "Epoch 4276: train loss: 0.1908, test loss 1.2856\n",
      "Epoch 4277: train loss: 0.1908, test loss 1.2854\n",
      "Epoch 4278: train loss: 0.1908, test loss 1.2853\n",
      "Epoch 4279: train loss: 0.1907, test loss 1.2851\n",
      "Epoch 4280: train loss: 0.1907, test loss 1.2849\n",
      "Epoch 4281: train loss: 0.1907, test loss 1.2848\n",
      "Epoch 4282: train loss: 0.1907, test loss 1.2846\n",
      "Epoch 4283: train loss: 0.1907, test loss 1.2845\n",
      "Epoch 4284: train loss: 0.1907, test loss 1.2843\n",
      "Epoch 4285: train loss: 0.1907, test loss 1.2841\n",
      "Epoch 4286: train loss: 0.1907, test loss 1.2840\n",
      "Epoch 4287: train loss: 0.1907, test loss 1.2838\n",
      "Epoch 4288: train loss: 0.1907, test loss 1.2837\n",
      "Epoch 4289: train loss: 0.1907, test loss 1.2835\n",
      "Epoch 4290: train loss: 0.1907, test loss 1.2834\n",
      "Epoch 4291: train loss: 0.1907, test loss 1.2832\n",
      "Epoch 4292: train loss: 0.1907, test loss 1.2830\n",
      "Epoch 4293: train loss: 0.1907, test loss 1.2829\n",
      "Epoch 4294: train loss: 0.1907, test loss 1.2827\n",
      "Epoch 4295: train loss: 0.1907, test loss 1.2826\n",
      "Epoch 4296: train loss: 0.1907, test loss 1.2824\n",
      "Epoch 4297: train loss: 0.1907, test loss 1.2822\n",
      "Epoch 4298: train loss: 0.1907, test loss 1.2821\n",
      "Epoch 4299: train loss: 0.1907, test loss 1.2819\n",
      "Epoch 4300: train loss: 0.1907, test loss 1.2818\n",
      "Epoch 4301: train loss: 0.1907, test loss 1.2816\n",
      "Epoch 4302: train loss: 0.1907, test loss 1.2815\n",
      "Epoch 4303: train loss: 0.1906, test loss 1.2813\n",
      "Epoch 4304: train loss: 0.1906, test loss 1.2811\n",
      "Epoch 4305: train loss: 0.1906, test loss 1.2810\n",
      "Epoch 4306: train loss: 0.1906, test loss 1.2808\n",
      "Epoch 4307: train loss: 0.1906, test loss 1.2807\n",
      "Epoch 4308: train loss: 0.1906, test loss 1.2805\n",
      "Epoch 4309: train loss: 0.1906, test loss 1.2804\n",
      "Epoch 4310: train loss: 0.1906, test loss 1.2802\n",
      "Epoch 4311: train loss: 0.1906, test loss 1.2801\n",
      "Epoch 4312: train loss: 0.1906, test loss 1.2799\n",
      "Epoch 4313: train loss: 0.1906, test loss 1.2797\n",
      "Epoch 4314: train loss: 0.1906, test loss 1.2796\n",
      "Epoch 4315: train loss: 0.1906, test loss 1.2794\n",
      "Epoch 4316: train loss: 0.1906, test loss 1.2793\n",
      "Epoch 4317: train loss: 0.1906, test loss 1.2791\n",
      "Epoch 4318: train loss: 0.1906, test loss 1.2790\n",
      "Epoch 4319: train loss: 0.1906, test loss 1.2788\n",
      "Epoch 4320: train loss: 0.1906, test loss 1.2786\n",
      "Epoch 4321: train loss: 0.1906, test loss 1.2785\n",
      "Epoch 4322: train loss: 0.1906, test loss 1.2783\n",
      "Epoch 4323: train loss: 0.1906, test loss 1.2782\n",
      "Epoch 4324: train loss: 0.1906, test loss 1.2780\n",
      "Epoch 4325: train loss: 0.1906, test loss 1.2779\n",
      "Epoch 4326: train loss: 0.1906, test loss 1.2777\n",
      "Epoch 4327: train loss: 0.1906, test loss 1.2776\n",
      "Epoch 4328: train loss: 0.1905, test loss 1.2774\n",
      "Epoch 4329: train loss: 0.1905, test loss 1.2773\n",
      "Epoch 4330: train loss: 0.1905, test loss 1.2771\n",
      "Epoch 4331: train loss: 0.1905, test loss 1.2769\n",
      "Epoch 4332: train loss: 0.1905, test loss 1.2768\n",
      "Epoch 4333: train loss: 0.1905, test loss 1.2766\n",
      "Epoch 4334: train loss: 0.1905, test loss 1.2765\n",
      "Epoch 4335: train loss: 0.1905, test loss 1.2763\n",
      "Epoch 4336: train loss: 0.1905, test loss 1.2762\n",
      "Epoch 4337: train loss: 0.1905, test loss 1.2760\n",
      "Epoch 4338: train loss: 0.1905, test loss 1.2759\n",
      "Epoch 4339: train loss: 0.1905, test loss 1.2757\n",
      "Epoch 4340: train loss: 0.1905, test loss 1.2756\n",
      "Epoch 4341: train loss: 0.1905, test loss 1.2754\n",
      "Epoch 4342: train loss: 0.1905, test loss 1.2752\n",
      "Epoch 4343: train loss: 0.1905, test loss 1.2751\n",
      "Epoch 4344: train loss: 0.1905, test loss 1.2749\n",
      "Epoch 4345: train loss: 0.1905, test loss 1.2748\n",
      "Epoch 4346: train loss: 0.1905, test loss 1.2746\n",
      "Epoch 4347: train loss: 0.1905, test loss 1.2745\n",
      "Epoch 4348: train loss: 0.1905, test loss 1.2743\n",
      "Epoch 4349: train loss: 0.1905, test loss 1.2742\n",
      "Epoch 4350: train loss: 0.1905, test loss 1.2740\n",
      "Epoch 4351: train loss: 0.1905, test loss 1.2739\n",
      "Epoch 4352: train loss: 0.1904, test loss 1.2737\n",
      "Epoch 4353: train loss: 0.1904, test loss 1.2736\n",
      "Epoch 4354: train loss: 0.1904, test loss 1.2734\n",
      "Epoch 4355: train loss: 0.1904, test loss 1.2732\n",
      "Epoch 4356: train loss: 0.1904, test loss 1.2731\n",
      "Epoch 4357: train loss: 0.1904, test loss 1.2729\n",
      "Epoch 4358: train loss: 0.1904, test loss 1.2728\n",
      "Epoch 4359: train loss: 0.1904, test loss 1.2726\n",
      "Epoch 4360: train loss: 0.1904, test loss 1.2725\n",
      "Epoch 4361: train loss: 0.1904, test loss 1.2723\n",
      "Epoch 4362: train loss: 0.1904, test loss 1.2722\n",
      "Epoch 4363: train loss: 0.1904, test loss 1.2720\n",
      "Epoch 4364: train loss: 0.1904, test loss 1.2719\n",
      "Epoch 4365: train loss: 0.1904, test loss 1.2717\n",
      "Epoch 4366: train loss: 0.1904, test loss 1.2716\n",
      "Epoch 4367: train loss: 0.1904, test loss 1.2714\n",
      "Epoch 4368: train loss: 0.1904, test loss 1.2713\n",
      "Epoch 4369: train loss: 0.1904, test loss 1.2711\n",
      "Epoch 4370: train loss: 0.1904, test loss 1.2710\n",
      "Epoch 4371: train loss: 0.1904, test loss 1.2708\n",
      "Epoch 4372: train loss: 0.1904, test loss 1.2707\n",
      "Epoch 4373: train loss: 0.1904, test loss 1.2705\n",
      "Epoch 4374: train loss: 0.1904, test loss 1.2704\n",
      "Epoch 4375: train loss: 0.1904, test loss 1.2702\n",
      "Epoch 4376: train loss: 0.1904, test loss 1.2701\n",
      "Epoch 4377: train loss: 0.1903, test loss 1.2699\n",
      "Epoch 4378: train loss: 0.1903, test loss 1.2698\n",
      "Epoch 4379: train loss: 0.1903, test loss 1.2696\n",
      "Epoch 4380: train loss: 0.1903, test loss 1.2694\n",
      "Epoch 4381: train loss: 0.1903, test loss 1.2693\n",
      "Epoch 4382: train loss: 0.1903, test loss 1.2691\n",
      "Epoch 4383: train loss: 0.1903, test loss 1.2690\n",
      "Epoch 4384: train loss: 0.1903, test loss 1.2688\n",
      "Epoch 4385: train loss: 0.1903, test loss 1.2687\n",
      "Epoch 4386: train loss: 0.1903, test loss 1.2685\n",
      "Epoch 4387: train loss: 0.1903, test loss 1.2684\n",
      "Epoch 4388: train loss: 0.1903, test loss 1.2682\n",
      "Epoch 4389: train loss: 0.1903, test loss 1.2681\n",
      "Epoch 4390: train loss: 0.1903, test loss 1.2679\n",
      "Epoch 4391: train loss: 0.1903, test loss 1.2678\n",
      "Epoch 4392: train loss: 0.1903, test loss 1.2676\n",
      "Epoch 4393: train loss: 0.1903, test loss 1.2675\n",
      "Epoch 4394: train loss: 0.1903, test loss 1.2673\n",
      "Epoch 4395: train loss: 0.1903, test loss 1.2672\n",
      "Epoch 4396: train loss: 0.1903, test loss 1.2670\n",
      "Epoch 4397: train loss: 0.1903, test loss 1.2669\n",
      "Epoch 4398: train loss: 0.1903, test loss 1.2667\n",
      "Epoch 4399: train loss: 0.1903, test loss 1.2666\n",
      "Epoch 4400: train loss: 0.1903, test loss 1.2664\n",
      "Epoch 4401: train loss: 0.1903, test loss 1.2663\n",
      "Epoch 4402: train loss: 0.1902, test loss 1.2661\n",
      "Epoch 4403: train loss: 0.1902, test loss 1.2660\n",
      "Epoch 4404: train loss: 0.1902, test loss 1.2658\n",
      "Epoch 4405: train loss: 0.1902, test loss 1.2657\n",
      "Epoch 4406: train loss: 0.1902, test loss 1.2655\n",
      "Epoch 4407: train loss: 0.1902, test loss 1.2654\n",
      "Epoch 4408: train loss: 0.1902, test loss 1.2652\n",
      "Epoch 4409: train loss: 0.1902, test loss 1.2651\n",
      "Epoch 4410: train loss: 0.1902, test loss 1.2649\n",
      "Epoch 4411: train loss: 0.1902, test loss 1.2648\n",
      "Epoch 4412: train loss: 0.1902, test loss 1.2646\n",
      "Epoch 4413: train loss: 0.1902, test loss 1.2645\n",
      "Epoch 4414: train loss: 0.1902, test loss 1.2643\n",
      "Epoch 4415: train loss: 0.1902, test loss 1.2642\n",
      "Epoch 4416: train loss: 0.1902, test loss 1.2640\n",
      "Epoch 4417: train loss: 0.1902, test loss 1.2639\n",
      "Epoch 4418: train loss: 0.1902, test loss 1.2637\n",
      "Epoch 4419: train loss: 0.1902, test loss 1.2636\n",
      "Epoch 4420: train loss: 0.1902, test loss 1.2634\n",
      "Epoch 4421: train loss: 0.1902, test loss 1.2633\n",
      "Epoch 4422: train loss: 0.1902, test loss 1.2631\n",
      "Epoch 4423: train loss: 0.1902, test loss 1.2630\n",
      "Epoch 4424: train loss: 0.1902, test loss 1.2628\n",
      "Epoch 4425: train loss: 0.1902, test loss 1.2627\n",
      "Epoch 4426: train loss: 0.1902, test loss 1.2625\n",
      "Epoch 4427: train loss: 0.1901, test loss 1.2624\n",
      "Epoch 4428: train loss: 0.1901, test loss 1.2622\n",
      "Epoch 4429: train loss: 0.1901, test loss 1.2621\n",
      "Epoch 4430: train loss: 0.1901, test loss 1.2620\n",
      "Epoch 4431: train loss: 0.1901, test loss 1.2618\n",
      "Epoch 4432: train loss: 0.1901, test loss 1.2617\n",
      "Epoch 4433: train loss: 0.1901, test loss 1.2615\n",
      "Epoch 4434: train loss: 0.1901, test loss 1.2614\n",
      "Epoch 4435: train loss: 0.1901, test loss 1.2612\n",
      "Epoch 4436: train loss: 0.1901, test loss 1.2611\n",
      "Epoch 4437: train loss: 0.1901, test loss 1.2609\n",
      "Epoch 4438: train loss: 0.1901, test loss 1.2608\n",
      "Epoch 4439: train loss: 0.1901, test loss 1.2606\n",
      "Epoch 4440: train loss: 0.1901, test loss 1.2605\n",
      "Epoch 4441: train loss: 0.1901, test loss 1.2603\n",
      "Epoch 4442: train loss: 0.1901, test loss 1.2602\n",
      "Epoch 4443: train loss: 0.1901, test loss 1.2600\n",
      "Epoch 4444: train loss: 0.1901, test loss 1.2599\n",
      "Epoch 4445: train loss: 0.1901, test loss 1.2597\n",
      "Epoch 4446: train loss: 0.1901, test loss 1.2596\n",
      "Epoch 4447: train loss: 0.1901, test loss 1.2595\n",
      "Epoch 4448: train loss: 0.1901, test loss 1.2593\n",
      "Epoch 4449: train loss: 0.1901, test loss 1.2592\n",
      "Epoch 4450: train loss: 0.1901, test loss 1.2590\n",
      "Epoch 4451: train loss: 0.1901, test loss 1.2589\n",
      "Epoch 4452: train loss: 0.1901, test loss 1.2587\n",
      "Epoch 4453: train loss: 0.1900, test loss 1.2586\n",
      "Epoch 4454: train loss: 0.1900, test loss 1.2584\n",
      "Epoch 4455: train loss: 0.1900, test loss 1.2583\n",
      "Epoch 4456: train loss: 0.1900, test loss 1.2581\n",
      "Epoch 4457: train loss: 0.1900, test loss 1.2580\n",
      "Epoch 4458: train loss: 0.1900, test loss 1.2578\n",
      "Epoch 4459: train loss: 0.1900, test loss 1.2577\n",
      "Epoch 4460: train loss: 0.1900, test loss 1.2576\n",
      "Epoch 4461: train loss: 0.1900, test loss 1.2574\n",
      "Epoch 4462: train loss: 0.1900, test loss 1.2573\n",
      "Epoch 4463: train loss: 0.1900, test loss 1.2571\n",
      "Epoch 4464: train loss: 0.1900, test loss 1.2570\n",
      "Epoch 4465: train loss: 0.1900, test loss 1.2568\n",
      "Epoch 4466: train loss: 0.1900, test loss 1.2567\n",
      "Epoch 4467: train loss: 0.1900, test loss 1.2565\n",
      "Epoch 4468: train loss: 0.1900, test loss 1.2564\n",
      "Epoch 4469: train loss: 0.1900, test loss 1.2562\n",
      "Epoch 4470: train loss: 0.1900, test loss 1.2561\n",
      "Epoch 4471: train loss: 0.1900, test loss 1.2560\n",
      "Epoch 4472: train loss: 0.1900, test loss 1.2558\n",
      "Epoch 4473: train loss: 0.1900, test loss 1.2557\n",
      "Epoch 4474: train loss: 0.1900, test loss 1.2555\n",
      "Epoch 4475: train loss: 0.1900, test loss 1.2554\n",
      "Epoch 4476: train loss: 0.1900, test loss 1.2552\n",
      "Epoch 4477: train loss: 0.1900, test loss 1.2551\n",
      "Epoch 4478: train loss: 0.1899, test loss 1.2549\n",
      "Epoch 4479: train loss: 0.1899, test loss 1.2548\n",
      "Epoch 4480: train loss: 0.1899, test loss 1.2547\n",
      "Epoch 4481: train loss: 0.1899, test loss 1.2545\n",
      "Epoch 4482: train loss: 0.1899, test loss 1.2544\n",
      "Epoch 4483: train loss: 0.1899, test loss 1.2542\n",
      "Epoch 4484: train loss: 0.1899, test loss 1.2541\n",
      "Epoch 4485: train loss: 0.1899, test loss 1.2539\n",
      "Epoch 4486: train loss: 0.1899, test loss 1.2538\n",
      "Epoch 4487: train loss: 0.1899, test loss 1.2536\n",
      "Epoch 4488: train loss: 0.1899, test loss 1.2535\n",
      "Epoch 4489: train loss: 0.1899, test loss 1.2534\n",
      "Epoch 4490: train loss: 0.1899, test loss 1.2532\n",
      "Epoch 4491: train loss: 0.1899, test loss 1.2531\n",
      "Epoch 4492: train loss: 0.1899, test loss 1.2529\n",
      "Epoch 4493: train loss: 0.1899, test loss 1.2528\n",
      "Epoch 4494: train loss: 0.1899, test loss 1.2526\n",
      "Epoch 4495: train loss: 0.1899, test loss 1.2525\n",
      "Epoch 4496: train loss: 0.1899, test loss 1.2524\n",
      "Epoch 4497: train loss: 0.1899, test loss 1.2522\n",
      "Epoch 4498: train loss: 0.1899, test loss 1.2521\n",
      "Epoch 4499: train loss: 0.1899, test loss 1.2519\n",
      "Epoch 4500: train loss: 0.1899, test loss 1.2518\n",
      "Epoch 4501: train loss: 0.1899, test loss 1.2516\n",
      "Epoch 4502: train loss: 0.1899, test loss 1.2515\n",
      "Epoch 4503: train loss: 0.1898, test loss 1.2514\n",
      "Epoch 4504: train loss: 0.1898, test loss 1.2512\n",
      "Epoch 4505: train loss: 0.1898, test loss 1.2511\n",
      "Epoch 4506: train loss: 0.1898, test loss 1.2509\n",
      "Epoch 4507: train loss: 0.1898, test loss 1.2508\n",
      "Epoch 4508: train loss: 0.1898, test loss 1.2506\n",
      "Epoch 4509: train loss: 0.1898, test loss 1.2505\n",
      "Epoch 4510: train loss: 0.1898, test loss 1.2504\n",
      "Epoch 4511: train loss: 0.1898, test loss 1.2502\n",
      "Epoch 4512: train loss: 0.1898, test loss 1.2501\n",
      "Epoch 4513: train loss: 0.1898, test loss 1.2499\n",
      "Epoch 4514: train loss: 0.1898, test loss 1.2498\n",
      "Epoch 4515: train loss: 0.1898, test loss 1.2496\n",
      "Epoch 4516: train loss: 0.1898, test loss 1.2495\n",
      "Epoch 4517: train loss: 0.1898, test loss 1.2493\n",
      "Epoch 4518: train loss: 0.1898, test loss 1.2492\n",
      "Epoch 4519: train loss: 0.1898, test loss 1.2490\n",
      "Epoch 4520: train loss: 0.1898, test loss 1.2489\n",
      "Epoch 4521: train loss: 0.1898, test loss 1.2487\n",
      "Epoch 4522: train loss: 0.1898, test loss 1.2486\n",
      "Epoch 4523: train loss: 0.1898, test loss 1.2484\n",
      "Epoch 4524: train loss: 0.1898, test loss 1.2483\n",
      "Epoch 4525: train loss: 0.1898, test loss 1.2482\n",
      "Epoch 4526: train loss: 0.1898, test loss 1.2480\n",
      "Epoch 4527: train loss: 0.1898, test loss 1.2479\n",
      "Epoch 4528: train loss: 0.1898, test loss 1.2477\n",
      "Epoch 4529: train loss: 0.1897, test loss 1.2476\n",
      "Epoch 4530: train loss: 0.1897, test loss 1.2474\n",
      "Epoch 4531: train loss: 0.1897, test loss 1.2473\n",
      "Epoch 4532: train loss: 0.1897, test loss 1.2471\n",
      "Epoch 4533: train loss: 0.1897, test loss 1.2470\n",
      "Epoch 4534: train loss: 0.1897, test loss 1.2468\n",
      "Epoch 4535: train loss: 0.1897, test loss 1.2467\n",
      "Epoch 4536: train loss: 0.1897, test loss 1.2465\n",
      "Epoch 4537: train loss: 0.1897, test loss 1.2464\n",
      "Epoch 4538: train loss: 0.1897, test loss 1.2462\n",
      "Epoch 4539: train loss: 0.1897, test loss 1.2461\n",
      "Epoch 4540: train loss: 0.1897, test loss 1.2459\n",
      "Epoch 4541: train loss: 0.1897, test loss 1.2458\n",
      "Epoch 4542: train loss: 0.1897, test loss 1.2456\n",
      "Epoch 4543: train loss: 0.1897, test loss 1.2455\n",
      "Epoch 4544: train loss: 0.1897, test loss 1.2453\n",
      "Epoch 4545: train loss: 0.1897, test loss 1.2452\n",
      "Epoch 4546: train loss: 0.1897, test loss 1.2451\n",
      "Epoch 4547: train loss: 0.1897, test loss 1.2449\n",
      "Epoch 4548: train loss: 0.1897, test loss 1.2448\n",
      "Epoch 4549: train loss: 0.1897, test loss 1.2446\n",
      "Epoch 4550: train loss: 0.1897, test loss 1.2445\n",
      "Epoch 4551: train loss: 0.1897, test loss 1.2443\n",
      "Epoch 4552: train loss: 0.1897, test loss 1.2442\n",
      "Epoch 4553: train loss: 0.1897, test loss 1.2440\n",
      "Epoch 4554: train loss: 0.1897, test loss 1.2439\n",
      "Epoch 4555: train loss: 0.1896, test loss 1.2437\n",
      "Epoch 4556: train loss: 0.1896, test loss 1.2436\n",
      "Epoch 4557: train loss: 0.1896, test loss 1.2434\n",
      "Epoch 4558: train loss: 0.1896, test loss 1.2433\n",
      "Epoch 4559: train loss: 0.1896, test loss 1.2432\n",
      "Epoch 4560: train loss: 0.1896, test loss 1.2430\n",
      "Epoch 4561: train loss: 0.1896, test loss 1.2429\n",
      "Epoch 4562: train loss: 0.1896, test loss 1.2427\n",
      "Epoch 4563: train loss: 0.1896, test loss 1.2426\n",
      "Epoch 4564: train loss: 0.1896, test loss 1.2424\n",
      "Epoch 4565: train loss: 0.1896, test loss 1.2423\n",
      "Epoch 4566: train loss: 0.1896, test loss 1.2421\n",
      "Epoch 4567: train loss: 0.1896, test loss 1.2420\n",
      "Epoch 4568: train loss: 0.1896, test loss 1.2418\n",
      "Epoch 4569: train loss: 0.1896, test loss 1.2417\n",
      "Epoch 4570: train loss: 0.1896, test loss 1.2416\n",
      "Epoch 4571: train loss: 0.1896, test loss 1.2414\n",
      "Epoch 4572: train loss: 0.1896, test loss 1.2413\n",
      "Epoch 4573: train loss: 0.1896, test loss 1.2411\n",
      "Epoch 4574: train loss: 0.1896, test loss 1.2410\n",
      "Epoch 4575: train loss: 0.1896, test loss 1.2408\n",
      "Epoch 4576: train loss: 0.1896, test loss 1.2407\n",
      "Epoch 4577: train loss: 0.1896, test loss 1.2405\n",
      "Epoch 4578: train loss: 0.1896, test loss 1.2404\n",
      "Epoch 4579: train loss: 0.1896, test loss 1.2403\n",
      "Epoch 4580: train loss: 0.1896, test loss 1.2401\n",
      "Epoch 4581: train loss: 0.1895, test loss 1.2400\n",
      "Epoch 4582: train loss: 0.1895, test loss 1.2398\n",
      "Epoch 4583: train loss: 0.1895, test loss 1.2397\n",
      "Epoch 4584: train loss: 0.1895, test loss 1.2395\n",
      "Epoch 4585: train loss: 0.1895, test loss 1.2394\n",
      "Epoch 4586: train loss: 0.1895, test loss 1.2392\n",
      "Epoch 4587: train loss: 0.1895, test loss 1.2391\n",
      "Epoch 4588: train loss: 0.1895, test loss 1.2390\n",
      "Epoch 4589: train loss: 0.1895, test loss 1.2388\n",
      "Epoch 4590: train loss: 0.1895, test loss 1.2387\n",
      "Epoch 4591: train loss: 0.1895, test loss 1.2385\n",
      "Epoch 4592: train loss: 0.1895, test loss 1.2384\n",
      "Epoch 4593: train loss: 0.1895, test loss 1.2383\n",
      "Epoch 4594: train loss: 0.1895, test loss 1.2381\n",
      "Epoch 4595: train loss: 0.1895, test loss 1.2380\n",
      "Epoch 4596: train loss: 0.1895, test loss 1.2378\n",
      "Epoch 4597: train loss: 0.1895, test loss 1.2377\n",
      "Epoch 4598: train loss: 0.1895, test loss 1.2375\n",
      "Epoch 4599: train loss: 0.1895, test loss 1.2374\n",
      "Epoch 4600: train loss: 0.1895, test loss 1.2372\n",
      "Epoch 4601: train loss: 0.1895, test loss 1.2371\n",
      "Epoch 4602: train loss: 0.1895, test loss 1.2370\n",
      "Epoch 4603: train loss: 0.1895, test loss 1.2368\n",
      "Epoch 4604: train loss: 0.1895, test loss 1.2367\n",
      "Epoch 4605: train loss: 0.1895, test loss 1.2365\n",
      "Epoch 4606: train loss: 0.1895, test loss 1.2364\n",
      "Epoch 4607: train loss: 0.1895, test loss 1.2362\n",
      "Epoch 4608: train loss: 0.1894, test loss 1.2361\n",
      "Epoch 4609: train loss: 0.1894, test loss 1.2360\n",
      "Epoch 4610: train loss: 0.1894, test loss 1.2358\n",
      "Epoch 4611: train loss: 0.1894, test loss 1.2357\n",
      "Epoch 4612: train loss: 0.1894, test loss 1.2355\n",
      "Epoch 4613: train loss: 0.1894, test loss 1.2354\n",
      "Epoch 4614: train loss: 0.1894, test loss 1.2352\n",
      "Epoch 4615: train loss: 0.1894, test loss 1.2351\n",
      "Epoch 4616: train loss: 0.1894, test loss 1.2350\n",
      "Epoch 4617: train loss: 0.1894, test loss 1.2348\n",
      "Epoch 4618: train loss: 0.1894, test loss 1.2347\n",
      "Epoch 4619: train loss: 0.1894, test loss 1.2345\n",
      "Epoch 4620: train loss: 0.1894, test loss 1.2344\n",
      "Epoch 4621: train loss: 0.1894, test loss 1.2342\n",
      "Epoch 4622: train loss: 0.1894, test loss 1.2341\n",
      "Epoch 4623: train loss: 0.1894, test loss 1.2340\n",
      "Epoch 4624: train loss: 0.1894, test loss 1.2338\n",
      "Epoch 4625: train loss: 0.1894, test loss 1.2337\n",
      "Epoch 4626: train loss: 0.1894, test loss 1.2335\n",
      "Epoch 4627: train loss: 0.1894, test loss 1.2334\n",
      "Epoch 4628: train loss: 0.1894, test loss 1.2333\n",
      "Epoch 4629: train loss: 0.1894, test loss 1.2331\n",
      "Epoch 4630: train loss: 0.1894, test loss 1.2330\n",
      "Epoch 4631: train loss: 0.1894, test loss 1.2328\n",
      "Epoch 4632: train loss: 0.1894, test loss 1.2327\n",
      "Epoch 4633: train loss: 0.1894, test loss 1.2325\n",
      "Epoch 4634: train loss: 0.1894, test loss 1.2324\n",
      "Epoch 4635: train loss: 0.1893, test loss 1.2323\n",
      "Epoch 4636: train loss: 0.1893, test loss 1.2321\n",
      "Epoch 4637: train loss: 0.1893, test loss 1.2320\n",
      "Epoch 4638: train loss: 0.1893, test loss 1.2318\n",
      "Epoch 4639: train loss: 0.1893, test loss 1.2317\n",
      "Epoch 4640: train loss: 0.1893, test loss 1.2315\n",
      "Epoch 4641: train loss: 0.1893, test loss 1.2314\n",
      "Epoch 4642: train loss: 0.1893, test loss 1.2313\n",
      "Epoch 4643: train loss: 0.1893, test loss 1.2311\n",
      "Epoch 4644: train loss: 0.1893, test loss 1.2310\n",
      "Epoch 4645: train loss: 0.1893, test loss 1.2308\n",
      "Epoch 4646: train loss: 0.1893, test loss 1.2307\n",
      "Epoch 4647: train loss: 0.1893, test loss 1.2306\n",
      "Epoch 4648: train loss: 0.1893, test loss 1.2304\n",
      "Epoch 4649: train loss: 0.1893, test loss 1.2303\n",
      "Epoch 4650: train loss: 0.1893, test loss 1.2301\n",
      "Epoch 4651: train loss: 0.1893, test loss 1.2300\n",
      "Epoch 4652: train loss: 0.1893, test loss 1.2299\n",
      "Epoch 4653: train loss: 0.1893, test loss 1.2297\n",
      "Epoch 4654: train loss: 0.1893, test loss 1.2296\n",
      "Epoch 4655: train loss: 0.1893, test loss 1.2294\n",
      "Epoch 4656: train loss: 0.1893, test loss 1.2293\n",
      "Epoch 4657: train loss: 0.1893, test loss 1.2292\n",
      "Epoch 4658: train loss: 0.1893, test loss 1.2290\n",
      "Epoch 4659: train loss: 0.1893, test loss 1.2289\n",
      "Epoch 4660: train loss: 0.1893, test loss 1.2287\n",
      "Epoch 4661: train loss: 0.1893, test loss 1.2286\n",
      "Epoch 4662: train loss: 0.1892, test loss 1.2285\n",
      "Epoch 4663: train loss: 0.1892, test loss 1.2283\n",
      "Epoch 4664: train loss: 0.1892, test loss 1.2282\n",
      "Epoch 4665: train loss: 0.1892, test loss 1.2280\n",
      "Epoch 4666: train loss: 0.1892, test loss 1.2279\n",
      "Epoch 4667: train loss: 0.1892, test loss 1.2278\n",
      "Epoch 4668: train loss: 0.1892, test loss 1.2276\n",
      "Epoch 4669: train loss: 0.1892, test loss 1.2275\n",
      "Epoch 4670: train loss: 0.1892, test loss 1.2274\n",
      "Epoch 4671: train loss: 0.1892, test loss 1.2272\n",
      "Epoch 4672: train loss: 0.1892, test loss 1.2271\n",
      "Epoch 4673: train loss: 0.1892, test loss 1.2269\n",
      "Epoch 4674: train loss: 0.1892, test loss 1.2268\n",
      "Epoch 4675: train loss: 0.1892, test loss 1.2267\n",
      "Epoch 4676: train loss: 0.1892, test loss 1.2265\n",
      "Epoch 4677: train loss: 0.1892, test loss 1.2264\n",
      "Epoch 4678: train loss: 0.1892, test loss 1.2262\n",
      "Epoch 4679: train loss: 0.1892, test loss 1.2261\n",
      "Epoch 4680: train loss: 0.1892, test loss 1.2260\n",
      "Epoch 4681: train loss: 0.1892, test loss 1.2258\n",
      "Epoch 4682: train loss: 0.1892, test loss 1.2257\n",
      "Epoch 4683: train loss: 0.1892, test loss 1.2256\n",
      "Epoch 4684: train loss: 0.1892, test loss 1.2254\n",
      "Epoch 4685: train loss: 0.1892, test loss 1.2253\n",
      "Epoch 4686: train loss: 0.1892, test loss 1.2251\n",
      "Epoch 4687: train loss: 0.1892, test loss 1.2250\n",
      "Epoch 4688: train loss: 0.1892, test loss 1.2249\n",
      "Epoch 4689: train loss: 0.1891, test loss 1.2247\n",
      "Epoch 4690: train loss: 0.1891, test loss 1.2246\n",
      "Epoch 4691: train loss: 0.1891, test loss 1.2244\n",
      "Epoch 4692: train loss: 0.1891, test loss 1.2243\n",
      "Epoch 4693: train loss: 0.1891, test loss 1.2242\n",
      "Epoch 4694: train loss: 0.1891, test loss 1.2240\n",
      "Epoch 4695: train loss: 0.1891, test loss 1.2239\n",
      "Epoch 4696: train loss: 0.1891, test loss 1.2238\n",
      "Epoch 4697: train loss: 0.1891, test loss 1.2236\n",
      "Epoch 4698: train loss: 0.1891, test loss 1.2235\n",
      "Epoch 4699: train loss: 0.1891, test loss 1.2233\n",
      "Epoch 4700: train loss: 0.1891, test loss 1.2232\n",
      "Epoch 4701: train loss: 0.1891, test loss 1.2231\n",
      "Epoch 4702: train loss: 0.1891, test loss 1.2229\n",
      "Epoch 4703: train loss: 0.1891, test loss 1.2228\n",
      "Epoch 4704: train loss: 0.1891, test loss 1.2226\n",
      "Epoch 4705: train loss: 0.1891, test loss 1.2225\n",
      "Epoch 4706: train loss: 0.1891, test loss 1.2224\n",
      "Epoch 4707: train loss: 0.1891, test loss 1.2222\n",
      "Epoch 4708: train loss: 0.1891, test loss 1.2221\n",
      "Epoch 4709: train loss: 0.1891, test loss 1.2220\n",
      "Epoch 4710: train loss: 0.1891, test loss 1.2218\n",
      "Epoch 4711: train loss: 0.1891, test loss 1.2217\n",
      "Epoch 4712: train loss: 0.1891, test loss 1.2216\n",
      "Epoch 4713: train loss: 0.1891, test loss 1.2214\n",
      "Epoch 4714: train loss: 0.1891, test loss 1.2213\n",
      "Epoch 4715: train loss: 0.1891, test loss 1.2211\n",
      "Epoch 4716: train loss: 0.1890, test loss 1.2210\n",
      "Epoch 4717: train loss: 0.1890, test loss 1.2209\n",
      "Epoch 4718: train loss: 0.1890, test loss 1.2207\n",
      "Epoch 4719: train loss: 0.1890, test loss 1.2206\n",
      "Epoch 4720: train loss: 0.1890, test loss 1.2205\n",
      "Epoch 4721: train loss: 0.1890, test loss 1.2203\n",
      "Epoch 4722: train loss: 0.1890, test loss 1.2202\n",
      "Epoch 4723: train loss: 0.1890, test loss 1.2200\n",
      "Epoch 4724: train loss: 0.1890, test loss 1.2199\n",
      "Epoch 4725: train loss: 0.1890, test loss 1.2198\n",
      "Epoch 4726: train loss: 0.1890, test loss 1.2196\n",
      "Epoch 4727: train loss: 0.1890, test loss 1.2195\n",
      "Epoch 4728: train loss: 0.1890, test loss 1.2194\n",
      "Epoch 4729: train loss: 0.1890, test loss 1.2192\n",
      "Epoch 4730: train loss: 0.1890, test loss 1.2191\n",
      "Epoch 4731: train loss: 0.1890, test loss 1.2190\n",
      "Epoch 4732: train loss: 0.1890, test loss 1.2188\n",
      "Epoch 4733: train loss: 0.1890, test loss 1.2187\n",
      "Epoch 4734: train loss: 0.1890, test loss 1.2185\n",
      "Epoch 4735: train loss: 0.1890, test loss 1.2184\n",
      "Epoch 4736: train loss: 0.1890, test loss 1.2183\n",
      "Epoch 4737: train loss: 0.1890, test loss 1.2181\n",
      "Epoch 4738: train loss: 0.1890, test loss 1.2180\n",
      "Epoch 4739: train loss: 0.1890, test loss 1.2179\n",
      "Epoch 4740: train loss: 0.1890, test loss 1.2177\n",
      "Epoch 4741: train loss: 0.1890, test loss 1.2176\n",
      "Epoch 4742: train loss: 0.1890, test loss 1.2175\n",
      "Epoch 4743: train loss: 0.1889, test loss 1.2173\n",
      "Epoch 4744: train loss: 0.1889, test loss 1.2172\n",
      "Epoch 4745: train loss: 0.1889, test loss 1.2170\n",
      "Epoch 4746: train loss: 0.1889, test loss 1.2169\n",
      "Epoch 4747: train loss: 0.1889, test loss 1.2168\n",
      "Epoch 4748: train loss: 0.1889, test loss 1.2166\n",
      "Epoch 4749: train loss: 0.1889, test loss 1.2165\n",
      "Epoch 4750: train loss: 0.1889, test loss 1.2164\n",
      "Epoch 4751: train loss: 0.1889, test loss 1.2162\n",
      "Epoch 4752: train loss: 0.1889, test loss 1.2161\n",
      "Epoch 4753: train loss: 0.1889, test loss 1.2160\n",
      "Epoch 4754: train loss: 0.1889, test loss 1.2158\n",
      "Epoch 4755: train loss: 0.1889, test loss 1.2157\n",
      "Epoch 4756: train loss: 0.1889, test loss 1.2156\n",
      "Epoch 4757: train loss: 0.1889, test loss 1.2154\n",
      "Epoch 4758: train loss: 0.1889, test loss 1.2153\n",
      "Epoch 4759: train loss: 0.1889, test loss 1.2152\n",
      "Epoch 4760: train loss: 0.1889, test loss 1.2150\n",
      "Epoch 4761: train loss: 0.1889, test loss 1.2149\n",
      "Epoch 4762: train loss: 0.1889, test loss 1.2147\n",
      "Epoch 4763: train loss: 0.1889, test loss 1.2146\n",
      "Epoch 4764: train loss: 0.1889, test loss 1.2145\n",
      "Epoch 4765: train loss: 0.1889, test loss 1.2143\n",
      "Epoch 4766: train loss: 0.1889, test loss 1.2142\n",
      "Epoch 4767: train loss: 0.1889, test loss 1.2141\n",
      "Epoch 4768: train loss: 0.1889, test loss 1.2139\n",
      "Epoch 4769: train loss: 0.1889, test loss 1.2138\n",
      "Epoch 4770: train loss: 0.1889, test loss 1.2137\n",
      "Epoch 4771: train loss: 0.1888, test loss 1.2135\n",
      "Epoch 4772: train loss: 0.1888, test loss 1.2134\n",
      "Epoch 4773: train loss: 0.1888, test loss 1.2132\n",
      "Epoch 4774: train loss: 0.1888, test loss 1.2131\n",
      "Epoch 4775: train loss: 0.1888, test loss 1.2130\n",
      "Epoch 4776: train loss: 0.1888, test loss 1.2128\n",
      "Epoch 4777: train loss: 0.1888, test loss 1.2127\n",
      "Epoch 4778: train loss: 0.1888, test loss 1.2126\n",
      "Epoch 4779: train loss: 0.1888, test loss 1.2124\n",
      "Epoch 4780: train loss: 0.1888, test loss 1.2123\n",
      "Epoch 4781: train loss: 0.1888, test loss 1.2122\n",
      "Epoch 4782: train loss: 0.1888, test loss 1.2120\n",
      "Epoch 4783: train loss: 0.1888, test loss 1.2119\n",
      "Epoch 4784: train loss: 0.1888, test loss 1.2117\n",
      "Epoch 4785: train loss: 0.1888, test loss 1.2116\n",
      "Epoch 4786: train loss: 0.1888, test loss 1.2115\n",
      "Epoch 4787: train loss: 0.1888, test loss 1.2113\n",
      "Epoch 4788: train loss: 0.1888, test loss 1.2112\n",
      "Epoch 4789: train loss: 0.1888, test loss 1.2111\n",
      "Epoch 4790: train loss: 0.1888, test loss 1.2109\n",
      "Epoch 4791: train loss: 0.1888, test loss 1.2108\n",
      "Epoch 4792: train loss: 0.1888, test loss 1.2107\n",
      "Epoch 4793: train loss: 0.1888, test loss 1.2105\n",
      "Epoch 4794: train loss: 0.1888, test loss 1.2104\n",
      "Epoch 4795: train loss: 0.1888, test loss 1.2103\n",
      "Epoch 4796: train loss: 0.1888, test loss 1.2101\n",
      "Epoch 4797: train loss: 0.1888, test loss 1.2100\n",
      "Epoch 4798: train loss: 0.1888, test loss 1.2099\n",
      "Epoch 4799: train loss: 0.1887, test loss 1.2097\n",
      "Epoch 4800: train loss: 0.1887, test loss 1.2096\n",
      "Epoch 4801: train loss: 0.1887, test loss 1.2094\n",
      "Epoch 4802: train loss: 0.1887, test loss 1.2093\n",
      "Epoch 4803: train loss: 0.1887, test loss 1.2092\n",
      "Epoch 4804: train loss: 0.1887, test loss 1.2090\n",
      "Epoch 4805: train loss: 0.1887, test loss 1.2089\n",
      "Epoch 4806: train loss: 0.1887, test loss 1.2088\n",
      "Epoch 4807: train loss: 0.1887, test loss 1.2086\n",
      "Epoch 4808: train loss: 0.1887, test loss 1.2085\n",
      "Epoch 4809: train loss: 0.1887, test loss 1.2084\n",
      "Epoch 4810: train loss: 0.1887, test loss 1.2082\n",
      "Epoch 4811: train loss: 0.1887, test loss 1.2081\n",
      "Epoch 4812: train loss: 0.1887, test loss 1.2080\n",
      "Epoch 4813: train loss: 0.1887, test loss 1.2078\n",
      "Epoch 4814: train loss: 0.1887, test loss 1.2077\n",
      "Epoch 4815: train loss: 0.1887, test loss 1.2076\n",
      "Epoch 4816: train loss: 0.1887, test loss 1.2074\n",
      "Epoch 4817: train loss: 0.1887, test loss 1.2073\n",
      "Epoch 4818: train loss: 0.1887, test loss 1.2072\n",
      "Epoch 4819: train loss: 0.1887, test loss 1.2070\n",
      "Epoch 4820: train loss: 0.1887, test loss 1.2069\n",
      "Epoch 4821: train loss: 0.1887, test loss 1.2068\n",
      "Epoch 4822: train loss: 0.1887, test loss 1.2066\n",
      "Epoch 4823: train loss: 0.1887, test loss 1.2065\n",
      "Epoch 4824: train loss: 0.1887, test loss 1.2064\n",
      "Epoch 4825: train loss: 0.1887, test loss 1.2062\n",
      "Epoch 4826: train loss: 0.1887, test loss 1.2061\n",
      "Epoch 4827: train loss: 0.1886, test loss 1.2060\n",
      "Epoch 4828: train loss: 0.1886, test loss 1.2058\n",
      "Epoch 4829: train loss: 0.1886, test loss 1.2057\n",
      "Epoch 4830: train loss: 0.1886, test loss 1.2055\n",
      "Epoch 4831: train loss: 0.1886, test loss 1.2054\n",
      "Epoch 4832: train loss: 0.1886, test loss 1.2053\n",
      "Epoch 4833: train loss: 0.1886, test loss 1.2052\n",
      "Epoch 4834: train loss: 0.1886, test loss 1.2050\n",
      "Epoch 4835: train loss: 0.1886, test loss 1.2049\n",
      "Epoch 4836: train loss: 0.1886, test loss 1.2047\n",
      "Epoch 4837: train loss: 0.1886, test loss 1.2046\n",
      "Epoch 4838: train loss: 0.1886, test loss 1.2045\n",
      "Epoch 4839: train loss: 0.1886, test loss 1.2044\n",
      "Epoch 4840: train loss: 0.1886, test loss 1.2042\n",
      "Epoch 4841: train loss: 0.1886, test loss 1.2041\n",
      "Epoch 4842: train loss: 0.1886, test loss 1.2040\n",
      "Epoch 4843: train loss: 0.1886, test loss 1.2038\n",
      "Epoch 4844: train loss: 0.1886, test loss 1.2037\n",
      "Epoch 4845: train loss: 0.1886, test loss 1.2036\n",
      "Epoch 4846: train loss: 0.1886, test loss 1.2034\n",
      "Epoch 4847: train loss: 0.1886, test loss 1.2033\n",
      "Epoch 4848: train loss: 0.1886, test loss 1.2032\n",
      "Epoch 4849: train loss: 0.1886, test loss 1.2030\n",
      "Epoch 4850: train loss: 0.1886, test loss 1.2029\n",
      "Epoch 4851: train loss: 0.1886, test loss 1.2028\n",
      "Epoch 4852: train loss: 0.1886, test loss 1.2026\n",
      "Epoch 4853: train loss: 0.1886, test loss 1.2025\n",
      "Epoch 4854: train loss: 0.1886, test loss 1.2024\n",
      "Epoch 4855: train loss: 0.1885, test loss 1.2022\n",
      "Epoch 4856: train loss: 0.1885, test loss 1.2021\n",
      "Epoch 4857: train loss: 0.1885, test loss 1.2020\n",
      "Epoch 4858: train loss: 0.1885, test loss 1.2018\n",
      "Epoch 4859: train loss: 0.1885, test loss 1.2017\n",
      "Epoch 4860: train loss: 0.1885, test loss 1.2016\n",
      "Epoch 4861: train loss: 0.1885, test loss 1.2014\n",
      "Epoch 4862: train loss: 0.1885, test loss 1.2013\n",
      "Epoch 4863: train loss: 0.1885, test loss 1.2012\n",
      "Epoch 4864: train loss: 0.1885, test loss 1.2010\n",
      "Epoch 4865: train loss: 0.1885, test loss 1.2009\n",
      "Epoch 4866: train loss: 0.1885, test loss 1.2008\n",
      "Epoch 4867: train loss: 0.1885, test loss 1.2006\n",
      "Epoch 4868: train loss: 0.1885, test loss 1.2005\n",
      "Epoch 4869: train loss: 0.1885, test loss 1.2004\n",
      "Epoch 4870: train loss: 0.1885, test loss 1.2003\n",
      "Epoch 4871: train loss: 0.1885, test loss 1.2001\n",
      "Epoch 4872: train loss: 0.1885, test loss 1.2000\n",
      "Epoch 4873: train loss: 0.1885, test loss 1.1999\n",
      "Epoch 4874: train loss: 0.1885, test loss 1.1997\n",
      "Epoch 4875: train loss: 0.1885, test loss 1.1996\n",
      "Epoch 4876: train loss: 0.1885, test loss 1.1995\n",
      "Epoch 4877: train loss: 0.1885, test loss 1.1993\n",
      "Epoch 4878: train loss: 0.1885, test loss 1.1992\n",
      "Epoch 4879: train loss: 0.1885, test loss 1.1991\n",
      "Epoch 4880: train loss: 0.1885, test loss 1.1989\n",
      "Epoch 4881: train loss: 0.1885, test loss 1.1988\n",
      "Epoch 4882: train loss: 0.1885, test loss 1.1987\n",
      "Epoch 4883: train loss: 0.1884, test loss 1.1985\n",
      "Epoch 4884: train loss: 0.1884, test loss 1.1984\n",
      "Epoch 4885: train loss: 0.1884, test loss 1.1983\n",
      "Epoch 4886: train loss: 0.1884, test loss 1.1981\n",
      "Epoch 4887: train loss: 0.1884, test loss 1.1980\n",
      "Epoch 4888: train loss: 0.1884, test loss 1.1979\n",
      "Epoch 4889: train loss: 0.1884, test loss 1.1977\n",
      "Epoch 4890: train loss: 0.1884, test loss 1.1976\n",
      "Epoch 4891: train loss: 0.1884, test loss 1.1975\n",
      "Epoch 4892: train loss: 0.1884, test loss 1.1973\n",
      "Epoch 4893: train loss: 0.1884, test loss 1.1972\n",
      "Epoch 4894: train loss: 0.1884, test loss 1.1971\n",
      "Epoch 4895: train loss: 0.1884, test loss 1.1970\n",
      "Epoch 4896: train loss: 0.1884, test loss 1.1968\n",
      "Epoch 4897: train loss: 0.1884, test loss 1.1967\n",
      "Epoch 4898: train loss: 0.1884, test loss 1.1966\n",
      "Epoch 4899: train loss: 0.1884, test loss 1.1964\n",
      "Epoch 4900: train loss: 0.1884, test loss 1.1963\n",
      "Epoch 4901: train loss: 0.1884, test loss 1.1962\n",
      "Epoch 4902: train loss: 0.1884, test loss 1.1960\n",
      "Epoch 4903: train loss: 0.1884, test loss 1.1959\n",
      "Epoch 4904: train loss: 0.1884, test loss 1.1958\n",
      "Epoch 4905: train loss: 0.1884, test loss 1.1956\n",
      "Epoch 4906: train loss: 0.1884, test loss 1.1955\n",
      "Epoch 4907: train loss: 0.1884, test loss 1.1954\n",
      "Epoch 4908: train loss: 0.1884, test loss 1.1953\n",
      "Epoch 4909: train loss: 0.1884, test loss 1.1951\n",
      "Epoch 4910: train loss: 0.1884, test loss 1.1950\n",
      "Epoch 4911: train loss: 0.1884, test loss 1.1949\n",
      "Epoch 4912: train loss: 0.1883, test loss 1.1947\n",
      "Epoch 4913: train loss: 0.1883, test loss 1.1946\n",
      "Epoch 4914: train loss: 0.1883, test loss 1.1945\n",
      "Epoch 4915: train loss: 0.1883, test loss 1.1943\n",
      "Epoch 4916: train loss: 0.1883, test loss 1.1942\n",
      "Epoch 4917: train loss: 0.1883, test loss 1.1941\n",
      "Epoch 4918: train loss: 0.1883, test loss 1.1939\n",
      "Epoch 4919: train loss: 0.1883, test loss 1.1938\n",
      "Epoch 4920: train loss: 0.1883, test loss 1.1937\n",
      "Epoch 4921: train loss: 0.1883, test loss 1.1935\n",
      "Epoch 4922: train loss: 0.1883, test loss 1.1934\n",
      "Epoch 4923: train loss: 0.1883, test loss 1.1933\n",
      "Epoch 4924: train loss: 0.1883, test loss 1.1932\n",
      "Epoch 4925: train loss: 0.1883, test loss 1.1930\n",
      "Epoch 4926: train loss: 0.1883, test loss 1.1929\n",
      "Epoch 4927: train loss: 0.1883, test loss 1.1928\n",
      "Epoch 4928: train loss: 0.1883, test loss 1.1926\n",
      "Epoch 4929: train loss: 0.1883, test loss 1.1925\n",
      "Epoch 4930: train loss: 0.1883, test loss 1.1924\n",
      "Epoch 4931: train loss: 0.1883, test loss 1.1923\n",
      "Epoch 4932: train loss: 0.1883, test loss 1.1921\n",
      "Epoch 4933: train loss: 0.1883, test loss 1.1920\n",
      "Epoch 4934: train loss: 0.1883, test loss 1.1919\n",
      "Epoch 4935: train loss: 0.1883, test loss 1.1917\n",
      "Epoch 4936: train loss: 0.1883, test loss 1.1916\n",
      "Epoch 4937: train loss: 0.1883, test loss 1.1915\n",
      "Epoch 4938: train loss: 0.1883, test loss 1.1913\n",
      "Epoch 4939: train loss: 0.1883, test loss 1.1912\n",
      "Epoch 4940: train loss: 0.1883, test loss 1.1911\n",
      "Epoch 4941: train loss: 0.1882, test loss 1.1910\n",
      "Epoch 4942: train loss: 0.1882, test loss 1.1908\n",
      "Epoch 4943: train loss: 0.1882, test loss 1.1907\n",
      "Epoch 4944: train loss: 0.1882, test loss 1.1906\n",
      "Epoch 4945: train loss: 0.1882, test loss 1.1904\n",
      "Epoch 4946: train loss: 0.1882, test loss 1.1903\n",
      "Epoch 4947: train loss: 0.1882, test loss 1.1902\n",
      "Epoch 4948: train loss: 0.1882, test loss 1.1901\n",
      "Epoch 4949: train loss: 0.1882, test loss 1.1899\n",
      "Epoch 4950: train loss: 0.1882, test loss 1.1898\n",
      "Epoch 4951: train loss: 0.1882, test loss 1.1897\n",
      "Epoch 4952: train loss: 0.1882, test loss 1.1895\n",
      "Epoch 4953: train loss: 0.1882, test loss 1.1894\n",
      "Epoch 4954: train loss: 0.1882, test loss 1.1893\n",
      "Epoch 4955: train loss: 0.1882, test loss 1.1892\n",
      "Epoch 4956: train loss: 0.1882, test loss 1.1890\n",
      "Epoch 4957: train loss: 0.1882, test loss 1.1889\n",
      "Epoch 4958: train loss: 0.1882, test loss 1.1888\n",
      "Epoch 4959: train loss: 0.1882, test loss 1.1886\n",
      "Epoch 4960: train loss: 0.1882, test loss 1.1885\n",
      "Epoch 4961: train loss: 0.1882, test loss 1.1884\n",
      "Epoch 4962: train loss: 0.1882, test loss 1.1883\n",
      "Epoch 4963: train loss: 0.1882, test loss 1.1881\n",
      "Epoch 4964: train loss: 0.1882, test loss 1.1880\n",
      "Epoch 4965: train loss: 0.1882, test loss 1.1879\n",
      "Epoch 4966: train loss: 0.1882, test loss 1.1877\n",
      "Epoch 4967: train loss: 0.1882, test loss 1.1876\n",
      "Epoch 4968: train loss: 0.1882, test loss 1.1875\n",
      "Epoch 4969: train loss: 0.1882, test loss 1.1874\n",
      "Epoch 4970: train loss: 0.1881, test loss 1.1872\n",
      "Epoch 4971: train loss: 0.1881, test loss 1.1871\n",
      "Epoch 4972: train loss: 0.1881, test loss 1.1870\n",
      "Epoch 4973: train loss: 0.1881, test loss 1.1869\n",
      "Epoch 4974: train loss: 0.1881, test loss 1.1867\n",
      "Epoch 4975: train loss: 0.1881, test loss 1.1866\n",
      "Epoch 4976: train loss: 0.1881, test loss 1.1865\n",
      "Epoch 4977: train loss: 0.1881, test loss 1.1863\n",
      "Epoch 4978: train loss: 0.1881, test loss 1.1862\n",
      "Epoch 4979: train loss: 0.1881, test loss 1.1861\n",
      "Epoch 4980: train loss: 0.1881, test loss 1.1860\n",
      "Epoch 4981: train loss: 0.1881, test loss 1.1858\n",
      "Epoch 4982: train loss: 0.1881, test loss 1.1857\n",
      "Epoch 4983: train loss: 0.1881, test loss 1.1856\n",
      "Epoch 4984: train loss: 0.1881, test loss 1.1854\n",
      "Epoch 4985: train loss: 0.1881, test loss 1.1853\n",
      "Epoch 4986: train loss: 0.1881, test loss 1.1852\n",
      "Epoch 4987: train loss: 0.1881, test loss 1.1851\n",
      "Epoch 4988: train loss: 0.1881, test loss 1.1849\n",
      "Epoch 4989: train loss: 0.1881, test loss 1.1848\n",
      "Epoch 4990: train loss: 0.1881, test loss 1.1847\n",
      "Epoch 4991: train loss: 0.1881, test loss 1.1845\n",
      "Epoch 4992: train loss: 0.1881, test loss 1.1844\n",
      "Epoch 4993: train loss: 0.1881, test loss 1.1843\n",
      "Epoch 4994: train loss: 0.1881, test loss 1.1841\n",
      "Epoch 4995: train loss: 0.1881, test loss 1.1840\n",
      "Epoch 4996: train loss: 0.1881, test loss 1.1839\n",
      "Epoch 4997: train loss: 0.1881, test loss 1.1837\n",
      "Epoch 4998: train loss: 0.1881, test loss 1.1836\n",
      "Epoch 4999: train loss: 0.1881, test loss 1.1835\n",
      "Epoch 5000: train loss: 0.1880, test loss 1.1833\n",
      "Epoch 5001: train loss: 0.1880, test loss 1.1832\n",
      "Epoch 5002: train loss: 0.1880, test loss 1.1831\n",
      "Epoch 5003: train loss: 0.1880, test loss 1.1829\n",
      "Epoch 5004: train loss: 0.1880, test loss 1.1828\n",
      "Epoch 5005: train loss: 0.1880, test loss 1.1827\n",
      "Epoch 5006: train loss: 0.1880, test loss 1.1825\n",
      "Epoch 5007: train loss: 0.1880, test loss 1.1824\n",
      "Epoch 5008: train loss: 0.1880, test loss 1.1822\n",
      "Epoch 5009: train loss: 0.1880, test loss 1.1821\n",
      "Epoch 5010: train loss: 0.1880, test loss 1.1820\n",
      "Epoch 5011: train loss: 0.1880, test loss 1.1818\n",
      "Epoch 5012: train loss: 0.1880, test loss 1.1817\n",
      "Epoch 5013: train loss: 0.1880, test loss 1.1816\n",
      "Epoch 5014: train loss: 0.1880, test loss 1.1814\n",
      "Epoch 5015: train loss: 0.1880, test loss 1.1813\n",
      "Epoch 5016: train loss: 0.1880, test loss 1.1812\n",
      "Epoch 5017: train loss: 0.1880, test loss 1.1810\n",
      "Epoch 5018: train loss: 0.1880, test loss 1.1809\n",
      "Epoch 5019: train loss: 0.1880, test loss 1.1808\n",
      "Epoch 5020: train loss: 0.1880, test loss 1.1807\n",
      "Epoch 5021: train loss: 0.1880, test loss 1.1805\n",
      "Epoch 5022: train loss: 0.1880, test loss 1.1804\n",
      "Epoch 5023: train loss: 0.1880, test loss 1.1803\n",
      "Epoch 5024: train loss: 0.1880, test loss 1.1801\n",
      "Epoch 5025: train loss: 0.1880, test loss 1.1800\n",
      "Epoch 5026: train loss: 0.1880, test loss 1.1799\n",
      "Epoch 5027: train loss: 0.1880, test loss 1.1797\n",
      "Epoch 5028: train loss: 0.1879, test loss 1.1796\n",
      "Epoch 5029: train loss: 0.1879, test loss 1.1795\n",
      "Epoch 5030: train loss: 0.1879, test loss 1.1793\n",
      "Epoch 5031: train loss: 0.1879, test loss 1.1792\n",
      "Epoch 5032: train loss: 0.1879, test loss 1.1791\n",
      "Epoch 5033: train loss: 0.1879, test loss 1.1789\n",
      "Epoch 5034: train loss: 0.1879, test loss 1.1788\n",
      "Epoch 5035: train loss: 0.1879, test loss 1.1787\n",
      "Epoch 5036: train loss: 0.1879, test loss 1.1785\n",
      "Epoch 5037: train loss: 0.1879, test loss 1.1784\n",
      "Epoch 5038: train loss: 0.1879, test loss 1.1783\n",
      "Epoch 5039: train loss: 0.1879, test loss 1.1781\n",
      "Epoch 5040: train loss: 0.1879, test loss 1.1780\n",
      "Epoch 5041: train loss: 0.1879, test loss 1.1779\n",
      "Epoch 5042: train loss: 0.1879, test loss 1.1777\n",
      "Epoch 5043: train loss: 0.1879, test loss 1.1776\n",
      "Epoch 5044: train loss: 0.1879, test loss 1.1775\n",
      "Epoch 5045: train loss: 0.1879, test loss 1.1773\n",
      "Epoch 5046: train loss: 0.1879, test loss 1.1772\n",
      "Epoch 5047: train loss: 0.1879, test loss 1.1771\n",
      "Epoch 5048: train loss: 0.1879, test loss 1.1769\n",
      "Epoch 5049: train loss: 0.1879, test loss 1.1768\n",
      "Epoch 5050: train loss: 0.1879, test loss 1.1767\n",
      "Epoch 5051: train loss: 0.1879, test loss 1.1765\n",
      "Epoch 5052: train loss: 0.1879, test loss 1.1764\n",
      "Epoch 5053: train loss: 0.1879, test loss 1.1763\n",
      "Epoch 5054: train loss: 0.1879, test loss 1.1762\n",
      "Epoch 5055: train loss: 0.1879, test loss 1.1760\n",
      "Epoch 5056: train loss: 0.1879, test loss 1.1759\n",
      "Epoch 5057: train loss: 0.1878, test loss 1.1758\n",
      "Epoch 5058: train loss: 0.1878, test loss 1.1756\n",
      "Epoch 5059: train loss: 0.1878, test loss 1.1755\n",
      "Epoch 5060: train loss: 0.1878, test loss 1.1754\n",
      "Epoch 5061: train loss: 0.1878, test loss 1.1752\n",
      "Epoch 5062: train loss: 0.1878, test loss 1.1751\n",
      "Epoch 5063: train loss: 0.1878, test loss 1.1750\n",
      "Epoch 5064: train loss: 0.1878, test loss 1.1748\n",
      "Epoch 5065: train loss: 0.1878, test loss 1.1747\n",
      "Epoch 5066: train loss: 0.1878, test loss 1.1746\n",
      "Epoch 5067: train loss: 0.1878, test loss 1.1745\n",
      "Epoch 5068: train loss: 0.1878, test loss 1.1743\n",
      "Epoch 5069: train loss: 0.1878, test loss 1.1742\n",
      "Epoch 5070: train loss: 0.1878, test loss 1.1741\n",
      "Epoch 5071: train loss: 0.1878, test loss 1.1739\n",
      "Epoch 5072: train loss: 0.1878, test loss 1.1738\n",
      "Epoch 5073: train loss: 0.1878, test loss 1.1737\n",
      "Epoch 5074: train loss: 0.1878, test loss 1.1735\n",
      "Epoch 5075: train loss: 0.1878, test loss 1.1734\n",
      "Epoch 5076: train loss: 0.1878, test loss 1.1733\n",
      "Epoch 5077: train loss: 0.1878, test loss 1.1732\n",
      "Epoch 5078: train loss: 0.1878, test loss 1.1730\n",
      "Epoch 5079: train loss: 0.1878, test loss 1.1729\n",
      "Epoch 5080: train loss: 0.1878, test loss 1.1728\n",
      "Epoch 5081: train loss: 0.1878, test loss 1.1726\n",
      "Epoch 5082: train loss: 0.1878, test loss 1.1725\n",
      "Epoch 5083: train loss: 0.1878, test loss 1.1724\n",
      "Epoch 5084: train loss: 0.1878, test loss 1.1723\n",
      "Epoch 5085: train loss: 0.1878, test loss 1.1721\n",
      "Epoch 5086: train loss: 0.1878, test loss 1.1720\n",
      "Epoch 5087: train loss: 0.1877, test loss 1.1719\n",
      "Epoch 5088: train loss: 0.1877, test loss 1.1717\n",
      "Epoch 5089: train loss: 0.1877, test loss 1.1716\n",
      "Epoch 5090: train loss: 0.1877, test loss 1.1715\n",
      "Epoch 5091: train loss: 0.1877, test loss 1.1714\n",
      "Epoch 5092: train loss: 0.1877, test loss 1.1712\n",
      "Epoch 5093: train loss: 0.1877, test loss 1.1711\n",
      "Epoch 5094: train loss: 0.1877, test loss 1.1710\n",
      "Epoch 5095: train loss: 0.1877, test loss 1.1708\n",
      "Epoch 5096: train loss: 0.1877, test loss 1.1707\n",
      "Epoch 5097: train loss: 0.1877, test loss 1.1706\n",
      "Epoch 5098: train loss: 0.1877, test loss 1.1705\n",
      "Epoch 5099: train loss: 0.1877, test loss 1.1703\n",
      "Epoch 5100: train loss: 0.1877, test loss 1.1702\n",
      "Epoch 5101: train loss: 0.1877, test loss 1.1701\n",
      "Epoch 5102: train loss: 0.1877, test loss 1.1699\n",
      "Epoch 5103: train loss: 0.1877, test loss 1.1698\n",
      "Epoch 5104: train loss: 0.1877, test loss 1.1697\n",
      "Epoch 5105: train loss: 0.1877, test loss 1.1696\n",
      "Epoch 5106: train loss: 0.1877, test loss 1.1694\n",
      "Epoch 5107: train loss: 0.1877, test loss 1.1693\n",
      "Epoch 5108: train loss: 0.1877, test loss 1.1692\n",
      "Epoch 5109: train loss: 0.1877, test loss 1.1690\n",
      "Epoch 5110: train loss: 0.1877, test loss 1.1689\n",
      "Epoch 5111: train loss: 0.1877, test loss 1.1688\n",
      "Epoch 5112: train loss: 0.1877, test loss 1.1687\n",
      "Epoch 5113: train loss: 0.1877, test loss 1.1685\n",
      "Epoch 5114: train loss: 0.1877, test loss 1.1684\n",
      "Epoch 5115: train loss: 0.1877, test loss 1.1683\n",
      "Epoch 5116: train loss: 0.1876, test loss 1.1681\n",
      "Epoch 5117: train loss: 0.1876, test loss 1.1680\n",
      "Epoch 5118: train loss: 0.1876, test loss 1.1679\n",
      "Epoch 5119: train loss: 0.1876, test loss 1.1678\n",
      "Epoch 5120: train loss: 0.1876, test loss 1.1676\n",
      "Epoch 5121: train loss: 0.1876, test loss 1.1675\n",
      "Epoch 5122: train loss: 0.1876, test loss 1.1674\n",
      "Epoch 5123: train loss: 0.1876, test loss 1.1672\n",
      "Epoch 5124: train loss: 0.1876, test loss 1.1671\n",
      "Epoch 5125: train loss: 0.1876, test loss 1.1670\n",
      "Epoch 5126: train loss: 0.1876, test loss 1.1668\n",
      "Epoch 5127: train loss: 0.1876, test loss 1.1667\n",
      "Epoch 5128: train loss: 0.1876, test loss 1.1666\n",
      "Epoch 5129: train loss: 0.1876, test loss 1.1665\n",
      "Epoch 5130: train loss: 0.1876, test loss 1.1663\n",
      "Epoch 5131: train loss: 0.1876, test loss 1.1662\n",
      "Epoch 5132: train loss: 0.1876, test loss 1.1661\n",
      "Epoch 5133: train loss: 0.1876, test loss 1.1659\n",
      "Epoch 5134: train loss: 0.1876, test loss 1.1658\n",
      "Epoch 5135: train loss: 0.1876, test loss 1.1657\n",
      "Epoch 5136: train loss: 0.1876, test loss 1.1655\n",
      "Epoch 5137: train loss: 0.1876, test loss 1.1654\n",
      "Epoch 5138: train loss: 0.1876, test loss 1.1653\n",
      "Epoch 5139: train loss: 0.1876, test loss 1.1652\n",
      "Epoch 5140: train loss: 0.1876, test loss 1.1650\n",
      "Epoch 5141: train loss: 0.1876, test loss 1.1649\n",
      "Epoch 5142: train loss: 0.1876, test loss 1.1648\n",
      "Epoch 5143: train loss: 0.1876, test loss 1.1646\n",
      "Epoch 5144: train loss: 0.1876, test loss 1.1645\n",
      "Epoch 5145: train loss: 0.1876, test loss 1.1644\n",
      "Epoch 5146: train loss: 0.1875, test loss 1.1643\n",
      "Epoch 5147: train loss: 0.1875, test loss 1.1641\n",
      "Epoch 5148: train loss: 0.1875, test loss 1.1640\n",
      "Epoch 5149: train loss: 0.1875, test loss 1.1639\n",
      "Epoch 5150: train loss: 0.1875, test loss 1.1637\n",
      "Epoch 5151: train loss: 0.1875, test loss 1.1636\n",
      "Epoch 5152: train loss: 0.1875, test loss 1.1635\n",
      "Epoch 5153: train loss: 0.1875, test loss 1.1634\n",
      "Epoch 5154: train loss: 0.1875, test loss 1.1632\n",
      "Epoch 5155: train loss: 0.1875, test loss 1.1631\n",
      "Epoch 5156: train loss: 0.1875, test loss 1.1630\n",
      "Epoch 5157: train loss: 0.1875, test loss 1.1628\n",
      "Epoch 5158: train loss: 0.1875, test loss 1.1627\n",
      "Epoch 5159: train loss: 0.1875, test loss 1.1626\n",
      "Epoch 5160: train loss: 0.1875, test loss 1.1625\n",
      "Epoch 5161: train loss: 0.1875, test loss 1.1623\n",
      "Epoch 5162: train loss: 0.1875, test loss 1.1622\n",
      "Epoch 5163: train loss: 0.1875, test loss 1.1621\n",
      "Epoch 5164: train loss: 0.1875, test loss 1.1619\n",
      "Epoch 5165: train loss: 0.1875, test loss 1.1618\n",
      "Epoch 5166: train loss: 0.1875, test loss 1.1617\n",
      "Epoch 5167: train loss: 0.1875, test loss 1.1616\n",
      "Epoch 5168: train loss: 0.1875, test loss 1.1614\n",
      "Epoch 5169: train loss: 0.1875, test loss 1.1613\n",
      "Epoch 5170: train loss: 0.1875, test loss 1.1612\n",
      "Epoch 5171: train loss: 0.1875, test loss 1.1611\n",
      "Epoch 5172: train loss: 0.1875, test loss 1.1609\n",
      "Epoch 5173: train loss: 0.1875, test loss 1.1608\n",
      "Epoch 5174: train loss: 0.1875, test loss 1.1607\n",
      "Epoch 5175: train loss: 0.1874, test loss 1.1605\n",
      "Epoch 5176: train loss: 0.1874, test loss 1.1604\n",
      "Epoch 5177: train loss: 0.1874, test loss 1.1603\n",
      "Epoch 5178: train loss: 0.1874, test loss 1.1602\n",
      "Epoch 5179: train loss: 0.1874, test loss 1.1600\n",
      "Epoch 5180: train loss: 0.1874, test loss 1.1599\n",
      "Epoch 5181: train loss: 0.1874, test loss 1.1598\n",
      "Epoch 5182: train loss: 0.1874, test loss 1.1597\n",
      "Epoch 5183: train loss: 0.1874, test loss 1.1595\n",
      "Epoch 5184: train loss: 0.1874, test loss 1.1594\n",
      "Epoch 5185: train loss: 0.1874, test loss 1.1593\n",
      "Epoch 5186: train loss: 0.1874, test loss 1.1591\n",
      "Epoch 5187: train loss: 0.1874, test loss 1.1590\n",
      "Epoch 5188: train loss: 0.1874, test loss 1.1589\n",
      "Epoch 5189: train loss: 0.1874, test loss 1.1587\n",
      "Epoch 5190: train loss: 0.1874, test loss 1.1586\n",
      "Epoch 5191: train loss: 0.1874, test loss 1.1585\n",
      "Epoch 5192: train loss: 0.1874, test loss 1.1584\n",
      "Epoch 5193: train loss: 0.1874, test loss 1.1582\n",
      "Epoch 5194: train loss: 0.1874, test loss 1.1581\n",
      "Epoch 5195: train loss: 0.1874, test loss 1.1580\n",
      "Epoch 5196: train loss: 0.1874, test loss 1.1578\n",
      "Epoch 5197: train loss: 0.1874, test loss 1.1577\n",
      "Epoch 5198: train loss: 0.1874, test loss 1.1576\n",
      "Epoch 5199: train loss: 0.1874, test loss 1.1575\n",
      "Epoch 5200: train loss: 0.1874, test loss 1.1573\n",
      "Epoch 5201: train loss: 0.1874, test loss 1.1572\n",
      "Epoch 5202: train loss: 0.1874, test loss 1.1571\n",
      "Epoch 5203: train loss: 0.1874, test loss 1.1570\n",
      "Epoch 5204: train loss: 0.1874, test loss 1.1568\n",
      "Epoch 5205: train loss: 0.1873, test loss 1.1567\n",
      "Epoch 5206: train loss: 0.1873, test loss 1.1566\n",
      "Epoch 5207: train loss: 0.1873, test loss 1.1564\n",
      "Epoch 5208: train loss: 0.1873, test loss 1.1563\n",
      "Epoch 5209: train loss: 0.1873, test loss 1.1562\n",
      "Epoch 5210: train loss: 0.1873, test loss 1.1561\n",
      "Epoch 5211: train loss: 0.1873, test loss 1.1559\n",
      "Epoch 5212: train loss: 0.1873, test loss 1.1558\n",
      "Epoch 5213: train loss: 0.1873, test loss 1.1557\n",
      "Epoch 5214: train loss: 0.1873, test loss 1.1556\n",
      "Epoch 5215: train loss: 0.1873, test loss 1.1554\n",
      "Epoch 5216: train loss: 0.1873, test loss 1.1553\n",
      "Epoch 5217: train loss: 0.1873, test loss 1.1552\n",
      "Epoch 5218: train loss: 0.1873, test loss 1.1551\n",
      "Epoch 5219: train loss: 0.1873, test loss 1.1549\n",
      "Epoch 5220: train loss: 0.1873, test loss 1.1548\n",
      "Epoch 5221: train loss: 0.1873, test loss 1.1547\n",
      "Epoch 5222: train loss: 0.1873, test loss 1.1546\n",
      "Epoch 5223: train loss: 0.1873, test loss 1.1544\n",
      "Epoch 5224: train loss: 0.1873, test loss 1.1543\n",
      "Epoch 5225: train loss: 0.1873, test loss 1.1542\n",
      "Epoch 5226: train loss: 0.1873, test loss 1.1540\n",
      "Epoch 5227: train loss: 0.1873, test loss 1.1539\n",
      "Epoch 5228: train loss: 0.1873, test loss 1.1538\n",
      "Epoch 5229: train loss: 0.1873, test loss 1.1537\n",
      "Epoch 5230: train loss: 0.1873, test loss 1.1535\n",
      "Epoch 5231: train loss: 0.1873, test loss 1.1534\n",
      "Epoch 5232: train loss: 0.1873, test loss 1.1533\n",
      "Epoch 5233: train loss: 0.1873, test loss 1.1532\n",
      "Epoch 5234: train loss: 0.1873, test loss 1.1530\n",
      "Epoch 5235: train loss: 0.1872, test loss 1.1529\n",
      "Epoch 5236: train loss: 0.1872, test loss 1.1528\n",
      "Epoch 5237: train loss: 0.1872, test loss 1.1527\n",
      "Epoch 5238: train loss: 0.1872, test loss 1.1525\n",
      "Epoch 5239: train loss: 0.1872, test loss 1.1524\n",
      "Epoch 5240: train loss: 0.1872, test loss 1.1523\n",
      "Epoch 5241: train loss: 0.1872, test loss 1.1522\n",
      "Epoch 5242: train loss: 0.1872, test loss 1.1520\n",
      "Epoch 5243: train loss: 0.1872, test loss 1.1519\n",
      "Epoch 5244: train loss: 0.1872, test loss 1.1518\n",
      "Epoch 5245: train loss: 0.1872, test loss 1.1517\n",
      "Epoch 5246: train loss: 0.1872, test loss 1.1516\n",
      "Epoch 5247: train loss: 0.1872, test loss 1.1514\n",
      "Epoch 5248: train loss: 0.1872, test loss 1.1513\n",
      "Epoch 5249: train loss: 0.1872, test loss 1.1512\n",
      "Epoch 5250: train loss: 0.1872, test loss 1.1511\n",
      "Epoch 5251: train loss: 0.1872, test loss 1.1509\n",
      "Epoch 5252: train loss: 0.1872, test loss 1.1508\n",
      "Epoch 5253: train loss: 0.1872, test loss 1.1507\n",
      "Epoch 5254: train loss: 0.1872, test loss 1.1506\n",
      "Epoch 5255: train loss: 0.1872, test loss 1.1504\n",
      "Epoch 5256: train loss: 0.1872, test loss 1.1503\n",
      "Epoch 5257: train loss: 0.1872, test loss 1.1502\n",
      "Epoch 5258: train loss: 0.1872, test loss 1.1501\n",
      "Epoch 5259: train loss: 0.1872, test loss 1.1500\n",
      "Epoch 5260: train loss: 0.1872, test loss 1.1498\n",
      "Epoch 5261: train loss: 0.1872, test loss 1.1497\n",
      "Epoch 5262: train loss: 0.1872, test loss 1.1496\n",
      "Epoch 5263: train loss: 0.1872, test loss 1.1495\n",
      "Epoch 5264: train loss: 0.1872, test loss 1.1493\n",
      "Epoch 5265: train loss: 0.1871, test loss 1.1492\n",
      "Epoch 5266: train loss: 0.1871, test loss 1.1491\n",
      "Epoch 5267: train loss: 0.1871, test loss 1.1490\n",
      "Epoch 5268: train loss: 0.1871, test loss 1.1489\n",
      "Epoch 5269: train loss: 0.1871, test loss 1.1487\n",
      "Epoch 5270: train loss: 0.1871, test loss 1.1486\n",
      "Epoch 5271: train loss: 0.1871, test loss 1.1485\n",
      "Epoch 5272: train loss: 0.1871, test loss 1.1484\n",
      "Epoch 5273: train loss: 0.1871, test loss 1.1482\n",
      "Epoch 5274: train loss: 0.1871, test loss 1.1481\n",
      "Epoch 5275: train loss: 0.1871, test loss 1.1480\n",
      "Epoch 5276: train loss: 0.1871, test loss 1.1479\n",
      "Epoch 5277: train loss: 0.1871, test loss 1.1478\n",
      "Epoch 5278: train loss: 0.1871, test loss 1.1476\n",
      "Epoch 5279: train loss: 0.1871, test loss 1.1475\n",
      "Epoch 5280: train loss: 0.1871, test loss 1.1474\n",
      "Epoch 5281: train loss: 0.1871, test loss 1.1473\n",
      "Epoch 5282: train loss: 0.1871, test loss 1.1471\n",
      "Epoch 5283: train loss: 0.1871, test loss 1.1470\n",
      "Epoch 5284: train loss: 0.1871, test loss 1.1469\n",
      "Epoch 5285: train loss: 0.1871, test loss 1.1468\n",
      "Epoch 5286: train loss: 0.1871, test loss 1.1467\n",
      "Epoch 5287: train loss: 0.1871, test loss 1.1465\n",
      "Epoch 5288: train loss: 0.1871, test loss 1.1464\n",
      "Epoch 5289: train loss: 0.1871, test loss 1.1463\n",
      "Epoch 5290: train loss: 0.1871, test loss 1.1462\n",
      "Epoch 5291: train loss: 0.1871, test loss 1.1460\n",
      "Epoch 5292: train loss: 0.1871, test loss 1.1459\n",
      "Epoch 5293: train loss: 0.1871, test loss 1.1458\n",
      "Epoch 5294: train loss: 0.1871, test loss 1.1457\n",
      "Epoch 5295: train loss: 0.1870, test loss 1.1456\n",
      "Epoch 5296: train loss: 0.1870, test loss 1.1454\n",
      "Epoch 5297: train loss: 0.1870, test loss 1.1453\n",
      "Epoch 5298: train loss: 0.1870, test loss 1.1452\n",
      "Epoch 5299: train loss: 0.1870, test loss 1.1451\n",
      "Epoch 5300: train loss: 0.1870, test loss 1.1449\n",
      "Epoch 5301: train loss: 0.1870, test loss 1.1448\n",
      "Epoch 5302: train loss: 0.1870, test loss 1.1447\n",
      "Epoch 5303: train loss: 0.1870, test loss 1.1446\n",
      "Epoch 5304: train loss: 0.1870, test loss 1.1445\n",
      "Epoch 5305: train loss: 0.1870, test loss 1.1443\n",
      "Epoch 5306: train loss: 0.1870, test loss 1.1442\n",
      "Epoch 5307: train loss: 0.1870, test loss 1.1441\n",
      "Epoch 5308: train loss: 0.1870, test loss 1.1440\n",
      "Epoch 5309: train loss: 0.1870, test loss 1.1438\n",
      "Epoch 5310: train loss: 0.1870, test loss 1.1437\n",
      "Epoch 5311: train loss: 0.1870, test loss 1.1436\n",
      "Epoch 5312: train loss: 0.1870, test loss 1.1435\n",
      "Epoch 5313: train loss: 0.1870, test loss 1.1434\n",
      "Epoch 5314: train loss: 0.1870, test loss 1.1432\n",
      "Epoch 5315: train loss: 0.1870, test loss 1.1431\n",
      "Epoch 5316: train loss: 0.1870, test loss 1.1430\n",
      "Epoch 5317: train loss: 0.1870, test loss 1.1429\n",
      "Epoch 5318: train loss: 0.1870, test loss 1.1427\n",
      "Epoch 5319: train loss: 0.1870, test loss 1.1426\n",
      "Epoch 5320: train loss: 0.1870, test loss 1.1425\n",
      "Epoch 5321: train loss: 0.1870, test loss 1.1424\n",
      "Epoch 5322: train loss: 0.1870, test loss 1.1423\n",
      "Epoch 5323: train loss: 0.1870, test loss 1.1421\n",
      "Epoch 5324: train loss: 0.1870, test loss 1.1420\n",
      "Epoch 5325: train loss: 0.1870, test loss 1.1419\n",
      "Epoch 5326: train loss: 0.1869, test loss 1.1418\n",
      "Epoch 5327: train loss: 0.1869, test loss 1.1417\n",
      "Epoch 5328: train loss: 0.1869, test loss 1.1415\n",
      "Epoch 5329: train loss: 0.1869, test loss 1.1414\n",
      "Epoch 5330: train loss: 0.1869, test loss 1.1413\n",
      "Epoch 5331: train loss: 0.1869, test loss 1.1412\n",
      "Epoch 5332: train loss: 0.1869, test loss 1.1411\n",
      "Epoch 5333: train loss: 0.1869, test loss 1.1410\n",
      "Epoch 5334: train loss: 0.1869, test loss 1.1408\n",
      "Epoch 5335: train loss: 0.1869, test loss 1.1407\n",
      "Epoch 5336: train loss: 0.1869, test loss 1.1406\n",
      "Epoch 5337: train loss: 0.1869, test loss 1.1405\n",
      "Epoch 5338: train loss: 0.1869, test loss 1.1404\n",
      "Epoch 5339: train loss: 0.1869, test loss 1.1402\n",
      "Epoch 5340: train loss: 0.1869, test loss 1.1401\n",
      "Epoch 5341: train loss: 0.1869, test loss 1.1400\n",
      "Epoch 5342: train loss: 0.1869, test loss 1.1399\n",
      "Epoch 5343: train loss: 0.1869, test loss 1.1398\n",
      "Epoch 5344: train loss: 0.1869, test loss 1.1397\n",
      "Epoch 5345: train loss: 0.1869, test loss 1.1395\n",
      "Epoch 5346: train loss: 0.1869, test loss 1.1394\n",
      "Epoch 5347: train loss: 0.1869, test loss 1.1393\n",
      "Epoch 5348: train loss: 0.1869, test loss 1.1392\n",
      "Epoch 5349: train loss: 0.1869, test loss 1.1391\n",
      "Epoch 5350: train loss: 0.1869, test loss 1.1390\n",
      "Epoch 5351: train loss: 0.1869, test loss 1.1388\n",
      "Epoch 5352: train loss: 0.1869, test loss 1.1387\n",
      "Epoch 5353: train loss: 0.1869, test loss 1.1386\n",
      "Epoch 5354: train loss: 0.1869, test loss 1.1385\n",
      "Epoch 5355: train loss: 0.1869, test loss 1.1384\n",
      "Epoch 5356: train loss: 0.1869, test loss 1.1382\n",
      "Epoch 5357: train loss: 0.1869, test loss 1.1381\n",
      "Epoch 5358: train loss: 0.1868, test loss 1.1380\n",
      "Epoch 5359: train loss: 0.1868, test loss 1.1379\n",
      "Epoch 5360: train loss: 0.1868, test loss 1.1378\n",
      "Epoch 5361: train loss: 0.1868, test loss 1.1377\n",
      "Epoch 5362: train loss: 0.1868, test loss 1.1375\n",
      "Epoch 5363: train loss: 0.1868, test loss 1.1374\n",
      "Epoch 5364: train loss: 0.1868, test loss 1.1373\n",
      "Epoch 5365: train loss: 0.1868, test loss 1.1372\n",
      "Epoch 5366: train loss: 0.1868, test loss 1.1371\n",
      "Epoch 5367: train loss: 0.1868, test loss 1.1370\n",
      "Epoch 5368: train loss: 0.1868, test loss 1.1368\n",
      "Epoch 5369: train loss: 0.1868, test loss 1.1367\n",
      "Epoch 5370: train loss: 0.1868, test loss 1.1366\n",
      "Epoch 5371: train loss: 0.1868, test loss 1.1365\n",
      "Epoch 5372: train loss: 0.1868, test loss 1.1364\n",
      "Epoch 5373: train loss: 0.1868, test loss 1.1363\n",
      "Epoch 5374: train loss: 0.1868, test loss 1.1361\n",
      "Epoch 5375: train loss: 0.1868, test loss 1.1360\n",
      "Epoch 5376: train loss: 0.1868, test loss 1.1359\n",
      "Epoch 5377: train loss: 0.1868, test loss 1.1358\n",
      "Epoch 5378: train loss: 0.1868, test loss 1.1357\n",
      "Epoch 5379: train loss: 0.1868, test loss 1.1356\n",
      "Epoch 5380: train loss: 0.1868, test loss 1.1354\n",
      "Epoch 5381: train loss: 0.1868, test loss 1.1353\n",
      "Epoch 5382: train loss: 0.1868, test loss 1.1352\n",
      "Epoch 5383: train loss: 0.1868, test loss 1.1351\n",
      "Epoch 5384: train loss: 0.1868, test loss 1.1350\n",
      "Epoch 5385: train loss: 0.1868, test loss 1.1349\n",
      "Epoch 5386: train loss: 0.1868, test loss 1.1348\n",
      "Epoch 5387: train loss: 0.1868, test loss 1.1346\n",
      "Epoch 5388: train loss: 0.1868, test loss 1.1345\n",
      "Epoch 5389: train loss: 0.1867, test loss 1.1344\n",
      "Epoch 5390: train loss: 0.1867, test loss 1.1343\n",
      "Epoch 5391: train loss: 0.1867, test loss 1.1342\n",
      "Epoch 5392: train loss: 0.1867, test loss 1.1341\n",
      "Epoch 5393: train loss: 0.1867, test loss 1.1339\n",
      "Epoch 5394: train loss: 0.1867, test loss 1.1338\n",
      "Epoch 5395: train loss: 0.1867, test loss 1.1337\n",
      "Epoch 5396: train loss: 0.1867, test loss 1.1336\n",
      "Epoch 5397: train loss: 0.1867, test loss 1.1335\n",
      "Epoch 5398: train loss: 0.1867, test loss 1.1334\n",
      "Epoch 5399: train loss: 0.1867, test loss 1.1333\n",
      "Epoch 5400: train loss: 0.1867, test loss 1.1331\n",
      "Epoch 5401: train loss: 0.1867, test loss 1.1330\n",
      "Epoch 5402: train loss: 0.1867, test loss 1.1329\n",
      "Epoch 5403: train loss: 0.1867, test loss 1.1328\n",
      "Epoch 5404: train loss: 0.1867, test loss 1.1327\n",
      "Epoch 5405: train loss: 0.1867, test loss 1.1326\n",
      "Epoch 5406: train loss: 0.1867, test loss 1.1324\n",
      "Epoch 5407: train loss: 0.1867, test loss 1.1323\n",
      "Epoch 5408: train loss: 0.1867, test loss 1.1322\n",
      "Epoch 5409: train loss: 0.1867, test loss 1.1321\n",
      "Epoch 5410: train loss: 0.1867, test loss 1.1320\n",
      "Epoch 5411: train loss: 0.1867, test loss 1.1319\n",
      "Epoch 5412: train loss: 0.1867, test loss 1.1318\n",
      "Epoch 5413: train loss: 0.1867, test loss 1.1316\n",
      "Epoch 5414: train loss: 0.1867, test loss 1.1315\n",
      "Epoch 5415: train loss: 0.1867, test loss 1.1314\n",
      "Epoch 5416: train loss: 0.1867, test loss 1.1313\n",
      "Epoch 5417: train loss: 0.1867, test loss 1.1312\n",
      "Epoch 5418: train loss: 0.1867, test loss 1.1311\n",
      "Epoch 5419: train loss: 0.1867, test loss 1.1310\n",
      "Epoch 5420: train loss: 0.1867, test loss 1.1308\n",
      "Epoch 5421: train loss: 0.1867, test loss 1.1307\n",
      "Epoch 5422: train loss: 0.1866, test loss 1.1306\n",
      "Epoch 5423: train loss: 0.1866, test loss 1.1305\n",
      "Epoch 5424: train loss: 0.1866, test loss 1.1304\n",
      "Epoch 5425: train loss: 0.1866, test loss 1.1303\n",
      "Epoch 5426: train loss: 0.1866, test loss 1.1302\n",
      "Epoch 5427: train loss: 0.1866, test loss 1.1300\n",
      "Epoch 5428: train loss: 0.1866, test loss 1.1299\n",
      "Epoch 5429: train loss: 0.1866, test loss 1.1298\n",
      "Epoch 5430: train loss: 0.1866, test loss 1.1297\n",
      "Epoch 5431: train loss: 0.1866, test loss 1.1296\n",
      "Epoch 5432: train loss: 0.1866, test loss 1.1295\n",
      "Epoch 5433: train loss: 0.1866, test loss 1.1294\n",
      "Epoch 5434: train loss: 0.1866, test loss 1.1292\n",
      "Epoch 5435: train loss: 0.1866, test loss 1.1291\n",
      "Epoch 5436: train loss: 0.1866, test loss 1.1290\n",
      "Epoch 5437: train loss: 0.1866, test loss 1.1289\n",
      "Epoch 5438: train loss: 0.1866, test loss 1.1288\n",
      "Epoch 5439: train loss: 0.1866, test loss 1.1287\n",
      "Epoch 5440: train loss: 0.1866, test loss 1.1286\n",
      "Epoch 5441: train loss: 0.1866, test loss 1.1285\n",
      "Epoch 5442: train loss: 0.1866, test loss 1.1283\n",
      "Epoch 5443: train loss: 0.1866, test loss 1.1282\n",
      "Epoch 5444: train loss: 0.1866, test loss 1.1281\n",
      "Epoch 5445: train loss: 0.1866, test loss 1.1280\n",
      "Epoch 5446: train loss: 0.1866, test loss 1.1279\n",
      "Epoch 5447: train loss: 0.1866, test loss 1.1278\n",
      "Epoch 5448: train loss: 0.1866, test loss 1.1277\n",
      "Epoch 5449: train loss: 0.1866, test loss 1.1276\n",
      "Epoch 5450: train loss: 0.1866, test loss 1.1274\n",
      "Epoch 5451: train loss: 0.1866, test loss 1.1273\n",
      "Epoch 5452: train loss: 0.1866, test loss 1.1272\n",
      "Epoch 5453: train loss: 0.1866, test loss 1.1271\n",
      "Epoch 5454: train loss: 0.1865, test loss 1.1270\n",
      "Epoch 5455: train loss: 0.1865, test loss 1.1269\n",
      "Epoch 5456: train loss: 0.1865, test loss 1.1268\n",
      "Epoch 5457: train loss: 0.1865, test loss 1.1267\n",
      "Epoch 5458: train loss: 0.1865, test loss 1.1265\n",
      "Epoch 5459: train loss: 0.1865, test loss 1.1264\n",
      "Epoch 5460: train loss: 0.1865, test loss 1.1263\n",
      "Epoch 5461: train loss: 0.1865, test loss 1.1262\n",
      "Epoch 5462: train loss: 0.1865, test loss 1.1261\n",
      "Epoch 5463: train loss: 0.1865, test loss 1.1260\n",
      "Epoch 5464: train loss: 0.1865, test loss 1.1259\n",
      "Epoch 5465: train loss: 0.1865, test loss 1.1258\n",
      "Epoch 5466: train loss: 0.1865, test loss 1.1256\n",
      "Epoch 5467: train loss: 0.1865, test loss 1.1255\n",
      "Epoch 5468: train loss: 0.1865, test loss 1.1254\n",
      "Epoch 5469: train loss: 0.1865, test loss 1.1253\n",
      "Epoch 5470: train loss: 0.1865, test loss 1.1252\n",
      "Epoch 5471: train loss: 0.1865, test loss 1.1251\n",
      "Epoch 5472: train loss: 0.1865, test loss 1.1250\n",
      "Epoch 5473: train loss: 0.1865, test loss 1.1249\n",
      "Epoch 5474: train loss: 0.1865, test loss 1.1248\n",
      "Epoch 5475: train loss: 0.1865, test loss 1.1246\n",
      "Epoch 5476: train loss: 0.1865, test loss 1.1245\n",
      "Epoch 5477: train loss: 0.1865, test loss 1.1244\n",
      "Epoch 5478: train loss: 0.1865, test loss 1.1243\n",
      "Epoch 5479: train loss: 0.1865, test loss 1.1242\n",
      "Epoch 5480: train loss: 0.1865, test loss 1.1241\n",
      "Epoch 5481: train loss: 0.1865, test loss 1.1240\n",
      "Epoch 5482: train loss: 0.1865, test loss 1.1239\n",
      "Epoch 5483: train loss: 0.1865, test loss 1.1238\n",
      "Epoch 5484: train loss: 0.1865, test loss 1.1236\n",
      "Epoch 5485: train loss: 0.1865, test loss 1.1235\n",
      "Epoch 5486: train loss: 0.1865, test loss 1.1234\n",
      "Epoch 5487: train loss: 0.1864, test loss 1.1233\n",
      "Epoch 5488: train loss: 0.1864, test loss 1.1232\n",
      "Epoch 5489: train loss: 0.1864, test loss 1.1231\n",
      "Epoch 5490: train loss: 0.1864, test loss 1.1230\n",
      "Epoch 5491: train loss: 0.1864, test loss 1.1229\n",
      "Epoch 5492: train loss: 0.1864, test loss 1.1228\n",
      "Epoch 5493: train loss: 0.1864, test loss 1.1226\n",
      "Epoch 5494: train loss: 0.1864, test loss 1.1225\n",
      "Epoch 5495: train loss: 0.1864, test loss 1.1224\n",
      "Epoch 5496: train loss: 0.1864, test loss 1.1223\n",
      "Epoch 5497: train loss: 0.1864, test loss 1.1222\n",
      "Epoch 5498: train loss: 0.1864, test loss 1.1221\n",
      "Epoch 5499: train loss: 0.1864, test loss 1.1220\n",
      "Epoch 5500: train loss: 0.1864, test loss 1.1219\n",
      "Epoch 5501: train loss: 0.1864, test loss 1.1218\n",
      "Epoch 5502: train loss: 0.1864, test loss 1.1217\n",
      "Epoch 5503: train loss: 0.1864, test loss 1.1215\n",
      "Epoch 5504: train loss: 0.1864, test loss 1.1214\n",
      "Epoch 5505: train loss: 0.1864, test loss 1.1213\n",
      "Epoch 5506: train loss: 0.1864, test loss 1.1212\n",
      "Epoch 5507: train loss: 0.1864, test loss 1.1211\n",
      "Epoch 5508: train loss: 0.1864, test loss 1.1210\n",
      "Epoch 5509: train loss: 0.1864, test loss 1.1209\n",
      "Epoch 5510: train loss: 0.1864, test loss 1.1208\n",
      "Epoch 5511: train loss: 0.1864, test loss 1.1207\n",
      "Epoch 5512: train loss: 0.1864, test loss 1.1206\n",
      "Epoch 5513: train loss: 0.1864, test loss 1.1204\n",
      "Epoch 5514: train loss: 0.1864, test loss 1.1203\n",
      "Epoch 5515: train loss: 0.1864, test loss 1.1202\n",
      "Epoch 5516: train loss: 0.1864, test loss 1.1201\n",
      "Epoch 5517: train loss: 0.1864, test loss 1.1200\n",
      "Epoch 5518: train loss: 0.1864, test loss 1.1199\n",
      "Epoch 5519: train loss: 0.1864, test loss 1.1198\n",
      "Epoch 5520: train loss: 0.1864, test loss 1.1197\n",
      "Epoch 5521: train loss: 0.1863, test loss 1.1196\n",
      "Epoch 5522: train loss: 0.1863, test loss 1.1195\n",
      "Epoch 5523: train loss: 0.1863, test loss 1.1193\n",
      "Epoch 5524: train loss: 0.1863, test loss 1.1192\n",
      "Epoch 5525: train loss: 0.1863, test loss 1.1191\n",
      "Epoch 5526: train loss: 0.1863, test loss 1.1190\n",
      "Epoch 5527: train loss: 0.1863, test loss 1.1189\n",
      "Epoch 5528: train loss: 0.1863, test loss 1.1188\n",
      "Epoch 5529: train loss: 0.1863, test loss 1.1187\n",
      "Epoch 5530: train loss: 0.1863, test loss 1.1186\n",
      "Epoch 5531: train loss: 0.1863, test loss 1.1185\n",
      "Epoch 5532: train loss: 0.1863, test loss 1.1184\n",
      "Epoch 5533: train loss: 0.1863, test loss 1.1183\n",
      "Epoch 5534: train loss: 0.1863, test loss 1.1181\n",
      "Epoch 5535: train loss: 0.1863, test loss 1.1180\n",
      "Epoch 5536: train loss: 0.1863, test loss 1.1179\n",
      "Epoch 5537: train loss: 0.1863, test loss 1.1178\n",
      "Epoch 5538: train loss: 0.1863, test loss 1.1177\n",
      "Epoch 5539: train loss: 0.1863, test loss 1.1176\n",
      "Epoch 5540: train loss: 0.1863, test loss 1.1175\n",
      "Epoch 5541: train loss: 0.1863, test loss 1.1174\n",
      "Epoch 5542: train loss: 0.1863, test loss 1.1173\n",
      "Epoch 5543: train loss: 0.1863, test loss 1.1172\n",
      "Epoch 5544: train loss: 0.1863, test loss 1.1171\n",
      "Epoch 5545: train loss: 0.1863, test loss 1.1170\n",
      "Epoch 5546: train loss: 0.1863, test loss 1.1168\n",
      "Epoch 5547: train loss: 0.1863, test loss 1.1167\n",
      "Epoch 5548: train loss: 0.1863, test loss 1.1166\n",
      "Epoch 5549: train loss: 0.1863, test loss 1.1165\n",
      "Epoch 5550: train loss: 0.1863, test loss 1.1164\n",
      "Epoch 5551: train loss: 0.1863, test loss 1.1163\n",
      "Epoch 5552: train loss: 0.1863, test loss 1.1162\n",
      "Epoch 5553: train loss: 0.1863, test loss 1.1161\n",
      "Epoch 5554: train loss: 0.1862, test loss 1.1160\n",
      "Epoch 5555: train loss: 0.1862, test loss 1.1159\n",
      "Epoch 5556: train loss: 0.1862, test loss 1.1158\n",
      "Epoch 5557: train loss: 0.1862, test loss 1.1157\n",
      "Epoch 5558: train loss: 0.1862, test loss 1.1156\n",
      "Epoch 5559: train loss: 0.1862, test loss 1.1154\n",
      "Epoch 5560: train loss: 0.1862, test loss 1.1153\n",
      "Epoch 5561: train loss: 0.1862, test loss 1.1152\n",
      "Epoch 5562: train loss: 0.1862, test loss 1.1151\n",
      "Epoch 5563: train loss: 0.1862, test loss 1.1150\n",
      "Epoch 5564: train loss: 0.1862, test loss 1.1149\n",
      "Epoch 5565: train loss: 0.1862, test loss 1.1148\n",
      "Epoch 5566: train loss: 0.1862, test loss 1.1147\n",
      "Epoch 5567: train loss: 0.1862, test loss 1.1146\n",
      "Epoch 5568: train loss: 0.1862, test loss 1.1145\n",
      "Epoch 5569: train loss: 0.1862, test loss 1.1144\n",
      "Epoch 5570: train loss: 0.1862, test loss 1.1143\n",
      "Epoch 5571: train loss: 0.1862, test loss 1.1142\n",
      "Epoch 5572: train loss: 0.1862, test loss 1.1140\n",
      "Epoch 5573: train loss: 0.1862, test loss 1.1139\n",
      "Epoch 5574: train loss: 0.1862, test loss 1.1138\n",
      "Epoch 5575: train loss: 0.1862, test loss 1.1137\n",
      "Epoch 5576: train loss: 0.1862, test loss 1.1136\n",
      "Epoch 5577: train loss: 0.1862, test loss 1.1135\n",
      "Epoch 5578: train loss: 0.1862, test loss 1.1134\n",
      "Epoch 5579: train loss: 0.1862, test loss 1.1133\n",
      "Epoch 5580: train loss: 0.1862, test loss 1.1132\n",
      "Epoch 5581: train loss: 0.1862, test loss 1.1131\n",
      "Epoch 5582: train loss: 0.1862, test loss 1.1130\n",
      "Epoch 5583: train loss: 0.1862, test loss 1.1129\n",
      "Epoch 5584: train loss: 0.1862, test loss 1.1127\n",
      "Epoch 5585: train loss: 0.1862, test loss 1.1126\n",
      "Epoch 5586: train loss: 0.1862, test loss 1.1125\n",
      "Epoch 5587: train loss: 0.1862, test loss 1.1124\n",
      "Epoch 5588: train loss: 0.1861, test loss 1.1123\n",
      "Epoch 5589: train loss: 0.1861, test loss 1.1122\n",
      "Epoch 5590: train loss: 0.1861, test loss 1.1121\n",
      "Epoch 5591: train loss: 0.1861, test loss 1.1120\n",
      "Epoch 5592: train loss: 0.1861, test loss 1.1119\n",
      "Epoch 5593: train loss: 0.1861, test loss 1.1118\n",
      "Epoch 5594: train loss: 0.1861, test loss 1.1117\n",
      "Epoch 5595: train loss: 0.1861, test loss 1.1115\n",
      "Epoch 5596: train loss: 0.1861, test loss 1.1114\n",
      "Epoch 5597: train loss: 0.1861, test loss 1.1113\n",
      "Epoch 5598: train loss: 0.1861, test loss 1.1112\n",
      "Epoch 5599: train loss: 0.1861, test loss 1.1111\n",
      "Epoch 5600: train loss: 0.1861, test loss 1.1110\n",
      "Epoch 5601: train loss: 0.1861, test loss 1.1109\n",
      "Epoch 5602: train loss: 0.1861, test loss 1.1108\n",
      "Epoch 5603: train loss: 0.1861, test loss 1.1107\n",
      "Epoch 5604: train loss: 0.1861, test loss 1.1106\n",
      "Epoch 5605: train loss: 0.1861, test loss 1.1105\n",
      "Epoch 5606: train loss: 0.1861, test loss 1.1104\n",
      "Epoch 5607: train loss: 0.1861, test loss 1.1103\n",
      "Epoch 5608: train loss: 0.1861, test loss 1.1101\n",
      "Epoch 5609: train loss: 0.1861, test loss 1.1100\n",
      "Epoch 5610: train loss: 0.1861, test loss 1.1099\n",
      "Epoch 5611: train loss: 0.1861, test loss 1.1098\n",
      "Epoch 5612: train loss: 0.1861, test loss 1.1097\n",
      "Epoch 5613: train loss: 0.1861, test loss 1.1096\n",
      "Epoch 5614: train loss: 0.1861, test loss 1.1095\n",
      "Epoch 5615: train loss: 0.1861, test loss 1.1094\n",
      "Epoch 5616: train loss: 0.1861, test loss 1.1093\n",
      "Epoch 5617: train loss: 0.1861, test loss 1.1092\n",
      "Epoch 5618: train loss: 0.1861, test loss 1.1091\n",
      "Epoch 5619: train loss: 0.1861, test loss 1.1090\n",
      "Epoch 5620: train loss: 0.1861, test loss 1.1089\n",
      "Epoch 5621: train loss: 0.1861, test loss 1.1087\n",
      "Epoch 5622: train loss: 0.1860, test loss 1.1086\n",
      "Epoch 5623: train loss: 0.1860, test loss 1.1085\n",
      "Epoch 5624: train loss: 0.1860, test loss 1.1084\n",
      "Epoch 5625: train loss: 0.1860, test loss 1.1083\n",
      "Epoch 5626: train loss: 0.1860, test loss 1.1082\n",
      "Epoch 5627: train loss: 0.1860, test loss 1.1081\n",
      "Epoch 5628: train loss: 0.1860, test loss 1.1080\n",
      "Epoch 5629: train loss: 0.1860, test loss 1.1079\n",
      "Epoch 5630: train loss: 0.1860, test loss 1.1078\n",
      "Epoch 5631: train loss: 0.1860, test loss 1.1077\n",
      "Epoch 5632: train loss: 0.1860, test loss 1.1076\n",
      "Epoch 5633: train loss: 0.1860, test loss 1.1075\n",
      "Epoch 5634: train loss: 0.1860, test loss 1.1074\n",
      "Epoch 5635: train loss: 0.1860, test loss 1.1073\n",
      "Epoch 5636: train loss: 0.1860, test loss 1.1072\n",
      "Epoch 5637: train loss: 0.1860, test loss 1.1071\n",
      "Epoch 5638: train loss: 0.1860, test loss 1.1070\n",
      "Epoch 5639: train loss: 0.1860, test loss 1.1068\n",
      "Epoch 5640: train loss: 0.1860, test loss 1.1067\n",
      "Epoch 5641: train loss: 0.1860, test loss 1.1066\n",
      "Epoch 5642: train loss: 0.1860, test loss 1.1065\n",
      "Epoch 5643: train loss: 0.1860, test loss 1.1064\n",
      "Epoch 5644: train loss: 0.1860, test loss 1.1063\n",
      "Epoch 5645: train loss: 0.1860, test loss 1.1062\n",
      "Epoch 5646: train loss: 0.1860, test loss 1.1061\n",
      "Epoch 5647: train loss: 0.1860, test loss 1.1060\n",
      "Epoch 5648: train loss: 0.1860, test loss 1.1059\n",
      "Epoch 5649: train loss: 0.1860, test loss 1.1058\n",
      "Epoch 5650: train loss: 0.1860, test loss 1.1057\n",
      "Epoch 5651: train loss: 0.1860, test loss 1.1056\n",
      "Epoch 5652: train loss: 0.1860, test loss 1.1055\n",
      "Epoch 5653: train loss: 0.1860, test loss 1.1054\n",
      "Epoch 5654: train loss: 0.1860, test loss 1.1053\n",
      "Epoch 5655: train loss: 0.1859, test loss 1.1052\n",
      "Epoch 5656: train loss: 0.1859, test loss 1.1051\n",
      "Epoch 5657: train loss: 0.1859, test loss 1.1050\n",
      "Epoch 5658: train loss: 0.1859, test loss 1.1049\n",
      "Epoch 5659: train loss: 0.1859, test loss 1.1048\n",
      "Epoch 5660: train loss: 0.1859, test loss 1.1047\n",
      "Epoch 5661: train loss: 0.1859, test loss 1.1046\n",
      "Epoch 5662: train loss: 0.1859, test loss 1.1045\n",
      "Epoch 5663: train loss: 0.1859, test loss 1.1043\n",
      "Epoch 5664: train loss: 0.1859, test loss 1.1042\n",
      "Epoch 5665: train loss: 0.1859, test loss 1.1041\n",
      "Epoch 5666: train loss: 0.1859, test loss 1.1040\n",
      "Epoch 5667: train loss: 0.1859, test loss 1.1039\n",
      "Epoch 5668: train loss: 0.1859, test loss 1.1038\n",
      "Epoch 5669: train loss: 0.1859, test loss 1.1037\n",
      "Epoch 5670: train loss: 0.1859, test loss 1.1036\n",
      "Epoch 5671: train loss: 0.1859, test loss 1.1035\n",
      "Epoch 5672: train loss: 0.1859, test loss 1.1034\n",
      "Epoch 5673: train loss: 0.1859, test loss 1.1033\n",
      "Epoch 5674: train loss: 0.1859, test loss 1.1032\n",
      "Epoch 5675: train loss: 0.1859, test loss 1.1031\n",
      "Epoch 5676: train loss: 0.1859, test loss 1.1030\n",
      "Epoch 5677: train loss: 0.1859, test loss 1.1029\n",
      "Epoch 5678: train loss: 0.1859, test loss 1.1028\n",
      "Epoch 5679: train loss: 0.1859, test loss 1.1027\n",
      "Epoch 5680: train loss: 0.1859, test loss 1.1026\n",
      "Epoch 5681: train loss: 0.1859, test loss 1.1025\n",
      "Epoch 5682: train loss: 0.1859, test loss 1.1024\n",
      "Epoch 5683: train loss: 0.1859, test loss 1.1023\n",
      "Epoch 5684: train loss: 0.1859, test loss 1.1022\n",
      "Epoch 5685: train loss: 0.1859, test loss 1.1021\n",
      "Epoch 5686: train loss: 0.1859, test loss 1.1020\n",
      "Epoch 5687: train loss: 0.1859, test loss 1.1019\n",
      "Epoch 5688: train loss: 0.1859, test loss 1.1018\n",
      "Epoch 5689: train loss: 0.1859, test loss 1.1017\n",
      "Epoch 5690: train loss: 0.1858, test loss 1.1016\n",
      "Epoch 5691: train loss: 0.1858, test loss 1.1015\n",
      "Epoch 5692: train loss: 0.1858, test loss 1.1014\n",
      "Epoch 5693: train loss: 0.1858, test loss 1.1013\n",
      "Epoch 5694: train loss: 0.1858, test loss 1.1012\n",
      "Epoch 5695: train loss: 0.1858, test loss 1.1011\n",
      "Epoch 5696: train loss: 0.1858, test loss 1.1009\n",
      "Epoch 5697: train loss: 0.1858, test loss 1.1008\n",
      "Epoch 5698: train loss: 0.1858, test loss 1.1007\n",
      "Epoch 5699: train loss: 0.1858, test loss 1.1006\n",
      "Epoch 5700: train loss: 0.1858, test loss 1.1005\n",
      "Epoch 5701: train loss: 0.1858, test loss 1.1004\n",
      "Epoch 5702: train loss: 0.1858, test loss 1.1003\n",
      "Epoch 5703: train loss: 0.1858, test loss 1.1002\n",
      "Epoch 5704: train loss: 0.1858, test loss 1.1001\n",
      "Epoch 5705: train loss: 0.1858, test loss 1.1000\n",
      "Epoch 5706: train loss: 0.1858, test loss 1.0999\n",
      "Epoch 5707: train loss: 0.1858, test loss 1.0998\n",
      "Epoch 5708: train loss: 0.1858, test loss 1.0997\n",
      "Epoch 5709: train loss: 0.1858, test loss 1.0996\n",
      "Epoch 5710: train loss: 0.1858, test loss 1.0995\n",
      "Epoch 5711: train loss: 0.1858, test loss 1.0994\n",
      "Epoch 5712: train loss: 0.1858, test loss 1.0993\n",
      "Epoch 5713: train loss: 0.1858, test loss 1.0992\n",
      "Epoch 5714: train loss: 0.1858, test loss 1.0991\n",
      "Epoch 5715: train loss: 0.1858, test loss 1.0990\n",
      "Epoch 5716: train loss: 0.1858, test loss 1.0989\n",
      "Epoch 5717: train loss: 0.1858, test loss 1.0988\n",
      "Epoch 5718: train loss: 0.1858, test loss 1.0987\n",
      "Epoch 5719: train loss: 0.1858, test loss 1.0986\n",
      "Epoch 5720: train loss: 0.1858, test loss 1.0985\n",
      "Epoch 5721: train loss: 0.1858, test loss 1.0984\n",
      "Epoch 5722: train loss: 0.1858, test loss 1.0983\n",
      "Epoch 5723: train loss: 0.1858, test loss 1.0982\n",
      "Epoch 5724: train loss: 0.1857, test loss 1.0981\n",
      "Epoch 5725: train loss: 0.1857, test loss 1.0980\n",
      "Epoch 5726: train loss: 0.1857, test loss 1.0979\n",
      "Epoch 5727: train loss: 0.1857, test loss 1.0978\n",
      "Epoch 5728: train loss: 0.1857, test loss 1.0977\n",
      "Epoch 5729: train loss: 0.1857, test loss 1.0976\n",
      "Epoch 5730: train loss: 0.1857, test loss 1.0975\n",
      "Epoch 5731: train loss: 0.1857, test loss 1.0974\n",
      "Epoch 5732: train loss: 0.1857, test loss 1.0973\n",
      "Epoch 5733: train loss: 0.1857, test loss 1.0972\n",
      "Epoch 5734: train loss: 0.1857, test loss 1.0971\n",
      "Epoch 5735: train loss: 0.1857, test loss 1.0970\n",
      "Epoch 5736: train loss: 0.1857, test loss 1.0969\n",
      "Epoch 5737: train loss: 0.1857, test loss 1.0968\n",
      "Epoch 5738: train loss: 0.1857, test loss 1.0967\n",
      "Epoch 5739: train loss: 0.1857, test loss 1.0966\n",
      "Epoch 5740: train loss: 0.1857, test loss 1.0965\n",
      "Epoch 5741: train loss: 0.1857, test loss 1.0964\n",
      "Epoch 5742: train loss: 0.1857, test loss 1.0963\n",
      "Epoch 5743: train loss: 0.1857, test loss 1.0962\n",
      "Epoch 5744: train loss: 0.1857, test loss 1.0961\n",
      "Epoch 5745: train loss: 0.1857, test loss 1.0960\n",
      "Epoch 5746: train loss: 0.1857, test loss 1.0959\n",
      "Epoch 5747: train loss: 0.1857, test loss 1.0958\n",
      "Epoch 5748: train loss: 0.1857, test loss 1.0957\n",
      "Epoch 5749: train loss: 0.1857, test loss 1.0956\n",
      "Epoch 5750: train loss: 0.1857, test loss 1.0955\n",
      "Epoch 5751: train loss: 0.1857, test loss 1.0954\n",
      "Epoch 5752: train loss: 0.1857, test loss 1.0953\n",
      "Epoch 5753: train loss: 0.1857, test loss 1.0952\n",
      "Epoch 5754: train loss: 0.1857, test loss 1.0951\n",
      "Epoch 5755: train loss: 0.1857, test loss 1.0950\n",
      "Epoch 5756: train loss: 0.1857, test loss 1.0949\n",
      "Epoch 5757: train loss: 0.1857, test loss 1.0948\n",
      "Epoch 5758: train loss: 0.1857, test loss 1.0947\n",
      "Epoch 5759: train loss: 0.1856, test loss 1.0946\n",
      "Epoch 5760: train loss: 0.1856, test loss 1.0945\n",
      "Epoch 5761: train loss: 0.1856, test loss 1.0944\n",
      "Epoch 5762: train loss: 0.1856, test loss 1.0943\n",
      "Epoch 5763: train loss: 0.1856, test loss 1.0942\n",
      "Epoch 5764: train loss: 0.1856, test loss 1.0941\n",
      "Epoch 5765: train loss: 0.1856, test loss 1.0940\n",
      "Epoch 5766: train loss: 0.1856, test loss 1.0939\n",
      "Epoch 5767: train loss: 0.1856, test loss 1.0938\n",
      "Epoch 5768: train loss: 0.1856, test loss 1.0937\n",
      "Epoch 5769: train loss: 0.1856, test loss 1.0936\n",
      "Epoch 5770: train loss: 0.1856, test loss 1.0935\n",
      "Epoch 5771: train loss: 0.1856, test loss 1.0934\n",
      "Epoch 5772: train loss: 0.1856, test loss 1.0933\n",
      "Epoch 5773: train loss: 0.1856, test loss 1.0932\n",
      "Epoch 5774: train loss: 0.1856, test loss 1.0931\n",
      "Epoch 5775: train loss: 0.1856, test loss 1.0930\n",
      "Epoch 5776: train loss: 0.1856, test loss 1.0929\n",
      "Epoch 5777: train loss: 0.1856, test loss 1.0928\n",
      "Epoch 5778: train loss: 0.1856, test loss 1.0927\n",
      "Epoch 5779: train loss: 0.1856, test loss 1.0926\n",
      "Epoch 5780: train loss: 0.1856, test loss 1.0925\n",
      "Epoch 5781: train loss: 0.1856, test loss 1.0924\n",
      "Epoch 5782: train loss: 0.1856, test loss 1.0923\n",
      "Epoch 5783: train loss: 0.1856, test loss 1.0922\n",
      "Epoch 5784: train loss: 0.1856, test loss 1.0921\n",
      "Epoch 5785: train loss: 0.1856, test loss 1.0920\n",
      "Epoch 5786: train loss: 0.1856, test loss 1.0919\n",
      "Epoch 5787: train loss: 0.1856, test loss 1.0918\n",
      "Epoch 5788: train loss: 0.1856, test loss 1.0917\n",
      "Epoch 5789: train loss: 0.1856, test loss 1.0916\n",
      "Epoch 5790: train loss: 0.1856, test loss 1.0915\n",
      "Epoch 5791: train loss: 0.1856, test loss 1.0914\n",
      "Epoch 5792: train loss: 0.1856, test loss 1.0913\n",
      "Epoch 5793: train loss: 0.1856, test loss 1.0912\n",
      "Epoch 5794: train loss: 0.1855, test loss 1.0911\n",
      "Epoch 5795: train loss: 0.1855, test loss 1.0910\n",
      "Epoch 5796: train loss: 0.1855, test loss 1.0909\n",
      "Epoch 5797: train loss: 0.1855, test loss 1.0908\n",
      "Epoch 5798: train loss: 0.1855, test loss 1.0907\n",
      "Epoch 5799: train loss: 0.1855, test loss 1.0906\n",
      "Epoch 5800: train loss: 0.1855, test loss 1.0906\n",
      "Epoch 5801: train loss: 0.1855, test loss 1.0905\n",
      "Epoch 5802: train loss: 0.1855, test loss 1.0904\n",
      "Epoch 5803: train loss: 0.1855, test loss 1.0903\n",
      "Epoch 5804: train loss: 0.1855, test loss 1.0902\n",
      "Epoch 5805: train loss: 0.1855, test loss 1.0901\n",
      "Epoch 5806: train loss: 0.1855, test loss 1.0900\n",
      "Epoch 5807: train loss: 0.1855, test loss 1.0899\n",
      "Epoch 5808: train loss: 0.1855, test loss 1.0898\n",
      "Epoch 5809: train loss: 0.1855, test loss 1.0897\n",
      "Epoch 5810: train loss: 0.1855, test loss 1.0896\n",
      "Epoch 5811: train loss: 0.1855, test loss 1.0895\n",
      "Epoch 5812: train loss: 0.1855, test loss 1.0894\n",
      "Epoch 5813: train loss: 0.1855, test loss 1.0893\n",
      "Epoch 5814: train loss: 0.1855, test loss 1.0892\n",
      "Epoch 5815: train loss: 0.1855, test loss 1.0891\n",
      "Epoch 5816: train loss: 0.1855, test loss 1.0890\n",
      "Epoch 5817: train loss: 0.1855, test loss 1.0889\n",
      "Epoch 5818: train loss: 0.1855, test loss 1.0888\n",
      "Epoch 5819: train loss: 0.1855, test loss 1.0887\n",
      "Epoch 5820: train loss: 0.1855, test loss 1.0886\n",
      "Epoch 5821: train loss: 0.1855, test loss 1.0885\n",
      "Epoch 5822: train loss: 0.1855, test loss 1.0884\n",
      "Epoch 5823: train loss: 0.1855, test loss 1.0883\n",
      "Epoch 5824: train loss: 0.1855, test loss 1.0882\n",
      "Epoch 5825: train loss: 0.1855, test loss 1.0881\n",
      "Epoch 5826: train loss: 0.1855, test loss 1.0880\n",
      "Epoch 5827: train loss: 0.1855, test loss 1.0879\n",
      "Epoch 5828: train loss: 0.1855, test loss 1.0878\n",
      "Epoch 5829: train loss: 0.1855, test loss 1.0877\n",
      "Epoch 5830: train loss: 0.1854, test loss 1.0876\n",
      "Epoch 5831: train loss: 0.1854, test loss 1.0876\n",
      "Epoch 5832: train loss: 0.1854, test loss 1.0875\n",
      "Epoch 5833: train loss: 0.1854, test loss 1.0874\n",
      "Epoch 5834: train loss: 0.1854, test loss 1.0873\n",
      "Epoch 5835: train loss: 0.1854, test loss 1.0872\n",
      "Epoch 5836: train loss: 0.1854, test loss 1.0871\n",
      "Epoch 5837: train loss: 0.1854, test loss 1.0870\n",
      "Epoch 5838: train loss: 0.1854, test loss 1.0869\n",
      "Epoch 5839: train loss: 0.1854, test loss 1.0868\n",
      "Epoch 5840: train loss: 0.1854, test loss 1.0867\n",
      "Epoch 5841: train loss: 0.1854, test loss 1.0866\n",
      "Epoch 5842: train loss: 0.1854, test loss 1.0865\n",
      "Epoch 5843: train loss: 0.1854, test loss 1.0864\n",
      "Epoch 5844: train loss: 0.1854, test loss 1.0863\n",
      "Epoch 5845: train loss: 0.1854, test loss 1.0862\n",
      "Epoch 5846: train loss: 0.1854, test loss 1.0861\n",
      "Epoch 5847: train loss: 0.1854, test loss 1.0860\n",
      "Epoch 5848: train loss: 0.1854, test loss 1.0859\n",
      "Epoch 5849: train loss: 0.1854, test loss 1.0858\n",
      "Epoch 5850: train loss: 0.1854, test loss 1.0857\n",
      "Epoch 5851: train loss: 0.1854, test loss 1.0856\n",
      "Epoch 5852: train loss: 0.1854, test loss 1.0855\n",
      "Epoch 5853: train loss: 0.1854, test loss 1.0854\n",
      "Epoch 5854: train loss: 0.1854, test loss 1.0853\n",
      "Epoch 5855: train loss: 0.1854, test loss 1.0852\n",
      "Epoch 5856: train loss: 0.1854, test loss 1.0851\n",
      "Epoch 5857: train loss: 0.1854, test loss 1.0850\n",
      "Epoch 5858: train loss: 0.1854, test loss 1.0849\n",
      "Epoch 5859: train loss: 0.1854, test loss 1.0848\n",
      "Epoch 5860: train loss: 0.1854, test loss 1.0847\n",
      "Epoch 5861: train loss: 0.1854, test loss 1.0846\n",
      "Epoch 5862: train loss: 0.1854, test loss 1.0845\n",
      "Epoch 5863: train loss: 0.1854, test loss 1.0844\n",
      "Epoch 5864: train loss: 0.1854, test loss 1.0843\n",
      "Epoch 5865: train loss: 0.1854, test loss 1.0843\n",
      "Epoch 5866: train loss: 0.1853, test loss 1.0842\n",
      "Epoch 5867: train loss: 0.1853, test loss 1.0841\n",
      "Epoch 5868: train loss: 0.1853, test loss 1.0840\n",
      "Epoch 5869: train loss: 0.1853, test loss 1.0839\n",
      "Epoch 5870: train loss: 0.1853, test loss 1.0838\n",
      "Epoch 5871: train loss: 0.1853, test loss 1.0837\n",
      "Epoch 5872: train loss: 0.1853, test loss 1.0836\n",
      "Epoch 5873: train loss: 0.1853, test loss 1.0835\n",
      "Epoch 5874: train loss: 0.1853, test loss 1.0834\n",
      "Epoch 5875: train loss: 0.1853, test loss 1.0833\n",
      "Epoch 5876: train loss: 0.1853, test loss 1.0832\n",
      "Epoch 5877: train loss: 0.1853, test loss 1.0831\n",
      "Epoch 5878: train loss: 0.1853, test loss 1.0830\n",
      "Epoch 5879: train loss: 0.1853, test loss 1.0829\n",
      "Epoch 5880: train loss: 0.1853, test loss 1.0828\n",
      "Epoch 5881: train loss: 0.1853, test loss 1.0827\n",
      "Epoch 5882: train loss: 0.1853, test loss 1.0826\n",
      "Epoch 5883: train loss: 0.1853, test loss 1.0825\n",
      "Epoch 5884: train loss: 0.1853, test loss 1.0824\n",
      "Epoch 5885: train loss: 0.1853, test loss 1.0823\n",
      "Epoch 5886: train loss: 0.1853, test loss 1.0822\n",
      "Epoch 5887: train loss: 0.1853, test loss 1.0821\n",
      "Epoch 5888: train loss: 0.1853, test loss 1.0820\n",
      "Epoch 5889: train loss: 0.1853, test loss 1.0819\n",
      "Epoch 5890: train loss: 0.1853, test loss 1.0818\n",
      "Epoch 5891: train loss: 0.1853, test loss 1.0818\n",
      "Epoch 5892: train loss: 0.1853, test loss 1.0817\n",
      "Epoch 5893: train loss: 0.1853, test loss 1.0816\n",
      "Epoch 5894: train loss: 0.1853, test loss 1.0815\n",
      "Epoch 5895: train loss: 0.1853, test loss 1.0814\n",
      "Epoch 5896: train loss: 0.1853, test loss 1.0813\n",
      "Epoch 5897: train loss: 0.1853, test loss 1.0812\n",
      "Epoch 5898: train loss: 0.1853, test loss 1.0811\n",
      "Epoch 5899: train loss: 0.1853, test loss 1.0810\n",
      "Epoch 5900: train loss: 0.1853, test loss 1.0809\n",
      "Epoch 5901: train loss: 0.1853, test loss 1.0808\n",
      "Epoch 5902: train loss: 0.1852, test loss 1.0807\n",
      "Epoch 5903: train loss: 0.1852, test loss 1.0806\n",
      "Epoch 5904: train loss: 0.1852, test loss 1.0805\n",
      "Epoch 5905: train loss: 0.1852, test loss 1.0804\n",
      "Epoch 5906: train loss: 0.1852, test loss 1.0803\n",
      "Epoch 5907: train loss: 0.1852, test loss 1.0802\n",
      "Epoch 5908: train loss: 0.1852, test loss 1.0801\n",
      "Epoch 5909: train loss: 0.1852, test loss 1.0800\n",
      "Epoch 5910: train loss: 0.1852, test loss 1.0799\n",
      "Epoch 5911: train loss: 0.1852, test loss 1.0799\n",
      "Epoch 5912: train loss: 0.1852, test loss 1.0798\n",
      "Epoch 5913: train loss: 0.1852, test loss 1.0797\n",
      "Epoch 5914: train loss: 0.1852, test loss 1.0796\n",
      "Epoch 5915: train loss: 0.1852, test loss 1.0795\n",
      "Epoch 5916: train loss: 0.1852, test loss 1.0794\n",
      "Epoch 5917: train loss: 0.1852, test loss 1.0793\n",
      "Epoch 5918: train loss: 0.1852, test loss 1.0792\n",
      "Epoch 5919: train loss: 0.1852, test loss 1.0791\n",
      "Epoch 5920: train loss: 0.1852, test loss 1.0790\n",
      "Epoch 5921: train loss: 0.1852, test loss 1.0789\n",
      "Epoch 5922: train loss: 0.1852, test loss 1.0788\n",
      "Epoch 5923: train loss: 0.1852, test loss 1.0787\n",
      "Epoch 5924: train loss: 0.1852, test loss 1.0786\n",
      "Epoch 5925: train loss: 0.1852, test loss 1.0785\n",
      "Epoch 5926: train loss: 0.1852, test loss 1.0784\n",
      "Epoch 5927: train loss: 0.1852, test loss 1.0784\n",
      "Epoch 5928: train loss: 0.1852, test loss 1.0783\n",
      "Epoch 5929: train loss: 0.1852, test loss 1.0782\n",
      "Epoch 5930: train loss: 0.1852, test loss 1.0781\n",
      "Epoch 5931: train loss: 0.1852, test loss 1.0780\n",
      "Epoch 5932: train loss: 0.1852, test loss 1.0779\n",
      "Epoch 5933: train loss: 0.1852, test loss 1.0778\n",
      "Epoch 5934: train loss: 0.1852, test loss 1.0777\n",
      "Epoch 5935: train loss: 0.1852, test loss 1.0776\n",
      "Epoch 5936: train loss: 0.1852, test loss 1.0775\n",
      "Epoch 5937: train loss: 0.1852, test loss 1.0774\n",
      "Epoch 5938: train loss: 0.1851, test loss 1.0773\n",
      "Epoch 5939: train loss: 0.1851, test loss 1.0772\n",
      "Epoch 5940: train loss: 0.1851, test loss 1.0771\n",
      "Epoch 5941: train loss: 0.1851, test loss 1.0770\n",
      "Epoch 5942: train loss: 0.1851, test loss 1.0769\n",
      "Epoch 5943: train loss: 0.1851, test loss 1.0768\n",
      "Epoch 5944: train loss: 0.1851, test loss 1.0767\n",
      "Epoch 5945: train loss: 0.1851, test loss 1.0766\n",
      "Epoch 5946: train loss: 0.1851, test loss 1.0765\n",
      "Epoch 5947: train loss: 0.1851, test loss 1.0764\n",
      "Epoch 5948: train loss: 0.1851, test loss 1.0763\n",
      "Epoch 5949: train loss: 0.1851, test loss 1.0763\n",
      "Epoch 5950: train loss: 0.1851, test loss 1.0762\n",
      "Epoch 5951: train loss: 0.1851, test loss 1.0761\n",
      "Epoch 5952: train loss: 0.1851, test loss 1.0760\n",
      "Epoch 5953: train loss: 0.1851, test loss 1.0759\n",
      "Epoch 5954: train loss: 0.1851, test loss 1.0758\n",
      "Epoch 5955: train loss: 0.1851, test loss 1.0757\n",
      "Epoch 5956: train loss: 0.1851, test loss 1.0756\n",
      "Epoch 5957: train loss: 0.1851, test loss 1.0755\n",
      "Epoch 5958: train loss: 0.1851, test loss 1.0754\n",
      "Epoch 5959: train loss: 0.1851, test loss 1.0753\n",
      "Epoch 5960: train loss: 0.1851, test loss 1.0752\n",
      "Epoch 5961: train loss: 0.1851, test loss 1.0751\n",
      "Epoch 5962: train loss: 0.1851, test loss 1.0750\n",
      "Epoch 5963: train loss: 0.1851, test loss 1.0749\n",
      "Epoch 5964: train loss: 0.1851, test loss 1.0748\n",
      "Epoch 5965: train loss: 0.1851, test loss 1.0747\n",
      "Epoch 5966: train loss: 0.1851, test loss 1.0746\n",
      "Epoch 5967: train loss: 0.1851, test loss 1.0746\n",
      "Epoch 5968: train loss: 0.1851, test loss 1.0745\n",
      "Epoch 5969: train loss: 0.1851, test loss 1.0744\n",
      "Epoch 5970: train loss: 0.1851, test loss 1.0743\n",
      "Epoch 5971: train loss: 0.1851, test loss 1.0742\n",
      "Epoch 5972: train loss: 0.1851, test loss 1.0741\n",
      "Epoch 5973: train loss: 0.1851, test loss 1.0740\n",
      "Epoch 5974: train loss: 0.1851, test loss 1.0739\n",
      "Epoch 5975: train loss: 0.1850, test loss 1.0738\n",
      "Epoch 5976: train loss: 0.1850, test loss 1.0737\n",
      "Epoch 5977: train loss: 0.1850, test loss 1.0736\n",
      "Epoch 5978: train loss: 0.1850, test loss 1.0735\n",
      "Epoch 5979: train loss: 0.1850, test loss 1.0734\n",
      "Epoch 5980: train loss: 0.1850, test loss 1.0733\n",
      "Epoch 5981: train loss: 0.1850, test loss 1.0732\n",
      "Epoch 5982: train loss: 0.1850, test loss 1.0732\n",
      "Epoch 5983: train loss: 0.1850, test loss 1.0731\n",
      "Epoch 5984: train loss: 0.1850, test loss 1.0730\n",
      "Epoch 5985: train loss: 0.1850, test loss 1.0729\n",
      "Epoch 5986: train loss: 0.1850, test loss 1.0728\n",
      "Epoch 5987: train loss: 0.1850, test loss 1.0727\n",
      "Epoch 5988: train loss: 0.1850, test loss 1.0726\n",
      "Epoch 5989: train loss: 0.1850, test loss 1.0725\n",
      "Epoch 5990: train loss: 0.1850, test loss 1.0724\n",
      "Epoch 5991: train loss: 0.1850, test loss 1.0723\n",
      "Epoch 5992: train loss: 0.1850, test loss 1.0722\n",
      "Epoch 5993: train loss: 0.1850, test loss 1.0721\n",
      "Epoch 5994: train loss: 0.1850, test loss 1.0720\n",
      "Epoch 5995: train loss: 0.1850, test loss 1.0719\n",
      "Epoch 5996: train loss: 0.1850, test loss 1.0719\n",
      "Epoch 5997: train loss: 0.1850, test loss 1.0718\n",
      "Epoch 5998: train loss: 0.1850, test loss 1.0717\n",
      "Epoch 5999: train loss: 0.1850, test loss 1.0716\n",
      "Epoch 6000: train loss: 0.1850, test loss 1.0715\n",
      "Epoch 6001: train loss: 0.1850, test loss 1.0714\n",
      "Epoch 6002: train loss: 0.1850, test loss 1.0713\n",
      "Epoch 6003: train loss: 0.1850, test loss 1.0712\n",
      "Epoch 6004: train loss: 0.1850, test loss 1.0711\n",
      "Epoch 6005: train loss: 0.1850, test loss 1.0710\n",
      "Epoch 6006: train loss: 0.1850, test loss 1.0709\n",
      "Epoch 6007: train loss: 0.1850, test loss 1.0708\n",
      "Epoch 6008: train loss: 0.1850, test loss 1.0707\n",
      "Epoch 6009: train loss: 0.1850, test loss 1.0706\n",
      "Epoch 6010: train loss: 0.1850, test loss 1.0705\n",
      "Epoch 6011: train loss: 0.1850, test loss 1.0704\n",
      "Epoch 6012: train loss: 0.1849, test loss 1.0703\n",
      "Epoch 6013: train loss: 0.1849, test loss 1.0702\n",
      "Epoch 6014: train loss: 0.1849, test loss 1.0701\n",
      "Epoch 6015: train loss: 0.1849, test loss 1.0700\n",
      "Epoch 6016: train loss: 0.1849, test loss 1.0699\n",
      "Epoch 6017: train loss: 0.1849, test loss 1.0699\n",
      "Epoch 6018: train loss: 0.1849, test loss 1.0698\n",
      "Epoch 6019: train loss: 0.1849, test loss 1.0697\n",
      "Epoch 6020: train loss: 0.1849, test loss 1.0696\n",
      "Epoch 6021: train loss: 0.1849, test loss 1.0695\n",
      "Epoch 6022: train loss: 0.1849, test loss 1.0694\n",
      "Epoch 6023: train loss: 0.1849, test loss 1.0693\n",
      "Epoch 6024: train loss: 0.1849, test loss 1.0692\n",
      "Epoch 6025: train loss: 0.1849, test loss 1.0691\n",
      "Epoch 6026: train loss: 0.1849, test loss 1.0690\n",
      "Epoch 6027: train loss: 0.1849, test loss 1.0689\n",
      "Epoch 6028: train loss: 0.1849, test loss 1.0688\n",
      "Epoch 6029: train loss: 0.1849, test loss 1.0687\n",
      "Epoch 6030: train loss: 0.1849, test loss 1.0686\n",
      "Epoch 6031: train loss: 0.1849, test loss 1.0685\n",
      "Epoch 6032: train loss: 0.1849, test loss 1.0684\n",
      "Epoch 6033: train loss: 0.1849, test loss 1.0683\n",
      "Epoch 6034: train loss: 0.1849, test loss 1.0683\n",
      "Epoch 6035: train loss: 0.1849, test loss 1.0682\n",
      "Epoch 6036: train loss: 0.1849, test loss 1.0681\n",
      "Epoch 6037: train loss: 0.1849, test loss 1.0680\n",
      "Epoch 6038: train loss: 0.1849, test loss 1.0679\n",
      "Epoch 6039: train loss: 0.1849, test loss 1.0678\n",
      "Epoch 6040: train loss: 0.1849, test loss 1.0677\n",
      "Epoch 6041: train loss: 0.1849, test loss 1.0676\n",
      "Epoch 6042: train loss: 0.1849, test loss 1.0675\n",
      "Epoch 6043: train loss: 0.1849, test loss 1.0674\n",
      "Epoch 6044: train loss: 0.1849, test loss 1.0673\n",
      "Epoch 6045: train loss: 0.1849, test loss 1.0672\n",
      "Epoch 6046: train loss: 0.1849, test loss 1.0671\n",
      "Epoch 6047: train loss: 0.1849, test loss 1.0670\n",
      "Epoch 6048: train loss: 0.1848, test loss 1.0669\n",
      "Epoch 6049: train loss: 0.1848, test loss 1.0669\n",
      "Epoch 6050: train loss: 0.1848, test loss 1.0668\n",
      "Epoch 6051: train loss: 0.1848, test loss 1.0667\n",
      "Epoch 6052: train loss: 0.1848, test loss 1.0666\n",
      "Epoch 6053: train loss: 0.1848, test loss 1.0665\n",
      "Epoch 6054: train loss: 0.1848, test loss 1.0664\n",
      "Epoch 6055: train loss: 0.1848, test loss 1.0663\n",
      "Epoch 6056: train loss: 0.1848, test loss 1.0662\n",
      "Epoch 6057: train loss: 0.1848, test loss 1.0661\n",
      "Epoch 6058: train loss: 0.1848, test loss 1.0660\n",
      "Epoch 6059: train loss: 0.1848, test loss 1.0659\n",
      "Epoch 6060: train loss: 0.1848, test loss 1.0659\n",
      "Epoch 6061: train loss: 0.1848, test loss 1.0658\n",
      "Epoch 6062: train loss: 0.1848, test loss 1.0657\n",
      "Epoch 6063: train loss: 0.1848, test loss 1.0656\n",
      "Epoch 6064: train loss: 0.1848, test loss 1.0655\n",
      "Epoch 6065: train loss: 0.1848, test loss 1.0654\n",
      "Epoch 6066: train loss: 0.1848, test loss 1.0653\n",
      "Epoch 6067: train loss: 0.1848, test loss 1.0652\n",
      "Epoch 6068: train loss: 0.1848, test loss 1.0651\n",
      "Epoch 6069: train loss: 0.1848, test loss 1.0650\n",
      "Epoch 6070: train loss: 0.1848, test loss 1.0650\n",
      "Epoch 6071: train loss: 0.1848, test loss 1.0649\n",
      "Epoch 6072: train loss: 0.1848, test loss 1.0648\n",
      "Epoch 6073: train loss: 0.1848, test loss 1.0647\n",
      "Epoch 6074: train loss: 0.1848, test loss 1.0646\n",
      "Epoch 6075: train loss: 0.1848, test loss 1.0645\n",
      "Epoch 6076: train loss: 0.1848, test loss 1.0644\n",
      "Epoch 6077: train loss: 0.1848, test loss 1.0643\n",
      "Epoch 6078: train loss: 0.1848, test loss 1.0642\n",
      "Epoch 6079: train loss: 0.1848, test loss 1.0642\n",
      "Epoch 6080: train loss: 0.1848, test loss 1.0641\n",
      "Epoch 6081: train loss: 0.1848, test loss 1.0640\n",
      "Epoch 6082: train loss: 0.1848, test loss 1.0639\n",
      "Epoch 6083: train loss: 0.1848, test loss 1.0638\n",
      "Epoch 6084: train loss: 0.1848, test loss 1.0637\n",
      "Epoch 6085: train loss: 0.1847, test loss 1.0636\n",
      "Epoch 6086: train loss: 0.1847, test loss 1.0635\n",
      "Epoch 6087: train loss: 0.1847, test loss 1.0634\n",
      "Epoch 6088: train loss: 0.1847, test loss 1.0634\n",
      "Epoch 6089: train loss: 0.1847, test loss 1.0633\n",
      "Epoch 6090: train loss: 0.1847, test loss 1.0632\n",
      "Epoch 6091: train loss: 0.1847, test loss 1.0631\n",
      "Epoch 6092: train loss: 0.1847, test loss 1.0630\n",
      "Epoch 6093: train loss: 0.1847, test loss 1.0629\n",
      "Epoch 6094: train loss: 0.1847, test loss 1.0628\n",
      "Epoch 6095: train loss: 0.1847, test loss 1.0627\n",
      "Epoch 6096: train loss: 0.1847, test loss 1.0626\n",
      "Epoch 6097: train loss: 0.1847, test loss 1.0626\n",
      "Epoch 6098: train loss: 0.1847, test loss 1.0625\n",
      "Epoch 6099: train loss: 0.1847, test loss 1.0624\n",
      "Epoch 6100: train loss: 0.1847, test loss 1.0623\n",
      "Epoch 6101: train loss: 0.1847, test loss 1.0622\n",
      "Epoch 6102: train loss: 0.1847, test loss 1.0621\n",
      "Epoch 6103: train loss: 0.1847, test loss 1.0620\n",
      "Epoch 6104: train loss: 0.1847, test loss 1.0619\n",
      "Epoch 6105: train loss: 0.1847, test loss 1.0618\n",
      "Epoch 6106: train loss: 0.1847, test loss 1.0618\n",
      "Epoch 6107: train loss: 0.1847, test loss 1.0617\n",
      "Epoch 6108: train loss: 0.1847, test loss 1.0616\n",
      "Epoch 6109: train loss: 0.1847, test loss 1.0615\n",
      "Epoch 6110: train loss: 0.1847, test loss 1.0614\n",
      "Epoch 6111: train loss: 0.1847, test loss 1.0613\n",
      "Epoch 6112: train loss: 0.1847, test loss 1.0612\n",
      "Epoch 6113: train loss: 0.1847, test loss 1.0611\n",
      "Epoch 6114: train loss: 0.1847, test loss 1.0610\n",
      "Epoch 6115: train loss: 0.1847, test loss 1.0610\n",
      "Epoch 6116: train loss: 0.1847, test loss 1.0609\n",
      "Epoch 6117: train loss: 0.1847, test loss 1.0608\n",
      "Epoch 6118: train loss: 0.1847, test loss 1.0607\n",
      "Epoch 6119: train loss: 0.1847, test loss 1.0606\n",
      "Epoch 6120: train loss: 0.1847, test loss 1.0605\n",
      "Epoch 6121: train loss: 0.1847, test loss 1.0604\n",
      "Epoch 6122: train loss: 0.1846, test loss 1.0603\n",
      "Epoch 6123: train loss: 0.1846, test loss 1.0603\n",
      "Epoch 6124: train loss: 0.1846, test loss 1.0602\n",
      "Epoch 6125: train loss: 0.1846, test loss 1.0601\n",
      "Epoch 6126: train loss: 0.1846, test loss 1.0600\n",
      "Epoch 6127: train loss: 0.1846, test loss 1.0599\n",
      "Epoch 6128: train loss: 0.1846, test loss 1.0598\n",
      "Epoch 6129: train loss: 0.1846, test loss 1.0597\n",
      "Epoch 6130: train loss: 0.1846, test loss 1.0596\n",
      "Epoch 6131: train loss: 0.1846, test loss 1.0596\n",
      "Epoch 6132: train loss: 0.1846, test loss 1.0595\n",
      "Epoch 6133: train loss: 0.1846, test loss 1.0594\n",
      "Epoch 6134: train loss: 0.1846, test loss 1.0593\n",
      "Epoch 6135: train loss: 0.1846, test loss 1.0592\n",
      "Epoch 6136: train loss: 0.1846, test loss 1.0591\n",
      "Epoch 6137: train loss: 0.1846, test loss 1.0590\n",
      "Epoch 6138: train loss: 0.1846, test loss 1.0589\n",
      "Epoch 6139: train loss: 0.1846, test loss 1.0589\n",
      "Epoch 6140: train loss: 0.1846, test loss 1.0588\n",
      "Epoch 6141: train loss: 0.1846, test loss 1.0587\n",
      "Epoch 6142: train loss: 0.1846, test loss 1.0586\n",
      "Epoch 6143: train loss: 0.1846, test loss 1.0585\n",
      "Epoch 6144: train loss: 0.1846, test loss 1.0584\n",
      "Epoch 6145: train loss: 0.1846, test loss 1.0583\n",
      "Epoch 6146: train loss: 0.1846, test loss 1.0582\n",
      "Epoch 6147: train loss: 0.1846, test loss 1.0581\n",
      "Epoch 6148: train loss: 0.1846, test loss 1.0581\n",
      "Epoch 6149: train loss: 0.1846, test loss 1.0580\n",
      "Epoch 6150: train loss: 0.1846, test loss 1.0579\n",
      "Epoch 6151: train loss: 0.1846, test loss 1.0578\n",
      "Epoch 6152: train loss: 0.1846, test loss 1.0577\n",
      "Epoch 6153: train loss: 0.1846, test loss 1.0576\n",
      "Epoch 6154: train loss: 0.1846, test loss 1.0575\n",
      "Epoch 6155: train loss: 0.1846, test loss 1.0574\n",
      "Epoch 6156: train loss: 0.1846, test loss 1.0574\n",
      "Epoch 6157: train loss: 0.1846, test loss 1.0573\n",
      "Epoch 6158: train loss: 0.1846, test loss 1.0572\n",
      "Epoch 6159: train loss: 0.1846, test loss 1.0571\n",
      "Epoch 6160: train loss: 0.1845, test loss 1.0570\n",
      "Epoch 6161: train loss: 0.1845, test loss 1.0569\n",
      "Epoch 6162: train loss: 0.1845, test loss 1.0568\n",
      "Epoch 6163: train loss: 0.1845, test loss 1.0567\n",
      "Epoch 6164: train loss: 0.1845, test loss 1.0567\n",
      "Epoch 6165: train loss: 0.1845, test loss 1.0566\n",
      "Epoch 6166: train loss: 0.1845, test loss 1.0565\n",
      "Epoch 6167: train loss: 0.1845, test loss 1.0564\n",
      "Epoch 6168: train loss: 0.1845, test loss 1.0563\n",
      "Epoch 6169: train loss: 0.1845, test loss 1.0562\n",
      "Epoch 6170: train loss: 0.1845, test loss 1.0561\n",
      "Epoch 6171: train loss: 0.1845, test loss 1.0561\n",
      "Epoch 6172: train loss: 0.1845, test loss 1.0560\n",
      "Epoch 6173: train loss: 0.1845, test loss 1.0559\n",
      "Epoch 6174: train loss: 0.1845, test loss 1.0558\n",
      "Epoch 6175: train loss: 0.1845, test loss 1.0557\n",
      "Epoch 6176: train loss: 0.1845, test loss 1.0556\n",
      "Epoch 6177: train loss: 0.1845, test loss 1.0555\n",
      "Epoch 6178: train loss: 0.1845, test loss 1.0554\n",
      "Epoch 6179: train loss: 0.1845, test loss 1.0554\n",
      "Epoch 6180: train loss: 0.1845, test loss 1.0553\n",
      "Epoch 6181: train loss: 0.1845, test loss 1.0552\n",
      "Epoch 6182: train loss: 0.1845, test loss 1.0551\n",
      "Epoch 6183: train loss: 0.1845, test loss 1.0550\n",
      "Epoch 6184: train loss: 0.1845, test loss 1.0549\n",
      "Epoch 6185: train loss: 0.1845, test loss 1.0548\n",
      "Epoch 6186: train loss: 0.1845, test loss 1.0548\n",
      "Epoch 6187: train loss: 0.1845, test loss 1.0547\n",
      "Epoch 6188: train loss: 0.1845, test loss 1.0546\n",
      "Epoch 6189: train loss: 0.1845, test loss 1.0545\n",
      "Epoch 6190: train loss: 0.1845, test loss 1.0544\n",
      "Epoch 6191: train loss: 0.1845, test loss 1.0543\n",
      "Epoch 6192: train loss: 0.1845, test loss 1.0542\n",
      "Epoch 6193: train loss: 0.1845, test loss 1.0542\n",
      "Epoch 6194: train loss: 0.1845, test loss 1.0541\n",
      "Epoch 6195: train loss: 0.1845, test loss 1.0540\n",
      "Epoch 6196: train loss: 0.1845, test loss 1.0539\n",
      "Epoch 6197: train loss: 0.1844, test loss 1.0538\n",
      "Epoch 6198: train loss: 0.1844, test loss 1.0537\n",
      "Epoch 6199: train loss: 0.1844, test loss 1.0536\n",
      "Epoch 6200: train loss: 0.1844, test loss 1.0535\n",
      "Epoch 6201: train loss: 0.1844, test loss 1.0535\n",
      "Epoch 6202: train loss: 0.1844, test loss 1.0534\n",
      "Epoch 6203: train loss: 0.1844, test loss 1.0533\n",
      "Epoch 6204: train loss: 0.1844, test loss 1.0532\n",
      "Epoch 6205: train loss: 0.1844, test loss 1.0531\n",
      "Epoch 6206: train loss: 0.1844, test loss 1.0530\n",
      "Epoch 6207: train loss: 0.1844, test loss 1.0529\n",
      "Epoch 6208: train loss: 0.1844, test loss 1.0529\n",
      "Epoch 6209: train loss: 0.1844, test loss 1.0528\n",
      "Epoch 6210: train loss: 0.1844, test loss 1.0527\n",
      "Epoch 6211: train loss: 0.1844, test loss 1.0526\n",
      "Epoch 6212: train loss: 0.1844, test loss 1.0525\n",
      "Epoch 6213: train loss: 0.1844, test loss 1.0524\n",
      "Epoch 6214: train loss: 0.1844, test loss 1.0523\n",
      "Epoch 6215: train loss: 0.1844, test loss 1.0523\n",
      "Epoch 6216: train loss: 0.1844, test loss 1.0522\n",
      "Epoch 6217: train loss: 0.1844, test loss 1.0521\n",
      "Epoch 6218: train loss: 0.1844, test loss 1.0520\n",
      "Epoch 6219: train loss: 0.1844, test loss 1.0519\n",
      "Epoch 6220: train loss: 0.1844, test loss 1.0518\n",
      "Epoch 6221: train loss: 0.1844, test loss 1.0517\n",
      "Epoch 6222: train loss: 0.1844, test loss 1.0516\n",
      "Epoch 6223: train loss: 0.1844, test loss 1.0516\n",
      "Epoch 6224: train loss: 0.1844, test loss 1.0515\n",
      "Epoch 6225: train loss: 0.1844, test loss 1.0514\n",
      "Epoch 6226: train loss: 0.1844, test loss 1.0513\n",
      "Epoch 6227: train loss: 0.1844, test loss 1.0512\n",
      "Epoch 6228: train loss: 0.1844, test loss 1.0511\n",
      "Epoch 6229: train loss: 0.1844, test loss 1.0510\n",
      "Epoch 6230: train loss: 0.1844, test loss 1.0510\n",
      "Epoch 6231: train loss: 0.1844, test loss 1.0509\n",
      "Epoch 6232: train loss: 0.1844, test loss 1.0508\n",
      "Epoch 6233: train loss: 0.1844, test loss 1.0507\n",
      "Epoch 6234: train loss: 0.1844, test loss 1.0506\n",
      "Epoch 6235: train loss: 0.1843, test loss 1.0505\n",
      "Epoch 6236: train loss: 0.1843, test loss 1.0504\n",
      "Epoch 6237: train loss: 0.1843, test loss 1.0504\n",
      "Epoch 6238: train loss: 0.1843, test loss 1.0503\n",
      "Epoch 6239: train loss: 0.1843, test loss 1.0502\n",
      "Epoch 6240: train loss: 0.1843, test loss 1.0501\n",
      "Epoch 6241: train loss: 0.1843, test loss 1.0500\n",
      "Epoch 6242: train loss: 0.1843, test loss 1.0499\n",
      "Epoch 6243: train loss: 0.1843, test loss 1.0498\n",
      "Epoch 6244: train loss: 0.1843, test loss 1.0497\n",
      "Epoch 6245: train loss: 0.1843, test loss 1.0497\n",
      "Epoch 6246: train loss: 0.1843, test loss 1.0496\n",
      "Epoch 6247: train loss: 0.1843, test loss 1.0495\n",
      "Epoch 6248: train loss: 0.1843, test loss 1.0494\n",
      "Epoch 6249: train loss: 0.1843, test loss 1.0493\n",
      "Epoch 6250: train loss: 0.1843, test loss 1.0492\n",
      "Epoch 6251: train loss: 0.1843, test loss 1.0491\n",
      "Epoch 6252: train loss: 0.1843, test loss 1.0491\n",
      "Epoch 6253: train loss: 0.1843, test loss 1.0490\n",
      "Epoch 6254: train loss: 0.1843, test loss 1.0489\n",
      "Epoch 6255: train loss: 0.1843, test loss 1.0488\n",
      "Epoch 6256: train loss: 0.1843, test loss 1.0487\n",
      "Epoch 6257: train loss: 0.1843, test loss 1.0486\n",
      "Epoch 6258: train loss: 0.1843, test loss 1.0485\n",
      "Epoch 6259: train loss: 0.1843, test loss 1.0484\n",
      "Epoch 6260: train loss: 0.1843, test loss 1.0484\n",
      "Epoch 6261: train loss: 0.1843, test loss 1.0483\n",
      "Epoch 6262: train loss: 0.1843, test loss 1.0482\n",
      "Epoch 6263: train loss: 0.1843, test loss 1.0481\n",
      "Epoch 6264: train loss: 0.1843, test loss 1.0480\n",
      "Epoch 6265: train loss: 0.1843, test loss 1.0479\n",
      "Epoch 6266: train loss: 0.1843, test loss 1.0478\n",
      "Epoch 6267: train loss: 0.1843, test loss 1.0477\n",
      "Epoch 6268: train loss: 0.1843, test loss 1.0477\n",
      "Epoch 6269: train loss: 0.1843, test loss 1.0476\n",
      "Epoch 6270: train loss: 0.1843, test loss 1.0475\n",
      "Epoch 6271: train loss: 0.1843, test loss 1.0474\n",
      "Epoch 6272: train loss: 0.1843, test loss 1.0473\n",
      "Epoch 6273: train loss: 0.1842, test loss 1.0472\n",
      "Epoch 6274: train loss: 0.1842, test loss 1.0471\n",
      "Epoch 6275: train loss: 0.1842, test loss 1.0470\n",
      "Epoch 6276: train loss: 0.1842, test loss 1.0470\n",
      "Epoch 6277: train loss: 0.1842, test loss 1.0469\n",
      "Epoch 6278: train loss: 0.1842, test loss 1.0468\n",
      "Epoch 6279: train loss: 0.1842, test loss 1.0467\n",
      "Epoch 6280: train loss: 0.1842, test loss 1.0466\n",
      "Epoch 6281: train loss: 0.1842, test loss 1.0465\n",
      "Epoch 6282: train loss: 0.1842, test loss 1.0464\n",
      "Epoch 6283: train loss: 0.1842, test loss 1.0463\n",
      "Epoch 6284: train loss: 0.1842, test loss 1.0463\n",
      "Epoch 6285: train loss: 0.1842, test loss 1.0462\n",
      "Epoch 6286: train loss: 0.1842, test loss 1.0461\n",
      "Epoch 6287: train loss: 0.1842, test loss 1.0460\n",
      "Epoch 6288: train loss: 0.1842, test loss 1.0459\n",
      "Epoch 6289: train loss: 0.1842, test loss 1.0458\n",
      "Epoch 6290: train loss: 0.1842, test loss 1.0457\n",
      "Epoch 6291: train loss: 0.1842, test loss 1.0457\n",
      "Epoch 6292: train loss: 0.1842, test loss 1.0456\n",
      "Epoch 6293: train loss: 0.1842, test loss 1.0455\n",
      "Epoch 6294: train loss: 0.1842, test loss 1.0454\n",
      "Epoch 6295: train loss: 0.1842, test loss 1.0453\n",
      "Epoch 6296: train loss: 0.1842, test loss 1.0452\n",
      "Epoch 6297: train loss: 0.1842, test loss 1.0451\n",
      "Epoch 6298: train loss: 0.1842, test loss 1.0451\n",
      "Epoch 6299: train loss: 0.1842, test loss 1.0450\n",
      "Epoch 6300: train loss: 0.1842, test loss 1.0449\n",
      "Epoch 6301: train loss: 0.1842, test loss 1.0448\n",
      "Epoch 6302: train loss: 0.1842, test loss 1.0447\n",
      "Epoch 6303: train loss: 0.1842, test loss 1.0446\n",
      "Epoch 6304: train loss: 0.1842, test loss 1.0445\n",
      "Epoch 6305: train loss: 0.1842, test loss 1.0445\n",
      "Epoch 6306: train loss: 0.1842, test loss 1.0444\n",
      "Epoch 6307: train loss: 0.1842, test loss 1.0443\n",
      "Epoch 6308: train loss: 0.1842, test loss 1.0442\n",
      "Epoch 6309: train loss: 0.1842, test loss 1.0441\n",
      "Epoch 6310: train loss: 0.1842, test loss 1.0440\n",
      "Epoch 6311: train loss: 0.1841, test loss 1.0439\n",
      "Epoch 6312: train loss: 0.1841, test loss 1.0439\n",
      "Epoch 6313: train loss: 0.1841, test loss 1.0438\n",
      "Epoch 6314: train loss: 0.1841, test loss 1.0437\n",
      "Epoch 6315: train loss: 0.1841, test loss 1.0436\n",
      "Epoch 6316: train loss: 0.1841, test loss 1.0435\n",
      "Epoch 6317: train loss: 0.1841, test loss 1.0434\n",
      "Epoch 6318: train loss: 0.1841, test loss 1.0433\n",
      "Epoch 6319: train loss: 0.1841, test loss 1.0433\n",
      "Epoch 6320: train loss: 0.1841, test loss 1.0432\n",
      "Epoch 6321: train loss: 0.1841, test loss 1.0431\n",
      "Epoch 6322: train loss: 0.1841, test loss 1.0430\n",
      "Epoch 6323: train loss: 0.1841, test loss 1.0429\n",
      "Epoch 6324: train loss: 0.1841, test loss 1.0428\n",
      "Epoch 6325: train loss: 0.1841, test loss 1.0428\n",
      "Epoch 6326: train loss: 0.1841, test loss 1.0427\n",
      "Epoch 6327: train loss: 0.1841, test loss 1.0426\n",
      "Epoch 6328: train loss: 0.1841, test loss 1.0425\n",
      "Epoch 6329: train loss: 0.1841, test loss 1.0424\n",
      "Epoch 6330: train loss: 0.1841, test loss 1.0423\n",
      "Epoch 6331: train loss: 0.1841, test loss 1.0422\n",
      "Epoch 6332: train loss: 0.1841, test loss 1.0422\n",
      "Epoch 6333: train loss: 0.1841, test loss 1.0421\n",
      "Epoch 6334: train loss: 0.1841, test loss 1.0420\n",
      "Epoch 6335: train loss: 0.1841, test loss 1.0419\n",
      "Epoch 6336: train loss: 0.1841, test loss 1.0418\n",
      "Epoch 6337: train loss: 0.1841, test loss 1.0417\n",
      "Epoch 6338: train loss: 0.1841, test loss 1.0417\n",
      "Epoch 6339: train loss: 0.1841, test loss 1.0416\n",
      "Epoch 6340: train loss: 0.1841, test loss 1.0415\n",
      "Epoch 6341: train loss: 0.1841, test loss 1.0414\n",
      "Epoch 6342: train loss: 0.1841, test loss 1.0413\n",
      "Epoch 6343: train loss: 0.1841, test loss 1.0412\n",
      "Epoch 6344: train loss: 0.1841, test loss 1.0412\n",
      "Epoch 6345: train loss: 0.1841, test loss 1.0411\n",
      "Epoch 6346: train loss: 0.1841, test loss 1.0410\n",
      "Epoch 6347: train loss: 0.1841, test loss 1.0409\n",
      "Epoch 6348: train loss: 0.1841, test loss 1.0408\n",
      "Epoch 6349: train loss: 0.1840, test loss 1.0407\n",
      "Epoch 6350: train loss: 0.1840, test loss 1.0407\n",
      "Epoch 6351: train loss: 0.1840, test loss 1.0406\n",
      "Epoch 6352: train loss: 0.1840, test loss 1.0405\n",
      "Epoch 6353: train loss: 0.1840, test loss 1.0404\n",
      "Epoch 6354: train loss: 0.1840, test loss 1.0403\n",
      "Epoch 6355: train loss: 0.1840, test loss 1.0402\n",
      "Epoch 6356: train loss: 0.1840, test loss 1.0402\n",
      "Epoch 6357: train loss: 0.1840, test loss 1.0401\n",
      "Epoch 6358: train loss: 0.1840, test loss 1.0400\n",
      "Epoch 6359: train loss: 0.1840, test loss 1.0399\n",
      "Epoch 6360: train loss: 0.1840, test loss 1.0398\n",
      "Epoch 6361: train loss: 0.1840, test loss 1.0397\n",
      "Epoch 6362: train loss: 0.1840, test loss 1.0397\n",
      "Epoch 6363: train loss: 0.1840, test loss 1.0396\n",
      "Epoch 6364: train loss: 0.1840, test loss 1.0395\n",
      "Epoch 6365: train loss: 0.1840, test loss 1.0394\n",
      "Epoch 6366: train loss: 0.1840, test loss 1.0393\n",
      "Epoch 6367: train loss: 0.1840, test loss 1.0393\n",
      "Epoch 6368: train loss: 0.1840, test loss 1.0392\n",
      "Epoch 6369: train loss: 0.1840, test loss 1.0391\n",
      "Epoch 6370: train loss: 0.1840, test loss 1.0390\n",
      "Epoch 6371: train loss: 0.1840, test loss 1.0389\n",
      "Epoch 6372: train loss: 0.1840, test loss 1.0388\n",
      "Epoch 6373: train loss: 0.1840, test loss 1.0388\n",
      "Epoch 6374: train loss: 0.1840, test loss 1.0387\n",
      "Epoch 6375: train loss: 0.1840, test loss 1.0386\n",
      "Epoch 6376: train loss: 0.1840, test loss 1.0385\n",
      "Epoch 6377: train loss: 0.1840, test loss 1.0384\n",
      "Epoch 6378: train loss: 0.1840, test loss 1.0384\n",
      "Epoch 6379: train loss: 0.1840, test loss 1.0383\n",
      "Epoch 6380: train loss: 0.1840, test loss 1.0382\n",
      "Epoch 6381: train loss: 0.1840, test loss 1.0381\n",
      "Epoch 6382: train loss: 0.1840, test loss 1.0380\n",
      "Epoch 6383: train loss: 0.1840, test loss 1.0379\n",
      "Epoch 6384: train loss: 0.1840, test loss 1.0379\n",
      "Epoch 6385: train loss: 0.1840, test loss 1.0378\n",
      "Epoch 6386: train loss: 0.1840, test loss 1.0377\n",
      "Epoch 6387: train loss: 0.1840, test loss 1.0376\n",
      "Epoch 6388: train loss: 0.1840, test loss 1.0375\n",
      "Epoch 6389: train loss: 0.1839, test loss 1.0375\n",
      "Epoch 6390: train loss: 0.1839, test loss 1.0374\n",
      "Epoch 6391: train loss: 0.1839, test loss 1.0373\n",
      "Epoch 6392: train loss: 0.1839, test loss 1.0372\n",
      "Epoch 6393: train loss: 0.1839, test loss 1.0371\n",
      "Epoch 6394: train loss: 0.1839, test loss 1.0371\n",
      "Epoch 6395: train loss: 0.1839, test loss 1.0370\n",
      "Epoch 6396: train loss: 0.1839, test loss 1.0369\n",
      "Epoch 6397: train loss: 0.1839, test loss 1.0368\n",
      "Epoch 6398: train loss: 0.1839, test loss 1.0367\n",
      "Epoch 6399: train loss: 0.1839, test loss 1.0367\n",
      "Epoch 6400: train loss: 0.1839, test loss 1.0366\n",
      "Epoch 6401: train loss: 0.1839, test loss 1.0365\n",
      "Epoch 6402: train loss: 0.1839, test loss 1.0364\n",
      "Epoch 6403: train loss: 0.1839, test loss 1.0363\n",
      "Epoch 6404: train loss: 0.1839, test loss 1.0363\n",
      "Epoch 6405: train loss: 0.1839, test loss 1.0362\n",
      "Epoch 6406: train loss: 0.1839, test loss 1.0361\n",
      "Epoch 6407: train loss: 0.1839, test loss 1.0360\n",
      "Epoch 6408: train loss: 0.1839, test loss 1.0359\n",
      "Epoch 6409: train loss: 0.1839, test loss 1.0359\n",
      "Epoch 6410: train loss: 0.1839, test loss 1.0358\n",
      "Epoch 6411: train loss: 0.1839, test loss 1.0357\n",
      "Epoch 6412: train loss: 0.1839, test loss 1.0356\n",
      "Epoch 6413: train loss: 0.1839, test loss 1.0355\n",
      "Epoch 6414: train loss: 0.1839, test loss 1.0355\n",
      "Epoch 6415: train loss: 0.1839, test loss 1.0354\n",
      "Epoch 6416: train loss: 0.1839, test loss 1.0353\n",
      "Epoch 6417: train loss: 0.1839, test loss 1.0352\n",
      "Epoch 6418: train loss: 0.1839, test loss 1.0352\n",
      "Epoch 6419: train loss: 0.1839, test loss 1.0351\n",
      "Epoch 6420: train loss: 0.1839, test loss 1.0350\n",
      "Epoch 6421: train loss: 0.1839, test loss 1.0349\n",
      "Epoch 6422: train loss: 0.1839, test loss 1.0348\n",
      "Epoch 6423: train loss: 0.1839, test loss 1.0348\n",
      "Epoch 6424: train loss: 0.1839, test loss 1.0347\n",
      "Epoch 6425: train loss: 0.1839, test loss 1.0346\n",
      "Epoch 6426: train loss: 0.1839, test loss 1.0345\n",
      "Epoch 6427: train loss: 0.1839, test loss 1.0344\n",
      "Epoch 6428: train loss: 0.1839, test loss 1.0344\n",
      "Epoch 6429: train loss: 0.1838, test loss 1.0343\n",
      "Epoch 6430: train loss: 0.1838, test loss 1.0342\n",
      "Epoch 6431: train loss: 0.1838, test loss 1.0341\n",
      "Epoch 6432: train loss: 0.1838, test loss 1.0340\n",
      "Epoch 6433: train loss: 0.1838, test loss 1.0340\n",
      "Epoch 6434: train loss: 0.1838, test loss 1.0339\n",
      "Epoch 6435: train loss: 0.1838, test loss 1.0338\n",
      "Epoch 6436: train loss: 0.1838, test loss 1.0337\n",
      "Epoch 6437: train loss: 0.1838, test loss 1.0337\n",
      "Epoch 6438: train loss: 0.1838, test loss 1.0336\n",
      "Epoch 6439: train loss: 0.1838, test loss 1.0335\n",
      "Epoch 6440: train loss: 0.1838, test loss 1.0334\n",
      "Epoch 6441: train loss: 0.1838, test loss 1.0333\n",
      "Epoch 6442: train loss: 0.1838, test loss 1.0333\n",
      "Epoch 6443: train loss: 0.1838, test loss 1.0332\n",
      "Epoch 6444: train loss: 0.1838, test loss 1.0331\n",
      "Epoch 6445: train loss: 0.1838, test loss 1.0330\n",
      "Epoch 6446: train loss: 0.1838, test loss 1.0330\n",
      "Epoch 6447: train loss: 0.1838, test loss 1.0329\n",
      "Epoch 6448: train loss: 0.1838, test loss 1.0328\n",
      "Epoch 6449: train loss: 0.1838, test loss 1.0327\n",
      "Epoch 6450: train loss: 0.1838, test loss 1.0326\n",
      "Epoch 6451: train loss: 0.1838, test loss 1.0326\n",
      "Epoch 6452: train loss: 0.1838, test loss 1.0325\n",
      "Epoch 6453: train loss: 0.1838, test loss 1.0324\n",
      "Epoch 6454: train loss: 0.1838, test loss 1.0323\n",
      "Epoch 6455: train loss: 0.1838, test loss 1.0323\n",
      "Epoch 6456: train loss: 0.1838, test loss 1.0322\n",
      "Epoch 6457: train loss: 0.1838, test loss 1.0321\n",
      "Epoch 6458: train loss: 0.1838, test loss 1.0320\n",
      "Epoch 6459: train loss: 0.1838, test loss 1.0319\n",
      "Epoch 6460: train loss: 0.1838, test loss 1.0319\n",
      "Epoch 6461: train loss: 0.1838, test loss 1.0318\n",
      "Epoch 6462: train loss: 0.1838, test loss 1.0317\n",
      "Epoch 6463: train loss: 0.1838, test loss 1.0316\n",
      "Epoch 6464: train loss: 0.1838, test loss 1.0316\n",
      "Epoch 6465: train loss: 0.1838, test loss 1.0315\n",
      "Epoch 6466: train loss: 0.1838, test loss 1.0314\n",
      "Epoch 6467: train loss: 0.1838, test loss 1.0313\n",
      "Epoch 6468: train loss: 0.1838, test loss 1.0313\n",
      "Epoch 6469: train loss: 0.1838, test loss 1.0312\n",
      "Epoch 6470: train loss: 0.1837, test loss 1.0311\n",
      "Epoch 6471: train loss: 0.1837, test loss 1.0310\n",
      "Epoch 6472: train loss: 0.1837, test loss 1.0310\n",
      "Epoch 6473: train loss: 0.1837, test loss 1.0309\n",
      "Epoch 6474: train loss: 0.1837, test loss 1.0308\n",
      "Epoch 6475: train loss: 0.1837, test loss 1.0307\n",
      "Epoch 6476: train loss: 0.1837, test loss 1.0307\n",
      "Epoch 6477: train loss: 0.1837, test loss 1.0306\n",
      "Epoch 6478: train loss: 0.1837, test loss 1.0305\n",
      "Epoch 6479: train loss: 0.1837, test loss 1.0304\n",
      "Epoch 6480: train loss: 0.1837, test loss 1.0304\n",
      "Epoch 6481: train loss: 0.1837, test loss 1.0303\n",
      "Epoch 6482: train loss: 0.1837, test loss 1.0302\n",
      "Epoch 6483: train loss: 0.1837, test loss 1.0301\n",
      "Epoch 6484: train loss: 0.1837, test loss 1.0301\n",
      "Epoch 6485: train loss: 0.1837, test loss 1.0300\n",
      "Epoch 6486: train loss: 0.1837, test loss 1.0299\n",
      "Epoch 6487: train loss: 0.1837, test loss 1.0298\n",
      "Epoch 6488: train loss: 0.1837, test loss 1.0298\n",
      "Epoch 6489: train loss: 0.1837, test loss 1.0297\n",
      "Epoch 6490: train loss: 0.1837, test loss 1.0296\n",
      "Epoch 6491: train loss: 0.1837, test loss 1.0295\n",
      "Epoch 6492: train loss: 0.1837, test loss 1.0295\n",
      "Epoch 6493: train loss: 0.1837, test loss 1.0294\n",
      "Epoch 6494: train loss: 0.1837, test loss 1.0293\n",
      "Epoch 6495: train loss: 0.1837, test loss 1.0292\n",
      "Epoch 6496: train loss: 0.1837, test loss 1.0292\n",
      "Epoch 6497: train loss: 0.1837, test loss 1.0291\n",
      "Epoch 6498: train loss: 0.1837, test loss 1.0290\n",
      "Epoch 6499: train loss: 0.1837, test loss 1.0289\n",
      "Epoch 6500: train loss: 0.1837, test loss 1.0289\n",
      "Epoch 6501: train loss: 0.1837, test loss 1.0288\n",
      "Epoch 6502: train loss: 0.1837, test loss 1.0287\n",
      "Epoch 6503: train loss: 0.1837, test loss 1.0286\n",
      "Epoch 6504: train loss: 0.1837, test loss 1.0286\n",
      "Epoch 6505: train loss: 0.1837, test loss 1.0285\n",
      "Epoch 6506: train loss: 0.1837, test loss 1.0284\n",
      "Epoch 6507: train loss: 0.1837, test loss 1.0283\n",
      "Epoch 6508: train loss: 0.1837, test loss 1.0283\n",
      "Epoch 6509: train loss: 0.1837, test loss 1.0282\n",
      "Epoch 6510: train loss: 0.1837, test loss 1.0281\n",
      "Epoch 6511: train loss: 0.1836, test loss 1.0280\n",
      "Epoch 6512: train loss: 0.1836, test loss 1.0280\n",
      "Epoch 6513: train loss: 0.1836, test loss 1.0279\n",
      "Epoch 6514: train loss: 0.1836, test loss 1.0278\n",
      "Epoch 6515: train loss: 0.1836, test loss 1.0277\n",
      "Epoch 6516: train loss: 0.1836, test loss 1.0277\n",
      "Epoch 6517: train loss: 0.1836, test loss 1.0276\n",
      "Epoch 6518: train loss: 0.1836, test loss 1.0275\n",
      "Epoch 6519: train loss: 0.1836, test loss 1.0274\n",
      "Epoch 6520: train loss: 0.1836, test loss 1.0274\n",
      "Epoch 6521: train loss: 0.1836, test loss 1.0273\n",
      "Epoch 6522: train loss: 0.1836, test loss 1.0272\n",
      "Epoch 6523: train loss: 0.1836, test loss 1.0271\n",
      "Epoch 6524: train loss: 0.1836, test loss 1.0271\n",
      "Epoch 6525: train loss: 0.1836, test loss 1.0270\n",
      "Epoch 6526: train loss: 0.1836, test loss 1.0269\n",
      "Epoch 6527: train loss: 0.1836, test loss 1.0268\n",
      "Epoch 6528: train loss: 0.1836, test loss 1.0268\n",
      "Epoch 6529: train loss: 0.1836, test loss 1.0267\n",
      "Epoch 6530: train loss: 0.1836, test loss 1.0266\n",
      "Epoch 6531: train loss: 0.1836, test loss 1.0266\n",
      "Epoch 6532: train loss: 0.1836, test loss 1.0265\n",
      "Epoch 6533: train loss: 0.1836, test loss 1.0264\n",
      "Epoch 6534: train loss: 0.1836, test loss 1.0263\n",
      "Epoch 6535: train loss: 0.1836, test loss 1.0263\n",
      "Epoch 6536: train loss: 0.1836, test loss 1.0262\n",
      "Epoch 6537: train loss: 0.1836, test loss 1.0261\n",
      "Epoch 6538: train loss: 0.1836, test loss 1.0260\n",
      "Epoch 6539: train loss: 0.1836, test loss 1.0260\n",
      "Epoch 6540: train loss: 0.1836, test loss 1.0259\n",
      "Epoch 6541: train loss: 0.1836, test loss 1.0258\n",
      "Epoch 6542: train loss: 0.1836, test loss 1.0257\n",
      "Epoch 6543: train loss: 0.1836, test loss 1.0257\n",
      "Epoch 6544: train loss: 0.1836, test loss 1.0256\n",
      "Epoch 6545: train loss: 0.1836, test loss 1.0255\n",
      "Epoch 6546: train loss: 0.1836, test loss 1.0255\n",
      "Epoch 6547: train loss: 0.1836, test loss 1.0254\n",
      "Epoch 6548: train loss: 0.1836, test loss 1.0253\n",
      "Epoch 6549: train loss: 0.1836, test loss 1.0252\n",
      "Epoch 6550: train loss: 0.1836, test loss 1.0252\n",
      "Epoch 6551: train loss: 0.1836, test loss 1.0251\n",
      "Epoch 6552: train loss: 0.1836, test loss 1.0250\n",
      "Epoch 6553: train loss: 0.1835, test loss 1.0249\n",
      "Epoch 6554: train loss: 0.1835, test loss 1.0249\n",
      "Epoch 6555: train loss: 0.1835, test loss 1.0248\n",
      "Epoch 6556: train loss: 0.1835, test loss 1.0247\n",
      "Epoch 6557: train loss: 0.1835, test loss 1.0246\n",
      "Epoch 6558: train loss: 0.1835, test loss 1.0246\n",
      "Epoch 6559: train loss: 0.1835, test loss 1.0245\n",
      "Epoch 6560: train loss: 0.1835, test loss 1.0244\n",
      "Epoch 6561: train loss: 0.1835, test loss 1.0244\n",
      "Epoch 6562: train loss: 0.1835, test loss 1.0243\n",
      "Epoch 6563: train loss: 0.1835, test loss 1.0242\n",
      "Epoch 6564: train loss: 0.1835, test loss 1.0241\n",
      "Epoch 6565: train loss: 0.1835, test loss 1.0241\n",
      "Epoch 6566: train loss: 0.1835, test loss 1.0240\n",
      "Epoch 6567: train loss: 0.1835, test loss 1.0239\n",
      "Epoch 6568: train loss: 0.1835, test loss 1.0238\n",
      "Epoch 6569: train loss: 0.1835, test loss 1.0238\n",
      "Epoch 6570: train loss: 0.1835, test loss 1.0237\n",
      "Epoch 6571: train loss: 0.1835, test loss 1.0236\n",
      "Epoch 6572: train loss: 0.1835, test loss 1.0236\n",
      "Epoch 6573: train loss: 0.1835, test loss 1.0235\n",
      "Epoch 6574: train loss: 0.1835, test loss 1.0234\n",
      "Epoch 6575: train loss: 0.1835, test loss 1.0233\n",
      "Epoch 6576: train loss: 0.1835, test loss 1.0233\n",
      "Epoch 6577: train loss: 0.1835, test loss 1.0232\n",
      "Epoch 6578: train loss: 0.1835, test loss 1.0231\n",
      "Epoch 6579: train loss: 0.1835, test loss 1.0231\n",
      "Epoch 6580: train loss: 0.1835, test loss 1.0230\n",
      "Epoch 6581: train loss: 0.1835, test loss 1.0229\n",
      "Epoch 6582: train loss: 0.1835, test loss 1.0228\n",
      "Epoch 6583: train loss: 0.1835, test loss 1.0228\n",
      "Epoch 6584: train loss: 0.1835, test loss 1.0227\n",
      "Epoch 6585: train loss: 0.1835, test loss 1.0226\n",
      "Epoch 6586: train loss: 0.1835, test loss 1.0226\n",
      "Epoch 6587: train loss: 0.1835, test loss 1.0225\n",
      "Epoch 6588: train loss: 0.1835, test loss 1.0224\n",
      "Epoch 6589: train loss: 0.1835, test loss 1.0223\n",
      "Epoch 6590: train loss: 0.1835, test loss 1.0223\n",
      "Epoch 6591: train loss: 0.1835, test loss 1.0222\n",
      "Epoch 6592: train loss: 0.1835, test loss 1.0221\n",
      "Epoch 6593: train loss: 0.1835, test loss 1.0221\n",
      "Epoch 6594: train loss: 0.1835, test loss 1.0220\n",
      "Epoch 6595: train loss: 0.1835, test loss 1.0219\n",
      "Epoch 6596: train loss: 0.1834, test loss 1.0218\n",
      "Epoch 6597: train loss: 0.1834, test loss 1.0218\n",
      "Epoch 6598: train loss: 0.1834, test loss 1.0217\n",
      "Epoch 6599: train loss: 0.1834, test loss 1.0216\n",
      "Epoch 6600: train loss: 0.1834, test loss 1.0216\n",
      "Epoch 6601: train loss: 0.1834, test loss 1.0215\n",
      "Epoch 6602: train loss: 0.1834, test loss 1.0214\n",
      "Epoch 6603: train loss: 0.1834, test loss 1.0213\n",
      "Epoch 6604: train loss: 0.1834, test loss 1.0213\n",
      "Epoch 6605: train loss: 0.1834, test loss 1.0212\n",
      "Epoch 6606: train loss: 0.1834, test loss 1.0211\n",
      "Epoch 6607: train loss: 0.1834, test loss 1.0211\n",
      "Epoch 6608: train loss: 0.1834, test loss 1.0210\n",
      "Epoch 6609: train loss: 0.1834, test loss 1.0209\n",
      "Epoch 6610: train loss: 0.1834, test loss 1.0208\n",
      "Epoch 6611: train loss: 0.1834, test loss 1.0208\n",
      "Epoch 6612: train loss: 0.1834, test loss 1.0207\n",
      "Epoch 6613: train loss: 0.1834, test loss 1.0206\n",
      "Epoch 6614: train loss: 0.1834, test loss 1.0206\n",
      "Epoch 6615: train loss: 0.1834, test loss 1.0205\n",
      "Epoch 6616: train loss: 0.1834, test loss 1.0204\n",
      "Epoch 6617: train loss: 0.1834, test loss 1.0203\n",
      "Epoch 6618: train loss: 0.1834, test loss 1.0203\n",
      "Epoch 6619: train loss: 0.1834, test loss 1.0202\n",
      "Epoch 6620: train loss: 0.1834, test loss 1.0201\n",
      "Epoch 6621: train loss: 0.1834, test loss 1.0201\n",
      "Epoch 6622: train loss: 0.1834, test loss 1.0200\n",
      "Epoch 6623: train loss: 0.1834, test loss 1.0199\n",
      "Epoch 6624: train loss: 0.1834, test loss 1.0198\n",
      "Epoch 6625: train loss: 0.1834, test loss 1.0198\n",
      "Epoch 6626: train loss: 0.1834, test loss 1.0197\n",
      "Epoch 6627: train loss: 0.1834, test loss 1.0196\n",
      "Epoch 6628: train loss: 0.1834, test loss 1.0196\n",
      "Epoch 6629: train loss: 0.1834, test loss 1.0195\n",
      "Epoch 6630: train loss: 0.1834, test loss 1.0194\n",
      "Epoch 6631: train loss: 0.1834, test loss 1.0193\n",
      "Epoch 6632: train loss: 0.1834, test loss 1.0193\n",
      "Epoch 6633: train loss: 0.1834, test loss 1.0192\n",
      "Epoch 6634: train loss: 0.1834, test loss 1.0191\n",
      "Epoch 6635: train loss: 0.1834, test loss 1.0191\n",
      "Epoch 6636: train loss: 0.1834, test loss 1.0190\n",
      "Epoch 6637: train loss: 0.1834, test loss 1.0189\n",
      "Epoch 6638: train loss: 0.1834, test loss 1.0188\n",
      "Epoch 6639: train loss: 0.1833, test loss 1.0188\n",
      "Epoch 6640: train loss: 0.1833, test loss 1.0187\n",
      "Epoch 6641: train loss: 0.1833, test loss 1.0186\n",
      "Epoch 6642: train loss: 0.1833, test loss 1.0186\n",
      "Epoch 6643: train loss: 0.1833, test loss 1.0185\n",
      "Epoch 6644: train loss: 0.1833, test loss 1.0184\n",
      "Epoch 6645: train loss: 0.1833, test loss 1.0183\n",
      "Epoch 6646: train loss: 0.1833, test loss 1.0183\n",
      "Epoch 6647: train loss: 0.1833, test loss 1.0182\n",
      "Epoch 6648: train loss: 0.1833, test loss 1.0181\n",
      "Epoch 6649: train loss: 0.1833, test loss 1.0181\n",
      "Epoch 6650: train loss: 0.1833, test loss 1.0180\n",
      "Epoch 6651: train loss: 0.1833, test loss 1.0179\n",
      "Epoch 6652: train loss: 0.1833, test loss 1.0178\n",
      "Epoch 6653: train loss: 0.1833, test loss 1.0178\n",
      "Epoch 6654: train loss: 0.1833, test loss 1.0177\n",
      "Epoch 6655: train loss: 0.1833, test loss 1.0176\n",
      "Epoch 6656: train loss: 0.1833, test loss 1.0176\n",
      "Epoch 6657: train loss: 0.1833, test loss 1.0175\n",
      "Epoch 6658: train loss: 0.1833, test loss 1.0174\n",
      "Epoch 6659: train loss: 0.1833, test loss 1.0174\n",
      "Epoch 6660: train loss: 0.1833, test loss 1.0173\n",
      "Epoch 6661: train loss: 0.1833, test loss 1.0172\n",
      "Epoch 6662: train loss: 0.1833, test loss 1.0171\n",
      "Epoch 6663: train loss: 0.1833, test loss 1.0171\n",
      "Epoch 6664: train loss: 0.1833, test loss 1.0170\n",
      "Epoch 6665: train loss: 0.1833, test loss 1.0169\n",
      "Epoch 6666: train loss: 0.1833, test loss 1.0169\n",
      "Epoch 6667: train loss: 0.1833, test loss 1.0168\n",
      "Epoch 6668: train loss: 0.1833, test loss 1.0167\n",
      "Epoch 6669: train loss: 0.1833, test loss 1.0167\n",
      "Epoch 6670: train loss: 0.1833, test loss 1.0166\n",
      "Epoch 6671: train loss: 0.1833, test loss 1.0165\n",
      "Epoch 6672: train loss: 0.1833, test loss 1.0164\n",
      "Epoch 6673: train loss: 0.1833, test loss 1.0164\n",
      "Epoch 6674: train loss: 0.1833, test loss 1.0163\n",
      "Epoch 6675: train loss: 0.1833, test loss 1.0162\n",
      "Epoch 6676: train loss: 0.1833, test loss 1.0162\n",
      "Epoch 6677: train loss: 0.1833, test loss 1.0161\n",
      "Epoch 6678: train loss: 0.1833, test loss 1.0160\n",
      "Epoch 6679: train loss: 0.1833, test loss 1.0160\n",
      "Epoch 6680: train loss: 0.1833, test loss 1.0159\n",
      "Epoch 6681: train loss: 0.1833, test loss 1.0158\n",
      "Epoch 6682: train loss: 0.1833, test loss 1.0157\n",
      "Epoch 6683: train loss: 0.1832, test loss 1.0157\n",
      "Epoch 6684: train loss: 0.1832, test loss 1.0156\n",
      "Epoch 6685: train loss: 0.1832, test loss 1.0155\n",
      "Epoch 6686: train loss: 0.1832, test loss 1.0155\n",
      "Epoch 6687: train loss: 0.1832, test loss 1.0154\n",
      "Epoch 6688: train loss: 0.1832, test loss 1.0153\n",
      "Epoch 6689: train loss: 0.1832, test loss 1.0153\n",
      "Epoch 6690: train loss: 0.1832, test loss 1.0152\n",
      "Epoch 6691: train loss: 0.1832, test loss 1.0151\n",
      "Epoch 6692: train loss: 0.1832, test loss 1.0150\n",
      "Epoch 6693: train loss: 0.1832, test loss 1.0150\n",
      "Epoch 6694: train loss: 0.1832, test loss 1.0149\n",
      "Epoch 6695: train loss: 0.1832, test loss 1.0148\n",
      "Epoch 6696: train loss: 0.1832, test loss 1.0148\n",
      "Epoch 6697: train loss: 0.1832, test loss 1.0147\n",
      "Epoch 6698: train loss: 0.1832, test loss 1.0146\n",
      "Epoch 6699: train loss: 0.1832, test loss 1.0146\n",
      "Epoch 6700: train loss: 0.1832, test loss 1.0145\n",
      "Epoch 6701: train loss: 0.1832, test loss 1.0144\n",
      "Epoch 6702: train loss: 0.1832, test loss 1.0143\n",
      "Epoch 6703: train loss: 0.1832, test loss 1.0143\n",
      "Epoch 6704: train loss: 0.1832, test loss 1.0142\n",
      "Epoch 6705: train loss: 0.1832, test loss 1.0141\n",
      "Epoch 6706: train loss: 0.1832, test loss 1.0141\n",
      "Epoch 6707: train loss: 0.1832, test loss 1.0140\n",
      "Epoch 6708: train loss: 0.1832, test loss 1.0139\n",
      "Epoch 6709: train loss: 0.1832, test loss 1.0139\n",
      "Epoch 6710: train loss: 0.1832, test loss 1.0138\n",
      "Epoch 6711: train loss: 0.1832, test loss 1.0137\n",
      "Epoch 6712: train loss: 0.1832, test loss 1.0136\n",
      "Epoch 6713: train loss: 0.1832, test loss 1.0136\n",
      "Epoch 6714: train loss: 0.1832, test loss 1.0135\n",
      "Epoch 6715: train loss: 0.1832, test loss 1.0134\n",
      "Epoch 6716: train loss: 0.1832, test loss 1.0134\n",
      "Epoch 6717: train loss: 0.1832, test loss 1.0133\n",
      "Epoch 6718: train loss: 0.1832, test loss 1.0132\n",
      "Epoch 6719: train loss: 0.1832, test loss 1.0131\n",
      "Epoch 6720: train loss: 0.1832, test loss 1.0131\n",
      "Epoch 6721: train loss: 0.1832, test loss 1.0130\n",
      "Epoch 6722: train loss: 0.1832, test loss 1.0129\n",
      "Epoch 6723: train loss: 0.1832, test loss 1.0129\n",
      "Epoch 6724: train loss: 0.1832, test loss 1.0128\n",
      "Epoch 6725: train loss: 0.1831, test loss 1.0127\n",
      "Epoch 6726: train loss: 0.1831, test loss 1.0127\n",
      "Epoch 6727: train loss: 0.1831, test loss 1.0126\n",
      "Epoch 6728: train loss: 0.1831, test loss 1.0125\n",
      "Epoch 6729: train loss: 0.1831, test loss 1.0124\n",
      "Epoch 6730: train loss: 0.1831, test loss 1.0124\n",
      "Epoch 6731: train loss: 0.1831, test loss 1.0123\n",
      "Epoch 6732: train loss: 0.1831, test loss 1.0122\n",
      "Epoch 6733: train loss: 0.1831, test loss 1.0122\n",
      "Epoch 6734: train loss: 0.1831, test loss 1.0121\n",
      "Epoch 6735: train loss: 0.1831, test loss 1.0120\n",
      "Epoch 6736: train loss: 0.1831, test loss 1.0120\n",
      "Epoch 6737: train loss: 0.1831, test loss 1.0119\n",
      "Epoch 6738: train loss: 0.1831, test loss 1.0118\n",
      "Epoch 6739: train loss: 0.1831, test loss 1.0117\n",
      "Epoch 6740: train loss: 0.1831, test loss 1.0117\n",
      "Epoch 6741: train loss: 0.1831, test loss 1.0116\n",
      "Epoch 6742: train loss: 0.1831, test loss 1.0115\n",
      "Epoch 6743: train loss: 0.1831, test loss 1.0115\n",
      "Epoch 6744: train loss: 0.1831, test loss 1.0114\n",
      "Epoch 6745: train loss: 0.1831, test loss 1.0113\n",
      "Epoch 6746: train loss: 0.1831, test loss 1.0113\n",
      "Epoch 6747: train loss: 0.1831, test loss 1.0112\n",
      "Epoch 6748: train loss: 0.1831, test loss 1.0111\n",
      "Epoch 6749: train loss: 0.1831, test loss 1.0110\n",
      "Epoch 6750: train loss: 0.1831, test loss 1.0110\n",
      "Epoch 6751: train loss: 0.1831, test loss 1.0109\n",
      "Epoch 6752: train loss: 0.1831, test loss 1.0108\n",
      "Epoch 6753: train loss: 0.1831, test loss 1.0108\n",
      "Epoch 6754: train loss: 0.1831, test loss 1.0107\n",
      "Epoch 6755: train loss: 0.1831, test loss 1.0106\n",
      "Epoch 6756: train loss: 0.1831, test loss 1.0106\n",
      "Epoch 6757: train loss: 0.1831, test loss 1.0105\n",
      "Epoch 6758: train loss: 0.1831, test loss 1.0104\n",
      "Epoch 6759: train loss: 0.1831, test loss 1.0103\n",
      "Epoch 6760: train loss: 0.1831, test loss 1.0103\n",
      "Epoch 6761: train loss: 0.1831, test loss 1.0102\n",
      "Epoch 6762: train loss: 0.1831, test loss 1.0101\n",
      "Epoch 6763: train loss: 0.1831, test loss 1.0101\n",
      "Epoch 6764: train loss: 0.1831, test loss 1.0100\n",
      "Epoch 6765: train loss: 0.1831, test loss 1.0099\n",
      "Epoch 6766: train loss: 0.1831, test loss 1.0099\n",
      "Epoch 6767: train loss: 0.1831, test loss 1.0098\n",
      "Epoch 6768: train loss: 0.1831, test loss 1.0097\n",
      "Epoch 6769: train loss: 0.1830, test loss 1.0097\n",
      "Epoch 6770: train loss: 0.1830, test loss 1.0096\n",
      "Epoch 6771: train loss: 0.1830, test loss 1.0095\n",
      "Epoch 6772: train loss: 0.1830, test loss 1.0094\n",
      "Epoch 6773: train loss: 0.1830, test loss 1.0094\n",
      "Epoch 6774: train loss: 0.1830, test loss 1.0093\n",
      "Epoch 6775: train loss: 0.1830, test loss 1.0092\n",
      "Epoch 6776: train loss: 0.1830, test loss 1.0092\n",
      "Epoch 6777: train loss: 0.1830, test loss 1.0091\n",
      "Epoch 6778: train loss: 0.1830, test loss 1.0090\n",
      "Epoch 6779: train loss: 0.1830, test loss 1.0090\n",
      "Epoch 6780: train loss: 0.1830, test loss 1.0089\n",
      "Epoch 6781: train loss: 0.1830, test loss 1.0088\n",
      "Epoch 6782: train loss: 0.1830, test loss 1.0088\n",
      "Epoch 6783: train loss: 0.1830, test loss 1.0087\n",
      "Epoch 6784: train loss: 0.1830, test loss 1.0086\n",
      "Epoch 6785: train loss: 0.1830, test loss 1.0085\n",
      "Epoch 6786: train loss: 0.1830, test loss 1.0085\n",
      "Epoch 6787: train loss: 0.1830, test loss 1.0084\n",
      "Epoch 6788: train loss: 0.1830, test loss 1.0083\n",
      "Epoch 6789: train loss: 0.1830, test loss 1.0083\n",
      "Epoch 6790: train loss: 0.1830, test loss 1.0082\n",
      "Epoch 6791: train loss: 0.1830, test loss 1.0081\n",
      "Epoch 6792: train loss: 0.1830, test loss 1.0081\n",
      "Epoch 6793: train loss: 0.1830, test loss 1.0080\n",
      "Epoch 6794: train loss: 0.1830, test loss 1.0079\n",
      "Epoch 6795: train loss: 0.1830, test loss 1.0079\n",
      "Epoch 6796: train loss: 0.1830, test loss 1.0078\n",
      "Epoch 6797: train loss: 0.1830, test loss 1.0077\n",
      "Epoch 6798: train loss: 0.1830, test loss 1.0077\n",
      "Epoch 6799: train loss: 0.1830, test loss 1.0076\n",
      "Epoch 6800: train loss: 0.1830, test loss 1.0075\n",
      "Epoch 6801: train loss: 0.1830, test loss 1.0075\n",
      "Epoch 6802: train loss: 0.1830, test loss 1.0074\n",
      "Epoch 6803: train loss: 0.1830, test loss 1.0073\n",
      "Epoch 6804: train loss: 0.1830, test loss 1.0072\n",
      "Epoch 6805: train loss: 0.1830, test loss 1.0072\n",
      "Epoch 6806: train loss: 0.1830, test loss 1.0071\n",
      "Epoch 6807: train loss: 0.1830, test loss 1.0070\n",
      "Epoch 6808: train loss: 0.1830, test loss 1.0069\n",
      "Epoch 6809: train loss: 0.1830, test loss 1.0069\n",
      "Epoch 6810: train loss: 0.1830, test loss 1.0068\n",
      "Epoch 6811: train loss: 0.1830, test loss 1.0067\n",
      "Epoch 6812: train loss: 0.1830, test loss 1.0066\n",
      "Epoch 6813: train loss: 0.1829, test loss 1.0066\n",
      "Epoch 6814: train loss: 0.1829, test loss 1.0065\n",
      "Epoch 6815: train loss: 0.1829, test loss 1.0064\n",
      "Epoch 6816: train loss: 0.1829, test loss 1.0063\n",
      "Epoch 6817: train loss: 0.1829, test loss 1.0063\n",
      "Epoch 6818: train loss: 0.1829, test loss 1.0062\n",
      "Epoch 6819: train loss: 0.1829, test loss 1.0061\n",
      "Epoch 6820: train loss: 0.1829, test loss 1.0060\n",
      "Epoch 6821: train loss: 0.1829, test loss 1.0060\n",
      "Epoch 6822: train loss: 0.1829, test loss 1.0059\n",
      "Epoch 6823: train loss: 0.1829, test loss 1.0058\n",
      "Epoch 6824: train loss: 0.1829, test loss 1.0058\n",
      "Epoch 6825: train loss: 0.1829, test loss 1.0057\n",
      "Epoch 6826: train loss: 0.1829, test loss 1.0056\n",
      "Epoch 6827: train loss: 0.1829, test loss 1.0055\n",
      "Epoch 6828: train loss: 0.1829, test loss 1.0055\n",
      "Epoch 6829: train loss: 0.1829, test loss 1.0054\n",
      "Epoch 6830: train loss: 0.1829, test loss 1.0053\n",
      "Epoch 6831: train loss: 0.1829, test loss 1.0052\n",
      "Epoch 6832: train loss: 0.1829, test loss 1.0052\n",
      "Epoch 6833: train loss: 0.1829, test loss 1.0051\n",
      "Epoch 6834: train loss: 0.1829, test loss 1.0050\n",
      "Epoch 6835: train loss: 0.1829, test loss 1.0049\n",
      "Epoch 6836: train loss: 0.1829, test loss 1.0049\n",
      "Epoch 6837: train loss: 0.1829, test loss 1.0048\n",
      "Epoch 6838: train loss: 0.1829, test loss 1.0047\n",
      "Epoch 6839: train loss: 0.1829, test loss 1.0046\n",
      "Epoch 6840: train loss: 0.1829, test loss 1.0046\n",
      "Epoch 6841: train loss: 0.1829, test loss 1.0045\n",
      "Epoch 6842: train loss: 0.1829, test loss 1.0044\n",
      "Epoch 6843: train loss: 0.1829, test loss 1.0044\n",
      "Epoch 6844: train loss: 0.1829, test loss 1.0043\n",
      "Epoch 6845: train loss: 0.1829, test loss 1.0042\n",
      "Epoch 6846: train loss: 0.1829, test loss 1.0041\n",
      "Epoch 6847: train loss: 0.1829, test loss 1.0041\n",
      "Epoch 6848: train loss: 0.1829, test loss 1.0040\n",
      "Epoch 6849: train loss: 0.1829, test loss 1.0039\n",
      "Epoch 6850: train loss: 0.1829, test loss 1.0038\n",
      "Epoch 6851: train loss: 0.1829, test loss 1.0038\n",
      "Epoch 6852: train loss: 0.1829, test loss 1.0037\n",
      "Epoch 6853: train loss: 0.1829, test loss 1.0036\n",
      "Epoch 6854: train loss: 0.1829, test loss 1.0036\n",
      "Epoch 6855: train loss: 0.1829, test loss 1.0035\n",
      "Epoch 6856: train loss: 0.1829, test loss 1.0034\n",
      "Epoch 6857: train loss: 0.1828, test loss 1.0033\n",
      "Epoch 6858: train loss: 0.1828, test loss 1.0033\n",
      "Epoch 6859: train loss: 0.1828, test loss 1.0032\n",
      "Epoch 6860: train loss: 0.1828, test loss 1.0031\n",
      "Epoch 6861: train loss: 0.1828, test loss 1.0030\n",
      "Epoch 6862: train loss: 0.1828, test loss 1.0030\n",
      "Epoch 6863: train loss: 0.1828, test loss 1.0029\n",
      "Epoch 6864: train loss: 0.1828, test loss 1.0028\n",
      "Epoch 6865: train loss: 0.1828, test loss 1.0028\n",
      "Epoch 6866: train loss: 0.1828, test loss 1.0027\n",
      "Epoch 6867: train loss: 0.1828, test loss 1.0026\n",
      "Epoch 6868: train loss: 0.1828, test loss 1.0025\n",
      "Epoch 6869: train loss: 0.1828, test loss 1.0025\n",
      "Epoch 6870: train loss: 0.1828, test loss 1.0024\n",
      "Epoch 6871: train loss: 0.1828, test loss 1.0023\n",
      "Epoch 6872: train loss: 0.1828, test loss 1.0022\n",
      "Epoch 6873: train loss: 0.1828, test loss 1.0022\n",
      "Epoch 6874: train loss: 0.1828, test loss 1.0021\n",
      "Epoch 6875: train loss: 0.1828, test loss 1.0020\n",
      "Epoch 6876: train loss: 0.1828, test loss 1.0020\n",
      "Epoch 6877: train loss: 0.1828, test loss 1.0019\n",
      "Epoch 6878: train loss: 0.1828, test loss 1.0018\n",
      "Epoch 6879: train loss: 0.1828, test loss 1.0017\n",
      "Epoch 6880: train loss: 0.1828, test loss 1.0017\n",
      "Epoch 6881: train loss: 0.1828, test loss 1.0016\n",
      "Epoch 6882: train loss: 0.1828, test loss 1.0015\n",
      "Epoch 6883: train loss: 0.1828, test loss 1.0014\n",
      "Epoch 6884: train loss: 0.1828, test loss 1.0014\n",
      "Epoch 6885: train loss: 0.1828, test loss 1.0013\n",
      "Epoch 6886: train loss: 0.1828, test loss 1.0012\n",
      "Epoch 6887: train loss: 0.1828, test loss 1.0012\n",
      "Epoch 6888: train loss: 0.1828, test loss 1.0011\n",
      "Epoch 6889: train loss: 0.1828, test loss 1.0010\n",
      "Epoch 6890: train loss: 0.1828, test loss 1.0009\n",
      "Epoch 6891: train loss: 0.1828, test loss 1.0009\n",
      "Epoch 6892: train loss: 0.1828, test loss 1.0008\n",
      "Epoch 6893: train loss: 0.1828, test loss 1.0007\n",
      "Epoch 6894: train loss: 0.1828, test loss 1.0006\n",
      "Epoch 6895: train loss: 0.1828, test loss 1.0006\n",
      "Epoch 6896: train loss: 0.1828, test loss 1.0005\n",
      "Epoch 6897: train loss: 0.1828, test loss 1.0004\n",
      "Epoch 6898: train loss: 0.1828, test loss 1.0004\n",
      "Epoch 6899: train loss: 0.1828, test loss 1.0003\n",
      "Epoch 6900: train loss: 0.1828, test loss 1.0002\n",
      "Epoch 6901: train loss: 0.1828, test loss 1.0001\n",
      "Epoch 6902: train loss: 0.1827, test loss 1.0001\n",
      "Epoch 6903: train loss: 0.1827, test loss 1.0000\n",
      "Epoch 6904: train loss: 0.1827, test loss 0.9999\n",
      "Epoch 6905: train loss: 0.1827, test loss 0.9998\n",
      "Epoch 6906: train loss: 0.1827, test loss 0.9998\n",
      "Epoch 6907: train loss: 0.1827, test loss 0.9997\n",
      "Epoch 6908: train loss: 0.1827, test loss 0.9996\n",
      "Epoch 6909: train loss: 0.1827, test loss 0.9995\n",
      "Epoch 6910: train loss: 0.1827, test loss 0.9995\n",
      "Epoch 6911: train loss: 0.1827, test loss 0.9994\n",
      "Epoch 6912: train loss: 0.1827, test loss 0.9993\n",
      "Epoch 6913: train loss: 0.1827, test loss 0.9993\n",
      "Epoch 6914: train loss: 0.1827, test loss 0.9992\n",
      "Epoch 6915: train loss: 0.1827, test loss 0.9991\n",
      "Epoch 6916: train loss: 0.1827, test loss 0.9990\n",
      "Epoch 6917: train loss: 0.1827, test loss 0.9990\n",
      "Epoch 6918: train loss: 0.1827, test loss 0.9989\n",
      "Epoch 6919: train loss: 0.1827, test loss 0.9988\n",
      "Epoch 6920: train loss: 0.1827, test loss 0.9987\n",
      "Epoch 6921: train loss: 0.1827, test loss 0.9987\n",
      "Epoch 6922: train loss: 0.1827, test loss 0.9986\n",
      "Epoch 6923: train loss: 0.1827, test loss 0.9985\n",
      "Epoch 6924: train loss: 0.1827, test loss 0.9985\n",
      "Epoch 6925: train loss: 0.1827, test loss 0.9984\n",
      "Epoch 6926: train loss: 0.1827, test loss 0.9983\n",
      "Epoch 6927: train loss: 0.1827, test loss 0.9982\n",
      "Epoch 6928: train loss: 0.1827, test loss 0.9982\n",
      "Epoch 6929: train loss: 0.1827, test loss 0.9981\n",
      "Epoch 6930: train loss: 0.1827, test loss 0.9980\n",
      "Epoch 6931: train loss: 0.1827, test loss 0.9979\n",
      "Epoch 6932: train loss: 0.1827, test loss 0.9979\n",
      "Epoch 6933: train loss: 0.1827, test loss 0.9978\n",
      "Epoch 6934: train loss: 0.1827, test loss 0.9977\n",
      "Epoch 6935: train loss: 0.1827, test loss 0.9977\n",
      "Epoch 6936: train loss: 0.1827, test loss 0.9976\n",
      "Epoch 6937: train loss: 0.1827, test loss 0.9975\n",
      "Epoch 6938: train loss: 0.1827, test loss 0.9974\n",
      "Epoch 6939: train loss: 0.1827, test loss 0.9974\n",
      "Epoch 6940: train loss: 0.1827, test loss 0.9973\n",
      "Epoch 6941: train loss: 0.1827, test loss 0.9972\n",
      "Epoch 6942: train loss: 0.1827, test loss 0.9972\n",
      "Epoch 6943: train loss: 0.1827, test loss 0.9971\n",
      "Epoch 6944: train loss: 0.1827, test loss 0.9970\n",
      "Epoch 6945: train loss: 0.1827, test loss 0.9969\n",
      "Epoch 6946: train loss: 0.1827, test loss 0.9969\n",
      "Epoch 6947: train loss: 0.1826, test loss 0.9968\n",
      "Epoch 6948: train loss: 0.1826, test loss 0.9967\n",
      "Epoch 6949: train loss: 0.1826, test loss 0.9966\n",
      "Epoch 6950: train loss: 0.1826, test loss 0.9966\n",
      "Epoch 6951: train loss: 0.1826, test loss 0.9965\n",
      "Epoch 6952: train loss: 0.1826, test loss 0.9964\n",
      "Epoch 6953: train loss: 0.1826, test loss 0.9964\n",
      "Epoch 6954: train loss: 0.1826, test loss 0.9963\n",
      "Epoch 6955: train loss: 0.1826, test loss 0.9962\n",
      "Epoch 6956: train loss: 0.1826, test loss 0.9961\n",
      "Epoch 6957: train loss: 0.1826, test loss 0.9961\n",
      "Epoch 6958: train loss: 0.1826, test loss 0.9960\n",
      "Epoch 6959: train loss: 0.1826, test loss 0.9959\n",
      "Epoch 6960: train loss: 0.1826, test loss 0.9959\n",
      "Epoch 6961: train loss: 0.1826, test loss 0.9958\n",
      "Epoch 6962: train loss: 0.1826, test loss 0.9957\n",
      "Epoch 6963: train loss: 0.1826, test loss 0.9956\n",
      "Epoch 6964: train loss: 0.1826, test loss 0.9956\n",
      "Epoch 6965: train loss: 0.1826, test loss 0.9955\n",
      "Epoch 6966: train loss: 0.1826, test loss 0.9954\n",
      "Epoch 6967: train loss: 0.1826, test loss 0.9953\n",
      "Epoch 6968: train loss: 0.1826, test loss 0.9953\n",
      "Epoch 6969: train loss: 0.1826, test loss 0.9952\n",
      "Epoch 6970: train loss: 0.1826, test loss 0.9951\n",
      "Epoch 6971: train loss: 0.1826, test loss 0.9951\n",
      "Epoch 6972: train loss: 0.1826, test loss 0.9950\n",
      "Epoch 6973: train loss: 0.1826, test loss 0.9949\n",
      "Epoch 6974: train loss: 0.1826, test loss 0.9949\n",
      "Epoch 6975: train loss: 0.1826, test loss 0.9948\n",
      "Epoch 6976: train loss: 0.1826, test loss 0.9947\n",
      "Epoch 6977: train loss: 0.1826, test loss 0.9946\n",
      "Epoch 6978: train loss: 0.1826, test loss 0.9946\n",
      "Epoch 6979: train loss: 0.1826, test loss 0.9945\n",
      "Epoch 6980: train loss: 0.1826, test loss 0.9944\n",
      "Epoch 6981: train loss: 0.1826, test loss 0.9943\n",
      "Epoch 6982: train loss: 0.1826, test loss 0.9943\n",
      "Epoch 6983: train loss: 0.1826, test loss 0.9942\n",
      "Epoch 6984: train loss: 0.1826, test loss 0.9941\n",
      "Epoch 6985: train loss: 0.1826, test loss 0.9941\n",
      "Epoch 6986: train loss: 0.1826, test loss 0.9940\n",
      "Epoch 6987: train loss: 0.1826, test loss 0.9939\n",
      "Epoch 6988: train loss: 0.1826, test loss 0.9938\n",
      "Epoch 6989: train loss: 0.1826, test loss 0.9938\n",
      "Epoch 6990: train loss: 0.1826, test loss 0.9937\n",
      "Epoch 6991: train loss: 0.1826, test loss 0.9936\n",
      "Epoch 6992: train loss: 0.1826, test loss 0.9936\n",
      "Epoch 6993: train loss: 0.1825, test loss 0.9935\n",
      "Epoch 6994: train loss: 0.1825, test loss 0.9934\n",
      "Epoch 6995: train loss: 0.1825, test loss 0.9934\n",
      "Epoch 6996: train loss: 0.1825, test loss 0.9933\n",
      "Epoch 6997: train loss: 0.1825, test loss 0.9932\n",
      "Epoch 6998: train loss: 0.1825, test loss 0.9931\n",
      "Epoch 6999: train loss: 0.1825, test loss 0.9931\n",
      "Epoch 7000: train loss: 0.1825, test loss 0.9930\n",
      "Epoch 7001: train loss: 0.1825, test loss 0.9929\n",
      "Epoch 7002: train loss: 0.1825, test loss 0.9928\n",
      "Epoch 7003: train loss: 0.1825, test loss 0.9928\n",
      "Epoch 7004: train loss: 0.1825, test loss 0.9927\n",
      "Epoch 7005: train loss: 0.1825, test loss 0.9926\n",
      "Epoch 7006: train loss: 0.1825, test loss 0.9926\n",
      "Epoch 7007: train loss: 0.1825, test loss 0.9925\n",
      "Epoch 7008: train loss: 0.1825, test loss 0.9924\n",
      "Epoch 7009: train loss: 0.1825, test loss 0.9923\n",
      "Epoch 7010: train loss: 0.1825, test loss 0.9923\n",
      "Epoch 7011: train loss: 0.1825, test loss 0.9922\n",
      "Epoch 7012: train loss: 0.1825, test loss 0.9921\n",
      "Epoch 7013: train loss: 0.1825, test loss 0.9921\n",
      "Epoch 7014: train loss: 0.1825, test loss 0.9920\n",
      "Epoch 7015: train loss: 0.1825, test loss 0.9919\n",
      "Epoch 7016: train loss: 0.1825, test loss 0.9919\n",
      "Epoch 7017: train loss: 0.1825, test loss 0.9918\n",
      "Epoch 7018: train loss: 0.1825, test loss 0.9917\n",
      "Epoch 7019: train loss: 0.1825, test loss 0.9916\n",
      "Epoch 7020: train loss: 0.1825, test loss 0.9916\n",
      "Epoch 7021: train loss: 0.1825, test loss 0.9915\n",
      "Epoch 7022: train loss: 0.1825, test loss 0.9914\n",
      "Epoch 7023: train loss: 0.1825, test loss 0.9914\n",
      "Epoch 7024: train loss: 0.1825, test loss 0.9913\n",
      "Epoch 7025: train loss: 0.1825, test loss 0.9912\n",
      "Epoch 7026: train loss: 0.1825, test loss 0.9911\n",
      "Epoch 7027: train loss: 0.1825, test loss 0.9911\n",
      "Epoch 7028: train loss: 0.1825, test loss 0.9910\n",
      "Epoch 7029: train loss: 0.1825, test loss 0.9909\n",
      "Epoch 7030: train loss: 0.1825, test loss 0.9909\n",
      "Epoch 7031: train loss: 0.1825, test loss 0.9908\n",
      "Epoch 7032: train loss: 0.1825, test loss 0.9907\n",
      "Epoch 7033: train loss: 0.1825, test loss 0.9906\n",
      "Epoch 7034: train loss: 0.1825, test loss 0.9906\n",
      "Epoch 7035: train loss: 0.1825, test loss 0.9905\n",
      "Epoch 7036: train loss: 0.1825, test loss 0.9904\n",
      "Epoch 7037: train loss: 0.1825, test loss 0.9904\n",
      "Epoch 7038: train loss: 0.1825, test loss 0.9903\n",
      "Epoch 7039: train loss: 0.1824, test loss 0.9902\n",
      "Epoch 7040: train loss: 0.1824, test loss 0.9901\n",
      "Epoch 7041: train loss: 0.1824, test loss 0.9901\n",
      "Epoch 7042: train loss: 0.1824, test loss 0.9900\n",
      "Epoch 7043: train loss: 0.1824, test loss 0.9899\n",
      "Epoch 7044: train loss: 0.1824, test loss 0.9899\n",
      "Epoch 7045: train loss: 0.1824, test loss 0.9898\n",
      "Epoch 7046: train loss: 0.1824, test loss 0.9897\n",
      "Epoch 7047: train loss: 0.1824, test loss 0.9897\n",
      "Epoch 7048: train loss: 0.1824, test loss 0.9896\n",
      "Epoch 7049: train loss: 0.1824, test loss 0.9895\n",
      "Epoch 7050: train loss: 0.1824, test loss 0.9895\n",
      "Epoch 7051: train loss: 0.1824, test loss 0.9894\n",
      "Epoch 7052: train loss: 0.1824, test loss 0.9893\n",
      "Epoch 7053: train loss: 0.1824, test loss 0.9892\n",
      "Epoch 7054: train loss: 0.1824, test loss 0.9892\n",
      "Epoch 7055: train loss: 0.1824, test loss 0.9891\n",
      "Epoch 7056: train loss: 0.1824, test loss 0.9890\n",
      "Epoch 7057: train loss: 0.1824, test loss 0.9890\n",
      "Epoch 7058: train loss: 0.1824, test loss 0.9889\n",
      "Epoch 7059: train loss: 0.1824, test loss 0.9888\n",
      "Epoch 7060: train loss: 0.1824, test loss 0.9887\n",
      "Epoch 7061: train loss: 0.1824, test loss 0.9887\n",
      "Epoch 7062: train loss: 0.1824, test loss 0.9886\n",
      "Epoch 7063: train loss: 0.1824, test loss 0.9885\n",
      "Epoch 7064: train loss: 0.1824, test loss 0.9885\n",
      "Epoch 7065: train loss: 0.1824, test loss 0.9884\n",
      "Epoch 7066: train loss: 0.1824, test loss 0.9883\n",
      "Epoch 7067: train loss: 0.1824, test loss 0.9883\n",
      "Epoch 7068: train loss: 0.1824, test loss 0.9882\n",
      "Epoch 7069: train loss: 0.1824, test loss 0.9881\n",
      "Epoch 7070: train loss: 0.1824, test loss 0.9880\n",
      "Epoch 7071: train loss: 0.1824, test loss 0.9880\n",
      "Epoch 7072: train loss: 0.1824, test loss 0.9879\n",
      "Epoch 7073: train loss: 0.1824, test loss 0.9878\n",
      "Epoch 7074: train loss: 0.1824, test loss 0.9878\n",
      "Epoch 7075: train loss: 0.1824, test loss 0.9877\n",
      "Epoch 7076: train loss: 0.1824, test loss 0.9876\n",
      "Epoch 7077: train loss: 0.1824, test loss 0.9876\n",
      "Epoch 7078: train loss: 0.1824, test loss 0.9875\n",
      "Epoch 7079: train loss: 0.1824, test loss 0.9874\n",
      "Epoch 7080: train loss: 0.1824, test loss 0.9873\n",
      "Epoch 7081: train loss: 0.1824, test loss 0.9873\n",
      "Epoch 7082: train loss: 0.1824, test loss 0.9872\n",
      "Epoch 7083: train loss: 0.1824, test loss 0.9871\n",
      "Epoch 7084: train loss: 0.1824, test loss 0.9871\n",
      "Epoch 7085: train loss: 0.1824, test loss 0.9870\n",
      "Epoch 7086: train loss: 0.1823, test loss 0.9869\n",
      "Epoch 7087: train loss: 0.1823, test loss 0.9869\n",
      "Epoch 7088: train loss: 0.1823, test loss 0.9868\n",
      "Epoch 7089: train loss: 0.1823, test loss 0.9867\n",
      "Epoch 7090: train loss: 0.1823, test loss 0.9867\n",
      "Epoch 7091: train loss: 0.1823, test loss 0.9866\n",
      "Epoch 7092: train loss: 0.1823, test loss 0.9865\n",
      "Epoch 7093: train loss: 0.1823, test loss 0.9864\n",
      "Epoch 7094: train loss: 0.1823, test loss 0.9864\n",
      "Epoch 7095: train loss: 0.1823, test loss 0.9863\n",
      "Epoch 7096: train loss: 0.1823, test loss 0.9862\n",
      "Epoch 7097: train loss: 0.1823, test loss 0.9862\n",
      "Epoch 7098: train loss: 0.1823, test loss 0.9861\n",
      "Epoch 7099: train loss: 0.1823, test loss 0.9860\n",
      "Epoch 7100: train loss: 0.1823, test loss 0.9860\n",
      "Epoch 7101: train loss: 0.1823, test loss 0.9859\n",
      "Epoch 7102: train loss: 0.1823, test loss 0.9858\n",
      "Epoch 7103: train loss: 0.1823, test loss 0.9857\n",
      "Epoch 7104: train loss: 0.1823, test loss 0.9857\n",
      "Epoch 7105: train loss: 0.1823, test loss 0.9856\n",
      "Epoch 7106: train loss: 0.1823, test loss 0.9855\n",
      "Epoch 7107: train loss: 0.1823, test loss 0.9855\n",
      "Epoch 7108: train loss: 0.1823, test loss 0.9854\n",
      "Epoch 7109: train loss: 0.1823, test loss 0.9853\n",
      "Epoch 7110: train loss: 0.1823, test loss 0.9853\n",
      "Epoch 7111: train loss: 0.1823, test loss 0.9852\n",
      "Epoch 7112: train loss: 0.1823, test loss 0.9851\n",
      "Epoch 7113: train loss: 0.1823, test loss 0.9851\n",
      "Epoch 7114: train loss: 0.1823, test loss 0.9850\n",
      "Epoch 7115: train loss: 0.1823, test loss 0.9849\n",
      "Epoch 7116: train loss: 0.1823, test loss 0.9848\n",
      "Epoch 7117: train loss: 0.1823, test loss 0.9848\n",
      "Epoch 7118: train loss: 0.1823, test loss 0.9847\n",
      "Epoch 7119: train loss: 0.1823, test loss 0.9846\n",
      "Epoch 7120: train loss: 0.1823, test loss 0.9846\n",
      "Epoch 7121: train loss: 0.1823, test loss 0.9845\n",
      "Epoch 7122: train loss: 0.1823, test loss 0.9844\n",
      "Epoch 7123: train loss: 0.1823, test loss 0.9844\n",
      "Epoch 7124: train loss: 0.1823, test loss 0.9843\n",
      "Epoch 7125: train loss: 0.1823, test loss 0.9842\n",
      "Epoch 7126: train loss: 0.1823, test loss 0.9842\n",
      "Epoch 7127: train loss: 0.1823, test loss 0.9841\n",
      "Epoch 7128: train loss: 0.1823, test loss 0.9840\n",
      "Epoch 7129: train loss: 0.1823, test loss 0.9840\n",
      "Epoch 7130: train loss: 0.1823, test loss 0.9839\n",
      "Epoch 7131: train loss: 0.1823, test loss 0.9838\n",
      "Epoch 7132: train loss: 0.1823, test loss 0.9837\n",
      "Epoch 7133: train loss: 0.1822, test loss 0.9837\n",
      "Epoch 7134: train loss: 0.1822, test loss 0.9836\n",
      "Epoch 7135: train loss: 0.1822, test loss 0.9835\n",
      "Epoch 7136: train loss: 0.1822, test loss 0.9835\n",
      "Epoch 7137: train loss: 0.1822, test loss 0.9834\n",
      "Epoch 7138: train loss: 0.1822, test loss 0.9833\n",
      "Epoch 7139: train loss: 0.1822, test loss 0.9833\n",
      "Epoch 7140: train loss: 0.1822, test loss 0.9832\n",
      "Epoch 7141: train loss: 0.1822, test loss 0.9831\n",
      "Epoch 7142: train loss: 0.1822, test loss 0.9831\n",
      "Epoch 7143: train loss: 0.1822, test loss 0.9830\n",
      "Epoch 7144: train loss: 0.1822, test loss 0.9829\n",
      "Epoch 7145: train loss: 0.1822, test loss 0.9828\n",
      "Epoch 7146: train loss: 0.1822, test loss 0.9828\n",
      "Epoch 7147: train loss: 0.1822, test loss 0.9827\n",
      "Epoch 7148: train loss: 0.1822, test loss 0.9826\n",
      "Epoch 7149: train loss: 0.1822, test loss 0.9826\n",
      "Epoch 7150: train loss: 0.1822, test loss 0.9825\n",
      "Epoch 7151: train loss: 0.1822, test loss 0.9824\n",
      "Epoch 7152: train loss: 0.1822, test loss 0.9824\n",
      "Epoch 7153: train loss: 0.1822, test loss 0.9823\n",
      "Epoch 7154: train loss: 0.1822, test loss 0.9822\n",
      "Epoch 7155: train loss: 0.1822, test loss 0.9822\n",
      "Epoch 7156: train loss: 0.1822, test loss 0.9821\n",
      "Epoch 7157: train loss: 0.1822, test loss 0.9820\n",
      "Epoch 7158: train loss: 0.1822, test loss 0.9820\n",
      "Epoch 7159: train loss: 0.1822, test loss 0.9819\n",
      "Epoch 7160: train loss: 0.1822, test loss 0.9818\n",
      "Epoch 7161: train loss: 0.1822, test loss 0.9817\n",
      "Epoch 7162: train loss: 0.1822, test loss 0.9817\n",
      "Epoch 7163: train loss: 0.1822, test loss 0.9816\n",
      "Epoch 7164: train loss: 0.1822, test loss 0.9815\n",
      "Epoch 7165: train loss: 0.1822, test loss 0.9815\n",
      "Epoch 7166: train loss: 0.1822, test loss 0.9814\n",
      "Epoch 7167: train loss: 0.1822, test loss 0.9813\n",
      "Epoch 7168: train loss: 0.1822, test loss 0.9813\n",
      "Epoch 7169: train loss: 0.1822, test loss 0.9812\n",
      "Epoch 7170: train loss: 0.1822, test loss 0.9811\n",
      "Epoch 7171: train loss: 0.1822, test loss 0.9811\n",
      "Epoch 7172: train loss: 0.1822, test loss 0.9810\n",
      "Epoch 7173: train loss: 0.1822, test loss 0.9809\n",
      "Epoch 7174: train loss: 0.1822, test loss 0.9809\n",
      "Epoch 7175: train loss: 0.1822, test loss 0.9808\n",
      "Epoch 7176: train loss: 0.1822, test loss 0.9807\n",
      "Epoch 7177: train loss: 0.1822, test loss 0.9807\n",
      "Epoch 7178: train loss: 0.1822, test loss 0.9806\n",
      "Epoch 7179: train loss: 0.1822, test loss 0.9805\n",
      "Epoch 7180: train loss: 0.1822, test loss 0.9805\n",
      "Epoch 7181: train loss: 0.1821, test loss 0.9804\n",
      "Epoch 7182: train loss: 0.1821, test loss 0.9803\n",
      "Epoch 7183: train loss: 0.1821, test loss 0.9803\n",
      "Epoch 7184: train loss: 0.1821, test loss 0.9802\n",
      "Epoch 7185: train loss: 0.1821, test loss 0.9801\n",
      "Epoch 7186: train loss: 0.1821, test loss 0.9800\n",
      "Epoch 7187: train loss: 0.1821, test loss 0.9800\n",
      "Epoch 7188: train loss: 0.1821, test loss 0.9799\n",
      "Epoch 7189: train loss: 0.1821, test loss 0.9798\n",
      "Epoch 7190: train loss: 0.1821, test loss 0.9798\n",
      "Epoch 7191: train loss: 0.1821, test loss 0.9797\n",
      "Epoch 7192: train loss: 0.1821, test loss 0.9796\n",
      "Epoch 7193: train loss: 0.1821, test loss 0.9796\n",
      "Epoch 7194: train loss: 0.1821, test loss 0.9795\n",
      "Epoch 7195: train loss: 0.1821, test loss 0.9794\n",
      "Epoch 7196: train loss: 0.1821, test loss 0.9794\n",
      "Epoch 7197: train loss: 0.1821, test loss 0.9793\n",
      "Epoch 7198: train loss: 0.1821, test loss 0.9792\n",
      "Epoch 7199: train loss: 0.1821, test loss 0.9792\n",
      "Epoch 7200: train loss: 0.1821, test loss 0.9791\n",
      "Epoch 7201: train loss: 0.1821, test loss 0.9790\n",
      "Epoch 7202: train loss: 0.1821, test loss 0.9790\n",
      "Epoch 7203: train loss: 0.1821, test loss 0.9789\n",
      "Epoch 7204: train loss: 0.1821, test loss 0.9788\n",
      "Epoch 7205: train loss: 0.1821, test loss 0.9788\n",
      "Epoch 7206: train loss: 0.1821, test loss 0.9787\n",
      "Epoch 7207: train loss: 0.1821, test loss 0.9786\n",
      "Epoch 7208: train loss: 0.1821, test loss 0.9786\n",
      "Epoch 7209: train loss: 0.1821, test loss 0.9785\n",
      "Epoch 7210: train loss: 0.1821, test loss 0.9784\n",
      "Epoch 7211: train loss: 0.1821, test loss 0.9784\n",
      "Epoch 7212: train loss: 0.1821, test loss 0.9783\n",
      "Epoch 7213: train loss: 0.1821, test loss 0.9782\n",
      "Epoch 7214: train loss: 0.1821, test loss 0.9782\n",
      "Epoch 7215: train loss: 0.1821, test loss 0.9781\n",
      "Epoch 7216: train loss: 0.1821, test loss 0.9780\n",
      "Epoch 7217: train loss: 0.1821, test loss 0.9780\n",
      "Epoch 7218: train loss: 0.1821, test loss 0.9779\n",
      "Epoch 7219: train loss: 0.1821, test loss 0.9778\n",
      "Epoch 7220: train loss: 0.1821, test loss 0.9778\n",
      "Epoch 7221: train loss: 0.1821, test loss 0.9777\n",
      "Epoch 7222: train loss: 0.1821, test loss 0.9776\n",
      "Epoch 7223: train loss: 0.1821, test loss 0.9776\n",
      "Epoch 7224: train loss: 0.1821, test loss 0.9775\n",
      "Epoch 7225: train loss: 0.1821, test loss 0.9774\n",
      "Epoch 7226: train loss: 0.1821, test loss 0.9774\n",
      "Epoch 7227: train loss: 0.1821, test loss 0.9773\n",
      "Epoch 7228: train loss: 0.1821, test loss 0.9772\n",
      "Epoch 7229: train loss: 0.1820, test loss 0.9772\n",
      "Epoch 7230: train loss: 0.1820, test loss 0.9771\n",
      "Epoch 7231: train loss: 0.1820, test loss 0.9770\n",
      "Epoch 7232: train loss: 0.1820, test loss 0.9770\n",
      "Epoch 7233: train loss: 0.1820, test loss 0.9769\n",
      "Epoch 7234: train loss: 0.1820, test loss 0.9768\n",
      "Epoch 7235: train loss: 0.1820, test loss 0.9768\n",
      "Epoch 7236: train loss: 0.1820, test loss 0.9767\n",
      "Epoch 7237: train loss: 0.1820, test loss 0.9766\n",
      "Epoch 7238: train loss: 0.1820, test loss 0.9766\n",
      "Epoch 7239: train loss: 0.1820, test loss 0.9765\n",
      "Epoch 7240: train loss: 0.1820, test loss 0.9764\n",
      "Epoch 7241: train loss: 0.1820, test loss 0.9764\n",
      "Epoch 7242: train loss: 0.1820, test loss 0.9763\n",
      "Epoch 7243: train loss: 0.1820, test loss 0.9762\n",
      "Epoch 7244: train loss: 0.1820, test loss 0.9762\n",
      "Epoch 7245: train loss: 0.1820, test loss 0.9761\n",
      "Epoch 7246: train loss: 0.1820, test loss 0.9760\n",
      "Epoch 7247: train loss: 0.1820, test loss 0.9760\n",
      "Epoch 7248: train loss: 0.1820, test loss 0.9759\n",
      "Epoch 7249: train loss: 0.1820, test loss 0.9758\n",
      "Epoch 7250: train loss: 0.1820, test loss 0.9758\n",
      "Epoch 7251: train loss: 0.1820, test loss 0.9757\n",
      "Epoch 7252: train loss: 0.1820, test loss 0.9756\n",
      "Epoch 7253: train loss: 0.1820, test loss 0.9756\n",
      "Epoch 7254: train loss: 0.1820, test loss 0.9755\n",
      "Epoch 7255: train loss: 0.1820, test loss 0.9754\n",
      "Epoch 7256: train loss: 0.1820, test loss 0.9754\n",
      "Epoch 7257: train loss: 0.1820, test loss 0.9753\n",
      "Epoch 7258: train loss: 0.1820, test loss 0.9752\n",
      "Epoch 7259: train loss: 0.1820, test loss 0.9752\n",
      "Epoch 7260: train loss: 0.1820, test loss 0.9751\n",
      "Epoch 7261: train loss: 0.1820, test loss 0.9750\n",
      "Epoch 7262: train loss: 0.1820, test loss 0.9750\n",
      "Epoch 7263: train loss: 0.1820, test loss 0.9749\n",
      "Epoch 7264: train loss: 0.1820, test loss 0.9748\n",
      "Epoch 7265: train loss: 0.1820, test loss 0.9748\n",
      "Epoch 7266: train loss: 0.1820, test loss 0.9747\n",
      "Epoch 7267: train loss: 0.1820, test loss 0.9746\n",
      "Epoch 7268: train loss: 0.1820, test loss 0.9746\n",
      "Epoch 7269: train loss: 0.1820, test loss 0.9745\n",
      "Epoch 7270: train loss: 0.1820, test loss 0.9744\n",
      "Epoch 7271: train loss: 0.1820, test loss 0.9744\n",
      "Epoch 7272: train loss: 0.1820, test loss 0.9743\n",
      "Epoch 7273: train loss: 0.1820, test loss 0.9742\n",
      "Epoch 7274: train loss: 0.1820, test loss 0.9742\n",
      "Epoch 7275: train loss: 0.1820, test loss 0.9741\n",
      "Epoch 7276: train loss: 0.1820, test loss 0.9740\n",
      "Epoch 7277: train loss: 0.1819, test loss 0.9740\n",
      "Epoch 7278: train loss: 0.1819, test loss 0.9739\n",
      "Epoch 7279: train loss: 0.1819, test loss 0.9738\n",
      "Epoch 7280: train loss: 0.1819, test loss 0.9738\n",
      "Epoch 7281: train loss: 0.1819, test loss 0.9737\n",
      "Epoch 7282: train loss: 0.1819, test loss 0.9736\n",
      "Epoch 7283: train loss: 0.1819, test loss 0.9736\n",
      "Epoch 7284: train loss: 0.1819, test loss 0.9735\n",
      "Epoch 7285: train loss: 0.1819, test loss 0.9734\n",
      "Epoch 7286: train loss: 0.1819, test loss 0.9734\n",
      "Epoch 7287: train loss: 0.1819, test loss 0.9733\n",
      "Epoch 7288: train loss: 0.1819, test loss 0.9733\n",
      "Epoch 7289: train loss: 0.1819, test loss 0.9732\n",
      "Epoch 7290: train loss: 0.1819, test loss 0.9731\n",
      "Epoch 7291: train loss: 0.1819, test loss 0.9731\n",
      "Epoch 7292: train loss: 0.1819, test loss 0.9730\n",
      "Epoch 7293: train loss: 0.1819, test loss 0.9729\n",
      "Epoch 7294: train loss: 0.1819, test loss 0.9729\n",
      "Epoch 7295: train loss: 0.1819, test loss 0.9728\n",
      "Epoch 7296: train loss: 0.1819, test loss 0.9727\n",
      "Epoch 7297: train loss: 0.1819, test loss 0.9727\n",
      "Epoch 7298: train loss: 0.1819, test loss 0.9726\n",
      "Epoch 7299: train loss: 0.1819, test loss 0.9725\n",
      "Epoch 7300: train loss: 0.1819, test loss 0.9725\n",
      "Epoch 7301: train loss: 0.1819, test loss 0.9724\n",
      "Epoch 7302: train loss: 0.1819, test loss 0.9723\n",
      "Epoch 7303: train loss: 0.1819, test loss 0.9723\n",
      "Epoch 7304: train loss: 0.1819, test loss 0.9722\n",
      "Epoch 7305: train loss: 0.1819, test loss 0.9721\n",
      "Epoch 7306: train loss: 0.1819, test loss 0.9721\n",
      "Epoch 7307: train loss: 0.1819, test loss 0.9720\n",
      "Epoch 7308: train loss: 0.1819, test loss 0.9719\n",
      "Epoch 7309: train loss: 0.1819, test loss 0.9719\n",
      "Epoch 7310: train loss: 0.1819, test loss 0.9718\n",
      "Epoch 7311: train loss: 0.1819, test loss 0.9717\n",
      "Epoch 7312: train loss: 0.1819, test loss 0.9717\n",
      "Epoch 7313: train loss: 0.1819, test loss 0.9716\n",
      "Epoch 7314: train loss: 0.1819, test loss 0.9715\n",
      "Epoch 7315: train loss: 0.1819, test loss 0.9715\n",
      "Epoch 7316: train loss: 0.1819, test loss 0.9714\n",
      "Epoch 7317: train loss: 0.1819, test loss 0.9714\n",
      "Epoch 7318: train loss: 0.1819, test loss 0.9713\n",
      "Epoch 7319: train loss: 0.1819, test loss 0.9712\n",
      "Epoch 7320: train loss: 0.1819, test loss 0.9712\n",
      "Epoch 7321: train loss: 0.1819, test loss 0.9711\n",
      "Epoch 7322: train loss: 0.1819, test loss 0.9710\n",
      "Epoch 7323: train loss: 0.1819, test loss 0.9710\n",
      "Epoch 7324: train loss: 0.1819, test loss 0.9709\n",
      "Epoch 7325: train loss: 0.1819, test loss 0.9708\n",
      "Epoch 7326: train loss: 0.1818, test loss 0.9708\n",
      "Epoch 7327: train loss: 0.1818, test loss 0.9707\n",
      "Epoch 7328: train loss: 0.1818, test loss 0.9706\n",
      "Epoch 7329: train loss: 0.1818, test loss 0.9706\n",
      "Epoch 7330: train loss: 0.1818, test loss 0.9705\n",
      "Epoch 7331: train loss: 0.1818, test loss 0.9704\n",
      "Epoch 7332: train loss: 0.1818, test loss 0.9704\n",
      "Epoch 7333: train loss: 0.1818, test loss 0.9703\n",
      "Epoch 7334: train loss: 0.1818, test loss 0.9703\n",
      "Epoch 7335: train loss: 0.1818, test loss 0.9702\n",
      "Epoch 7336: train loss: 0.1818, test loss 0.9701\n",
      "Epoch 7337: train loss: 0.1818, test loss 0.9701\n",
      "Epoch 7338: train loss: 0.1818, test loss 0.9700\n",
      "Epoch 7339: train loss: 0.1818, test loss 0.9699\n",
      "Epoch 7340: train loss: 0.1818, test loss 0.9699\n",
      "Epoch 7341: train loss: 0.1818, test loss 0.9698\n",
      "Epoch 7342: train loss: 0.1818, test loss 0.9697\n",
      "Epoch 7343: train loss: 0.1818, test loss 0.9697\n",
      "Epoch 7344: train loss: 0.1818, test loss 0.9696\n",
      "Epoch 7345: train loss: 0.1818, test loss 0.9695\n",
      "Epoch 7346: train loss: 0.1818, test loss 0.9695\n",
      "Epoch 7347: train loss: 0.1818, test loss 0.9694\n",
      "Epoch 7348: train loss: 0.1818, test loss 0.9693\n",
      "Epoch 7349: train loss: 0.1818, test loss 0.9693\n",
      "Epoch 7350: train loss: 0.1818, test loss 0.9692\n",
      "Epoch 7351: train loss: 0.1818, test loss 0.9692\n",
      "Epoch 7352: train loss: 0.1818, test loss 0.9691\n",
      "Epoch 7353: train loss: 0.1818, test loss 0.9690\n",
      "Epoch 7354: train loss: 0.1818, test loss 0.9690\n",
      "Epoch 7355: train loss: 0.1818, test loss 0.9689\n",
      "Epoch 7356: train loss: 0.1818, test loss 0.9688\n",
      "Epoch 7357: train loss: 0.1818, test loss 0.9688\n",
      "Epoch 7358: train loss: 0.1818, test loss 0.9687\n",
      "Epoch 7359: train loss: 0.1818, test loss 0.9686\n",
      "Epoch 7360: train loss: 0.1818, test loss 0.9686\n",
      "Epoch 7361: train loss: 0.1818, test loss 0.9685\n",
      "Epoch 7362: train loss: 0.1818, test loss 0.9684\n",
      "Epoch 7363: train loss: 0.1818, test loss 0.9684\n",
      "Epoch 7364: train loss: 0.1818, test loss 0.9683\n",
      "Epoch 7365: train loss: 0.1818, test loss 0.9683\n",
      "Epoch 7366: train loss: 0.1818, test loss 0.9682\n",
      "Epoch 7367: train loss: 0.1818, test loss 0.9681\n",
      "Epoch 7368: train loss: 0.1818, test loss 0.9681\n",
      "Epoch 7369: train loss: 0.1818, test loss 0.9680\n",
      "Epoch 7370: train loss: 0.1818, test loss 0.9679\n",
      "Epoch 7371: train loss: 0.1818, test loss 0.9679\n",
      "Epoch 7372: train loss: 0.1818, test loss 0.9678\n",
      "Epoch 7373: train loss: 0.1818, test loss 0.9677\n",
      "Epoch 7374: train loss: 0.1818, test loss 0.9677\n",
      "Epoch 7375: train loss: 0.1818, test loss 0.9676\n",
      "Epoch 7376: train loss: 0.1817, test loss 0.9676\n",
      "Epoch 7377: train loss: 0.1817, test loss 0.9675\n",
      "Epoch 7378: train loss: 0.1817, test loss 0.9674\n",
      "Epoch 7379: train loss: 0.1817, test loss 0.9674\n",
      "Epoch 7380: train loss: 0.1817, test loss 0.9673\n",
      "Epoch 7381: train loss: 0.1817, test loss 0.9672\n",
      "Epoch 7382: train loss: 0.1817, test loss 0.9672\n",
      "Epoch 7383: train loss: 0.1817, test loss 0.9671\n",
      "Epoch 7384: train loss: 0.1817, test loss 0.9671\n",
      "Epoch 7385: train loss: 0.1817, test loss 0.9670\n",
      "Epoch 7386: train loss: 0.1817, test loss 0.9669\n",
      "Epoch 7387: train loss: 0.1817, test loss 0.9669\n",
      "Epoch 7388: train loss: 0.1817, test loss 0.9668\n",
      "Epoch 7389: train loss: 0.1817, test loss 0.9668\n",
      "Epoch 7390: train loss: 0.1817, test loss 0.9667\n",
      "Epoch 7391: train loss: 0.1817, test loss 0.9666\n",
      "Epoch 7392: train loss: 0.1817, test loss 0.9666\n",
      "Epoch 7393: train loss: 0.1817, test loss 0.9665\n",
      "Epoch 7394: train loss: 0.1817, test loss 0.9665\n",
      "Epoch 7395: train loss: 0.1817, test loss 0.9664\n",
      "Epoch 7396: train loss: 0.1817, test loss 0.9664\n",
      "Epoch 7397: train loss: 0.1817, test loss 0.9663\n",
      "Epoch 7398: train loss: 0.1817, test loss 0.9662\n",
      "Epoch 7399: train loss: 0.1817, test loss 0.9662\n",
      "Epoch 7400: train loss: 0.1817, test loss 0.9661\n",
      "Epoch 7401: train loss: 0.1817, test loss 0.9661\n",
      "Epoch 7402: train loss: 0.1817, test loss 0.9660\n",
      "Epoch 7403: train loss: 0.1817, test loss 0.9659\n",
      "Epoch 7404: train loss: 0.1817, test loss 0.9659\n",
      "Epoch 7405: train loss: 0.1817, test loss 0.9658\n",
      "Epoch 7406: train loss: 0.1817, test loss 0.9658\n",
      "Epoch 7407: train loss: 0.1817, test loss 0.9657\n",
      "Epoch 7408: train loss: 0.1817, test loss 0.9656\n",
      "Epoch 7409: train loss: 0.1817, test loss 0.9656\n",
      "Epoch 7410: train loss: 0.1817, test loss 0.9655\n",
      "Epoch 7411: train loss: 0.1817, test loss 0.9655\n",
      "Epoch 7412: train loss: 0.1817, test loss 0.9654\n",
      "Epoch 7413: train loss: 0.1817, test loss 0.9653\n",
      "Epoch 7414: train loss: 0.1817, test loss 0.9653\n",
      "Epoch 7415: train loss: 0.1817, test loss 0.9652\n",
      "Epoch 7416: train loss: 0.1817, test loss 0.9652\n",
      "Epoch 7417: train loss: 0.1817, test loss 0.9651\n",
      "Epoch 7418: train loss: 0.1817, test loss 0.9650\n",
      "Epoch 7419: train loss: 0.1817, test loss 0.9650\n",
      "Epoch 7420: train loss: 0.1817, test loss 0.9649\n",
      "Epoch 7421: train loss: 0.1817, test loss 0.9649\n",
      "Epoch 7422: train loss: 0.1817, test loss 0.9648\n",
      "Epoch 7423: train loss: 0.1817, test loss 0.9647\n",
      "Epoch 7424: train loss: 0.1817, test loss 0.9647\n",
      "Epoch 7425: train loss: 0.1817, test loss 0.9646\n",
      "Epoch 7426: train loss: 0.1817, test loss 0.9646\n",
      "Epoch 7427: train loss: 0.1817, test loss 0.9645\n",
      "Epoch 7428: train loss: 0.1816, test loss 0.9645\n",
      "Epoch 7429: train loss: 0.1816, test loss 0.9644\n",
      "Epoch 7430: train loss: 0.1816, test loss 0.9643\n",
      "Epoch 7431: train loss: 0.1816, test loss 0.9643\n",
      "Epoch 7432: train loss: 0.1816, test loss 0.9642\n",
      "Epoch 7433: train loss: 0.1816, test loss 0.9642\n",
      "Epoch 7434: train loss: 0.1816, test loss 0.9641\n",
      "Epoch 7435: train loss: 0.1816, test loss 0.9640\n",
      "Epoch 7436: train loss: 0.1816, test loss 0.9640\n",
      "Epoch 7437: train loss: 0.1816, test loss 0.9639\n",
      "Epoch 7438: train loss: 0.1816, test loss 0.9639\n",
      "Epoch 7439: train loss: 0.1816, test loss 0.9638\n",
      "Epoch 7440: train loss: 0.1816, test loss 0.9638\n",
      "Epoch 7441: train loss: 0.1816, test loss 0.9637\n",
      "Epoch 7442: train loss: 0.1816, test loss 0.9636\n",
      "Epoch 7443: train loss: 0.1816, test loss 0.9636\n",
      "Epoch 7444: train loss: 0.1816, test loss 0.9635\n",
      "Epoch 7445: train loss: 0.1816, test loss 0.9635\n",
      "Epoch 7446: train loss: 0.1816, test loss 0.9634\n",
      "Epoch 7447: train loss: 0.1816, test loss 0.9634\n",
      "Epoch 7448: train loss: 0.1816, test loss 0.9633\n",
      "Epoch 7449: train loss: 0.1816, test loss 0.9632\n",
      "Epoch 7450: train loss: 0.1816, test loss 0.9632\n",
      "Epoch 7451: train loss: 0.1816, test loss 0.9631\n",
      "Epoch 7452: train loss: 0.1816, test loss 0.9631\n",
      "Epoch 7453: train loss: 0.1816, test loss 0.9630\n",
      "Epoch 7454: train loss: 0.1816, test loss 0.9630\n",
      "Epoch 7455: train loss: 0.1816, test loss 0.9629\n",
      "Epoch 7456: train loss: 0.1816, test loss 0.9628\n",
      "Epoch 7457: train loss: 0.1816, test loss 0.9628\n",
      "Epoch 7458: train loss: 0.1816, test loss 0.9627\n",
      "Epoch 7459: train loss: 0.1816, test loss 0.9627\n",
      "Epoch 7460: train loss: 0.1816, test loss 0.9626\n",
      "Epoch 7461: train loss: 0.1816, test loss 0.9625\n",
      "Epoch 7462: train loss: 0.1816, test loss 0.9625\n",
      "Epoch 7463: train loss: 0.1816, test loss 0.9624\n",
      "Epoch 7464: train loss: 0.1816, test loss 0.9624\n",
      "Epoch 7465: train loss: 0.1816, test loss 0.9623\n",
      "Epoch 7466: train loss: 0.1816, test loss 0.9623\n",
      "Epoch 7467: train loss: 0.1816, test loss 0.9622\n",
      "Epoch 7468: train loss: 0.1816, test loss 0.9621\n",
      "Epoch 7469: train loss: 0.1816, test loss 0.9621\n",
      "Epoch 7470: train loss: 0.1816, test loss 0.9620\n",
      "Epoch 7471: train loss: 0.1816, test loss 0.9620\n",
      "Epoch 7472: train loss: 0.1816, test loss 0.9619\n",
      "Epoch 7473: train loss: 0.1816, test loss 0.9618\n",
      "Epoch 7474: train loss: 0.1816, test loss 0.9618\n",
      "Epoch 7475: train loss: 0.1816, test loss 0.9617\n",
      "Epoch 7476: train loss: 0.1816, test loss 0.9617\n",
      "Epoch 7477: train loss: 0.1816, test loss 0.9616\n",
      "Epoch 7478: train loss: 0.1816, test loss 0.9616\n",
      "Epoch 7479: train loss: 0.1816, test loss 0.9615\n",
      "Epoch 7480: train loss: 0.1815, test loss 0.9614\n",
      "Epoch 7481: train loss: 0.1815, test loss 0.9614\n",
      "Epoch 7482: train loss: 0.1815, test loss 0.9613\n",
      "Epoch 7483: train loss: 0.1815, test loss 0.9613\n",
      "Epoch 7484: train loss: 0.1815, test loss 0.9612\n",
      "Epoch 7485: train loss: 0.1815, test loss 0.9612\n",
      "Epoch 7486: train loss: 0.1815, test loss 0.9611\n",
      "Epoch 7487: train loss: 0.1815, test loss 0.9610\n",
      "Epoch 7488: train loss: 0.1815, test loss 0.9610\n",
      "Epoch 7489: train loss: 0.1815, test loss 0.9609\n",
      "Epoch 7490: train loss: 0.1815, test loss 0.9609\n",
      "Epoch 7491: train loss: 0.1815, test loss 0.9608\n",
      "Epoch 7492: train loss: 0.1815, test loss 0.9608\n",
      "Epoch 7493: train loss: 0.1815, test loss 0.9607\n",
      "Epoch 7494: train loss: 0.1815, test loss 0.9606\n",
      "Epoch 7495: train loss: 0.1815, test loss 0.9606\n",
      "Epoch 7496: train loss: 0.1815, test loss 0.9605\n",
      "Epoch 7497: train loss: 0.1815, test loss 0.9605\n",
      "Epoch 7498: train loss: 0.1815, test loss 0.9604\n",
      "Epoch 7499: train loss: 0.1815, test loss 0.9604\n",
      "Epoch 7500: train loss: 0.1815, test loss 0.9603\n",
      "Epoch 7501: train loss: 0.1815, test loss 0.9602\n",
      "Epoch 7502: train loss: 0.1815, test loss 0.9602\n",
      "Epoch 7503: train loss: 0.1815, test loss 0.9601\n",
      "Epoch 7504: train loss: 0.1815, test loss 0.9601\n",
      "Epoch 7505: train loss: 0.1815, test loss 0.9600\n",
      "Epoch 7506: train loss: 0.1815, test loss 0.9599\n",
      "Epoch 7507: train loss: 0.1815, test loss 0.9599\n",
      "Epoch 7508: train loss: 0.1815, test loss 0.9598\n",
      "Epoch 7509: train loss: 0.1815, test loss 0.9598\n",
      "Epoch 7510: train loss: 0.1815, test loss 0.9597\n",
      "Epoch 7511: train loss: 0.1815, test loss 0.9597\n",
      "Epoch 7512: train loss: 0.1815, test loss 0.9596\n",
      "Epoch 7513: train loss: 0.1815, test loss 0.9595\n",
      "Epoch 7514: train loss: 0.1815, test loss 0.9595\n",
      "Epoch 7515: train loss: 0.1815, test loss 0.9594\n",
      "Epoch 7516: train loss: 0.1815, test loss 0.9594\n",
      "Epoch 7517: train loss: 0.1815, test loss 0.9593\n",
      "Epoch 7518: train loss: 0.1815, test loss 0.9593\n",
      "Epoch 7519: train loss: 0.1815, test loss 0.9592\n",
      "Epoch 7520: train loss: 0.1815, test loss 0.9591\n",
      "Epoch 7521: train loss: 0.1815, test loss 0.9591\n",
      "Epoch 7522: train loss: 0.1815, test loss 0.9590\n",
      "Epoch 7523: train loss: 0.1815, test loss 0.9590\n",
      "Epoch 7524: train loss: 0.1815, test loss 0.9589\n",
      "Epoch 7525: train loss: 0.1815, test loss 0.9589\n",
      "Epoch 7526: train loss: 0.1815, test loss 0.9588\n",
      "Epoch 7527: train loss: 0.1815, test loss 0.9587\n",
      "Epoch 7528: train loss: 0.1815, test loss 0.9587\n",
      "Epoch 7529: train loss: 0.1815, test loss 0.9586\n",
      "Epoch 7530: train loss: 0.1815, test loss 0.9586\n",
      "Epoch 7531: train loss: 0.1815, test loss 0.9585\n",
      "Epoch 7532: train loss: 0.1815, test loss 0.9585\n",
      "Epoch 7533: train loss: 0.1814, test loss 0.9584\n",
      "Epoch 7534: train loss: 0.1814, test loss 0.9583\n",
      "Epoch 7535: train loss: 0.1814, test loss 0.9583\n",
      "Epoch 7536: train loss: 0.1814, test loss 0.9582\n",
      "Epoch 7537: train loss: 0.1814, test loss 0.9582\n",
      "Epoch 7538: train loss: 0.1814, test loss 0.9581\n",
      "Epoch 7539: train loss: 0.1814, test loss 0.9581\n",
      "Epoch 7540: train loss: 0.1814, test loss 0.9580\n",
      "Epoch 7541: train loss: 0.1814, test loss 0.9579\n",
      "Epoch 7542: train loss: 0.1814, test loss 0.9579\n",
      "Epoch 7543: train loss: 0.1814, test loss 0.9578\n",
      "Epoch 7544: train loss: 0.1814, test loss 0.9578\n",
      "Epoch 7545: train loss: 0.1814, test loss 0.9577\n",
      "Epoch 7546: train loss: 0.1814, test loss 0.9577\n",
      "Epoch 7547: train loss: 0.1814, test loss 0.9576\n",
      "Epoch 7548: train loss: 0.1814, test loss 0.9575\n",
      "Epoch 7549: train loss: 0.1814, test loss 0.9575\n",
      "Epoch 7550: train loss: 0.1814, test loss 0.9574\n",
      "Epoch 7551: train loss: 0.1814, test loss 0.9574\n",
      "Epoch 7552: train loss: 0.1814, test loss 0.9573\n",
      "Epoch 7553: train loss: 0.1814, test loss 0.9573\n",
      "Epoch 7554: train loss: 0.1814, test loss 0.9572\n",
      "Epoch 7555: train loss: 0.1814, test loss 0.9571\n",
      "Epoch 7556: train loss: 0.1814, test loss 0.9571\n",
      "Epoch 7557: train loss: 0.1814, test loss 0.9570\n",
      "Epoch 7558: train loss: 0.1814, test loss 0.9570\n",
      "Epoch 7559: train loss: 0.1814, test loss 0.9569\n",
      "Epoch 7560: train loss: 0.1814, test loss 0.9569\n",
      "Epoch 7561: train loss: 0.1814, test loss 0.9568\n",
      "Epoch 7562: train loss: 0.1814, test loss 0.9568\n",
      "Epoch 7563: train loss: 0.1814, test loss 0.9567\n",
      "Epoch 7564: train loss: 0.1814, test loss 0.9566\n",
      "Epoch 7565: train loss: 0.1814, test loss 0.9566\n",
      "Epoch 7566: train loss: 0.1814, test loss 0.9565\n",
      "Epoch 7567: train loss: 0.1814, test loss 0.9565\n",
      "Epoch 7568: train loss: 0.1814, test loss 0.9564\n",
      "Epoch 7569: train loss: 0.1814, test loss 0.9564\n",
      "Epoch 7570: train loss: 0.1814, test loss 0.9563\n",
      "Epoch 7571: train loss: 0.1814, test loss 0.9562\n",
      "Epoch 7572: train loss: 0.1814, test loss 0.9562\n",
      "Epoch 7573: train loss: 0.1814, test loss 0.9561\n",
      "Epoch 7574: train loss: 0.1814, test loss 0.9561\n",
      "Epoch 7575: train loss: 0.1814, test loss 0.9560\n",
      "Epoch 7576: train loss: 0.1814, test loss 0.9560\n",
      "Epoch 7577: train loss: 0.1814, test loss 0.9559\n",
      "Epoch 7578: train loss: 0.1814, test loss 0.9558\n",
      "Epoch 7579: train loss: 0.1814, test loss 0.9558\n",
      "Epoch 7580: train loss: 0.1814, test loss 0.9557\n",
      "Epoch 7581: train loss: 0.1814, test loss 0.9557\n",
      "Epoch 7582: train loss: 0.1814, test loss 0.9556\n",
      "Epoch 7583: train loss: 0.1814, test loss 0.9556\n",
      "Epoch 7584: train loss: 0.1814, test loss 0.9555\n",
      "Epoch 7585: train loss: 0.1814, test loss 0.9555\n",
      "Epoch 7586: train loss: 0.1814, test loss 0.9554\n",
      "Epoch 7587: train loss: 0.1813, test loss 0.9553\n",
      "Epoch 7588: train loss: 0.1813, test loss 0.9553\n",
      "Epoch 7589: train loss: 0.1813, test loss 0.9552\n",
      "Epoch 7590: train loss: 0.1813, test loss 0.9552\n",
      "Epoch 7591: train loss: 0.1813, test loss 0.9551\n",
      "Epoch 7592: train loss: 0.1813, test loss 0.9551\n",
      "Epoch 7593: train loss: 0.1813, test loss 0.9550\n",
      "Epoch 7594: train loss: 0.1813, test loss 0.9549\n",
      "Epoch 7595: train loss: 0.1813, test loss 0.9549\n",
      "Epoch 7596: train loss: 0.1813, test loss 0.9548\n",
      "Epoch 7597: train loss: 0.1813, test loss 0.9548\n",
      "Epoch 7598: train loss: 0.1813, test loss 0.9547\n",
      "Epoch 7599: train loss: 0.1813, test loss 0.9547\n",
      "Epoch 7600: train loss: 0.1813, test loss 0.9546\n",
      "Epoch 7601: train loss: 0.1813, test loss 0.9546\n",
      "Epoch 7602: train loss: 0.1813, test loss 0.9545\n",
      "Epoch 7603: train loss: 0.1813, test loss 0.9544\n",
      "Epoch 7604: train loss: 0.1813, test loss 0.9544\n",
      "Epoch 7605: train loss: 0.1813, test loss 0.9543\n",
      "Epoch 7606: train loss: 0.1813, test loss 0.9543\n",
      "Epoch 7607: train loss: 0.1813, test loss 0.9542\n",
      "Epoch 7608: train loss: 0.1813, test loss 0.9542\n",
      "Epoch 7609: train loss: 0.1813, test loss 0.9541\n",
      "Epoch 7610: train loss: 0.1813, test loss 0.9540\n",
      "Epoch 7611: train loss: 0.1813, test loss 0.9540\n",
      "Epoch 7612: train loss: 0.1813, test loss 0.9539\n",
      "Epoch 7613: train loss: 0.1813, test loss 0.9539\n",
      "Epoch 7614: train loss: 0.1813, test loss 0.9538\n",
      "Epoch 7615: train loss: 0.1813, test loss 0.9538\n",
      "Epoch 7616: train loss: 0.1813, test loss 0.9537\n",
      "Epoch 7617: train loss: 0.1813, test loss 0.9536\n",
      "Epoch 7618: train loss: 0.1813, test loss 0.9536\n",
      "Epoch 7619: train loss: 0.1813, test loss 0.9535\n",
      "Epoch 7620: train loss: 0.1813, test loss 0.9535\n",
      "Epoch 7621: train loss: 0.1813, test loss 0.9534\n",
      "Epoch 7622: train loss: 0.1813, test loss 0.9534\n",
      "Epoch 7623: train loss: 0.1813, test loss 0.9533\n",
      "Epoch 7624: train loss: 0.1813, test loss 0.9533\n",
      "Epoch 7625: train loss: 0.1813, test loss 0.9532\n",
      "Epoch 7626: train loss: 0.1813, test loss 0.9532\n",
      "Epoch 7627: train loss: 0.1813, test loss 0.9531\n",
      "Epoch 7628: train loss: 0.1813, test loss 0.9530\n",
      "Epoch 7629: train loss: 0.1813, test loss 0.9530\n",
      "Epoch 7630: train loss: 0.1813, test loss 0.9529\n",
      "Epoch 7631: train loss: 0.1813, test loss 0.9529\n",
      "Epoch 7632: train loss: 0.1813, test loss 0.9528\n",
      "Epoch 7633: train loss: 0.1813, test loss 0.9528\n",
      "Epoch 7634: train loss: 0.1813, test loss 0.9527\n",
      "Epoch 7635: train loss: 0.1813, test loss 0.9526\n",
      "Epoch 7636: train loss: 0.1813, test loss 0.9526\n",
      "Epoch 7637: train loss: 0.1813, test loss 0.9525\n",
      "Epoch 7638: train loss: 0.1813, test loss 0.9525\n",
      "Epoch 7639: train loss: 0.1813, test loss 0.9524\n",
      "Epoch 7640: train loss: 0.1813, test loss 0.9524\n",
      "Epoch 7641: train loss: 0.1813, test loss 0.9523\n",
      "Epoch 7642: train loss: 0.1812, test loss 0.9523\n",
      "Epoch 7643: train loss: 0.1812, test loss 0.9522\n",
      "Epoch 7644: train loss: 0.1812, test loss 0.9521\n",
      "Epoch 7645: train loss: 0.1812, test loss 0.9521\n",
      "Epoch 7646: train loss: 0.1812, test loss 0.9520\n",
      "Epoch 7647: train loss: 0.1812, test loss 0.9520\n",
      "Epoch 7648: train loss: 0.1812, test loss 0.9519\n",
      "Epoch 7649: train loss: 0.1812, test loss 0.9519\n",
      "Epoch 7650: train loss: 0.1812, test loss 0.9518\n",
      "Epoch 7651: train loss: 0.1812, test loss 0.9518\n",
      "Epoch 7652: train loss: 0.1812, test loss 0.9517\n",
      "Epoch 7653: train loss: 0.1812, test loss 0.9517\n",
      "Epoch 7654: train loss: 0.1812, test loss 0.9516\n",
      "Epoch 7655: train loss: 0.1812, test loss 0.9515\n",
      "Epoch 7656: train loss: 0.1812, test loss 0.9515\n",
      "Epoch 7657: train loss: 0.1812, test loss 0.9514\n",
      "Epoch 7658: train loss: 0.1812, test loss 0.9514\n",
      "Epoch 7659: train loss: 0.1812, test loss 0.9513\n",
      "Epoch 7660: train loss: 0.1812, test loss 0.9513\n",
      "Epoch 7661: train loss: 0.1812, test loss 0.9512\n",
      "Epoch 7662: train loss: 0.1812, test loss 0.9512\n",
      "Epoch 7663: train loss: 0.1812, test loss 0.9511\n",
      "Epoch 7664: train loss: 0.1812, test loss 0.9511\n",
      "Epoch 7665: train loss: 0.1812, test loss 0.9510\n",
      "Epoch 7666: train loss: 0.1812, test loss 0.9509\n",
      "Epoch 7667: train loss: 0.1812, test loss 0.9509\n",
      "Epoch 7668: train loss: 0.1812, test loss 0.9508\n",
      "Epoch 7669: train loss: 0.1812, test loss 0.9508\n",
      "Epoch 7670: train loss: 0.1812, test loss 0.9507\n",
      "Epoch 7671: train loss: 0.1812, test loss 0.9507\n",
      "Epoch 7672: train loss: 0.1812, test loss 0.9506\n",
      "Epoch 7673: train loss: 0.1812, test loss 0.9506\n",
      "Epoch 7674: train loss: 0.1812, test loss 0.9505\n",
      "Epoch 7675: train loss: 0.1812, test loss 0.9505\n",
      "Epoch 7676: train loss: 0.1812, test loss 0.9504\n",
      "Epoch 7677: train loss: 0.1812, test loss 0.9504\n",
      "Epoch 7678: train loss: 0.1812, test loss 0.9503\n",
      "Epoch 7679: train loss: 0.1812, test loss 0.9502\n",
      "Epoch 7680: train loss: 0.1812, test loss 0.9502\n",
      "Epoch 7681: train loss: 0.1812, test loss 0.9501\n",
      "Epoch 7682: train loss: 0.1812, test loss 0.9501\n",
      "Epoch 7683: train loss: 0.1812, test loss 0.9500\n",
      "Epoch 7684: train loss: 0.1812, test loss 0.9500\n",
      "Epoch 7685: train loss: 0.1812, test loss 0.9499\n",
      "Epoch 7686: train loss: 0.1812, test loss 0.9499\n",
      "Epoch 7687: train loss: 0.1812, test loss 0.9498\n",
      "Epoch 7688: train loss: 0.1812, test loss 0.9498\n",
      "Epoch 7689: train loss: 0.1812, test loss 0.9497\n",
      "Epoch 7690: train loss: 0.1812, test loss 0.9496\n",
      "Epoch 7691: train loss: 0.1812, test loss 0.9496\n",
      "Epoch 7692: train loss: 0.1812, test loss 0.9495\n",
      "Epoch 7693: train loss: 0.1812, test loss 0.9495\n",
      "Epoch 7694: train loss: 0.1812, test loss 0.9494\n",
      "Epoch 7695: train loss: 0.1812, test loss 0.9494\n",
      "Epoch 7696: train loss: 0.1812, test loss 0.9493\n",
      "Epoch 7697: train loss: 0.1811, test loss 0.9493\n",
      "Epoch 7698: train loss: 0.1811, test loss 0.9492\n",
      "Epoch 7699: train loss: 0.1811, test loss 0.9491\n",
      "Epoch 7700: train loss: 0.1811, test loss 0.9491\n",
      "Epoch 7701: train loss: 0.1811, test loss 0.9490\n",
      "Epoch 7702: train loss: 0.1811, test loss 0.9490\n",
      "Epoch 7703: train loss: 0.1811, test loss 0.9489\n",
      "Epoch 7704: train loss: 0.1811, test loss 0.9489\n",
      "Epoch 7705: train loss: 0.1811, test loss 0.9488\n",
      "Epoch 7706: train loss: 0.1811, test loss 0.9488\n",
      "Epoch 7707: train loss: 0.1811, test loss 0.9487\n",
      "Epoch 7708: train loss: 0.1811, test loss 0.9487\n",
      "Epoch 7709: train loss: 0.1811, test loss 0.9486\n",
      "Epoch 7710: train loss: 0.1811, test loss 0.9486\n",
      "Epoch 7711: train loss: 0.1811, test loss 0.9485\n",
      "Epoch 7712: train loss: 0.1811, test loss 0.9485\n",
      "Epoch 7713: train loss: 0.1811, test loss 0.9484\n",
      "Epoch 7714: train loss: 0.1811, test loss 0.9483\n",
      "Epoch 7715: train loss: 0.1811, test loss 0.9483\n",
      "Epoch 7716: train loss: 0.1811, test loss 0.9482\n",
      "Epoch 7717: train loss: 0.1811, test loss 0.9482\n",
      "Epoch 7718: train loss: 0.1811, test loss 0.9481\n",
      "Epoch 7719: train loss: 0.1811, test loss 0.9481\n",
      "Epoch 7720: train loss: 0.1811, test loss 0.9480\n",
      "Epoch 7721: train loss: 0.1811, test loss 0.9480\n",
      "Epoch 7722: train loss: 0.1811, test loss 0.9479\n",
      "Epoch 7723: train loss: 0.1811, test loss 0.9479\n",
      "Epoch 7724: train loss: 0.1811, test loss 0.9478\n",
      "Epoch 7725: train loss: 0.1811, test loss 0.9477\n",
      "Epoch 7726: train loss: 0.1811, test loss 0.9477\n",
      "Epoch 7727: train loss: 0.1811, test loss 0.9476\n",
      "Epoch 7728: train loss: 0.1811, test loss 0.9476\n",
      "Epoch 7729: train loss: 0.1811, test loss 0.9475\n",
      "Epoch 7730: train loss: 0.1811, test loss 0.9475\n",
      "Epoch 7731: train loss: 0.1811, test loss 0.9474\n",
      "Epoch 7732: train loss: 0.1811, test loss 0.9474\n",
      "Epoch 7733: train loss: 0.1811, test loss 0.9473\n",
      "Epoch 7734: train loss: 0.1811, test loss 0.9473\n",
      "Epoch 7735: train loss: 0.1811, test loss 0.9472\n",
      "Epoch 7736: train loss: 0.1811, test loss 0.9472\n",
      "Epoch 7737: train loss: 0.1811, test loss 0.9471\n",
      "Epoch 7738: train loss: 0.1811, test loss 0.9470\n",
      "Epoch 7739: train loss: 0.1811, test loss 0.9470\n",
      "Epoch 7740: train loss: 0.1811, test loss 0.9469\n",
      "Epoch 7741: train loss: 0.1811, test loss 0.9469\n",
      "Epoch 7742: train loss: 0.1811, test loss 0.9468\n",
      "Epoch 7743: train loss: 0.1811, test loss 0.9468\n",
      "Epoch 7744: train loss: 0.1811, test loss 0.9467\n",
      "Epoch 7745: train loss: 0.1811, test loss 0.9467\n",
      "Epoch 7746: train loss: 0.1811, test loss 0.9466\n",
      "Epoch 7747: train loss: 0.1811, test loss 0.9466\n",
      "Epoch 7748: train loss: 0.1811, test loss 0.9465\n",
      "Epoch 7749: train loss: 0.1811, test loss 0.9465\n",
      "Epoch 7750: train loss: 0.1811, test loss 0.9464\n",
      "Epoch 7751: train loss: 0.1811, test loss 0.9464\n",
      "Epoch 7752: train loss: 0.1811, test loss 0.9463\n",
      "Epoch 7753: train loss: 0.1810, test loss 0.9463\n",
      "Epoch 7754: train loss: 0.1810, test loss 0.9462\n",
      "Epoch 7755: train loss: 0.1810, test loss 0.9461\n",
      "Epoch 7756: train loss: 0.1810, test loss 0.9461\n",
      "Epoch 7757: train loss: 0.1810, test loss 0.9460\n",
      "Epoch 7758: train loss: 0.1810, test loss 0.9460\n",
      "Epoch 7759: train loss: 0.1810, test loss 0.9459\n",
      "Epoch 7760: train loss: 0.1810, test loss 0.9459\n",
      "Epoch 7761: train loss: 0.1810, test loss 0.9458\n",
      "Epoch 7762: train loss: 0.1810, test loss 0.9458\n",
      "Epoch 7763: train loss: 0.1810, test loss 0.9457\n",
      "Epoch 7764: train loss: 0.1810, test loss 0.9457\n",
      "Epoch 7765: train loss: 0.1810, test loss 0.9456\n",
      "Epoch 7766: train loss: 0.1810, test loss 0.9456\n",
      "Epoch 7767: train loss: 0.1810, test loss 0.9455\n",
      "Epoch 7768: train loss: 0.1810, test loss 0.9454\n",
      "Epoch 7769: train loss: 0.1810, test loss 0.9454\n",
      "Epoch 7770: train loss: 0.1810, test loss 0.9453\n",
      "Epoch 7771: train loss: 0.1810, test loss 0.9453\n",
      "Epoch 7772: train loss: 0.1810, test loss 0.9452\n",
      "Epoch 7773: train loss: 0.1810, test loss 0.9452\n",
      "Epoch 7774: train loss: 0.1810, test loss 0.9451\n",
      "Epoch 7775: train loss: 0.1810, test loss 0.9451\n",
      "Epoch 7776: train loss: 0.1810, test loss 0.9450\n",
      "Epoch 7777: train loss: 0.1810, test loss 0.9450\n",
      "Epoch 7778: train loss: 0.1810, test loss 0.9449\n",
      "Epoch 7779: train loss: 0.1810, test loss 0.9449\n",
      "Epoch 7780: train loss: 0.1810, test loss 0.9448\n",
      "Epoch 7781: train loss: 0.1810, test loss 0.9448\n",
      "Epoch 7782: train loss: 0.1810, test loss 0.9447\n",
      "Epoch 7783: train loss: 0.1810, test loss 0.9447\n",
      "Epoch 7784: train loss: 0.1810, test loss 0.9446\n",
      "Epoch 7785: train loss: 0.1810, test loss 0.9446\n",
      "Epoch 7786: train loss: 0.1810, test loss 0.9445\n",
      "Epoch 7787: train loss: 0.1810, test loss 0.9445\n",
      "Epoch 7788: train loss: 0.1810, test loss 0.9444\n",
      "Epoch 7789: train loss: 0.1810, test loss 0.9443\n",
      "Epoch 7790: train loss: 0.1810, test loss 0.9443\n",
      "Epoch 7791: train loss: 0.1810, test loss 0.9442\n",
      "Epoch 7792: train loss: 0.1810, test loss 0.9442\n",
      "Epoch 7793: train loss: 0.1810, test loss 0.9441\n",
      "Epoch 7794: train loss: 0.1810, test loss 0.9441\n",
      "Epoch 7795: train loss: 0.1810, test loss 0.9440\n",
      "Epoch 7796: train loss: 0.1810, test loss 0.9440\n",
      "Epoch 7797: train loss: 0.1810, test loss 0.9439\n",
      "Epoch 7798: train loss: 0.1810, test loss 0.9439\n",
      "Epoch 7799: train loss: 0.1810, test loss 0.9438\n",
      "Epoch 7800: train loss: 0.1810, test loss 0.9438\n",
      "Epoch 7801: train loss: 0.1810, test loss 0.9437\n",
      "Epoch 7802: train loss: 0.1810, test loss 0.9437\n",
      "Epoch 7803: train loss: 0.1810, test loss 0.9436\n",
      "Epoch 7804: train loss: 0.1810, test loss 0.9436\n",
      "Epoch 7805: train loss: 0.1810, test loss 0.9435\n",
      "Epoch 7806: train loss: 0.1810, test loss 0.9435\n",
      "Epoch 7807: train loss: 0.1810, test loss 0.9434\n",
      "Epoch 7808: train loss: 0.1810, test loss 0.9434\n",
      "Epoch 7809: train loss: 0.1810, test loss 0.9433\n",
      "Epoch 7810: train loss: 0.1809, test loss 0.9433\n",
      "Epoch 7811: train loss: 0.1809, test loss 0.9432\n",
      "Epoch 7812: train loss: 0.1809, test loss 0.9432\n",
      "Epoch 7813: train loss: 0.1809, test loss 0.9431\n",
      "Epoch 7814: train loss: 0.1809, test loss 0.9431\n",
      "Epoch 7815: train loss: 0.1809, test loss 0.9430\n",
      "Epoch 7816: train loss: 0.1809, test loss 0.9430\n",
      "Epoch 7817: train loss: 0.1809, test loss 0.9429\n",
      "Epoch 7818: train loss: 0.1809, test loss 0.9429\n",
      "Epoch 7819: train loss: 0.1809, test loss 0.9428\n",
      "Epoch 7820: train loss: 0.1809, test loss 0.9428\n",
      "Epoch 7821: train loss: 0.1809, test loss 0.9427\n",
      "Epoch 7822: train loss: 0.1809, test loss 0.9427\n",
      "Epoch 7823: train loss: 0.1809, test loss 0.9426\n",
      "Epoch 7824: train loss: 0.1809, test loss 0.9426\n",
      "Epoch 7825: train loss: 0.1809, test loss 0.9425\n",
      "Epoch 7826: train loss: 0.1809, test loss 0.9425\n",
      "Epoch 7827: train loss: 0.1809, test loss 0.9424\n",
      "Epoch 7828: train loss: 0.1809, test loss 0.9424\n",
      "Epoch 7829: train loss: 0.1809, test loss 0.9423\n",
      "Epoch 7830: train loss: 0.1809, test loss 0.9423\n",
      "Epoch 7831: train loss: 0.1809, test loss 0.9422\n",
      "Epoch 7832: train loss: 0.1809, test loss 0.9422\n",
      "Epoch 7833: train loss: 0.1809, test loss 0.9421\n",
      "Epoch 7834: train loss: 0.1809, test loss 0.9421\n",
      "Epoch 7835: train loss: 0.1809, test loss 0.9420\n",
      "Epoch 7836: train loss: 0.1809, test loss 0.9420\n",
      "Epoch 7837: train loss: 0.1809, test loss 0.9419\n",
      "Epoch 7838: train loss: 0.1809, test loss 0.9419\n",
      "Epoch 7839: train loss: 0.1809, test loss 0.9418\n",
      "Epoch 7840: train loss: 0.1809, test loss 0.9418\n",
      "Epoch 7841: train loss: 0.1809, test loss 0.9417\n",
      "Epoch 7842: train loss: 0.1809, test loss 0.9417\n",
      "Epoch 7843: train loss: 0.1809, test loss 0.9416\n",
      "Epoch 7844: train loss: 0.1809, test loss 0.9416\n",
      "Epoch 7845: train loss: 0.1809, test loss 0.9415\n",
      "Epoch 7846: train loss: 0.1809, test loss 0.9415\n",
      "Epoch 7847: train loss: 0.1809, test loss 0.9414\n",
      "Epoch 7848: train loss: 0.1809, test loss 0.9414\n",
      "Epoch 7849: train loss: 0.1809, test loss 0.9413\n",
      "Epoch 7850: train loss: 0.1809, test loss 0.9413\n",
      "Epoch 7851: train loss: 0.1809, test loss 0.9412\n",
      "Epoch 7852: train loss: 0.1809, test loss 0.9412\n",
      "Epoch 7853: train loss: 0.1809, test loss 0.9411\n",
      "Epoch 7854: train loss: 0.1809, test loss 0.9411\n",
      "Epoch 7855: train loss: 0.1809, test loss 0.9410\n",
      "Epoch 7856: train loss: 0.1809, test loss 0.9410\n",
      "Epoch 7857: train loss: 0.1809, test loss 0.9409\n",
      "Epoch 7858: train loss: 0.1809, test loss 0.9409\n",
      "Epoch 7859: train loss: 0.1809, test loss 0.9408\n",
      "Epoch 7860: train loss: 0.1809, test loss 0.9408\n",
      "Epoch 7861: train loss: 0.1809, test loss 0.9407\n",
      "Epoch 7862: train loss: 0.1809, test loss 0.9407\n",
      "Epoch 7863: train loss: 0.1809, test loss 0.9406\n",
      "Epoch 7864: train loss: 0.1809, test loss 0.9406\n",
      "Epoch 7865: train loss: 0.1809, test loss 0.9405\n",
      "Epoch 7866: train loss: 0.1809, test loss 0.9405\n",
      "Epoch 7867: train loss: 0.1808, test loss 0.9404\n",
      "Epoch 7868: train loss: 0.1808, test loss 0.9404\n",
      "Epoch 7869: train loss: 0.1808, test loss 0.9403\n",
      "Epoch 7870: train loss: 0.1808, test loss 0.9403\n",
      "Epoch 7871: train loss: 0.1808, test loss 0.9402\n",
      "Epoch 7872: train loss: 0.1808, test loss 0.9402\n",
      "Epoch 7873: train loss: 0.1808, test loss 0.9401\n",
      "Epoch 7874: train loss: 0.1808, test loss 0.9401\n",
      "Epoch 7875: train loss: 0.1808, test loss 0.9400\n",
      "Epoch 7876: train loss: 0.1808, test loss 0.9400\n",
      "Epoch 7877: train loss: 0.1808, test loss 0.9399\n",
      "Epoch 7878: train loss: 0.1808, test loss 0.9399\n",
      "Epoch 7879: train loss: 0.1808, test loss 0.9398\n",
      "Epoch 7880: train loss: 0.1808, test loss 0.9398\n",
      "Epoch 7881: train loss: 0.1808, test loss 0.9397\n",
      "Epoch 7882: train loss: 0.1808, test loss 0.9397\n",
      "Epoch 7883: train loss: 0.1808, test loss 0.9396\n",
      "Epoch 7884: train loss: 0.1808, test loss 0.9396\n",
      "Epoch 7885: train loss: 0.1808, test loss 0.9395\n",
      "Epoch 7886: train loss: 0.1808, test loss 0.9395\n",
      "Epoch 7887: train loss: 0.1808, test loss 0.9394\n",
      "Epoch 7888: train loss: 0.1808, test loss 0.9394\n",
      "Epoch 7889: train loss: 0.1808, test loss 0.9393\n",
      "Epoch 7890: train loss: 0.1808, test loss 0.9393\n",
      "Epoch 7891: train loss: 0.1808, test loss 0.9392\n",
      "Epoch 7892: train loss: 0.1808, test loss 0.9392\n",
      "Epoch 7893: train loss: 0.1808, test loss 0.9391\n",
      "Epoch 7894: train loss: 0.1808, test loss 0.9391\n",
      "Epoch 7895: train loss: 0.1808, test loss 0.9391\n",
      "Epoch 7896: train loss: 0.1808, test loss 0.9390\n",
      "Epoch 7897: train loss: 0.1808, test loss 0.9390\n",
      "Epoch 7898: train loss: 0.1808, test loss 0.9389\n",
      "Epoch 7899: train loss: 0.1808, test loss 0.9389\n",
      "Epoch 7900: train loss: 0.1808, test loss 0.9388\n",
      "Epoch 7901: train loss: 0.1808, test loss 0.9388\n",
      "Epoch 7902: train loss: 0.1808, test loss 0.9387\n",
      "Epoch 7903: train loss: 0.1808, test loss 0.9387\n",
      "Epoch 7904: train loss: 0.1808, test loss 0.9386\n",
      "Epoch 7905: train loss: 0.1808, test loss 0.9386\n",
      "Epoch 7906: train loss: 0.1808, test loss 0.9385\n",
      "Epoch 7907: train loss: 0.1808, test loss 0.9385\n",
      "Epoch 7908: train loss: 0.1808, test loss 0.9384\n",
      "Epoch 7909: train loss: 0.1808, test loss 0.9384\n",
      "Epoch 7910: train loss: 0.1808, test loss 0.9383\n",
      "Epoch 7911: train loss: 0.1808, test loss 0.9383\n",
      "Epoch 7912: train loss: 0.1808, test loss 0.9382\n",
      "Epoch 7913: train loss: 0.1808, test loss 0.9382\n",
      "Epoch 7914: train loss: 0.1808, test loss 0.9381\n",
      "Epoch 7915: train loss: 0.1808, test loss 0.9381\n",
      "Epoch 7916: train loss: 0.1808, test loss 0.9380\n",
      "Epoch 7917: train loss: 0.1808, test loss 0.9380\n",
      "Epoch 7918: train loss: 0.1808, test loss 0.9379\n",
      "Epoch 7919: train loss: 0.1808, test loss 0.9379\n",
      "Epoch 7920: train loss: 0.1808, test loss 0.9378\n",
      "Epoch 7921: train loss: 0.1808, test loss 0.9378\n",
      "Epoch 7922: train loss: 0.1808, test loss 0.9377\n",
      "Epoch 7923: train loss: 0.1808, test loss 0.9377\n",
      "Epoch 7924: train loss: 0.1808, test loss 0.9376\n",
      "Epoch 7925: train loss: 0.1808, test loss 0.9376\n",
      "Epoch 7926: train loss: 0.1807, test loss 0.9375\n",
      "Epoch 7927: train loss: 0.1807, test loss 0.9375\n",
      "Epoch 7928: train loss: 0.1807, test loss 0.9374\n",
      "Epoch 7929: train loss: 0.1807, test loss 0.9374\n",
      "Epoch 7930: train loss: 0.1807, test loss 0.9373\n",
      "Epoch 7931: train loss: 0.1807, test loss 0.9373\n",
      "Epoch 7932: train loss: 0.1807, test loss 0.9372\n",
      "Epoch 7933: train loss: 0.1807, test loss 0.9372\n",
      "Epoch 7934: train loss: 0.1807, test loss 0.9371\n",
      "Epoch 7935: train loss: 0.1807, test loss 0.9371\n",
      "Epoch 7936: train loss: 0.1807, test loss 0.9370\n",
      "Epoch 7937: train loss: 0.1807, test loss 0.9370\n",
      "Epoch 7938: train loss: 0.1807, test loss 0.9369\n",
      "Epoch 7939: train loss: 0.1807, test loss 0.9369\n",
      "Epoch 7940: train loss: 0.1807, test loss 0.9368\n",
      "Epoch 7941: train loss: 0.1807, test loss 0.9368\n",
      "Epoch 7942: train loss: 0.1807, test loss 0.9367\n",
      "Epoch 7943: train loss: 0.1807, test loss 0.9367\n",
      "Epoch 7944: train loss: 0.1807, test loss 0.9366\n",
      "Epoch 7945: train loss: 0.1807, test loss 0.9366\n",
      "Epoch 7946: train loss: 0.1807, test loss 0.9365\n",
      "Epoch 7947: train loss: 0.1807, test loss 0.9365\n",
      "Epoch 7948: train loss: 0.1807, test loss 0.9364\n",
      "Epoch 7949: train loss: 0.1807, test loss 0.9364\n",
      "Epoch 7950: train loss: 0.1807, test loss 0.9363\n",
      "Epoch 7951: train loss: 0.1807, test loss 0.9363\n",
      "Epoch 7952: train loss: 0.1807, test loss 0.9362\n",
      "Epoch 7953: train loss: 0.1807, test loss 0.9362\n",
      "Epoch 7954: train loss: 0.1807, test loss 0.9361\n",
      "Epoch 7955: train loss: 0.1807, test loss 0.9361\n",
      "Epoch 7956: train loss: 0.1807, test loss 0.9360\n",
      "Epoch 7957: train loss: 0.1807, test loss 0.9360\n",
      "Epoch 7958: train loss: 0.1807, test loss 0.9359\n",
      "Epoch 7959: train loss: 0.1807, test loss 0.9359\n",
      "Epoch 7960: train loss: 0.1807, test loss 0.9358\n",
      "Epoch 7961: train loss: 0.1807, test loss 0.9358\n",
      "Epoch 7962: train loss: 0.1807, test loss 0.9357\n",
      "Epoch 7963: train loss: 0.1807, test loss 0.9357\n",
      "Epoch 7964: train loss: 0.1807, test loss 0.9357\n",
      "Epoch 7965: train loss: 0.1807, test loss 0.9356\n",
      "Epoch 7966: train loss: 0.1807, test loss 0.9356\n",
      "Epoch 7967: train loss: 0.1807, test loss 0.9355\n",
      "Epoch 7968: train loss: 0.1807, test loss 0.9355\n",
      "Epoch 7969: train loss: 0.1807, test loss 0.9354\n",
      "Epoch 7970: train loss: 0.1807, test loss 0.9354\n",
      "Epoch 7971: train loss: 0.1807, test loss 0.9353\n",
      "Epoch 7972: train loss: 0.1807, test loss 0.9353\n",
      "Epoch 7973: train loss: 0.1807, test loss 0.9352\n",
      "Epoch 7974: train loss: 0.1807, test loss 0.9352\n",
      "Epoch 7975: train loss: 0.1807, test loss 0.9351\n",
      "Epoch 7976: train loss: 0.1807, test loss 0.9351\n",
      "Epoch 7977: train loss: 0.1807, test loss 0.9350\n",
      "Epoch 7978: train loss: 0.1807, test loss 0.9350\n",
      "Epoch 7979: train loss: 0.1807, test loss 0.9349\n",
      "Epoch 7980: train loss: 0.1807, test loss 0.9349\n",
      "Epoch 7981: train loss: 0.1807, test loss 0.9348\n",
      "Epoch 7982: train loss: 0.1807, test loss 0.9348\n",
      "Epoch 7983: train loss: 0.1807, test loss 0.9347\n",
      "Epoch 7984: train loss: 0.1807, test loss 0.9347\n",
      "Epoch 7985: train loss: 0.1807, test loss 0.9346\n",
      "Epoch 7986: train loss: 0.1806, test loss 0.9346\n",
      "Epoch 7987: train loss: 0.1806, test loss 0.9345\n",
      "Epoch 7988: train loss: 0.1806, test loss 0.9345\n",
      "Epoch 7989: train loss: 0.1806, test loss 0.9344\n",
      "Epoch 7990: train loss: 0.1806, test loss 0.9344\n",
      "Epoch 7991: train loss: 0.1806, test loss 0.9343\n",
      "Epoch 7992: train loss: 0.1806, test loss 0.9343\n",
      "Epoch 7993: train loss: 0.1806, test loss 0.9342\n",
      "Epoch 7994: train loss: 0.1806, test loss 0.9342\n",
      "Epoch 7995: train loss: 0.1806, test loss 0.9341\n",
      "Epoch 7996: train loss: 0.1806, test loss 0.9341\n",
      "Epoch 7997: train loss: 0.1806, test loss 0.9340\n",
      "Epoch 7998: train loss: 0.1806, test loss 0.9340\n",
      "Epoch 7999: train loss: 0.1806, test loss 0.9339\n",
      "Epoch 8000: train loss: 0.1806, test loss 0.9339\n",
      "Epoch 8001: train loss: 0.1806, test loss 0.9338\n",
      "Epoch 8002: train loss: 0.1806, test loss 0.9338\n",
      "Epoch 8003: train loss: 0.1806, test loss 0.9337\n",
      "Epoch 8004: train loss: 0.1806, test loss 0.9337\n",
      "Epoch 8005: train loss: 0.1806, test loss 0.9337\n",
      "Epoch 8006: train loss: 0.1806, test loss 0.9336\n",
      "Epoch 8007: train loss: 0.1806, test loss 0.9336\n",
      "Epoch 8008: train loss: 0.1806, test loss 0.9335\n",
      "Epoch 8009: train loss: 0.1806, test loss 0.9335\n",
      "Epoch 8010: train loss: 0.1806, test loss 0.9334\n",
      "Epoch 8011: train loss: 0.1806, test loss 0.9334\n",
      "Epoch 8012: train loss: 0.1806, test loss 0.9333\n",
      "Epoch 8013: train loss: 0.1806, test loss 0.9333\n",
      "Epoch 8014: train loss: 0.1806, test loss 0.9332\n",
      "Epoch 8015: train loss: 0.1806, test loss 0.9332\n",
      "Epoch 8016: train loss: 0.1806, test loss 0.9331\n",
      "Epoch 8017: train loss: 0.1806, test loss 0.9331\n",
      "Epoch 8018: train loss: 0.1806, test loss 0.9330\n",
      "Epoch 8019: train loss: 0.1806, test loss 0.9330\n",
      "Epoch 8020: train loss: 0.1806, test loss 0.9329\n",
      "Epoch 8021: train loss: 0.1806, test loss 0.9329\n",
      "Epoch 8022: train loss: 0.1806, test loss 0.9328\n",
      "Epoch 8023: train loss: 0.1806, test loss 0.9328\n",
      "Epoch 8024: train loss: 0.1806, test loss 0.9327\n",
      "Epoch 8025: train loss: 0.1806, test loss 0.9327\n",
      "Epoch 8026: train loss: 0.1806, test loss 0.9326\n",
      "Epoch 8027: train loss: 0.1806, test loss 0.9326\n",
      "Epoch 8028: train loss: 0.1806, test loss 0.9325\n",
      "Epoch 8029: train loss: 0.1806, test loss 0.9325\n",
      "Epoch 8030: train loss: 0.1806, test loss 0.9325\n",
      "Epoch 8031: train loss: 0.1806, test loss 0.9324\n",
      "Epoch 8032: train loss: 0.1806, test loss 0.9324\n",
      "Epoch 8033: train loss: 0.1806, test loss 0.9323\n",
      "Epoch 8034: train loss: 0.1806, test loss 0.9323\n",
      "Epoch 8035: train loss: 0.1806, test loss 0.9322\n",
      "Epoch 8036: train loss: 0.1806, test loss 0.9322\n",
      "Epoch 8037: train loss: 0.1806, test loss 0.9321\n",
      "Epoch 8038: train loss: 0.1806, test loss 0.9321\n",
      "Epoch 8039: train loss: 0.1806, test loss 0.9320\n",
      "Epoch 8040: train loss: 0.1806, test loss 0.9320\n",
      "Epoch 8041: train loss: 0.1806, test loss 0.9319\n",
      "Epoch 8042: train loss: 0.1806, test loss 0.9319\n",
      "Epoch 8043: train loss: 0.1806, test loss 0.9318\n",
      "Epoch 8044: train loss: 0.1806, test loss 0.9318\n",
      "Epoch 8045: train loss: 0.1806, test loss 0.9317\n",
      "Epoch 8046: train loss: 0.1806, test loss 0.9317\n",
      "Epoch 8047: train loss: 0.1805, test loss 0.9316\n",
      "Epoch 8048: train loss: 0.1805, test loss 0.9316\n",
      "Epoch 8049: train loss: 0.1805, test loss 0.9316\n",
      "Epoch 8050: train loss: 0.1805, test loss 0.9315\n",
      "Epoch 8051: train loss: 0.1805, test loss 0.9315\n",
      "Epoch 8052: train loss: 0.1805, test loss 0.9314\n",
      "Epoch 8053: train loss: 0.1805, test loss 0.9314\n",
      "Epoch 8054: train loss: 0.1805, test loss 0.9313\n",
      "Epoch 8055: train loss: 0.1805, test loss 0.9313\n",
      "Epoch 8056: train loss: 0.1805, test loss 0.9312\n",
      "Epoch 8057: train loss: 0.1805, test loss 0.9312\n",
      "Epoch 8058: train loss: 0.1805, test loss 0.9311\n",
      "Epoch 8059: train loss: 0.1805, test loss 0.9311\n",
      "Epoch 8060: train loss: 0.1805, test loss 0.9310\n",
      "Epoch 8061: train loss: 0.1805, test loss 0.9310\n",
      "Epoch 8062: train loss: 0.1805, test loss 0.9309\n",
      "Epoch 8063: train loss: 0.1805, test loss 0.9309\n",
      "Epoch 8064: train loss: 0.1805, test loss 0.9308\n",
      "Epoch 8065: train loss: 0.1805, test loss 0.9308\n",
      "Epoch 8066: train loss: 0.1805, test loss 0.9308\n",
      "Epoch 8067: train loss: 0.1805, test loss 0.9307\n",
      "Epoch 8068: train loss: 0.1805, test loss 0.9307\n",
      "Epoch 8069: train loss: 0.1805, test loss 0.9306\n",
      "Epoch 8070: train loss: 0.1805, test loss 0.9306\n",
      "Epoch 8071: train loss: 0.1805, test loss 0.9305\n",
      "Epoch 8072: train loss: 0.1805, test loss 0.9305\n",
      "Epoch 8073: train loss: 0.1805, test loss 0.9304\n",
      "Epoch 8074: train loss: 0.1805, test loss 0.9304\n",
      "Epoch 8075: train loss: 0.1805, test loss 0.9303\n",
      "Epoch 8076: train loss: 0.1805, test loss 0.9303\n",
      "Epoch 8077: train loss: 0.1805, test loss 0.9302\n",
      "Epoch 8078: train loss: 0.1805, test loss 0.9302\n",
      "Epoch 8079: train loss: 0.1805, test loss 0.9302\n",
      "Epoch 8080: train loss: 0.1805, test loss 0.9301\n",
      "Epoch 8081: train loss: 0.1805, test loss 0.9301\n",
      "Epoch 8082: train loss: 0.1805, test loss 0.9300\n",
      "Epoch 8083: train loss: 0.1805, test loss 0.9300\n",
      "Epoch 8084: train loss: 0.1805, test loss 0.9299\n",
      "Epoch 8085: train loss: 0.1805, test loss 0.9299\n",
      "Epoch 8086: train loss: 0.1805, test loss 0.9298\n",
      "Epoch 8087: train loss: 0.1805, test loss 0.9298\n",
      "Epoch 8088: train loss: 0.1805, test loss 0.9297\n",
      "Epoch 8089: train loss: 0.1805, test loss 0.9297\n",
      "Epoch 8090: train loss: 0.1805, test loss 0.9296\n",
      "Epoch 8091: train loss: 0.1805, test loss 0.9296\n",
      "Epoch 8092: train loss: 0.1805, test loss 0.9295\n",
      "Epoch 8093: train loss: 0.1805, test loss 0.9295\n",
      "Epoch 8094: train loss: 0.1805, test loss 0.9295\n",
      "Epoch 8095: train loss: 0.1805, test loss 0.9294\n",
      "Epoch 8096: train loss: 0.1805, test loss 0.9294\n",
      "Epoch 8097: train loss: 0.1805, test loss 0.9293\n",
      "Epoch 8098: train loss: 0.1805, test loss 0.9293\n",
      "Epoch 8099: train loss: 0.1805, test loss 0.9292\n",
      "Epoch 8100: train loss: 0.1805, test loss 0.9292\n",
      "Epoch 8101: train loss: 0.1805, test loss 0.9291\n",
      "Epoch 8102: train loss: 0.1805, test loss 0.9291\n",
      "Epoch 8103: train loss: 0.1805, test loss 0.9290\n",
      "Epoch 8104: train loss: 0.1805, test loss 0.9290\n",
      "Epoch 8105: train loss: 0.1805, test loss 0.9289\n",
      "Epoch 8106: train loss: 0.1805, test loss 0.9289\n",
      "Epoch 8107: train loss: 0.1805, test loss 0.9289\n",
      "Epoch 8108: train loss: 0.1805, test loss 0.9288\n",
      "Epoch 8109: train loss: 0.1804, test loss 0.9288\n",
      "Epoch 8110: train loss: 0.1804, test loss 0.9287\n",
      "Epoch 8111: train loss: 0.1804, test loss 0.9287\n",
      "Epoch 8112: train loss: 0.1804, test loss 0.9286\n",
      "Epoch 8113: train loss: 0.1804, test loss 0.9286\n",
      "Epoch 8114: train loss: 0.1804, test loss 0.9285\n",
      "Epoch 8115: train loss: 0.1804, test loss 0.9285\n",
      "Epoch 8116: train loss: 0.1804, test loss 0.9284\n",
      "Epoch 8117: train loss: 0.1804, test loss 0.9284\n",
      "Epoch 8118: train loss: 0.1804, test loss 0.9283\n",
      "Epoch 8119: train loss: 0.1804, test loss 0.9283\n",
      "Epoch 8120: train loss: 0.1804, test loss 0.9283\n",
      "Epoch 8121: train loss: 0.1804, test loss 0.9282\n",
      "Epoch 8122: train loss: 0.1804, test loss 0.9282\n",
      "Epoch 8123: train loss: 0.1804, test loss 0.9281\n",
      "Epoch 8124: train loss: 0.1804, test loss 0.9281\n",
      "Epoch 8125: train loss: 0.1804, test loss 0.9280\n",
      "Epoch 8126: train loss: 0.1804, test loss 0.9280\n",
      "Epoch 8127: train loss: 0.1804, test loss 0.9279\n",
      "Epoch 8128: train loss: 0.1804, test loss 0.9279\n",
      "Epoch 8129: train loss: 0.1804, test loss 0.9278\n",
      "Epoch 8130: train loss: 0.1804, test loss 0.9278\n",
      "Epoch 8131: train loss: 0.1804, test loss 0.9277\n",
      "Epoch 8132: train loss: 0.1804, test loss 0.9277\n",
      "Epoch 8133: train loss: 0.1804, test loss 0.9276\n",
      "Epoch 8134: train loss: 0.1804, test loss 0.9276\n",
      "Epoch 8135: train loss: 0.1804, test loss 0.9276\n",
      "Epoch 8136: train loss: 0.1804, test loss 0.9275\n",
      "Epoch 8137: train loss: 0.1804, test loss 0.9275\n",
      "Epoch 8138: train loss: 0.1804, test loss 0.9274\n",
      "Epoch 8139: train loss: 0.1804, test loss 0.9274\n",
      "Epoch 8140: train loss: 0.1804, test loss 0.9273\n",
      "Epoch 8141: train loss: 0.1804, test loss 0.9273\n",
      "Epoch 8142: train loss: 0.1804, test loss 0.9272\n",
      "Epoch 8143: train loss: 0.1804, test loss 0.9272\n",
      "Epoch 8144: train loss: 0.1804, test loss 0.9271\n",
      "Epoch 8145: train loss: 0.1804, test loss 0.9271\n",
      "Epoch 8146: train loss: 0.1804, test loss 0.9271\n",
      "Epoch 8147: train loss: 0.1804, test loss 0.9270\n",
      "Epoch 8148: train loss: 0.1804, test loss 0.9270\n",
      "Epoch 8149: train loss: 0.1804, test loss 0.9269\n",
      "Epoch 8150: train loss: 0.1804, test loss 0.9269\n",
      "Epoch 8151: train loss: 0.1804, test loss 0.9268\n",
      "Epoch 8152: train loss: 0.1804, test loss 0.9268\n",
      "Epoch 8153: train loss: 0.1804, test loss 0.9267\n",
      "Epoch 8154: train loss: 0.1804, test loss 0.9267\n",
      "Epoch 8155: train loss: 0.1804, test loss 0.9266\n",
      "Epoch 8156: train loss: 0.1804, test loss 0.9266\n",
      "Epoch 8157: train loss: 0.1804, test loss 0.9266\n",
      "Epoch 8158: train loss: 0.1804, test loss 0.9265\n",
      "Epoch 8159: train loss: 0.1804, test loss 0.9265\n",
      "Epoch 8160: train loss: 0.1804, test loss 0.9264\n",
      "Epoch 8161: train loss: 0.1804, test loss 0.9264\n",
      "Epoch 8162: train loss: 0.1804, test loss 0.9263\n",
      "Epoch 8163: train loss: 0.1804, test loss 0.9263\n",
      "Epoch 8164: train loss: 0.1804, test loss 0.9262\n",
      "Epoch 8165: train loss: 0.1804, test loss 0.9262\n",
      "Epoch 8166: train loss: 0.1804, test loss 0.9261\n",
      "Epoch 8167: train loss: 0.1804, test loss 0.9261\n",
      "Epoch 8168: train loss: 0.1804, test loss 0.9261\n",
      "Epoch 8169: train loss: 0.1804, test loss 0.9260\n",
      "Epoch 8170: train loss: 0.1804, test loss 0.9260\n",
      "Epoch 8171: train loss: 0.1803, test loss 0.9259\n",
      "Epoch 8172: train loss: 0.1803, test loss 0.9259\n",
      "Epoch 8173: train loss: 0.1803, test loss 0.9258\n",
      "Epoch 8174: train loss: 0.1803, test loss 0.9258\n",
      "Epoch 8175: train loss: 0.1803, test loss 0.9257\n",
      "Epoch 8176: train loss: 0.1803, test loss 0.9257\n",
      "Epoch 8177: train loss: 0.1803, test loss 0.9257\n",
      "Epoch 8178: train loss: 0.1803, test loss 0.9256\n",
      "Epoch 8179: train loss: 0.1803, test loss 0.9256\n",
      "Epoch 8180: train loss: 0.1803, test loss 0.9255\n",
      "Epoch 8181: train loss: 0.1803, test loss 0.9255\n",
      "Epoch 8182: train loss: 0.1803, test loss 0.9254\n",
      "Epoch 8183: train loss: 0.1803, test loss 0.9254\n",
      "Epoch 8184: train loss: 0.1803, test loss 0.9253\n",
      "Epoch 8185: train loss: 0.1803, test loss 0.9253\n",
      "Epoch 8186: train loss: 0.1803, test loss 0.9253\n",
      "Epoch 8187: train loss: 0.1803, test loss 0.9252\n",
      "Epoch 8188: train loss: 0.1803, test loss 0.9252\n",
      "Epoch 8189: train loss: 0.1803, test loss 0.9251\n",
      "Epoch 8190: train loss: 0.1803, test loss 0.9251\n",
      "Epoch 8191: train loss: 0.1803, test loss 0.9250\n",
      "Epoch 8192: train loss: 0.1803, test loss 0.9250\n",
      "Epoch 8193: train loss: 0.1803, test loss 0.9249\n",
      "Epoch 8194: train loss: 0.1803, test loss 0.9249\n",
      "Epoch 8195: train loss: 0.1803, test loss 0.9249\n",
      "Epoch 8196: train loss: 0.1803, test loss 0.9248\n",
      "Epoch 8197: train loss: 0.1803, test loss 0.9248\n",
      "Epoch 8198: train loss: 0.1803, test loss 0.9247\n",
      "Epoch 8199: train loss: 0.1803, test loss 0.9247\n",
      "Epoch 8200: train loss: 0.1803, test loss 0.9246\n",
      "Epoch 8201: train loss: 0.1803, test loss 0.9246\n",
      "Epoch 8202: train loss: 0.1803, test loss 0.9245\n",
      "Epoch 8203: train loss: 0.1803, test loss 0.9245\n",
      "Epoch 8204: train loss: 0.1803, test loss 0.9244\n",
      "Epoch 8205: train loss: 0.1803, test loss 0.9244\n",
      "Epoch 8206: train loss: 0.1803, test loss 0.9243\n",
      "Epoch 8207: train loss: 0.1803, test loss 0.9243\n",
      "Epoch 8208: train loss: 0.1803, test loss 0.9243\n",
      "Epoch 8209: train loss: 0.1803, test loss 0.9242\n",
      "Epoch 8210: train loss: 0.1803, test loss 0.9242\n",
      "Epoch 8211: train loss: 0.1803, test loss 0.9241\n",
      "Epoch 8212: train loss: 0.1803, test loss 0.9241\n",
      "Epoch 8213: train loss: 0.1803, test loss 0.9240\n",
      "Epoch 8214: train loss: 0.1803, test loss 0.9240\n",
      "Epoch 8215: train loss: 0.1803, test loss 0.9239\n",
      "Epoch 8216: train loss: 0.1803, test loss 0.9239\n",
      "Epoch 8217: train loss: 0.1803, test loss 0.9238\n",
      "Epoch 8218: train loss: 0.1803, test loss 0.9238\n",
      "Epoch 8219: train loss: 0.1803, test loss 0.9237\n",
      "Epoch 8220: train loss: 0.1803, test loss 0.9237\n",
      "Epoch 8221: train loss: 0.1803, test loss 0.9237\n",
      "Epoch 8222: train loss: 0.1803, test loss 0.9236\n",
      "Epoch 8223: train loss: 0.1803, test loss 0.9236\n",
      "Epoch 8224: train loss: 0.1803, test loss 0.9235\n",
      "Epoch 8225: train loss: 0.1803, test loss 0.9235\n",
      "Epoch 8226: train loss: 0.1803, test loss 0.9234\n",
      "Epoch 8227: train loss: 0.1803, test loss 0.9234\n",
      "Epoch 8228: train loss: 0.1803, test loss 0.9233\n",
      "Epoch 8229: train loss: 0.1803, test loss 0.9233\n",
      "Epoch 8230: train loss: 0.1803, test loss 0.9232\n",
      "Epoch 8231: train loss: 0.1803, test loss 0.9232\n",
      "Epoch 8232: train loss: 0.1803, test loss 0.9231\n",
      "Epoch 8233: train loss: 0.1803, test loss 0.9231\n",
      "Epoch 8234: train loss: 0.1802, test loss 0.9231\n",
      "Epoch 8235: train loss: 0.1802, test loss 0.9230\n",
      "Epoch 8236: train loss: 0.1802, test loss 0.9230\n",
      "Epoch 8237: train loss: 0.1802, test loss 0.9229\n",
      "Epoch 8238: train loss: 0.1802, test loss 0.9229\n",
      "Epoch 8239: train loss: 0.1802, test loss 0.9228\n",
      "Epoch 8240: train loss: 0.1802, test loss 0.9228\n",
      "Epoch 8241: train loss: 0.1802, test loss 0.9227\n",
      "Epoch 8242: train loss: 0.1802, test loss 0.9227\n",
      "Epoch 8243: train loss: 0.1802, test loss 0.9226\n",
      "Epoch 8244: train loss: 0.1802, test loss 0.9226\n",
      "Epoch 8245: train loss: 0.1802, test loss 0.9225\n",
      "Epoch 8246: train loss: 0.1802, test loss 0.9225\n",
      "Epoch 8247: train loss: 0.1802, test loss 0.9225\n",
      "Epoch 8248: train loss: 0.1802, test loss 0.9224\n",
      "Epoch 8249: train loss: 0.1802, test loss 0.9224\n",
      "Epoch 8250: train loss: 0.1802, test loss 0.9223\n",
      "Epoch 8251: train loss: 0.1802, test loss 0.9223\n",
      "Epoch 8252: train loss: 0.1802, test loss 0.9222\n",
      "Epoch 8253: train loss: 0.1802, test loss 0.9222\n",
      "Epoch 8254: train loss: 0.1802, test loss 0.9221\n",
      "Epoch 8255: train loss: 0.1802, test loss 0.9221\n",
      "Epoch 8256: train loss: 0.1802, test loss 0.9220\n",
      "Epoch 8257: train loss: 0.1802, test loss 0.9220\n",
      "Epoch 8258: train loss: 0.1802, test loss 0.9219\n",
      "Epoch 8259: train loss: 0.1802, test loss 0.9219\n",
      "Epoch 8260: train loss: 0.1802, test loss 0.9219\n",
      "Epoch 8261: train loss: 0.1802, test loss 0.9218\n",
      "Epoch 8262: train loss: 0.1802, test loss 0.9218\n",
      "Epoch 8263: train loss: 0.1802, test loss 0.9217\n",
      "Epoch 8264: train loss: 0.1802, test loss 0.9217\n",
      "Epoch 8265: train loss: 0.1802, test loss 0.9216\n",
      "Epoch 8266: train loss: 0.1802, test loss 0.9216\n",
      "Epoch 8267: train loss: 0.1802, test loss 0.9215\n",
      "Epoch 8268: train loss: 0.1802, test loss 0.9215\n",
      "Epoch 8269: train loss: 0.1802, test loss 0.9214\n",
      "Epoch 8270: train loss: 0.1802, test loss 0.9214\n",
      "Epoch 8271: train loss: 0.1802, test loss 0.9214\n",
      "Epoch 8272: train loss: 0.1802, test loss 0.9213\n",
      "Epoch 8273: train loss: 0.1802, test loss 0.9213\n",
      "Epoch 8274: train loss: 0.1802, test loss 0.9212\n",
      "Epoch 8275: train loss: 0.1802, test loss 0.9212\n",
      "Epoch 8276: train loss: 0.1802, test loss 0.9211\n",
      "Epoch 8277: train loss: 0.1802, test loss 0.9211\n",
      "Epoch 8278: train loss: 0.1802, test loss 0.9210\n",
      "Epoch 8279: train loss: 0.1802, test loss 0.9210\n",
      "Epoch 8280: train loss: 0.1802, test loss 0.9209\n",
      "Epoch 8281: train loss: 0.1802, test loss 0.9209\n",
      "Epoch 8282: train loss: 0.1802, test loss 0.9209\n",
      "Epoch 8283: train loss: 0.1802, test loss 0.9208\n",
      "Epoch 8284: train loss: 0.1802, test loss 0.9208\n",
      "Epoch 8285: train loss: 0.1802, test loss 0.9207\n",
      "Epoch 8286: train loss: 0.1802, test loss 0.9207\n",
      "Epoch 8287: train loss: 0.1802, test loss 0.9206\n",
      "Epoch 8288: train loss: 0.1802, test loss 0.9206\n",
      "Epoch 8289: train loss: 0.1802, test loss 0.9205\n",
      "Epoch 8290: train loss: 0.1802, test loss 0.9205\n",
      "Epoch 8291: train loss: 0.1802, test loss 0.9204\n",
      "Epoch 8292: train loss: 0.1802, test loss 0.9204\n",
      "Epoch 8293: train loss: 0.1802, test loss 0.9204\n",
      "Epoch 8294: train loss: 0.1802, test loss 0.9203\n",
      "Epoch 8295: train loss: 0.1802, test loss 0.9203\n",
      "Epoch 8296: train loss: 0.1802, test loss 0.9202\n",
      "Epoch 8297: train loss: 0.1802, test loss 0.9202\n",
      "Epoch 8298: train loss: 0.1801, test loss 0.9201\n",
      "Epoch 8299: train loss: 0.1801, test loss 0.9201\n",
      "Epoch 8300: train loss: 0.1801, test loss 0.9200\n",
      "Epoch 8301: train loss: 0.1801, test loss 0.9200\n",
      "Epoch 8302: train loss: 0.1801, test loss 0.9199\n",
      "Epoch 8303: train loss: 0.1801, test loss 0.9199\n",
      "Epoch 8304: train loss: 0.1801, test loss 0.9199\n",
      "Epoch 8305: train loss: 0.1801, test loss 0.9198\n",
      "Epoch 8306: train loss: 0.1801, test loss 0.9198\n",
      "Epoch 8307: train loss: 0.1801, test loss 0.9197\n",
      "Epoch 8308: train loss: 0.1801, test loss 0.9197\n",
      "Epoch 8309: train loss: 0.1801, test loss 0.9196\n",
      "Epoch 8310: train loss: 0.1801, test loss 0.9196\n",
      "Epoch 8311: train loss: 0.1801, test loss 0.9195\n",
      "Epoch 8312: train loss: 0.1801, test loss 0.9195\n",
      "Epoch 8313: train loss: 0.1801, test loss 0.9195\n",
      "Epoch 8314: train loss: 0.1801, test loss 0.9194\n",
      "Epoch 8315: train loss: 0.1801, test loss 0.9194\n",
      "Epoch 8316: train loss: 0.1801, test loss 0.9193\n",
      "Epoch 8317: train loss: 0.1801, test loss 0.9193\n",
      "Epoch 8318: train loss: 0.1801, test loss 0.9192\n",
      "Epoch 8319: train loss: 0.1801, test loss 0.9192\n",
      "Epoch 8320: train loss: 0.1801, test loss 0.9191\n",
      "Epoch 8321: train loss: 0.1801, test loss 0.9191\n",
      "Epoch 8322: train loss: 0.1801, test loss 0.9190\n",
      "Epoch 8323: train loss: 0.1801, test loss 0.9190\n",
      "Epoch 8324: train loss: 0.1801, test loss 0.9190\n",
      "Epoch 8325: train loss: 0.1801, test loss 0.9189\n",
      "Epoch 8326: train loss: 0.1801, test loss 0.9189\n",
      "Epoch 8327: train loss: 0.1801, test loss 0.9188\n",
      "Epoch 8328: train loss: 0.1801, test loss 0.9188\n",
      "Epoch 8329: train loss: 0.1801, test loss 0.9187\n",
      "Epoch 8330: train loss: 0.1801, test loss 0.9187\n",
      "Epoch 8331: train loss: 0.1801, test loss 0.9186\n",
      "Epoch 8332: train loss: 0.1801, test loss 0.9186\n",
      "Epoch 8333: train loss: 0.1801, test loss 0.9186\n",
      "Epoch 8334: train loss: 0.1801, test loss 0.9185\n",
      "Epoch 8335: train loss: 0.1801, test loss 0.9185\n",
      "Epoch 8336: train loss: 0.1801, test loss 0.9184\n",
      "Epoch 8337: train loss: 0.1801, test loss 0.9184\n",
      "Epoch 8338: train loss: 0.1801, test loss 0.9183\n",
      "Epoch 8339: train loss: 0.1801, test loss 0.9183\n",
      "Epoch 8340: train loss: 0.1801, test loss 0.9182\n",
      "Epoch 8341: train loss: 0.1801, test loss 0.9182\n",
      "Epoch 8342: train loss: 0.1801, test loss 0.9182\n",
      "Epoch 8343: train loss: 0.1801, test loss 0.9181\n",
      "Epoch 8344: train loss: 0.1801, test loss 0.9181\n",
      "Epoch 8345: train loss: 0.1801, test loss 0.9180\n",
      "Epoch 8346: train loss: 0.1801, test loss 0.9180\n",
      "Epoch 8347: train loss: 0.1801, test loss 0.9179\n",
      "Epoch 8348: train loss: 0.1801, test loss 0.9179\n",
      "Epoch 8349: train loss: 0.1801, test loss 0.9178\n",
      "Epoch 8350: train loss: 0.1801, test loss 0.9178\n",
      "Epoch 8351: train loss: 0.1801, test loss 0.9177\n",
      "Epoch 8352: train loss: 0.1801, test loss 0.9177\n",
      "Epoch 8353: train loss: 0.1801, test loss 0.9177\n",
      "Epoch 8354: train loss: 0.1801, test loss 0.9176\n",
      "Epoch 8355: train loss: 0.1801, test loss 0.9176\n",
      "Epoch 8356: train loss: 0.1801, test loss 0.9175\n",
      "Epoch 8357: train loss: 0.1801, test loss 0.9175\n",
      "Epoch 8358: train loss: 0.1801, test loss 0.9174\n",
      "Epoch 8359: train loss: 0.1801, test loss 0.9174\n",
      "Epoch 8360: train loss: 0.1801, test loss 0.9173\n",
      "Epoch 8361: train loss: 0.1801, test loss 0.9173\n",
      "Epoch 8362: train loss: 0.1800, test loss 0.9173\n",
      "Epoch 8363: train loss: 0.1800, test loss 0.9172\n",
      "Epoch 8364: train loss: 0.1800, test loss 0.9172\n",
      "Epoch 8365: train loss: 0.1800, test loss 0.9171\n",
      "Epoch 8366: train loss: 0.1800, test loss 0.9171\n",
      "Epoch 8367: train loss: 0.1800, test loss 0.9170\n",
      "Epoch 8368: train loss: 0.1800, test loss 0.9170\n",
      "Epoch 8369: train loss: 0.1800, test loss 0.9169\n",
      "Epoch 8370: train loss: 0.1800, test loss 0.9169\n",
      "Epoch 8371: train loss: 0.1800, test loss 0.9169\n",
      "Epoch 8372: train loss: 0.1800, test loss 0.9168\n",
      "Epoch 8373: train loss: 0.1800, test loss 0.9168\n",
      "Epoch 8374: train loss: 0.1800, test loss 0.9167\n",
      "Epoch 8375: train loss: 0.1800, test loss 0.9167\n",
      "Epoch 8376: train loss: 0.1800, test loss 0.9166\n",
      "Epoch 8377: train loss: 0.1800, test loss 0.9166\n",
      "Epoch 8378: train loss: 0.1800, test loss 0.9165\n",
      "Epoch 8379: train loss: 0.1800, test loss 0.9165\n",
      "Epoch 8380: train loss: 0.1800, test loss 0.9164\n",
      "Epoch 8381: train loss: 0.1800, test loss 0.9164\n",
      "Epoch 8382: train loss: 0.1800, test loss 0.9163\n",
      "Epoch 8383: train loss: 0.1800, test loss 0.9163\n",
      "Epoch 8384: train loss: 0.1800, test loss 0.9163\n",
      "Epoch 8385: train loss: 0.1800, test loss 0.9162\n",
      "Epoch 8386: train loss: 0.1800, test loss 0.9162\n",
      "Epoch 8387: train loss: 0.1800, test loss 0.9161\n",
      "Epoch 8388: train loss: 0.1800, test loss 0.9161\n",
      "Epoch 8389: train loss: 0.1800, test loss 0.9160\n",
      "Epoch 8390: train loss: 0.1800, test loss 0.9160\n",
      "Epoch 8391: train loss: 0.1800, test loss 0.9159\n",
      "Epoch 8392: train loss: 0.1800, test loss 0.9159\n",
      "Epoch 8393: train loss: 0.1800, test loss 0.9158\n",
      "Epoch 8394: train loss: 0.1800, test loss 0.9158\n",
      "Epoch 8395: train loss: 0.1800, test loss 0.9158\n",
      "Epoch 8396: train loss: 0.1800, test loss 0.9157\n",
      "Epoch 8397: train loss: 0.1800, test loss 0.9157\n",
      "Epoch 8398: train loss: 0.1800, test loss 0.9156\n",
      "Epoch 8399: train loss: 0.1800, test loss 0.9156\n",
      "Epoch 8400: train loss: 0.1800, test loss 0.9155\n",
      "Epoch 8401: train loss: 0.1800, test loss 0.9155\n",
      "Epoch 8402: train loss: 0.1800, test loss 0.9154\n",
      "Epoch 8403: train loss: 0.1800, test loss 0.9154\n",
      "Epoch 8404: train loss: 0.1800, test loss 0.9153\n",
      "Epoch 8405: train loss: 0.1800, test loss 0.9153\n",
      "Epoch 8406: train loss: 0.1800, test loss 0.9153\n",
      "Epoch 8407: train loss: 0.1800, test loss 0.9152\n",
      "Epoch 8408: train loss: 0.1800, test loss 0.9152\n",
      "Epoch 8409: train loss: 0.1800, test loss 0.9151\n",
      "Epoch 8410: train loss: 0.1800, test loss 0.9151\n",
      "Epoch 8411: train loss: 0.1800, test loss 0.9150\n",
      "Epoch 8412: train loss: 0.1800, test loss 0.9150\n",
      "Epoch 8413: train loss: 0.1800, test loss 0.9149\n",
      "Epoch 8414: train loss: 0.1800, test loss 0.9149\n",
      "Epoch 8415: train loss: 0.1800, test loss 0.9149\n",
      "Epoch 8416: train loss: 0.1800, test loss 0.9148\n",
      "Epoch 8417: train loss: 0.1800, test loss 0.9148\n",
      "Epoch 8418: train loss: 0.1800, test loss 0.9147\n",
      "Epoch 8419: train loss: 0.1800, test loss 0.9147\n",
      "Epoch 8420: train loss: 0.1800, test loss 0.9146\n",
      "Epoch 8421: train loss: 0.1800, test loss 0.9146\n",
      "Epoch 8422: train loss: 0.1800, test loss 0.9145\n",
      "Epoch 8423: train loss: 0.1800, test loss 0.9145\n",
      "Epoch 8424: train loss: 0.1800, test loss 0.9144\n",
      "Epoch 8425: train loss: 0.1800, test loss 0.9144\n",
      "Epoch 8426: train loss: 0.1799, test loss 0.9144\n",
      "Epoch 8427: train loss: 0.1799, test loss 0.9143\n",
      "Epoch 8428: train loss: 0.1799, test loss 0.9143\n",
      "Epoch 8429: train loss: 0.1799, test loss 0.9142\n",
      "Epoch 8430: train loss: 0.1799, test loss 0.9142\n",
      "Epoch 8431: train loss: 0.1799, test loss 0.9141\n",
      "Epoch 8432: train loss: 0.1799, test loss 0.9141\n",
      "Epoch 8433: train loss: 0.1799, test loss 0.9140\n",
      "Epoch 8434: train loss: 0.1799, test loss 0.9140\n",
      "Epoch 8435: train loss: 0.1799, test loss 0.9140\n",
      "Epoch 8436: train loss: 0.1799, test loss 0.9139\n",
      "Epoch 8437: train loss: 0.1799, test loss 0.9139\n",
      "Epoch 8438: train loss: 0.1799, test loss 0.9138\n",
      "Epoch 8439: train loss: 0.1799, test loss 0.9138\n",
      "Epoch 8440: train loss: 0.1799, test loss 0.9137\n",
      "Epoch 8441: train loss: 0.1799, test loss 0.9137\n",
      "Epoch 8442: train loss: 0.1799, test loss 0.9136\n",
      "Epoch 8443: train loss: 0.1799, test loss 0.9136\n",
      "Epoch 8444: train loss: 0.1799, test loss 0.9136\n",
      "Epoch 8445: train loss: 0.1799, test loss 0.9135\n",
      "Epoch 8446: train loss: 0.1799, test loss 0.9135\n",
      "Epoch 8447: train loss: 0.1799, test loss 0.9134\n",
      "Epoch 8448: train loss: 0.1799, test loss 0.9134\n",
      "Epoch 8449: train loss: 0.1799, test loss 0.9133\n",
      "Epoch 8450: train loss: 0.1799, test loss 0.9133\n",
      "Epoch 8451: train loss: 0.1799, test loss 0.9132\n",
      "Epoch 8452: train loss: 0.1799, test loss 0.9132\n",
      "Epoch 8453: train loss: 0.1799, test loss 0.9132\n",
      "Epoch 8454: train loss: 0.1799, test loss 0.9131\n",
      "Epoch 8455: train loss: 0.1799, test loss 0.9131\n",
      "Epoch 8456: train loss: 0.1799, test loss 0.9130\n",
      "Epoch 8457: train loss: 0.1799, test loss 0.9130\n",
      "Epoch 8458: train loss: 0.1799, test loss 0.9129\n",
      "Epoch 8459: train loss: 0.1799, test loss 0.9129\n",
      "Epoch 8460: train loss: 0.1799, test loss 0.9128\n",
      "Epoch 8461: train loss: 0.1799, test loss 0.9128\n",
      "Epoch 8462: train loss: 0.1799, test loss 0.9128\n",
      "Epoch 8463: train loss: 0.1799, test loss 0.9127\n",
      "Epoch 8464: train loss: 0.1799, test loss 0.9127\n",
      "Epoch 8465: train loss: 0.1799, test loss 0.9126\n",
      "Epoch 8466: train loss: 0.1799, test loss 0.9126\n",
      "Epoch 8467: train loss: 0.1799, test loss 0.9125\n",
      "Epoch 8468: train loss: 0.1799, test loss 0.9125\n",
      "Epoch 8469: train loss: 0.1799, test loss 0.9124\n",
      "Epoch 8470: train loss: 0.1799, test loss 0.9124\n",
      "Epoch 8471: train loss: 0.1799, test loss 0.9124\n",
      "Epoch 8472: train loss: 0.1799, test loss 0.9123\n",
      "Epoch 8473: train loss: 0.1799, test loss 0.9123\n",
      "Epoch 8474: train loss: 0.1799, test loss 0.9122\n",
      "Epoch 8475: train loss: 0.1799, test loss 0.9122\n",
      "Epoch 8476: train loss: 0.1799, test loss 0.9121\n",
      "Epoch 8477: train loss: 0.1799, test loss 0.9121\n",
      "Epoch 8478: train loss: 0.1799, test loss 0.9120\n",
      "Epoch 8479: train loss: 0.1799, test loss 0.9120\n",
      "Epoch 8480: train loss: 0.1799, test loss 0.9120\n",
      "Epoch 8481: train loss: 0.1799, test loss 0.9119\n",
      "Epoch 8482: train loss: 0.1799, test loss 0.9119\n",
      "Epoch 8483: train loss: 0.1799, test loss 0.9118\n",
      "Epoch 8484: train loss: 0.1799, test loss 0.9118\n",
      "Epoch 8485: train loss: 0.1799, test loss 0.9117\n",
      "Epoch 8486: train loss: 0.1799, test loss 0.9117\n",
      "Epoch 8487: train loss: 0.1799, test loss 0.9117\n",
      "Epoch 8488: train loss: 0.1799, test loss 0.9116\n",
      "Epoch 8489: train loss: 0.1799, test loss 0.9116\n",
      "Epoch 8490: train loss: 0.1799, test loss 0.9115\n",
      "Epoch 8491: train loss: 0.1799, test loss 0.9115\n",
      "Epoch 8492: train loss: 0.1799, test loss 0.9114\n",
      "Epoch 8493: train loss: 0.1798, test loss 0.9114\n",
      "Epoch 8494: train loss: 0.1798, test loss 0.9113\n",
      "Epoch 8495: train loss: 0.1798, test loss 0.9113\n",
      "Epoch 8496: train loss: 0.1798, test loss 0.9113\n",
      "Epoch 8497: train loss: 0.1798, test loss 0.9112\n",
      "Epoch 8498: train loss: 0.1798, test loss 0.9112\n",
      "Epoch 8499: train loss: 0.1798, test loss 0.9111\n",
      "Epoch 8500: train loss: 0.1798, test loss 0.9111\n",
      "Epoch 8501: train loss: 0.1798, test loss 0.9110\n",
      "Epoch 8502: train loss: 0.1798, test loss 0.9110\n",
      "Epoch 8503: train loss: 0.1798, test loss 0.9110\n",
      "Epoch 8504: train loss: 0.1798, test loss 0.9109\n",
      "Epoch 8505: train loss: 0.1798, test loss 0.9109\n",
      "Epoch 8506: train loss: 0.1798, test loss 0.9108\n",
      "Epoch 8507: train loss: 0.1798, test loss 0.9108\n",
      "Epoch 8508: train loss: 0.1798, test loss 0.9107\n",
      "Epoch 8509: train loss: 0.1798, test loss 0.9107\n",
      "Epoch 8510: train loss: 0.1798, test loss 0.9107\n",
      "Epoch 8511: train loss: 0.1798, test loss 0.9106\n",
      "Epoch 8512: train loss: 0.1798, test loss 0.9106\n",
      "Epoch 8513: train loss: 0.1798, test loss 0.9105\n",
      "Epoch 8514: train loss: 0.1798, test loss 0.9105\n",
      "Epoch 8515: train loss: 0.1798, test loss 0.9104\n",
      "Epoch 8516: train loss: 0.1798, test loss 0.9104\n",
      "Epoch 8517: train loss: 0.1798, test loss 0.9104\n",
      "Epoch 8518: train loss: 0.1798, test loss 0.9103\n",
      "Epoch 8519: train loss: 0.1798, test loss 0.9103\n",
      "Epoch 8520: train loss: 0.1798, test loss 0.9102\n",
      "Epoch 8521: train loss: 0.1798, test loss 0.9102\n",
      "Epoch 8522: train loss: 0.1798, test loss 0.9101\n",
      "Epoch 8523: train loss: 0.1798, test loss 0.9101\n",
      "Epoch 8524: train loss: 0.1798, test loss 0.9101\n",
      "Epoch 8525: train loss: 0.1798, test loss 0.9100\n",
      "Epoch 8526: train loss: 0.1798, test loss 0.9100\n",
      "Epoch 8527: train loss: 0.1798, test loss 0.9099\n",
      "Epoch 8528: train loss: 0.1798, test loss 0.9099\n",
      "Epoch 8529: train loss: 0.1798, test loss 0.9098\n",
      "Epoch 8530: train loss: 0.1798, test loss 0.9098\n",
      "Epoch 8531: train loss: 0.1798, test loss 0.9098\n",
      "Epoch 8532: train loss: 0.1798, test loss 0.9097\n",
      "Epoch 8533: train loss: 0.1798, test loss 0.9097\n",
      "Epoch 8534: train loss: 0.1798, test loss 0.9096\n",
      "Epoch 8535: train loss: 0.1798, test loss 0.9096\n",
      "Epoch 8536: train loss: 0.1798, test loss 0.9095\n",
      "Epoch 8537: train loss: 0.1798, test loss 0.9095\n",
      "Epoch 8538: train loss: 0.1798, test loss 0.9095\n",
      "Epoch 8539: train loss: 0.1798, test loss 0.9094\n",
      "Epoch 8540: train loss: 0.1798, test loss 0.9094\n",
      "Epoch 8541: train loss: 0.1798, test loss 0.9093\n",
      "Epoch 8542: train loss: 0.1798, test loss 0.9093\n",
      "Epoch 8543: train loss: 0.1798, test loss 0.9093\n",
      "Epoch 8544: train loss: 0.1798, test loss 0.9092\n",
      "Epoch 8545: train loss: 0.1798, test loss 0.9092\n",
      "Epoch 8546: train loss: 0.1798, test loss 0.9091\n",
      "Epoch 8547: train loss: 0.1798, test loss 0.9091\n",
      "Epoch 8548: train loss: 0.1798, test loss 0.9090\n",
      "Epoch 8549: train loss: 0.1798, test loss 0.9090\n",
      "Epoch 8550: train loss: 0.1798, test loss 0.9090\n",
      "Epoch 8551: train loss: 0.1798, test loss 0.9089\n",
      "Epoch 8552: train loss: 0.1798, test loss 0.9089\n",
      "Epoch 8553: train loss: 0.1798, test loss 0.9088\n",
      "Epoch 8554: train loss: 0.1798, test loss 0.9088\n",
      "Epoch 8555: train loss: 0.1798, test loss 0.9087\n",
      "Epoch 8556: train loss: 0.1798, test loss 0.9087\n",
      "Epoch 8557: train loss: 0.1798, test loss 0.9087\n",
      "Epoch 8558: train loss: 0.1798, test loss 0.9086\n",
      "Epoch 8559: train loss: 0.1798, test loss 0.9086\n",
      "Epoch 8560: train loss: 0.1797, test loss 0.9085\n",
      "Epoch 8561: train loss: 0.1797, test loss 0.9085\n",
      "Epoch 8562: train loss: 0.1797, test loss 0.9084\n",
      "Epoch 8563: train loss: 0.1797, test loss 0.9084\n",
      "Epoch 8564: train loss: 0.1797, test loss 0.9084\n",
      "Epoch 8565: train loss: 0.1797, test loss 0.9083\n",
      "Epoch 8566: train loss: 0.1797, test loss 0.9083\n",
      "Epoch 8567: train loss: 0.1797, test loss 0.9082\n",
      "Epoch 8568: train loss: 0.1797, test loss 0.9082\n",
      "Epoch 8569: train loss: 0.1797, test loss 0.9082\n",
      "Epoch 8570: train loss: 0.1797, test loss 0.9081\n",
      "Epoch 8571: train loss: 0.1797, test loss 0.9081\n",
      "Epoch 8572: train loss: 0.1797, test loss 0.9080\n",
      "Epoch 8573: train loss: 0.1797, test loss 0.9080\n",
      "Epoch 8574: train loss: 0.1797, test loss 0.9079\n",
      "Epoch 8575: train loss: 0.1797, test loss 0.9079\n",
      "Epoch 8576: train loss: 0.1797, test loss 0.9079\n",
      "Epoch 8577: train loss: 0.1797, test loss 0.9078\n",
      "Epoch 8578: train loss: 0.1797, test loss 0.9078\n",
      "Epoch 8579: train loss: 0.1797, test loss 0.9077\n",
      "Epoch 8580: train loss: 0.1797, test loss 0.9077\n",
      "Epoch 8581: train loss: 0.1797, test loss 0.9076\n",
      "Epoch 8582: train loss: 0.1797, test loss 0.9076\n",
      "Epoch 8583: train loss: 0.1797, test loss 0.9076\n",
      "Epoch 8584: train loss: 0.1797, test loss 0.9075\n",
      "Epoch 8585: train loss: 0.1797, test loss 0.9075\n",
      "Epoch 8586: train loss: 0.1797, test loss 0.9074\n",
      "Epoch 8587: train loss: 0.1797, test loss 0.9074\n",
      "Epoch 8588: train loss: 0.1797, test loss 0.9074\n",
      "Epoch 8589: train loss: 0.1797, test loss 0.9073\n",
      "Epoch 8590: train loss: 0.1797, test loss 0.9073\n",
      "Epoch 8591: train loss: 0.1797, test loss 0.9072\n",
      "Epoch 8592: train loss: 0.1797, test loss 0.9072\n",
      "Epoch 8593: train loss: 0.1797, test loss 0.9071\n",
      "Epoch 8594: train loss: 0.1797, test loss 0.9071\n",
      "Epoch 8595: train loss: 0.1797, test loss 0.9071\n",
      "Epoch 8596: train loss: 0.1797, test loss 0.9070\n",
      "Epoch 8597: train loss: 0.1797, test loss 0.9070\n",
      "Epoch 8598: train loss: 0.1797, test loss 0.9069\n",
      "Epoch 8599: train loss: 0.1797, test loss 0.9069\n",
      "Epoch 8600: train loss: 0.1797, test loss 0.9069\n",
      "Epoch 8601: train loss: 0.1797, test loss 0.9068\n",
      "Epoch 8602: train loss: 0.1797, test loss 0.9068\n",
      "Epoch 8603: train loss: 0.1797, test loss 0.9067\n",
      "Epoch 8604: train loss: 0.1797, test loss 0.9067\n",
      "Epoch 8605: train loss: 0.1797, test loss 0.9066\n",
      "Epoch 8606: train loss: 0.1797, test loss 0.9066\n",
      "Epoch 8607: train loss: 0.1797, test loss 0.9066\n",
      "Epoch 8608: train loss: 0.1797, test loss 0.9065\n",
      "Epoch 8609: train loss: 0.1797, test loss 0.9065\n",
      "Epoch 8610: train loss: 0.1797, test loss 0.9064\n",
      "Epoch 8611: train loss: 0.1797, test loss 0.9064\n",
      "Epoch 8612: train loss: 0.1797, test loss 0.9064\n",
      "Epoch 8613: train loss: 0.1797, test loss 0.9063\n",
      "Epoch 8614: train loss: 0.1797, test loss 0.9063\n",
      "Epoch 8615: train loss: 0.1797, test loss 0.9062\n",
      "Epoch 8616: train loss: 0.1797, test loss 0.9062\n",
      "Epoch 8617: train loss: 0.1797, test loss 0.9062\n",
      "Epoch 8618: train loss: 0.1797, test loss 0.9061\n",
      "Epoch 8619: train loss: 0.1797, test loss 0.9061\n",
      "Epoch 8620: train loss: 0.1797, test loss 0.9060\n",
      "Epoch 8621: train loss: 0.1797, test loss 0.9060\n",
      "Epoch 8622: train loss: 0.1797, test loss 0.9059\n",
      "Epoch 8623: train loss: 0.1797, test loss 0.9059\n",
      "Epoch 8624: train loss: 0.1797, test loss 0.9059\n",
      "Epoch 8625: train loss: 0.1797, test loss 0.9058\n",
      "Epoch 8626: train loss: 0.1797, test loss 0.9058\n",
      "Epoch 8627: train loss: 0.1797, test loss 0.9057\n",
      "Epoch 8628: train loss: 0.1797, test loss 0.9057\n",
      "Epoch 8629: train loss: 0.1796, test loss 0.9057\n",
      "Epoch 8630: train loss: 0.1796, test loss 0.9056\n",
      "Epoch 8631: train loss: 0.1796, test loss 0.9056\n",
      "Epoch 8632: train loss: 0.1796, test loss 0.9055\n",
      "Epoch 8633: train loss: 0.1796, test loss 0.9055\n",
      "Epoch 8634: train loss: 0.1796, test loss 0.9054\n",
      "Epoch 8635: train loss: 0.1796, test loss 0.9054\n",
      "Epoch 8636: train loss: 0.1796, test loss 0.9054\n",
      "Epoch 8637: train loss: 0.1796, test loss 0.9053\n",
      "Epoch 8638: train loss: 0.1796, test loss 0.9053\n",
      "Epoch 8639: train loss: 0.1796, test loss 0.9052\n",
      "Epoch 8640: train loss: 0.1796, test loss 0.9052\n",
      "Epoch 8641: train loss: 0.1796, test loss 0.9052\n",
      "Epoch 8642: train loss: 0.1796, test loss 0.9051\n",
      "Epoch 8643: train loss: 0.1796, test loss 0.9051\n",
      "Epoch 8644: train loss: 0.1796, test loss 0.9050\n",
      "Epoch 8645: train loss: 0.1796, test loss 0.9050\n",
      "Epoch 8646: train loss: 0.1796, test loss 0.9050\n",
      "Epoch 8647: train loss: 0.1796, test loss 0.9049\n",
      "Epoch 8648: train loss: 0.1796, test loss 0.9049\n",
      "Epoch 8649: train loss: 0.1796, test loss 0.9048\n",
      "Epoch 8650: train loss: 0.1796, test loss 0.9048\n",
      "Epoch 8651: train loss: 0.1796, test loss 0.9047\n",
      "Epoch 8652: train loss: 0.1796, test loss 0.9047\n",
      "Epoch 8653: train loss: 0.1796, test loss 0.9047\n",
      "Epoch 8654: train loss: 0.1796, test loss 0.9046\n",
      "Epoch 8655: train loss: 0.1796, test loss 0.9046\n",
      "Epoch 8656: train loss: 0.1796, test loss 0.9045\n",
      "Epoch 8657: train loss: 0.1796, test loss 0.9045\n",
      "Epoch 8658: train loss: 0.1796, test loss 0.9045\n",
      "Epoch 8659: train loss: 0.1796, test loss 0.9044\n",
      "Epoch 8660: train loss: 0.1796, test loss 0.9044\n",
      "Epoch 8661: train loss: 0.1796, test loss 0.9043\n",
      "Epoch 8662: train loss: 0.1796, test loss 0.9043\n",
      "Epoch 8663: train loss: 0.1796, test loss 0.9043\n",
      "Epoch 8664: train loss: 0.1796, test loss 0.9042\n",
      "Epoch 8665: train loss: 0.1796, test loss 0.9042\n",
      "Epoch 8666: train loss: 0.1796, test loss 0.9041\n",
      "Epoch 8667: train loss: 0.1796, test loss 0.9041\n",
      "Epoch 8668: train loss: 0.1796, test loss 0.9041\n",
      "Epoch 8669: train loss: 0.1796, test loss 0.9040\n",
      "Epoch 8670: train loss: 0.1796, test loss 0.9040\n",
      "Epoch 8671: train loss: 0.1796, test loss 0.9039\n",
      "Epoch 8672: train loss: 0.1796, test loss 0.9039\n",
      "Epoch 8673: train loss: 0.1796, test loss 0.9039\n",
      "Epoch 8674: train loss: 0.1796, test loss 0.9038\n",
      "Epoch 8675: train loss: 0.1796, test loss 0.9038\n",
      "Epoch 8676: train loss: 0.1796, test loss 0.9037\n",
      "Epoch 8677: train loss: 0.1796, test loss 0.9037\n",
      "Epoch 8678: train loss: 0.1796, test loss 0.9036\n",
      "Epoch 8679: train loss: 0.1796, test loss 0.9036\n",
      "Epoch 8680: train loss: 0.1796, test loss 0.9036\n",
      "Epoch 8681: train loss: 0.1796, test loss 0.9035\n",
      "Epoch 8682: train loss: 0.1796, test loss 0.9035\n",
      "Epoch 8683: train loss: 0.1796, test loss 0.9034\n",
      "Epoch 8684: train loss: 0.1796, test loss 0.9034\n",
      "Epoch 8685: train loss: 0.1796, test loss 0.9034\n",
      "Epoch 8686: train loss: 0.1796, test loss 0.9033\n",
      "Epoch 8687: train loss: 0.1796, test loss 0.9033\n",
      "Epoch 8688: train loss: 0.1796, test loss 0.9032\n",
      "Epoch 8689: train loss: 0.1796, test loss 0.9032\n",
      "Epoch 8690: train loss: 0.1796, test loss 0.9032\n",
      "Epoch 8691: train loss: 0.1796, test loss 0.9031\n",
      "Epoch 8692: train loss: 0.1796, test loss 0.9031\n",
      "Epoch 8693: train loss: 0.1796, test loss 0.9030\n",
      "Epoch 8694: train loss: 0.1796, test loss 0.9030\n",
      "Epoch 8695: train loss: 0.1796, test loss 0.9030\n",
      "Epoch 8696: train loss: 0.1796, test loss 0.9029\n",
      "Epoch 8697: train loss: 0.1796, test loss 0.9029\n",
      "Epoch 8698: train loss: 0.1796, test loss 0.9028\n",
      "Epoch 8699: train loss: 0.1796, test loss 0.9028\n",
      "Epoch 8700: train loss: 0.1795, test loss 0.9028\n",
      "Epoch 8701: train loss: 0.1795, test loss 0.9027\n",
      "Epoch 8702: train loss: 0.1795, test loss 0.9027\n",
      "Epoch 8703: train loss: 0.1795, test loss 0.9026\n",
      "Epoch 8704: train loss: 0.1795, test loss 0.9026\n",
      "Epoch 8705: train loss: 0.1795, test loss 0.9026\n",
      "Epoch 8706: train loss: 0.1795, test loss 0.9025\n",
      "Epoch 8707: train loss: 0.1795, test loss 0.9025\n",
      "Epoch 8708: train loss: 0.1795, test loss 0.9024\n",
      "Epoch 8709: train loss: 0.1795, test loss 0.9024\n",
      "Epoch 8710: train loss: 0.1795, test loss 0.9023\n",
      "Epoch 8711: train loss: 0.1795, test loss 0.9023\n",
      "Epoch 8712: train loss: 0.1795, test loss 0.9023\n",
      "Epoch 8713: train loss: 0.1795, test loss 0.9022\n",
      "Epoch 8714: train loss: 0.1795, test loss 0.9022\n",
      "Epoch 8715: train loss: 0.1795, test loss 0.9021\n",
      "Epoch 8716: train loss: 0.1795, test loss 0.9021\n",
      "Epoch 8717: train loss: 0.1795, test loss 0.9021\n",
      "Epoch 8718: train loss: 0.1795, test loss 0.9020\n",
      "Epoch 8719: train loss: 0.1795, test loss 0.9020\n",
      "Epoch 8720: train loss: 0.1795, test loss 0.9019\n",
      "Epoch 8721: train loss: 0.1795, test loss 0.9019\n",
      "Epoch 8722: train loss: 0.1795, test loss 0.9019\n",
      "Epoch 8723: train loss: 0.1795, test loss 0.9018\n",
      "Epoch 8724: train loss: 0.1795, test loss 0.9018\n",
      "Epoch 8725: train loss: 0.1795, test loss 0.9017\n",
      "Epoch 8726: train loss: 0.1795, test loss 0.9017\n",
      "Epoch 8727: train loss: 0.1795, test loss 0.9017\n",
      "Epoch 8728: train loss: 0.1795, test loss 0.9016\n",
      "Epoch 8729: train loss: 0.1795, test loss 0.9016\n",
      "Epoch 8730: train loss: 0.1795, test loss 0.9015\n",
      "Epoch 8731: train loss: 0.1795, test loss 0.9015\n",
      "Epoch 8732: train loss: 0.1795, test loss 0.9015\n",
      "Epoch 8733: train loss: 0.1795, test loss 0.9014\n",
      "Epoch 8734: train loss: 0.1795, test loss 0.9014\n",
      "Epoch 8735: train loss: 0.1795, test loss 0.9013\n",
      "Epoch 8736: train loss: 0.1795, test loss 0.9013\n",
      "Epoch 8737: train loss: 0.1795, test loss 0.9013\n",
      "Epoch 8738: train loss: 0.1795, test loss 0.9012\n",
      "Epoch 8739: train loss: 0.1795, test loss 0.9012\n",
      "Epoch 8740: train loss: 0.1795, test loss 0.9011\n",
      "Epoch 8741: train loss: 0.1795, test loss 0.9011\n",
      "Epoch 8742: train loss: 0.1795, test loss 0.9011\n",
      "Epoch 8743: train loss: 0.1795, test loss 0.9010\n",
      "Epoch 8744: train loss: 0.1795, test loss 0.9010\n",
      "Epoch 8745: train loss: 0.1795, test loss 0.9009\n",
      "Epoch 8746: train loss: 0.1795, test loss 0.9009\n",
      "Epoch 8747: train loss: 0.1795, test loss 0.9009\n",
      "Epoch 8748: train loss: 0.1795, test loss 0.9008\n",
      "Epoch 8749: train loss: 0.1795, test loss 0.9008\n",
      "Epoch 8750: train loss: 0.1795, test loss 0.9007\n",
      "Epoch 8751: train loss: 0.1795, test loss 0.9007\n",
      "Epoch 8752: train loss: 0.1795, test loss 0.9007\n",
      "Epoch 8753: train loss: 0.1795, test loss 0.9006\n",
      "Epoch 8754: train loss: 0.1795, test loss 0.9006\n",
      "Epoch 8755: train loss: 0.1795, test loss 0.9005\n",
      "Epoch 8756: train loss: 0.1795, test loss 0.9005\n",
      "Epoch 8757: train loss: 0.1795, test loss 0.9005\n",
      "Epoch 8758: train loss: 0.1795, test loss 0.9004\n",
      "Epoch 8759: train loss: 0.1795, test loss 0.9004\n",
      "Epoch 8760: train loss: 0.1795, test loss 0.9003\n",
      "Epoch 8761: train loss: 0.1795, test loss 0.9003\n",
      "Epoch 8762: train loss: 0.1795, test loss 0.9003\n",
      "Epoch 8763: train loss: 0.1795, test loss 0.9002\n",
      "Epoch 8764: train loss: 0.1795, test loss 0.9002\n",
      "Epoch 8765: train loss: 0.1795, test loss 0.9001\n",
      "Epoch 8766: train loss: 0.1795, test loss 0.9001\n",
      "Epoch 8767: train loss: 0.1795, test loss 0.9001\n",
      "Epoch 8768: train loss: 0.1795, test loss 0.9000\n",
      "Epoch 8769: train loss: 0.1795, test loss 0.9000\n",
      "Epoch 8770: train loss: 0.1795, test loss 0.8999\n",
      "Epoch 8771: train loss: 0.1794, test loss 0.8999\n",
      "Epoch 8772: train loss: 0.1794, test loss 0.8999\n",
      "Epoch 8773: train loss: 0.1794, test loss 0.8998\n",
      "Epoch 8774: train loss: 0.1794, test loss 0.8998\n",
      "Epoch 8775: train loss: 0.1794, test loss 0.8998\n",
      "Epoch 8776: train loss: 0.1794, test loss 0.8997\n",
      "Epoch 8777: train loss: 0.1794, test loss 0.8997\n",
      "Epoch 8778: train loss: 0.1794, test loss 0.8996\n",
      "Epoch 8779: train loss: 0.1794, test loss 0.8996\n",
      "Epoch 8780: train loss: 0.1794, test loss 0.8996\n",
      "Epoch 8781: train loss: 0.1794, test loss 0.8995\n",
      "Epoch 8782: train loss: 0.1794, test loss 0.8995\n",
      "Epoch 8783: train loss: 0.1794, test loss 0.8994\n",
      "Epoch 8784: train loss: 0.1794, test loss 0.8994\n",
      "Epoch 8785: train loss: 0.1794, test loss 0.8994\n",
      "Epoch 8786: train loss: 0.1794, test loss 0.8993\n",
      "Epoch 8787: train loss: 0.1794, test loss 0.8993\n",
      "Epoch 8788: train loss: 0.1794, test loss 0.8992\n",
      "Epoch 8789: train loss: 0.1794, test loss 0.8992\n",
      "Epoch 8790: train loss: 0.1794, test loss 0.8992\n",
      "Epoch 8791: train loss: 0.1794, test loss 0.8991\n",
      "Epoch 8792: train loss: 0.1794, test loss 0.8991\n",
      "Epoch 8793: train loss: 0.1794, test loss 0.8991\n",
      "Epoch 8794: train loss: 0.1794, test loss 0.8990\n",
      "Epoch 8795: train loss: 0.1794, test loss 0.8990\n",
      "Epoch 8796: train loss: 0.1794, test loss 0.8989\n",
      "Epoch 8797: train loss: 0.1794, test loss 0.8989\n",
      "Epoch 8798: train loss: 0.1794, test loss 0.8989\n",
      "Epoch 8799: train loss: 0.1794, test loss 0.8988\n",
      "Epoch 8800: train loss: 0.1794, test loss 0.8988\n",
      "Epoch 8801: train loss: 0.1794, test loss 0.8987\n",
      "Epoch 8802: train loss: 0.1794, test loss 0.8987\n",
      "Epoch 8803: train loss: 0.1794, test loss 0.8987\n",
      "Epoch 8804: train loss: 0.1794, test loss 0.8986\n",
      "Epoch 8805: train loss: 0.1794, test loss 0.8986\n",
      "Epoch 8806: train loss: 0.1794, test loss 0.8985\n",
      "Epoch 8807: train loss: 0.1794, test loss 0.8985\n",
      "Epoch 8808: train loss: 0.1794, test loss 0.8985\n",
      "Epoch 8809: train loss: 0.1794, test loss 0.8984\n",
      "Epoch 8810: train loss: 0.1794, test loss 0.8984\n",
      "Epoch 8811: train loss: 0.1794, test loss 0.8983\n",
      "Epoch 8812: train loss: 0.1794, test loss 0.8983\n",
      "Epoch 8813: train loss: 0.1794, test loss 0.8983\n",
      "Epoch 8814: train loss: 0.1794, test loss 0.8982\n",
      "Epoch 8815: train loss: 0.1794, test loss 0.8982\n",
      "Epoch 8816: train loss: 0.1794, test loss 0.8982\n",
      "Epoch 8817: train loss: 0.1794, test loss 0.8981\n",
      "Epoch 8818: train loss: 0.1794, test loss 0.8981\n",
      "Epoch 8819: train loss: 0.1794, test loss 0.8980\n",
      "Epoch 8820: train loss: 0.1794, test loss 0.8980\n",
      "Epoch 8821: train loss: 0.1794, test loss 0.8980\n",
      "Epoch 8822: train loss: 0.1794, test loss 0.8979\n",
      "Epoch 8823: train loss: 0.1794, test loss 0.8979\n",
      "Epoch 8824: train loss: 0.1794, test loss 0.8978\n",
      "Epoch 8825: train loss: 0.1794, test loss 0.8978\n",
      "Epoch 8826: train loss: 0.1794, test loss 0.8978\n",
      "Epoch 8827: train loss: 0.1794, test loss 0.8977\n",
      "Epoch 8828: train loss: 0.1794, test loss 0.8977\n",
      "Epoch 8829: train loss: 0.1794, test loss 0.8976\n",
      "Epoch 8830: train loss: 0.1794, test loss 0.8976\n",
      "Epoch 8831: train loss: 0.1794, test loss 0.8976\n",
      "Epoch 8832: train loss: 0.1794, test loss 0.8975\n",
      "Epoch 8833: train loss: 0.1794, test loss 0.8975\n",
      "Epoch 8834: train loss: 0.1794, test loss 0.8974\n",
      "Epoch 8835: train loss: 0.1794, test loss 0.8974\n",
      "Epoch 8836: train loss: 0.1794, test loss 0.8974\n",
      "Epoch 8837: train loss: 0.1794, test loss 0.8973\n",
      "Epoch 8838: train loss: 0.1794, test loss 0.8973\n",
      "Epoch 8839: train loss: 0.1794, test loss 0.8972\n",
      "Epoch 8840: train loss: 0.1794, test loss 0.8972\n",
      "Epoch 8841: train loss: 0.1794, test loss 0.8972\n",
      "Epoch 8842: train loss: 0.1793, test loss 0.8971\n",
      "Epoch 8843: train loss: 0.1793, test loss 0.8971\n",
      "Epoch 8844: train loss: 0.1793, test loss 0.8971\n",
      "Epoch 8845: train loss: 0.1793, test loss 0.8970\n",
      "Epoch 8846: train loss: 0.1793, test loss 0.8970\n",
      "Epoch 8847: train loss: 0.1793, test loss 0.8969\n",
      "Epoch 8848: train loss: 0.1793, test loss 0.8969\n",
      "Epoch 8849: train loss: 0.1793, test loss 0.8969\n",
      "Epoch 8850: train loss: 0.1793, test loss 0.8968\n",
      "Epoch 8851: train loss: 0.1793, test loss 0.8968\n",
      "Epoch 8852: train loss: 0.1793, test loss 0.8967\n",
      "Epoch 8853: train loss: 0.1793, test loss 0.8967\n",
      "Epoch 8854: train loss: 0.1793, test loss 0.8967\n",
      "Epoch 8855: train loss: 0.1793, test loss 0.8966\n",
      "Epoch 8856: train loss: 0.1793, test loss 0.8966\n",
      "Epoch 8857: train loss: 0.1793, test loss 0.8965\n",
      "Epoch 8858: train loss: 0.1793, test loss 0.8965\n",
      "Epoch 8859: train loss: 0.1793, test loss 0.8965\n",
      "Epoch 8860: train loss: 0.1793, test loss 0.8964\n",
      "Epoch 8861: train loss: 0.1793, test loss 0.8964\n",
      "Epoch 8862: train loss: 0.1793, test loss 0.8963\n",
      "Epoch 8863: train loss: 0.1793, test loss 0.8963\n",
      "Epoch 8864: train loss: 0.1793, test loss 0.8963\n",
      "Epoch 8865: train loss: 0.1793, test loss 0.8962\n",
      "Epoch 8866: train loss: 0.1793, test loss 0.8962\n",
      "Epoch 8867: train loss: 0.1793, test loss 0.8961\n",
      "Epoch 8868: train loss: 0.1793, test loss 0.8961\n",
      "Epoch 8869: train loss: 0.1793, test loss 0.8961\n",
      "Epoch 8870: train loss: 0.1793, test loss 0.8960\n",
      "Epoch 8871: train loss: 0.1793, test loss 0.8960\n",
      "Epoch 8872: train loss: 0.1793, test loss 0.8960\n",
      "Epoch 8873: train loss: 0.1793, test loss 0.8959\n",
      "Epoch 8874: train loss: 0.1793, test loss 0.8959\n",
      "Epoch 8875: train loss: 0.1793, test loss 0.8958\n",
      "Epoch 8876: train loss: 0.1793, test loss 0.8958\n",
      "Epoch 8877: train loss: 0.1793, test loss 0.8958\n",
      "Epoch 8878: train loss: 0.1793, test loss 0.8957\n",
      "Epoch 8879: train loss: 0.1793, test loss 0.8957\n",
      "Epoch 8880: train loss: 0.1793, test loss 0.8956\n",
      "Epoch 8881: train loss: 0.1793, test loss 0.8956\n",
      "Epoch 8882: train loss: 0.1793, test loss 0.8956\n",
      "Epoch 8883: train loss: 0.1793, test loss 0.8955\n",
      "Epoch 8884: train loss: 0.1793, test loss 0.8955\n",
      "Epoch 8885: train loss: 0.1793, test loss 0.8954\n",
      "Epoch 8886: train loss: 0.1793, test loss 0.8954\n",
      "Epoch 8887: train loss: 0.1793, test loss 0.8954\n",
      "Epoch 8888: train loss: 0.1793, test loss 0.8953\n",
      "Epoch 8889: train loss: 0.1793, test loss 0.8953\n",
      "Epoch 8890: train loss: 0.1793, test loss 0.8952\n",
      "Epoch 8891: train loss: 0.1793, test loss 0.8952\n",
      "Epoch 8892: train loss: 0.1793, test loss 0.8952\n",
      "Epoch 8893: train loss: 0.1793, test loss 0.8951\n",
      "Epoch 8894: train loss: 0.1793, test loss 0.8951\n",
      "Epoch 8895: train loss: 0.1793, test loss 0.8951\n",
      "Epoch 8896: train loss: 0.1793, test loss 0.8950\n",
      "Epoch 8897: train loss: 0.1793, test loss 0.8950\n",
      "Epoch 8898: train loss: 0.1793, test loss 0.8949\n",
      "Epoch 8899: train loss: 0.1793, test loss 0.8949\n",
      "Epoch 8900: train loss: 0.1793, test loss 0.8949\n",
      "Epoch 8901: train loss: 0.1793, test loss 0.8948\n",
      "Epoch 8902: train loss: 0.1793, test loss 0.8948\n",
      "Epoch 8903: train loss: 0.1793, test loss 0.8947\n",
      "Epoch 8904: train loss: 0.1793, test loss 0.8947\n",
      "Epoch 8905: train loss: 0.1793, test loss 0.8947\n",
      "Epoch 8906: train loss: 0.1793, test loss 0.8946\n",
      "Epoch 8907: train loss: 0.1793, test loss 0.8946\n",
      "Epoch 8908: train loss: 0.1793, test loss 0.8945\n",
      "Epoch 8909: train loss: 0.1793, test loss 0.8945\n",
      "Epoch 8910: train loss: 0.1793, test loss 0.8945\n",
      "Epoch 8911: train loss: 0.1793, test loss 0.8944\n",
      "Epoch 8912: train loss: 0.1793, test loss 0.8944\n",
      "Epoch 8913: train loss: 0.1793, test loss 0.8943\n",
      "Epoch 8914: train loss: 0.1792, test loss 0.8943\n",
      "Epoch 8915: train loss: 0.1792, test loss 0.8943\n",
      "Epoch 8916: train loss: 0.1792, test loss 0.8942\n",
      "Epoch 8917: train loss: 0.1792, test loss 0.8942\n",
      "Epoch 8918: train loss: 0.1792, test loss 0.8942\n",
      "Epoch 8919: train loss: 0.1792, test loss 0.8941\n",
      "Epoch 8920: train loss: 0.1792, test loss 0.8941\n",
      "Epoch 8921: train loss: 0.1792, test loss 0.8940\n",
      "Epoch 8922: train loss: 0.1792, test loss 0.8940\n",
      "Epoch 8923: train loss: 0.1792, test loss 0.8940\n",
      "Epoch 8924: train loss: 0.1792, test loss 0.8939\n",
      "Epoch 8925: train loss: 0.1792, test loss 0.8939\n",
      "Epoch 8926: train loss: 0.1792, test loss 0.8938\n",
      "Epoch 8927: train loss: 0.1792, test loss 0.8938\n",
      "Epoch 8928: train loss: 0.1792, test loss 0.8938\n",
      "Epoch 8929: train loss: 0.1792, test loss 0.8937\n",
      "Epoch 8930: train loss: 0.1792, test loss 0.8937\n",
      "Epoch 8931: train loss: 0.1792, test loss 0.8936\n",
      "Epoch 8932: train loss: 0.1792, test loss 0.8936\n",
      "Epoch 8933: train loss: 0.1792, test loss 0.8936\n",
      "Epoch 8934: train loss: 0.1792, test loss 0.8935\n",
      "Epoch 8935: train loss: 0.1792, test loss 0.8935\n",
      "Epoch 8936: train loss: 0.1792, test loss 0.8934\n",
      "Epoch 8937: train loss: 0.1792, test loss 0.8934\n",
      "Epoch 8938: train loss: 0.1792, test loss 0.8934\n",
      "Epoch 8939: train loss: 0.1792, test loss 0.8933\n",
      "Epoch 8940: train loss: 0.1792, test loss 0.8933\n",
      "Epoch 8941: train loss: 0.1792, test loss 0.8933\n",
      "Epoch 8942: train loss: 0.1792, test loss 0.8932\n",
      "Epoch 8943: train loss: 0.1792, test loss 0.8932\n",
      "Epoch 8944: train loss: 0.1792, test loss 0.8931\n",
      "Epoch 8945: train loss: 0.1792, test loss 0.8931\n",
      "Epoch 8946: train loss: 0.1792, test loss 0.8931\n",
      "Epoch 8947: train loss: 0.1792, test loss 0.8930\n",
      "Epoch 8948: train loss: 0.1792, test loss 0.8930\n",
      "Epoch 8949: train loss: 0.1792, test loss 0.8929\n",
      "Epoch 8950: train loss: 0.1792, test loss 0.8929\n",
      "Epoch 8951: train loss: 0.1792, test loss 0.8929\n",
      "Epoch 8952: train loss: 0.1792, test loss 0.8928\n",
      "Epoch 8953: train loss: 0.1792, test loss 0.8928\n",
      "Epoch 8954: train loss: 0.1792, test loss 0.8928\n",
      "Epoch 8955: train loss: 0.1792, test loss 0.8927\n",
      "Epoch 8956: train loss: 0.1792, test loss 0.8927\n",
      "Epoch 8957: train loss: 0.1792, test loss 0.8926\n",
      "Epoch 8958: train loss: 0.1792, test loss 0.8926\n",
      "Epoch 8959: train loss: 0.1792, test loss 0.8926\n",
      "Epoch 8960: train loss: 0.1792, test loss 0.8925\n",
      "Epoch 8961: train loss: 0.1792, test loss 0.8925\n",
      "Epoch 8962: train loss: 0.1792, test loss 0.8924\n",
      "Epoch 8963: train loss: 0.1792, test loss 0.8924\n",
      "Epoch 8964: train loss: 0.1792, test loss 0.8924\n",
      "Epoch 8965: train loss: 0.1792, test loss 0.8923\n",
      "Epoch 8966: train loss: 0.1792, test loss 0.8923\n",
      "Epoch 8967: train loss: 0.1792, test loss 0.8922\n",
      "Epoch 8968: train loss: 0.1792, test loss 0.8922\n",
      "Epoch 8969: train loss: 0.1792, test loss 0.8922\n",
      "Epoch 8970: train loss: 0.1792, test loss 0.8921\n",
      "Epoch 8971: train loss: 0.1792, test loss 0.8921\n",
      "Epoch 8972: train loss: 0.1792, test loss 0.8921\n",
      "Epoch 8973: train loss: 0.1792, test loss 0.8920\n",
      "Epoch 8974: train loss: 0.1792, test loss 0.8920\n",
      "Epoch 8975: train loss: 0.1792, test loss 0.8919\n",
      "Epoch 8976: train loss: 0.1792, test loss 0.8919\n",
      "Epoch 8977: train loss: 0.1792, test loss 0.8919\n",
      "Epoch 8978: train loss: 0.1792, test loss 0.8918\n",
      "Epoch 8979: train loss: 0.1792, test loss 0.8918\n",
      "Epoch 8980: train loss: 0.1792, test loss 0.8917\n",
      "Epoch 8981: train loss: 0.1792, test loss 0.8917\n",
      "Epoch 8982: train loss: 0.1792, test loss 0.8917\n",
      "Epoch 8983: train loss: 0.1792, test loss 0.8916\n",
      "Epoch 8984: train loss: 0.1792, test loss 0.8916\n",
      "Epoch 8985: train loss: 0.1792, test loss 0.8916\n",
      "Epoch 8986: train loss: 0.1792, test loss 0.8915\n",
      "Epoch 8987: train loss: 0.1791, test loss 0.8915\n",
      "Epoch 8988: train loss: 0.1791, test loss 0.8914\n",
      "Epoch 8989: train loss: 0.1791, test loss 0.8914\n",
      "Epoch 8990: train loss: 0.1791, test loss 0.8914\n",
      "Epoch 8991: train loss: 0.1791, test loss 0.8913\n",
      "Epoch 8992: train loss: 0.1791, test loss 0.8913\n",
      "Epoch 8993: train loss: 0.1791, test loss 0.8912\n",
      "Epoch 8994: train loss: 0.1791, test loss 0.8912\n",
      "Epoch 8995: train loss: 0.1791, test loss 0.8912\n",
      "Epoch 8996: train loss: 0.1791, test loss 0.8911\n",
      "Epoch 8997: train loss: 0.1791, test loss 0.8911\n",
      "Epoch 8998: train loss: 0.1791, test loss 0.8911\n",
      "Epoch 8999: train loss: 0.1791, test loss 0.8910\n",
      "Epoch 9000: train loss: 0.1791, test loss 0.8910\n",
      "Epoch 9001: train loss: 0.1791, test loss 0.8909\n",
      "Epoch 9002: train loss: 0.1791, test loss 0.8909\n",
      "Epoch 9003: train loss: 0.1791, test loss 0.8909\n",
      "Epoch 9004: train loss: 0.1791, test loss 0.8908\n",
      "Epoch 9005: train loss: 0.1791, test loss 0.8908\n",
      "Epoch 9006: train loss: 0.1791, test loss 0.8907\n",
      "Epoch 9007: train loss: 0.1791, test loss 0.8907\n",
      "Epoch 9008: train loss: 0.1791, test loss 0.8907\n",
      "Epoch 9009: train loss: 0.1791, test loss 0.8906\n",
      "Epoch 9010: train loss: 0.1791, test loss 0.8906\n",
      "Epoch 9011: train loss: 0.1791, test loss 0.8906\n",
      "Epoch 9012: train loss: 0.1791, test loss 0.8905\n",
      "Epoch 9013: train loss: 0.1791, test loss 0.8905\n",
      "Epoch 9014: train loss: 0.1791, test loss 0.8904\n",
      "Epoch 9015: train loss: 0.1791, test loss 0.8904\n",
      "Epoch 9016: train loss: 0.1791, test loss 0.8904\n",
      "Epoch 9017: train loss: 0.1791, test loss 0.8903\n",
      "Epoch 9018: train loss: 0.1791, test loss 0.8903\n",
      "Epoch 9019: train loss: 0.1791, test loss 0.8902\n",
      "Epoch 9020: train loss: 0.1791, test loss 0.8902\n",
      "Epoch 9021: train loss: 0.1791, test loss 0.8902\n",
      "Epoch 9022: train loss: 0.1791, test loss 0.8901\n",
      "Epoch 9023: train loss: 0.1791, test loss 0.8901\n",
      "Epoch 9024: train loss: 0.1791, test loss 0.8900\n",
      "Epoch 9025: train loss: 0.1791, test loss 0.8900\n",
      "Epoch 9026: train loss: 0.1791, test loss 0.8900\n",
      "Epoch 9027: train loss: 0.1791, test loss 0.8899\n",
      "Epoch 9028: train loss: 0.1791, test loss 0.8899\n",
      "Epoch 9029: train loss: 0.1791, test loss 0.8899\n",
      "Epoch 9030: train loss: 0.1791, test loss 0.8898\n",
      "Epoch 9031: train loss: 0.1791, test loss 0.8898\n",
      "Epoch 9032: train loss: 0.1791, test loss 0.8897\n",
      "Epoch 9033: train loss: 0.1791, test loss 0.8897\n",
      "Epoch 9034: train loss: 0.1791, test loss 0.8897\n",
      "Epoch 9035: train loss: 0.1791, test loss 0.8896\n",
      "Epoch 9036: train loss: 0.1791, test loss 0.8896\n",
      "Epoch 9037: train loss: 0.1791, test loss 0.8895\n",
      "Epoch 9038: train loss: 0.1791, test loss 0.8895\n",
      "Epoch 9039: train loss: 0.1791, test loss 0.8895\n",
      "Epoch 9040: train loss: 0.1791, test loss 0.8894\n",
      "Epoch 9041: train loss: 0.1791, test loss 0.8894\n",
      "Epoch 9042: train loss: 0.1791, test loss 0.8894\n",
      "Epoch 9043: train loss: 0.1791, test loss 0.8893\n",
      "Epoch 9044: train loss: 0.1791, test loss 0.8893\n",
      "Epoch 9045: train loss: 0.1791, test loss 0.8892\n",
      "Epoch 9046: train loss: 0.1791, test loss 0.8892\n",
      "Epoch 9047: train loss: 0.1791, test loss 0.8892\n",
      "Epoch 9048: train loss: 0.1791, test loss 0.8891\n",
      "Epoch 9049: train loss: 0.1791, test loss 0.8891\n",
      "Epoch 9050: train loss: 0.1791, test loss 0.8890\n",
      "Epoch 9051: train loss: 0.1791, test loss 0.8890\n",
      "Epoch 9052: train loss: 0.1791, test loss 0.8890\n",
      "Epoch 9053: train loss: 0.1791, test loss 0.8889\n",
      "Epoch 9054: train loss: 0.1791, test loss 0.8889\n",
      "Epoch 9055: train loss: 0.1791, test loss 0.8889\n",
      "Epoch 9056: train loss: 0.1791, test loss 0.8888\n",
      "Epoch 9057: train loss: 0.1791, test loss 0.8888\n",
      "Epoch 9058: train loss: 0.1791, test loss 0.8887\n",
      "Epoch 9059: train loss: 0.1791, test loss 0.8887\n",
      "Epoch 9060: train loss: 0.1791, test loss 0.8887\n",
      "Epoch 9061: train loss: 0.1790, test loss 0.8886\n",
      "Epoch 9062: train loss: 0.1790, test loss 0.8886\n",
      "Epoch 9063: train loss: 0.1790, test loss 0.8886\n",
      "Epoch 9064: train loss: 0.1790, test loss 0.8885\n",
      "Epoch 9065: train loss: 0.1790, test loss 0.8885\n",
      "Epoch 9066: train loss: 0.1790, test loss 0.8884\n",
      "Epoch 9067: train loss: 0.1790, test loss 0.8884\n",
      "Epoch 9068: train loss: 0.1790, test loss 0.8884\n",
      "Epoch 9069: train loss: 0.1790, test loss 0.8883\n",
      "Epoch 9070: train loss: 0.1790, test loss 0.8883\n",
      "Epoch 9071: train loss: 0.1790, test loss 0.8882\n",
      "Epoch 9072: train loss: 0.1790, test loss 0.8882\n",
      "Epoch 9073: train loss: 0.1790, test loss 0.8882\n",
      "Epoch 9074: train loss: 0.1790, test loss 0.8881\n",
      "Epoch 9075: train loss: 0.1790, test loss 0.8881\n",
      "Epoch 9076: train loss: 0.1790, test loss 0.8881\n",
      "Epoch 9077: train loss: 0.1790, test loss 0.8880\n",
      "Epoch 9078: train loss: 0.1790, test loss 0.8880\n",
      "Epoch 9079: train loss: 0.1790, test loss 0.8879\n",
      "Epoch 9080: train loss: 0.1790, test loss 0.8879\n",
      "Epoch 9081: train loss: 0.1790, test loss 0.8879\n",
      "Epoch 9082: train loss: 0.1790, test loss 0.8878\n",
      "Epoch 9083: train loss: 0.1790, test loss 0.8878\n",
      "Epoch 9084: train loss: 0.1790, test loss 0.8877\n",
      "Epoch 9085: train loss: 0.1790, test loss 0.8877\n",
      "Epoch 9086: train loss: 0.1790, test loss 0.8877\n",
      "Epoch 9087: train loss: 0.1790, test loss 0.8876\n",
      "Epoch 9088: train loss: 0.1790, test loss 0.8876\n",
      "Epoch 9089: train loss: 0.1790, test loss 0.8876\n",
      "Epoch 9090: train loss: 0.1790, test loss 0.8875\n",
      "Epoch 9091: train loss: 0.1790, test loss 0.8875\n",
      "Epoch 9092: train loss: 0.1790, test loss 0.8874\n",
      "Epoch 9093: train loss: 0.1790, test loss 0.8874\n",
      "Epoch 9094: train loss: 0.1790, test loss 0.8874\n",
      "Epoch 9095: train loss: 0.1790, test loss 0.8873\n",
      "Epoch 9096: train loss: 0.1790, test loss 0.8873\n",
      "Epoch 9097: train loss: 0.1790, test loss 0.8873\n",
      "Epoch 9098: train loss: 0.1790, test loss 0.8872\n",
      "Epoch 9099: train loss: 0.1790, test loss 0.8872\n",
      "Epoch 9100: train loss: 0.1790, test loss 0.8871\n",
      "Epoch 9101: train loss: 0.1790, test loss 0.8871\n",
      "Epoch 9102: train loss: 0.1790, test loss 0.8871\n",
      "Epoch 9103: train loss: 0.1790, test loss 0.8870\n",
      "Epoch 9104: train loss: 0.1790, test loss 0.8870\n",
      "Epoch 9105: train loss: 0.1790, test loss 0.8870\n",
      "Epoch 9106: train loss: 0.1790, test loss 0.8869\n",
      "Epoch 9107: train loss: 0.1790, test loss 0.8869\n",
      "Epoch 9108: train loss: 0.1790, test loss 0.8868\n",
      "Epoch 9109: train loss: 0.1790, test loss 0.8868\n",
      "Epoch 9110: train loss: 0.1790, test loss 0.8868\n",
      "Epoch 9111: train loss: 0.1790, test loss 0.8867\n",
      "Epoch 9112: train loss: 0.1790, test loss 0.8867\n",
      "Epoch 9113: train loss: 0.1790, test loss 0.8867\n",
      "Epoch 9114: train loss: 0.1790, test loss 0.8866\n",
      "Epoch 9115: train loss: 0.1790, test loss 0.8866\n",
      "Epoch 9116: train loss: 0.1790, test loss 0.8865\n",
      "Epoch 9117: train loss: 0.1790, test loss 0.8865\n",
      "Epoch 9118: train loss: 0.1790, test loss 0.8865\n",
      "Epoch 9119: train loss: 0.1790, test loss 0.8864\n",
      "Epoch 9120: train loss: 0.1790, test loss 0.8864\n",
      "Epoch 9121: train loss: 0.1790, test loss 0.8863\n",
      "Epoch 9122: train loss: 0.1790, test loss 0.8863\n",
      "Epoch 9123: train loss: 0.1790, test loss 0.8863\n",
      "Epoch 9124: train loss: 0.1790, test loss 0.8862\n",
      "Epoch 9125: train loss: 0.1790, test loss 0.8862\n",
      "Epoch 9126: train loss: 0.1790, test loss 0.8862\n",
      "Epoch 9127: train loss: 0.1790, test loss 0.8861\n",
      "Epoch 9128: train loss: 0.1790, test loss 0.8861\n",
      "Epoch 9129: train loss: 0.1790, test loss 0.8860\n",
      "Epoch 9130: train loss: 0.1790, test loss 0.8860\n",
      "Epoch 9131: train loss: 0.1790, test loss 0.8860\n",
      "Epoch 9132: train loss: 0.1790, test loss 0.8859\n",
      "Epoch 9133: train loss: 0.1790, test loss 0.8859\n",
      "Epoch 9134: train loss: 0.1790, test loss 0.8859\n",
      "Epoch 9135: train loss: 0.1790, test loss 0.8858\n",
      "Epoch 9136: train loss: 0.1789, test loss 0.8858\n",
      "Epoch 9137: train loss: 0.1789, test loss 0.8857\n",
      "Epoch 9138: train loss: 0.1789, test loss 0.8857\n",
      "Epoch 9139: train loss: 0.1789, test loss 0.8857\n",
      "Epoch 9140: train loss: 0.1789, test loss 0.8856\n",
      "Epoch 9141: train loss: 0.1789, test loss 0.8856\n",
      "Epoch 9142: train loss: 0.1789, test loss 0.8856\n",
      "Epoch 9143: train loss: 0.1789, test loss 0.8855\n",
      "Epoch 9144: train loss: 0.1789, test loss 0.8855\n",
      "Epoch 9145: train loss: 0.1789, test loss 0.8854\n",
      "Epoch 9146: train loss: 0.1789, test loss 0.8854\n",
      "Epoch 9147: train loss: 0.1789, test loss 0.8854\n",
      "Epoch 9148: train loss: 0.1789, test loss 0.8853\n",
      "Epoch 9149: train loss: 0.1789, test loss 0.8853\n",
      "Epoch 9150: train loss: 0.1789, test loss 0.8853\n",
      "Epoch 9151: train loss: 0.1789, test loss 0.8852\n",
      "Epoch 9152: train loss: 0.1789, test loss 0.8852\n",
      "Epoch 9153: train loss: 0.1789, test loss 0.8851\n",
      "Epoch 9154: train loss: 0.1789, test loss 0.8851\n",
      "Epoch 9155: train loss: 0.1789, test loss 0.8851\n",
      "Epoch 9156: train loss: 0.1789, test loss 0.8850\n",
      "Epoch 9157: train loss: 0.1789, test loss 0.8850\n",
      "Epoch 9158: train loss: 0.1789, test loss 0.8850\n",
      "Epoch 9159: train loss: 0.1789, test loss 0.8849\n",
      "Epoch 9160: train loss: 0.1789, test loss 0.8849\n",
      "Epoch 9161: train loss: 0.1789, test loss 0.8848\n",
      "Epoch 9162: train loss: 0.1789, test loss 0.8848\n",
      "Epoch 9163: train loss: 0.1789, test loss 0.8848\n",
      "Epoch 9164: train loss: 0.1789, test loss 0.8847\n",
      "Epoch 9165: train loss: 0.1789, test loss 0.8847\n",
      "Epoch 9166: train loss: 0.1789, test loss 0.8847\n",
      "Epoch 9167: train loss: 0.1789, test loss 0.8846\n",
      "Epoch 9168: train loss: 0.1789, test loss 0.8846\n",
      "Epoch 9169: train loss: 0.1789, test loss 0.8845\n",
      "Epoch 9170: train loss: 0.1789, test loss 0.8845\n",
      "Epoch 9171: train loss: 0.1789, test loss 0.8845\n",
      "Epoch 9172: train loss: 0.1789, test loss 0.8844\n",
      "Epoch 9173: train loss: 0.1789, test loss 0.8844\n",
      "Epoch 9174: train loss: 0.1789, test loss 0.8844\n",
      "Epoch 9175: train loss: 0.1789, test loss 0.8843\n",
      "Epoch 9176: train loss: 0.1789, test loss 0.8843\n",
      "Epoch 9177: train loss: 0.1789, test loss 0.8842\n",
      "Epoch 9178: train loss: 0.1789, test loss 0.8842\n",
      "Epoch 9179: train loss: 0.1789, test loss 0.8842\n",
      "Epoch 9180: train loss: 0.1789, test loss 0.8841\n",
      "Epoch 9181: train loss: 0.1789, test loss 0.8841\n",
      "Epoch 9182: train loss: 0.1789, test loss 0.8841\n",
      "Epoch 9183: train loss: 0.1789, test loss 0.8840\n",
      "Epoch 9184: train loss: 0.1789, test loss 0.8840\n",
      "Epoch 9185: train loss: 0.1789, test loss 0.8840\n",
      "Epoch 9186: train loss: 0.1789, test loss 0.8839\n",
      "Epoch 9187: train loss: 0.1789, test loss 0.8839\n",
      "Epoch 9188: train loss: 0.1789, test loss 0.8838\n",
      "Epoch 9189: train loss: 0.1789, test loss 0.8838\n",
      "Epoch 9190: train loss: 0.1789, test loss 0.8838\n",
      "Epoch 9191: train loss: 0.1789, test loss 0.8837\n",
      "Epoch 9192: train loss: 0.1789, test loss 0.8837\n",
      "Epoch 9193: train loss: 0.1789, test loss 0.8837\n",
      "Epoch 9194: train loss: 0.1789, test loss 0.8836\n",
      "Epoch 9195: train loss: 0.1789, test loss 0.8836\n",
      "Epoch 9196: train loss: 0.1789, test loss 0.8835\n",
      "Epoch 9197: train loss: 0.1789, test loss 0.8835\n",
      "Epoch 9198: train loss: 0.1789, test loss 0.8835\n",
      "Epoch 9199: train loss: 0.1789, test loss 0.8834\n",
      "Epoch 9200: train loss: 0.1789, test loss 0.8834\n",
      "Epoch 9201: train loss: 0.1789, test loss 0.8834\n",
      "Epoch 9202: train loss: 0.1789, test loss 0.8833\n",
      "Epoch 9203: train loss: 0.1789, test loss 0.8833\n",
      "Epoch 9204: train loss: 0.1789, test loss 0.8833\n",
      "Epoch 9205: train loss: 0.1789, test loss 0.8832\n",
      "Epoch 9206: train loss: 0.1789, test loss 0.8832\n",
      "Epoch 9207: train loss: 0.1789, test loss 0.8832\n",
      "Epoch 9208: train loss: 0.1789, test loss 0.8831\n",
      "Epoch 9209: train loss: 0.1789, test loss 0.8831\n",
      "Epoch 9210: train loss: 0.1789, test loss 0.8830\n",
      "Epoch 9211: train loss: 0.1789, test loss 0.8830\n",
      "Epoch 9212: train loss: 0.1788, test loss 0.8830\n",
      "Epoch 9213: train loss: 0.1788, test loss 0.8829\n",
      "Epoch 9214: train loss: 0.1788, test loss 0.8829\n",
      "Epoch 9215: train loss: 0.1788, test loss 0.8829\n",
      "Epoch 9216: train loss: 0.1788, test loss 0.8828\n",
      "Epoch 9217: train loss: 0.1788, test loss 0.8828\n",
      "Epoch 9218: train loss: 0.1788, test loss 0.8828\n",
      "Epoch 9219: train loss: 0.1788, test loss 0.8827\n",
      "Epoch 9220: train loss: 0.1788, test loss 0.8827\n",
      "Epoch 9221: train loss: 0.1788, test loss 0.8826\n",
      "Epoch 9222: train loss: 0.1788, test loss 0.8826\n",
      "Epoch 9223: train loss: 0.1788, test loss 0.8826\n",
      "Epoch 9224: train loss: 0.1788, test loss 0.8825\n",
      "Epoch 9225: train loss: 0.1788, test loss 0.8825\n",
      "Epoch 9226: train loss: 0.1788, test loss 0.8825\n",
      "Epoch 9227: train loss: 0.1788, test loss 0.8824\n",
      "Epoch 9228: train loss: 0.1788, test loss 0.8824\n",
      "Epoch 9229: train loss: 0.1788, test loss 0.8824\n",
      "Epoch 9230: train loss: 0.1788, test loss 0.8823\n",
      "Epoch 9231: train loss: 0.1788, test loss 0.8823\n",
      "Epoch 9232: train loss: 0.1788, test loss 0.8822\n",
      "Epoch 9233: train loss: 0.1788, test loss 0.8822\n",
      "Epoch 9234: train loss: 0.1788, test loss 0.8822\n",
      "Epoch 9235: train loss: 0.1788, test loss 0.8821\n",
      "Epoch 9236: train loss: 0.1788, test loss 0.8821\n",
      "Epoch 9237: train loss: 0.1788, test loss 0.8821\n",
      "Epoch 9238: train loss: 0.1788, test loss 0.8820\n",
      "Epoch 9239: train loss: 0.1788, test loss 0.8820\n",
      "Epoch 9240: train loss: 0.1788, test loss 0.8820\n",
      "Epoch 9241: train loss: 0.1788, test loss 0.8819\n",
      "Epoch 9242: train loss: 0.1788, test loss 0.8819\n",
      "Epoch 9243: train loss: 0.1788, test loss 0.8818\n",
      "Epoch 9244: train loss: 0.1788, test loss 0.8818\n",
      "Epoch 9245: train loss: 0.1788, test loss 0.8818\n",
      "Epoch 9246: train loss: 0.1788, test loss 0.8817\n",
      "Epoch 9247: train loss: 0.1788, test loss 0.8817\n",
      "Epoch 9248: train loss: 0.1788, test loss 0.8817\n",
      "Epoch 9249: train loss: 0.1788, test loss 0.8816\n",
      "Epoch 9250: train loss: 0.1788, test loss 0.8816\n",
      "Epoch 9251: train loss: 0.1788, test loss 0.8816\n",
      "Epoch 9252: train loss: 0.1788, test loss 0.8815\n",
      "Epoch 9253: train loss: 0.1788, test loss 0.8815\n",
      "Epoch 9254: train loss: 0.1788, test loss 0.8815\n",
      "Epoch 9255: train loss: 0.1788, test loss 0.8814\n",
      "Epoch 9256: train loss: 0.1788, test loss 0.8814\n",
      "Epoch 9257: train loss: 0.1788, test loss 0.8813\n",
      "Epoch 9258: train loss: 0.1788, test loss 0.8813\n",
      "Epoch 9259: train loss: 0.1788, test loss 0.8813\n",
      "Epoch 9260: train loss: 0.1788, test loss 0.8812\n",
      "Epoch 9261: train loss: 0.1788, test loss 0.8812\n",
      "Epoch 9262: train loss: 0.1788, test loss 0.8812\n",
      "Epoch 9263: train loss: 0.1788, test loss 0.8811\n",
      "Epoch 9264: train loss: 0.1788, test loss 0.8811\n",
      "Epoch 9265: train loss: 0.1788, test loss 0.8811\n",
      "Epoch 9266: train loss: 0.1788, test loss 0.8810\n",
      "Epoch 9267: train loss: 0.1788, test loss 0.8810\n",
      "Epoch 9268: train loss: 0.1788, test loss 0.8809\n",
      "Epoch 9269: train loss: 0.1788, test loss 0.8809\n",
      "Epoch 9270: train loss: 0.1788, test loss 0.8809\n",
      "Epoch 9271: train loss: 0.1788, test loss 0.8808\n",
      "Epoch 9272: train loss: 0.1788, test loss 0.8808\n",
      "Epoch 9273: train loss: 0.1788, test loss 0.8808\n",
      "Epoch 9274: train loss: 0.1788, test loss 0.8807\n",
      "Epoch 9275: train loss: 0.1788, test loss 0.8807\n",
      "Epoch 9276: train loss: 0.1788, test loss 0.8807\n",
      "Epoch 9277: train loss: 0.1788, test loss 0.8806\n",
      "Epoch 9278: train loss: 0.1788, test loss 0.8806\n",
      "Epoch 9279: train loss: 0.1788, test loss 0.8805\n",
      "Epoch 9280: train loss: 0.1788, test loss 0.8805\n",
      "Epoch 9281: train loss: 0.1788, test loss 0.8805\n",
      "Epoch 9282: train loss: 0.1788, test loss 0.8804\n",
      "Epoch 9283: train loss: 0.1788, test loss 0.8804\n",
      "Epoch 9284: train loss: 0.1788, test loss 0.8804\n",
      "Epoch 9285: train loss: 0.1788, test loss 0.8803\n",
      "Epoch 9286: train loss: 0.1788, test loss 0.8803\n",
      "Epoch 9287: train loss: 0.1788, test loss 0.8803\n",
      "Epoch 9288: train loss: 0.1788, test loss 0.8802\n",
      "Epoch 9289: train loss: 0.1787, test loss 0.8802\n",
      "Epoch 9290: train loss: 0.1787, test loss 0.8802\n",
      "Epoch 9291: train loss: 0.1787, test loss 0.8801\n",
      "Epoch 9292: train loss: 0.1787, test loss 0.8801\n",
      "Epoch 9293: train loss: 0.1787, test loss 0.8800\n",
      "Epoch 9294: train loss: 0.1787, test loss 0.8800\n",
      "Epoch 9295: train loss: 0.1787, test loss 0.8800\n",
      "Epoch 9296: train loss: 0.1787, test loss 0.8799\n",
      "Epoch 9297: train loss: 0.1787, test loss 0.8799\n",
      "Epoch 9298: train loss: 0.1787, test loss 0.8799\n",
      "Epoch 9299: train loss: 0.1787, test loss 0.8798\n",
      "Epoch 9300: train loss: 0.1787, test loss 0.8798\n",
      "Epoch 9301: train loss: 0.1787, test loss 0.8798\n",
      "Epoch 9302: train loss: 0.1787, test loss 0.8797\n",
      "Epoch 9303: train loss: 0.1787, test loss 0.8797\n",
      "Epoch 9304: train loss: 0.1787, test loss 0.8797\n",
      "Epoch 9305: train loss: 0.1787, test loss 0.8796\n",
      "Epoch 9306: train loss: 0.1787, test loss 0.8796\n",
      "Epoch 9307: train loss: 0.1787, test loss 0.8795\n",
      "Epoch 9308: train loss: 0.1787, test loss 0.8795\n",
      "Epoch 9309: train loss: 0.1787, test loss 0.8795\n",
      "Epoch 9310: train loss: 0.1787, test loss 0.8794\n",
      "Epoch 9311: train loss: 0.1787, test loss 0.8794\n",
      "Epoch 9312: train loss: 0.1787, test loss 0.8794\n",
      "Epoch 9313: train loss: 0.1787, test loss 0.8793\n",
      "Epoch 9314: train loss: 0.1787, test loss 0.8793\n",
      "Epoch 9315: train loss: 0.1787, test loss 0.8793\n",
      "Epoch 9316: train loss: 0.1787, test loss 0.8792\n",
      "Epoch 9317: train loss: 0.1787, test loss 0.8792\n",
      "Epoch 9318: train loss: 0.1787, test loss 0.8792\n",
      "Epoch 9319: train loss: 0.1787, test loss 0.8791\n",
      "Epoch 9320: train loss: 0.1787, test loss 0.8791\n",
      "Epoch 9321: train loss: 0.1787, test loss 0.8790\n",
      "Epoch 9322: train loss: 0.1787, test loss 0.8790\n",
      "Epoch 9323: train loss: 0.1787, test loss 0.8790\n",
      "Epoch 9324: train loss: 0.1787, test loss 0.8789\n",
      "Epoch 9325: train loss: 0.1787, test loss 0.8789\n",
      "Epoch 9326: train loss: 0.1787, test loss 0.8789\n",
      "Epoch 9327: train loss: 0.1787, test loss 0.8788\n",
      "Epoch 9328: train loss: 0.1787, test loss 0.8788\n",
      "Epoch 9329: train loss: 0.1787, test loss 0.8788\n",
      "Epoch 9330: train loss: 0.1787, test loss 0.8787\n",
      "Epoch 9331: train loss: 0.1787, test loss 0.8787\n",
      "Epoch 9332: train loss: 0.1787, test loss 0.8787\n",
      "Epoch 9333: train loss: 0.1787, test loss 0.8786\n",
      "Epoch 9334: train loss: 0.1787, test loss 0.8786\n",
      "Epoch 9335: train loss: 0.1787, test loss 0.8785\n",
      "Epoch 9336: train loss: 0.1787, test loss 0.8785\n",
      "Epoch 9337: train loss: 0.1787, test loss 0.8785\n",
      "Epoch 9338: train loss: 0.1787, test loss 0.8784\n",
      "Epoch 9339: train loss: 0.1787, test loss 0.8784\n",
      "Epoch 9340: train loss: 0.1787, test loss 0.8784\n",
      "Epoch 9341: train loss: 0.1787, test loss 0.8783\n",
      "Epoch 9342: train loss: 0.1787, test loss 0.8783\n",
      "Epoch 9343: train loss: 0.1787, test loss 0.8783\n",
      "Epoch 9344: train loss: 0.1787, test loss 0.8782\n",
      "Epoch 9345: train loss: 0.1787, test loss 0.8782\n",
      "Epoch 9346: train loss: 0.1787, test loss 0.8782\n",
      "Epoch 9347: train loss: 0.1787, test loss 0.8781\n",
      "Epoch 9348: train loss: 0.1787, test loss 0.8781\n",
      "Epoch 9349: train loss: 0.1787, test loss 0.8780\n",
      "Epoch 9350: train loss: 0.1787, test loss 0.8780\n",
      "Epoch 9351: train loss: 0.1787, test loss 0.8780\n",
      "Epoch 9352: train loss: 0.1787, test loss 0.8779\n",
      "Epoch 9353: train loss: 0.1787, test loss 0.8779\n",
      "Epoch 9354: train loss: 0.1787, test loss 0.8779\n",
      "Epoch 9355: train loss: 0.1787, test loss 0.8778\n",
      "Epoch 9356: train loss: 0.1787, test loss 0.8778\n",
      "Epoch 9357: train loss: 0.1787, test loss 0.8778\n",
      "Epoch 9358: train loss: 0.1787, test loss 0.8777\n",
      "Epoch 9359: train loss: 0.1787, test loss 0.8777\n",
      "Epoch 9360: train loss: 0.1787, test loss 0.8777\n",
      "Epoch 9361: train loss: 0.1787, test loss 0.8776\n",
      "Epoch 9362: train loss: 0.1787, test loss 0.8776\n",
      "Epoch 9363: train loss: 0.1787, test loss 0.8775\n",
      "Epoch 9364: train loss: 0.1787, test loss 0.8775\n",
      "Epoch 9365: train loss: 0.1787, test loss 0.8775\n",
      "Epoch 9366: train loss: 0.1787, test loss 0.8774\n",
      "Epoch 9367: train loss: 0.1786, test loss 0.8774\n",
      "Epoch 9368: train loss: 0.1786, test loss 0.8774\n",
      "Epoch 9369: train loss: 0.1786, test loss 0.8773\n",
      "Epoch 9370: train loss: 0.1786, test loss 0.8773\n",
      "Epoch 9371: train loss: 0.1786, test loss 0.8773\n",
      "Epoch 9372: train loss: 0.1786, test loss 0.8772\n",
      "Epoch 9373: train loss: 0.1786, test loss 0.8772\n",
      "Epoch 9374: train loss: 0.1786, test loss 0.8771\n",
      "Epoch 9375: train loss: 0.1786, test loss 0.8771\n",
      "Epoch 9376: train loss: 0.1786, test loss 0.8771\n",
      "Epoch 9377: train loss: 0.1786, test loss 0.8770\n",
      "Epoch 9378: train loss: 0.1786, test loss 0.8770\n",
      "Epoch 9379: train loss: 0.1786, test loss 0.8770\n",
      "Epoch 9380: train loss: 0.1786, test loss 0.8769\n",
      "Epoch 9381: train loss: 0.1786, test loss 0.8769\n",
      "Epoch 9382: train loss: 0.1786, test loss 0.8769\n",
      "Epoch 9383: train loss: 0.1786, test loss 0.8768\n",
      "Epoch 9384: train loss: 0.1786, test loss 0.8768\n",
      "Epoch 9385: train loss: 0.1786, test loss 0.8768\n",
      "Epoch 9386: train loss: 0.1786, test loss 0.8767\n",
      "Epoch 9387: train loss: 0.1786, test loss 0.8767\n",
      "Epoch 9388: train loss: 0.1786, test loss 0.8766\n",
      "Epoch 9389: train loss: 0.1786, test loss 0.8766\n",
      "Epoch 9390: train loss: 0.1786, test loss 0.8766\n",
      "Epoch 9391: train loss: 0.1786, test loss 0.8765\n",
      "Epoch 9392: train loss: 0.1786, test loss 0.8765\n",
      "Epoch 9393: train loss: 0.1786, test loss 0.8765\n",
      "Epoch 9394: train loss: 0.1786, test loss 0.8764\n",
      "Epoch 9395: train loss: 0.1786, test loss 0.8764\n",
      "Epoch 9396: train loss: 0.1786, test loss 0.8764\n",
      "Epoch 9397: train loss: 0.1786, test loss 0.8763\n",
      "Epoch 9398: train loss: 0.1786, test loss 0.8763\n",
      "Epoch 9399: train loss: 0.1786, test loss 0.8762\n",
      "Epoch 9400: train loss: 0.1786, test loss 0.8762\n",
      "Epoch 9401: train loss: 0.1786, test loss 0.8762\n",
      "Epoch 9402: train loss: 0.1786, test loss 0.8761\n",
      "Epoch 9403: train loss: 0.1786, test loss 0.8761\n",
      "Epoch 9404: train loss: 0.1786, test loss 0.8761\n",
      "Epoch 9405: train loss: 0.1786, test loss 0.8760\n",
      "Epoch 9406: train loss: 0.1786, test loss 0.8760\n",
      "Epoch 9407: train loss: 0.1786, test loss 0.8760\n",
      "Epoch 9408: train loss: 0.1786, test loss 0.8759\n",
      "Epoch 9409: train loss: 0.1786, test loss 0.8759\n",
      "Epoch 9410: train loss: 0.1786, test loss 0.8759\n",
      "Epoch 9411: train loss: 0.1786, test loss 0.8758\n",
      "Epoch 9412: train loss: 0.1786, test loss 0.8758\n",
      "Epoch 9413: train loss: 0.1786, test loss 0.8758\n",
      "Epoch 9414: train loss: 0.1786, test loss 0.8757\n",
      "Epoch 9415: train loss: 0.1786, test loss 0.8757\n",
      "Epoch 9416: train loss: 0.1786, test loss 0.8756\n",
      "Epoch 9417: train loss: 0.1786, test loss 0.8756\n",
      "Epoch 9418: train loss: 0.1786, test loss 0.8756\n",
      "Epoch 9419: train loss: 0.1786, test loss 0.8755\n",
      "Epoch 9420: train loss: 0.1786, test loss 0.8755\n",
      "Epoch 9421: train loss: 0.1786, test loss 0.8755\n",
      "Epoch 9422: train loss: 0.1786, test loss 0.8754\n",
      "Epoch 9423: train loss: 0.1786, test loss 0.8754\n",
      "Epoch 9424: train loss: 0.1786, test loss 0.8754\n",
      "Epoch 9425: train loss: 0.1786, test loss 0.8753\n",
      "Epoch 9426: train loss: 0.1786, test loss 0.8753\n",
      "Epoch 9427: train loss: 0.1786, test loss 0.8753\n",
      "Epoch 9428: train loss: 0.1786, test loss 0.8752\n",
      "Epoch 9429: train loss: 0.1786, test loss 0.8752\n",
      "Epoch 9430: train loss: 0.1786, test loss 0.8751\n",
      "Epoch 9431: train loss: 0.1786, test loss 0.8751\n",
      "Epoch 9432: train loss: 0.1786, test loss 0.8751\n",
      "Epoch 9433: train loss: 0.1786, test loss 0.8750\n",
      "Epoch 9434: train loss: 0.1786, test loss 0.8750\n",
      "Epoch 9435: train loss: 0.1786, test loss 0.8750\n",
      "Epoch 9436: train loss: 0.1786, test loss 0.8749\n",
      "Epoch 9437: train loss: 0.1786, test loss 0.8749\n",
      "Epoch 9438: train loss: 0.1786, test loss 0.8749\n",
      "Epoch 9439: train loss: 0.1786, test loss 0.8748\n",
      "Epoch 9440: train loss: 0.1786, test loss 0.8748\n",
      "Epoch 9441: train loss: 0.1786, test loss 0.8748\n",
      "Epoch 9442: train loss: 0.1786, test loss 0.8747\n",
      "Epoch 9443: train loss: 0.1786, test loss 0.8747\n",
      "Epoch 9444: train loss: 0.1786, test loss 0.8747\n",
      "Epoch 9445: train loss: 0.1786, test loss 0.8746\n",
      "Epoch 9446: train loss: 0.1786, test loss 0.8746\n",
      "Epoch 9447: train loss: 0.1786, test loss 0.8745\n",
      "Epoch 9448: train loss: 0.1786, test loss 0.8745\n",
      "Epoch 9449: train loss: 0.1785, test loss 0.8745\n",
      "Epoch 9450: train loss: 0.1785, test loss 0.8744\n",
      "Epoch 9451: train loss: 0.1785, test loss 0.8744\n",
      "Epoch 9452: train loss: 0.1785, test loss 0.8744\n",
      "Epoch 9453: train loss: 0.1785, test loss 0.8743\n",
      "Epoch 9454: train loss: 0.1785, test loss 0.8743\n",
      "Epoch 9455: train loss: 0.1785, test loss 0.8743\n",
      "Epoch 9456: train loss: 0.1785, test loss 0.8742\n",
      "Epoch 9457: train loss: 0.1785, test loss 0.8742\n",
      "Epoch 9458: train loss: 0.1785, test loss 0.8742\n",
      "Epoch 9459: train loss: 0.1785, test loss 0.8741\n",
      "Epoch 9460: train loss: 0.1785, test loss 0.8741\n",
      "Epoch 9461: train loss: 0.1785, test loss 0.8741\n",
      "Epoch 9462: train loss: 0.1785, test loss 0.8740\n",
      "Epoch 9463: train loss: 0.1785, test loss 0.8740\n",
      "Epoch 9464: train loss: 0.1785, test loss 0.8739\n",
      "Epoch 9465: train loss: 0.1785, test loss 0.8739\n",
      "Epoch 9466: train loss: 0.1785, test loss 0.8739\n",
      "Epoch 9467: train loss: 0.1785, test loss 0.8738\n",
      "Epoch 9468: train loss: 0.1785, test loss 0.8738\n",
      "Epoch 9469: train loss: 0.1785, test loss 0.8738\n",
      "Epoch 9470: train loss: 0.1785, test loss 0.8737\n",
      "Epoch 9471: train loss: 0.1785, test loss 0.8737\n",
      "Epoch 9472: train loss: 0.1785, test loss 0.8737\n",
      "Epoch 9473: train loss: 0.1785, test loss 0.8736\n",
      "Epoch 9474: train loss: 0.1785, test loss 0.8736\n",
      "Epoch 9475: train loss: 0.1785, test loss 0.8736\n",
      "Epoch 9476: train loss: 0.1785, test loss 0.8735\n",
      "Epoch 9477: train loss: 0.1785, test loss 0.8735\n",
      "Epoch 9478: train loss: 0.1785, test loss 0.8735\n",
      "Epoch 9479: train loss: 0.1785, test loss 0.8734\n",
      "Epoch 9480: train loss: 0.1785, test loss 0.8734\n",
      "Epoch 9481: train loss: 0.1785, test loss 0.8734\n",
      "Epoch 9482: train loss: 0.1785, test loss 0.8733\n",
      "Epoch 9483: train loss: 0.1785, test loss 0.8733\n",
      "Epoch 9484: train loss: 0.1785, test loss 0.8732\n",
      "Epoch 9485: train loss: 0.1785, test loss 0.8732\n",
      "Epoch 9486: train loss: 0.1785, test loss 0.8732\n",
      "Epoch 9487: train loss: 0.1785, test loss 0.8731\n",
      "Epoch 9488: train loss: 0.1785, test loss 0.8731\n",
      "Epoch 9489: train loss: 0.1785, test loss 0.8731\n",
      "Epoch 9490: train loss: 0.1785, test loss 0.8730\n",
      "Epoch 9491: train loss: 0.1785, test loss 0.8730\n",
      "Epoch 9492: train loss: 0.1785, test loss 0.8730\n",
      "Epoch 9493: train loss: 0.1785, test loss 0.8729\n",
      "Epoch 9494: train loss: 0.1785, test loss 0.8729\n",
      "Epoch 9495: train loss: 0.1785, test loss 0.8729\n",
      "Epoch 9496: train loss: 0.1785, test loss 0.8728\n",
      "Epoch 9497: train loss: 0.1785, test loss 0.8728\n",
      "Epoch 9498: train loss: 0.1785, test loss 0.8728\n",
      "Epoch 9499: train loss: 0.1785, test loss 0.8727\n",
      "Epoch 9500: train loss: 0.1785, test loss 0.8727\n",
      "Epoch 9501: train loss: 0.1785, test loss 0.8727\n",
      "Epoch 9502: train loss: 0.1785, test loss 0.8726\n",
      "Epoch 9503: train loss: 0.1785, test loss 0.8726\n",
      "Epoch 9504: train loss: 0.1785, test loss 0.8726\n",
      "Epoch 9505: train loss: 0.1785, test loss 0.8725\n",
      "Epoch 9506: train loss: 0.1785, test loss 0.8725\n",
      "Epoch 9507: train loss: 0.1785, test loss 0.8725\n",
      "Epoch 9508: train loss: 0.1785, test loss 0.8724\n",
      "Epoch 9509: train loss: 0.1785, test loss 0.8724\n",
      "Epoch 9510: train loss: 0.1785, test loss 0.8724\n",
      "Epoch 9511: train loss: 0.1785, test loss 0.8723\n",
      "Epoch 9512: train loss: 0.1785, test loss 0.8723\n",
      "Epoch 9513: train loss: 0.1785, test loss 0.8723\n",
      "Epoch 9514: train loss: 0.1785, test loss 0.8722\n",
      "Epoch 9515: train loss: 0.1785, test loss 0.8722\n",
      "Epoch 9516: train loss: 0.1785, test loss 0.8722\n",
      "Epoch 9517: train loss: 0.1785, test loss 0.8721\n",
      "Epoch 9518: train loss: 0.1785, test loss 0.8721\n",
      "Epoch 9519: train loss: 0.1785, test loss 0.8721\n",
      "Epoch 9520: train loss: 0.1785, test loss 0.8720\n",
      "Epoch 9521: train loss: 0.1785, test loss 0.8720\n",
      "Epoch 9522: train loss: 0.1785, test loss 0.8720\n",
      "Epoch 9523: train loss: 0.1785, test loss 0.8719\n",
      "Epoch 9524: train loss: 0.1785, test loss 0.8719\n",
      "Epoch 9525: train loss: 0.1785, test loss 0.8718\n",
      "Epoch 9526: train loss: 0.1785, test loss 0.8718\n",
      "Epoch 9527: train loss: 0.1785, test loss 0.8718\n",
      "Epoch 9528: train loss: 0.1785, test loss 0.8718\n",
      "Epoch 9529: train loss: 0.1785, test loss 0.8717\n",
      "Epoch 9530: train loss: 0.1785, test loss 0.8717\n",
      "Epoch 9531: train loss: 0.1784, test loss 0.8717\n",
      "Epoch 9532: train loss: 0.1784, test loss 0.8716\n",
      "Epoch 9533: train loss: 0.1784, test loss 0.8716\n",
      "Epoch 9534: train loss: 0.1784, test loss 0.8715\n",
      "Epoch 9535: train loss: 0.1784, test loss 0.8715\n",
      "Epoch 9536: train loss: 0.1784, test loss 0.8715\n",
      "Epoch 9537: train loss: 0.1784, test loss 0.8714\n",
      "Epoch 9538: train loss: 0.1784, test loss 0.8714\n",
      "Epoch 9539: train loss: 0.1784, test loss 0.8714\n",
      "Epoch 9540: train loss: 0.1784, test loss 0.8713\n",
      "Epoch 9541: train loss: 0.1784, test loss 0.8713\n",
      "Epoch 9542: train loss: 0.1784, test loss 0.8713\n",
      "Epoch 9543: train loss: 0.1784, test loss 0.8712\n",
      "Epoch 9544: train loss: 0.1784, test loss 0.8712\n",
      "Epoch 9545: train loss: 0.1784, test loss 0.8712\n",
      "Epoch 9546: train loss: 0.1784, test loss 0.8711\n",
      "Epoch 9547: train loss: 0.1784, test loss 0.8711\n",
      "Epoch 9548: train loss: 0.1784, test loss 0.8711\n",
      "Epoch 9549: train loss: 0.1784, test loss 0.8710\n",
      "Epoch 9550: train loss: 0.1784, test loss 0.8710\n",
      "Epoch 9551: train loss: 0.1784, test loss 0.8710\n",
      "Epoch 9552: train loss: 0.1784, test loss 0.8709\n",
      "Epoch 9553: train loss: 0.1784, test loss 0.8709\n",
      "Epoch 9554: train loss: 0.1784, test loss 0.8709\n",
      "Epoch 9555: train loss: 0.1784, test loss 0.8708\n",
      "Epoch 9556: train loss: 0.1784, test loss 0.8708\n",
      "Epoch 9557: train loss: 0.1784, test loss 0.8708\n",
      "Epoch 9558: train loss: 0.1784, test loss 0.8707\n",
      "Epoch 9559: train loss: 0.1784, test loss 0.8707\n",
      "Epoch 9560: train loss: 0.1784, test loss 0.8707\n",
      "Epoch 9561: train loss: 0.1784, test loss 0.8706\n",
      "Epoch 9562: train loss: 0.1784, test loss 0.8706\n",
      "Epoch 9563: train loss: 0.1784, test loss 0.8706\n",
      "Epoch 9564: train loss: 0.1784, test loss 0.8705\n",
      "Epoch 9565: train loss: 0.1784, test loss 0.8705\n",
      "Epoch 9566: train loss: 0.1784, test loss 0.8705\n",
      "Epoch 9567: train loss: 0.1784, test loss 0.8704\n",
      "Epoch 9568: train loss: 0.1784, test loss 0.8704\n",
      "Epoch 9569: train loss: 0.1784, test loss 0.8704\n",
      "Epoch 9570: train loss: 0.1784, test loss 0.8703\n",
      "Epoch 9571: train loss: 0.1784, test loss 0.8703\n",
      "Epoch 9572: train loss: 0.1784, test loss 0.8703\n",
      "Epoch 9573: train loss: 0.1784, test loss 0.8702\n",
      "Epoch 9574: train loss: 0.1784, test loss 0.8702\n",
      "Epoch 9575: train loss: 0.1784, test loss 0.8702\n",
      "Epoch 9576: train loss: 0.1784, test loss 0.8701\n",
      "Epoch 9577: train loss: 0.1784, test loss 0.8701\n",
      "Epoch 9578: train loss: 0.1784, test loss 0.8701\n",
      "Epoch 9579: train loss: 0.1784, test loss 0.8700\n",
      "Epoch 9580: train loss: 0.1784, test loss 0.8700\n",
      "Epoch 9581: train loss: 0.1784, test loss 0.8700\n",
      "Epoch 9582: train loss: 0.1784, test loss 0.8699\n",
      "Epoch 9583: train loss: 0.1784, test loss 0.8699\n",
      "Epoch 9584: train loss: 0.1784, test loss 0.8698\n",
      "Epoch 9585: train loss: 0.1784, test loss 0.8698\n",
      "Epoch 9586: train loss: 0.1784, test loss 0.8698\n",
      "Epoch 9587: train loss: 0.1784, test loss 0.8697\n",
      "Epoch 9588: train loss: 0.1784, test loss 0.8697\n",
      "Epoch 9589: train loss: 0.1784, test loss 0.8697\n",
      "Epoch 9590: train loss: 0.1784, test loss 0.8696\n",
      "Epoch 9591: train loss: 0.1784, test loss 0.8696\n",
      "Epoch 9592: train loss: 0.1784, test loss 0.8696\n",
      "Epoch 9593: train loss: 0.1784, test loss 0.8695\n",
      "Epoch 9594: train loss: 0.1784, test loss 0.8695\n",
      "Epoch 9595: train loss: 0.1784, test loss 0.8695\n",
      "Epoch 9596: train loss: 0.1784, test loss 0.8694\n",
      "Epoch 9597: train loss: 0.1784, test loss 0.8694\n",
      "Epoch 9598: train loss: 0.1784, test loss 0.8694\n",
      "Epoch 9599: train loss: 0.1784, test loss 0.8693\n",
      "Epoch 9600: train loss: 0.1784, test loss 0.8693\n",
      "Epoch 9601: train loss: 0.1784, test loss 0.8693\n",
      "Epoch 9602: train loss: 0.1784, test loss 0.8692\n",
      "Epoch 9603: train loss: 0.1784, test loss 0.8692\n",
      "Epoch 9604: train loss: 0.1784, test loss 0.8692\n",
      "Epoch 9605: train loss: 0.1784, test loss 0.8691\n",
      "Epoch 9606: train loss: 0.1784, test loss 0.8691\n",
      "Epoch 9607: train loss: 0.1784, test loss 0.8691\n",
      "Epoch 9608: train loss: 0.1784, test loss 0.8690\n",
      "Epoch 9609: train loss: 0.1784, test loss 0.8690\n",
      "Epoch 9610: train loss: 0.1784, test loss 0.8690\n",
      "Epoch 9611: train loss: 0.1784, test loss 0.8689\n",
      "Epoch 9612: train loss: 0.1784, test loss 0.8689\n",
      "Epoch 9613: train loss: 0.1783, test loss 0.8689\n",
      "Epoch 9614: train loss: 0.1783, test loss 0.8688\n",
      "Epoch 9615: train loss: 0.1783, test loss 0.8688\n",
      "Epoch 9616: train loss: 0.1783, test loss 0.8688\n",
      "Epoch 9617: train loss: 0.1783, test loss 0.8687\n",
      "Epoch 9618: train loss: 0.1783, test loss 0.8687\n",
      "Epoch 9619: train loss: 0.1783, test loss 0.8687\n",
      "Epoch 9620: train loss: 0.1783, test loss 0.8686\n",
      "Epoch 9621: train loss: 0.1783, test loss 0.8686\n",
      "Epoch 9622: train loss: 0.1783, test loss 0.8686\n",
      "Epoch 9623: train loss: 0.1783, test loss 0.8685\n",
      "Epoch 9624: train loss: 0.1783, test loss 0.8685\n",
      "Epoch 9625: train loss: 0.1783, test loss 0.8685\n",
      "Epoch 9626: train loss: 0.1783, test loss 0.8684\n",
      "Epoch 9627: train loss: 0.1783, test loss 0.8684\n",
      "Epoch 9628: train loss: 0.1783, test loss 0.8684\n",
      "Epoch 9629: train loss: 0.1783, test loss 0.8683\n",
      "Epoch 9630: train loss: 0.1783, test loss 0.8683\n",
      "Epoch 9631: train loss: 0.1783, test loss 0.8683\n",
      "Epoch 9632: train loss: 0.1783, test loss 0.8682\n",
      "Epoch 9633: train loss: 0.1783, test loss 0.8682\n",
      "Epoch 9634: train loss: 0.1783, test loss 0.8682\n",
      "Epoch 9635: train loss: 0.1783, test loss 0.8681\n",
      "Epoch 9636: train loss: 0.1783, test loss 0.8681\n",
      "Epoch 9637: train loss: 0.1783, test loss 0.8681\n",
      "Epoch 9638: train loss: 0.1783, test loss 0.8680\n",
      "Epoch 9639: train loss: 0.1783, test loss 0.8680\n",
      "Epoch 9640: train loss: 0.1783, test loss 0.8680\n",
      "Epoch 9641: train loss: 0.1783, test loss 0.8679\n",
      "Epoch 9642: train loss: 0.1783, test loss 0.8679\n",
      "Epoch 9643: train loss: 0.1783, test loss 0.8679\n",
      "Epoch 9644: train loss: 0.1783, test loss 0.8678\n",
      "Epoch 9645: train loss: 0.1783, test loss 0.8678\n",
      "Epoch 9646: train loss: 0.1783, test loss 0.8678\n",
      "Epoch 9647: train loss: 0.1783, test loss 0.8677\n",
      "Epoch 9648: train loss: 0.1783, test loss 0.8677\n",
      "Epoch 9649: train loss: 0.1783, test loss 0.8677\n",
      "Epoch 9650: train loss: 0.1783, test loss 0.8676\n",
      "Epoch 9651: train loss: 0.1783, test loss 0.8676\n",
      "Epoch 9652: train loss: 0.1783, test loss 0.8676\n",
      "Epoch 9653: train loss: 0.1783, test loss 0.8675\n",
      "Epoch 9654: train loss: 0.1783, test loss 0.8675\n",
      "Epoch 9655: train loss: 0.1783, test loss 0.8675\n",
      "Epoch 9656: train loss: 0.1783, test loss 0.8674\n",
      "Epoch 9657: train loss: 0.1783, test loss 0.8674\n",
      "Epoch 9658: train loss: 0.1783, test loss 0.8674\n",
      "Epoch 9659: train loss: 0.1783, test loss 0.8673\n",
      "Epoch 9660: train loss: 0.1783, test loss 0.8673\n",
      "Epoch 9661: train loss: 0.1783, test loss 0.8673\n",
      "Epoch 9662: train loss: 0.1783, test loss 0.8672\n",
      "Epoch 9663: train loss: 0.1783, test loss 0.8672\n",
      "Epoch 9664: train loss: 0.1783, test loss 0.8672\n",
      "Epoch 9665: train loss: 0.1783, test loss 0.8671\n",
      "Epoch 9666: train loss: 0.1783, test loss 0.8671\n",
      "Epoch 9667: train loss: 0.1783, test loss 0.8671\n",
      "Epoch 9668: train loss: 0.1783, test loss 0.8670\n",
      "Epoch 9669: train loss: 0.1783, test loss 0.8670\n",
      "Epoch 9670: train loss: 0.1783, test loss 0.8670\n",
      "Epoch 9671: train loss: 0.1783, test loss 0.8669\n",
      "Epoch 9672: train loss: 0.1783, test loss 0.8669\n",
      "Epoch 9673: train loss: 0.1783, test loss 0.8669\n",
      "Epoch 9674: train loss: 0.1783, test loss 0.8668\n",
      "Epoch 9675: train loss: 0.1783, test loss 0.8668\n",
      "Epoch 9676: train loss: 0.1783, test loss 0.8668\n",
      "Epoch 9677: train loss: 0.1783, test loss 0.8667\n",
      "Epoch 9678: train loss: 0.1783, test loss 0.8667\n",
      "Epoch 9679: train loss: 0.1783, test loss 0.8667\n",
      "Epoch 9680: train loss: 0.1783, test loss 0.8666\n",
      "Epoch 9681: train loss: 0.1783, test loss 0.8666\n",
      "Epoch 9682: train loss: 0.1783, test loss 0.8666\n",
      "Epoch 9683: train loss: 0.1783, test loss 0.8665\n",
      "Epoch 9684: train loss: 0.1783, test loss 0.8665\n",
      "Epoch 9685: train loss: 0.1783, test loss 0.8665\n",
      "Epoch 9686: train loss: 0.1783, test loss 0.8664\n",
      "Epoch 9687: train loss: 0.1783, test loss 0.8664\n",
      "Epoch 9688: train loss: 0.1783, test loss 0.8664\n",
      "Epoch 9689: train loss: 0.1783, test loss 0.8663\n",
      "Epoch 9690: train loss: 0.1783, test loss 0.8663\n",
      "Epoch 9691: train loss: 0.1783, test loss 0.8663\n",
      "Epoch 9692: train loss: 0.1783, test loss 0.8662\n",
      "Epoch 9693: train loss: 0.1783, test loss 0.8662\n",
      "Epoch 9694: train loss: 0.1783, test loss 0.8662\n",
      "Epoch 9695: train loss: 0.1783, test loss 0.8661\n",
      "Epoch 9696: train loss: 0.1782, test loss 0.8661\n",
      "Epoch 9697: train loss: 0.1782, test loss 0.8661\n",
      "Epoch 9698: train loss: 0.1782, test loss 0.8660\n",
      "Epoch 9699: train loss: 0.1782, test loss 0.8660\n",
      "Epoch 9700: train loss: 0.1782, test loss 0.8660\n",
      "Epoch 9701: train loss: 0.1782, test loss 0.8659\n",
      "Epoch 9702: train loss: 0.1782, test loss 0.8659\n",
      "Epoch 9703: train loss: 0.1782, test loss 0.8659\n",
      "Epoch 9704: train loss: 0.1782, test loss 0.8658\n",
      "Epoch 9705: train loss: 0.1782, test loss 0.8658\n",
      "Epoch 9706: train loss: 0.1782, test loss 0.8658\n",
      "Epoch 9707: train loss: 0.1782, test loss 0.8657\n",
      "Epoch 9708: train loss: 0.1782, test loss 0.8657\n",
      "Epoch 9709: train loss: 0.1782, test loss 0.8657\n",
      "Epoch 9710: train loss: 0.1782, test loss 0.8656\n",
      "Epoch 9711: train loss: 0.1782, test loss 0.8656\n",
      "Epoch 9712: train loss: 0.1782, test loss 0.8656\n",
      "Epoch 9713: train loss: 0.1782, test loss 0.8655\n",
      "Epoch 9714: train loss: 0.1782, test loss 0.8655\n",
      "Epoch 9715: train loss: 0.1782, test loss 0.8655\n",
      "Epoch 9716: train loss: 0.1782, test loss 0.8654\n",
      "Epoch 9717: train loss: 0.1782, test loss 0.8654\n",
      "Epoch 9718: train loss: 0.1782, test loss 0.8654\n",
      "Epoch 9719: train loss: 0.1782, test loss 0.8653\n",
      "Epoch 9720: train loss: 0.1782, test loss 0.8653\n",
      "Epoch 9721: train loss: 0.1782, test loss 0.8653\n",
      "Epoch 9722: train loss: 0.1782, test loss 0.8652\n",
      "Epoch 9723: train loss: 0.1782, test loss 0.8652\n",
      "Epoch 9724: train loss: 0.1782, test loss 0.8652\n",
      "Epoch 9725: train loss: 0.1782, test loss 0.8651\n",
      "Epoch 9726: train loss: 0.1782, test loss 0.8651\n",
      "Epoch 9727: train loss: 0.1782, test loss 0.8651\n",
      "Epoch 9728: train loss: 0.1782, test loss 0.8650\n",
      "Epoch 9729: train loss: 0.1782, test loss 0.8650\n",
      "Epoch 9730: train loss: 0.1782, test loss 0.8650\n",
      "Epoch 9731: train loss: 0.1782, test loss 0.8649\n",
      "Epoch 9732: train loss: 0.1782, test loss 0.8649\n",
      "Epoch 9733: train loss: 0.1782, test loss 0.8649\n",
      "Epoch 9734: train loss: 0.1782, test loss 0.8648\n",
      "Epoch 9735: train loss: 0.1782, test loss 0.8648\n",
      "Epoch 9736: train loss: 0.1782, test loss 0.8648\n",
      "Epoch 9737: train loss: 0.1782, test loss 0.8647\n",
      "Epoch 9738: train loss: 0.1782, test loss 0.8647\n",
      "Epoch 9739: train loss: 0.1782, test loss 0.8647\n",
      "Epoch 9740: train loss: 0.1782, test loss 0.8646\n",
      "Epoch 9741: train loss: 0.1782, test loss 0.8646\n",
      "Epoch 9742: train loss: 0.1782, test loss 0.8646\n",
      "Epoch 9743: train loss: 0.1782, test loss 0.8645\n",
      "Epoch 9744: train loss: 0.1782, test loss 0.8645\n",
      "Epoch 9745: train loss: 0.1782, test loss 0.8645\n",
      "Epoch 9746: train loss: 0.1782, test loss 0.8644\n",
      "Epoch 9747: train loss: 0.1782, test loss 0.8644\n",
      "Epoch 9748: train loss: 0.1782, test loss 0.8644\n",
      "Epoch 9749: train loss: 0.1782, test loss 0.8643\n",
      "Epoch 9750: train loss: 0.1782, test loss 0.8643\n",
      "Epoch 9751: train loss: 0.1782, test loss 0.8643\n",
      "Epoch 9752: train loss: 0.1782, test loss 0.8642\n",
      "Epoch 9753: train loss: 0.1782, test loss 0.8642\n",
      "Epoch 9754: train loss: 0.1782, test loss 0.8642\n",
      "Epoch 9755: train loss: 0.1782, test loss 0.8641\n",
      "Epoch 9756: train loss: 0.1782, test loss 0.8641\n",
      "Epoch 9757: train loss: 0.1782, test loss 0.8641\n",
      "Epoch 9758: train loss: 0.1782, test loss 0.8640\n",
      "Epoch 9759: train loss: 0.1782, test loss 0.8640\n",
      "Epoch 9760: train loss: 0.1782, test loss 0.8640\n",
      "Epoch 9761: train loss: 0.1782, test loss 0.8639\n",
      "Epoch 9762: train loss: 0.1782, test loss 0.8639\n",
      "Epoch 9763: train loss: 0.1782, test loss 0.8639\n",
      "Epoch 9764: train loss: 0.1782, test loss 0.8639\n",
      "Epoch 9765: train loss: 0.1782, test loss 0.8638\n",
      "Epoch 9766: train loss: 0.1782, test loss 0.8638\n",
      "Epoch 9767: train loss: 0.1782, test loss 0.8638\n",
      "Epoch 9768: train loss: 0.1782, test loss 0.8637\n",
      "Epoch 9769: train loss: 0.1782, test loss 0.8637\n",
      "Epoch 9770: train loss: 0.1782, test loss 0.8637\n",
      "Epoch 9771: train loss: 0.1782, test loss 0.8636\n",
      "Epoch 9772: train loss: 0.1782, test loss 0.8636\n",
      "Epoch 9773: train loss: 0.1782, test loss 0.8636\n",
      "Epoch 9774: train loss: 0.1782, test loss 0.8635\n",
      "Epoch 9775: train loss: 0.1782, test loss 0.8635\n",
      "Epoch 9776: train loss: 0.1782, test loss 0.8635\n",
      "Epoch 9777: train loss: 0.1782, test loss 0.8634\n",
      "Epoch 9778: train loss: 0.1782, test loss 0.8634\n",
      "Epoch 9779: train loss: 0.1781, test loss 0.8634\n",
      "Epoch 9780: train loss: 0.1781, test loss 0.8633\n",
      "Epoch 9781: train loss: 0.1781, test loss 0.8633\n",
      "Epoch 9782: train loss: 0.1781, test loss 0.8633\n",
      "Epoch 9783: train loss: 0.1781, test loss 0.8632\n",
      "Epoch 9784: train loss: 0.1781, test loss 0.8632\n",
      "Epoch 9785: train loss: 0.1781, test loss 0.8632\n",
      "Epoch 9786: train loss: 0.1781, test loss 0.8631\n",
      "Epoch 9787: train loss: 0.1781, test loss 0.8631\n",
      "Epoch 9788: train loss: 0.1781, test loss 0.8631\n",
      "Epoch 9789: train loss: 0.1781, test loss 0.8630\n",
      "Epoch 9790: train loss: 0.1781, test loss 0.8630\n",
      "Epoch 9791: train loss: 0.1781, test loss 0.8630\n",
      "Epoch 9792: train loss: 0.1781, test loss 0.8629\n",
      "Epoch 9793: train loss: 0.1781, test loss 0.8629\n",
      "Epoch 9794: train loss: 0.1781, test loss 0.8629\n",
      "Epoch 9795: train loss: 0.1781, test loss 0.8629\n",
      "Epoch 9796: train loss: 0.1781, test loss 0.8628\n",
      "Epoch 9797: train loss: 0.1781, test loss 0.8628\n",
      "Epoch 9798: train loss: 0.1781, test loss 0.8628\n",
      "Epoch 9799: train loss: 0.1781, test loss 0.8627\n",
      "Epoch 9800: train loss: 0.1781, test loss 0.8627\n",
      "Epoch 9801: train loss: 0.1781, test loss 0.8627\n",
      "Epoch 9802: train loss: 0.1781, test loss 0.8626\n",
      "Epoch 9803: train loss: 0.1781, test loss 0.8626\n",
      "Epoch 9804: train loss: 0.1781, test loss 0.8626\n",
      "Epoch 9805: train loss: 0.1781, test loss 0.8625\n",
      "Epoch 9806: train loss: 0.1781, test loss 0.8625\n",
      "Epoch 9807: train loss: 0.1781, test loss 0.8625\n",
      "Epoch 9808: train loss: 0.1781, test loss 0.8624\n",
      "Epoch 9809: train loss: 0.1781, test loss 0.8624\n",
      "Epoch 9810: train loss: 0.1781, test loss 0.8624\n",
      "Epoch 9811: train loss: 0.1781, test loss 0.8623\n",
      "Epoch 9812: train loss: 0.1781, test loss 0.8623\n",
      "Epoch 9813: train loss: 0.1781, test loss 0.8623\n",
      "Epoch 9814: train loss: 0.1781, test loss 0.8622\n",
      "Epoch 9815: train loss: 0.1781, test loss 0.8622\n",
      "Epoch 9816: train loss: 0.1781, test loss 0.8622\n",
      "Epoch 9817: train loss: 0.1781, test loss 0.8621\n",
      "Epoch 9818: train loss: 0.1781, test loss 0.8621\n",
      "Epoch 9819: train loss: 0.1781, test loss 0.8621\n",
      "Epoch 9820: train loss: 0.1781, test loss 0.8620\n",
      "Epoch 9821: train loss: 0.1781, test loss 0.8620\n",
      "Epoch 9822: train loss: 0.1781, test loss 0.8620\n",
      "Epoch 9823: train loss: 0.1781, test loss 0.8620\n",
      "Epoch 9824: train loss: 0.1781, test loss 0.8619\n",
      "Epoch 9825: train loss: 0.1781, test loss 0.8619\n",
      "Epoch 9826: train loss: 0.1781, test loss 0.8619\n",
      "Epoch 9827: train loss: 0.1781, test loss 0.8618\n",
      "Epoch 9828: train loss: 0.1781, test loss 0.8618\n",
      "Epoch 9829: train loss: 0.1781, test loss 0.8618\n",
      "Epoch 9830: train loss: 0.1781, test loss 0.8617\n",
      "Epoch 9831: train loss: 0.1781, test loss 0.8617\n",
      "Epoch 9832: train loss: 0.1781, test loss 0.8617\n",
      "Epoch 9833: train loss: 0.1781, test loss 0.8616\n",
      "Epoch 9834: train loss: 0.1781, test loss 0.8616\n",
      "Epoch 9835: train loss: 0.1781, test loss 0.8616\n",
      "Epoch 9836: train loss: 0.1781, test loss 0.8615\n",
      "Epoch 9837: train loss: 0.1781, test loss 0.8615\n",
      "Epoch 9838: train loss: 0.1781, test loss 0.8615\n",
      "Epoch 9839: train loss: 0.1781, test loss 0.8614\n",
      "Epoch 9840: train loss: 0.1781, test loss 0.8614\n",
      "Epoch 9841: train loss: 0.1781, test loss 0.8614\n",
      "Epoch 9842: train loss: 0.1781, test loss 0.8613\n",
      "Epoch 9843: train loss: 0.1781, test loss 0.8613\n",
      "Epoch 9844: train loss: 0.1781, test loss 0.8613\n",
      "Epoch 9845: train loss: 0.1781, test loss 0.8612\n",
      "Epoch 9846: train loss: 0.1781, test loss 0.8612\n",
      "Epoch 9847: train loss: 0.1781, test loss 0.8612\n",
      "Epoch 9848: train loss: 0.1781, test loss 0.8612\n",
      "Epoch 9849: train loss: 0.1781, test loss 0.8611\n",
      "Epoch 9850: train loss: 0.1781, test loss 0.8611\n",
      "Epoch 9851: train loss: 0.1781, test loss 0.8611\n",
      "Epoch 9852: train loss: 0.1781, test loss 0.8610\n",
      "Epoch 9853: train loss: 0.1781, test loss 0.8610\n",
      "Epoch 9854: train loss: 0.1781, test loss 0.8610\n",
      "Epoch 9855: train loss: 0.1781, test loss 0.8609\n",
      "Epoch 9856: train loss: 0.1781, test loss 0.8609\n",
      "Epoch 9857: train loss: 0.1781, test loss 0.8609\n",
      "Epoch 9858: train loss: 0.1781, test loss 0.8608\n",
      "Epoch 9859: train loss: 0.1781, test loss 0.8608\n",
      "Epoch 9860: train loss: 0.1781, test loss 0.8608\n",
      "Epoch 9861: train loss: 0.1781, test loss 0.8607\n",
      "Epoch 9862: train loss: 0.1780, test loss 0.8607\n",
      "Epoch 9863: train loss: 0.1780, test loss 0.8607\n",
      "Epoch 9864: train loss: 0.1780, test loss 0.8606\n",
      "Epoch 9865: train loss: 0.1780, test loss 0.8606\n",
      "Epoch 9866: train loss: 0.1780, test loss 0.8606\n",
      "Epoch 9867: train loss: 0.1780, test loss 0.8605\n",
      "Epoch 9868: train loss: 0.1780, test loss 0.8605\n",
      "Epoch 9869: train loss: 0.1780, test loss 0.8605\n",
      "Epoch 9870: train loss: 0.1780, test loss 0.8605\n",
      "Epoch 9871: train loss: 0.1780, test loss 0.8604\n",
      "Epoch 9872: train loss: 0.1780, test loss 0.8604\n",
      "Epoch 9873: train loss: 0.1780, test loss 0.8604\n",
      "Epoch 9874: train loss: 0.1780, test loss 0.8603\n",
      "Epoch 9875: train loss: 0.1780, test loss 0.8603\n",
      "Epoch 9876: train loss: 0.1780, test loss 0.8603\n",
      "Epoch 9877: train loss: 0.1780, test loss 0.8602\n",
      "Epoch 9878: train loss: 0.1780, test loss 0.8602\n",
      "Epoch 9879: train loss: 0.1780, test loss 0.8602\n",
      "Epoch 9880: train loss: 0.1780, test loss 0.8601\n",
      "Epoch 9881: train loss: 0.1780, test loss 0.8601\n",
      "Epoch 9882: train loss: 0.1780, test loss 0.8601\n",
      "Epoch 9883: train loss: 0.1780, test loss 0.8600\n",
      "Epoch 9884: train loss: 0.1780, test loss 0.8600\n",
      "Epoch 9885: train loss: 0.1780, test loss 0.8600\n",
      "Epoch 9886: train loss: 0.1780, test loss 0.8599\n",
      "Epoch 9887: train loss: 0.1780, test loss 0.8599\n",
      "Epoch 9888: train loss: 0.1780, test loss 0.8599\n",
      "Epoch 9889: train loss: 0.1780, test loss 0.8598\n",
      "Epoch 9890: train loss: 0.1780, test loss 0.8598\n",
      "Epoch 9891: train loss: 0.1780, test loss 0.8598\n",
      "Epoch 9892: train loss: 0.1780, test loss 0.8598\n",
      "Epoch 9893: train loss: 0.1780, test loss 0.8597\n",
      "Epoch 9894: train loss: 0.1780, test loss 0.8597\n",
      "Epoch 9895: train loss: 0.1780, test loss 0.8597\n",
      "Epoch 9896: train loss: 0.1780, test loss 0.8596\n",
      "Epoch 9897: train loss: 0.1780, test loss 0.8596\n",
      "Epoch 9898: train loss: 0.1780, test loss 0.8596\n",
      "Epoch 9899: train loss: 0.1780, test loss 0.8595\n",
      "Epoch 9900: train loss: 0.1780, test loss 0.8595\n",
      "Epoch 9901: train loss: 0.1780, test loss 0.8595\n",
      "Epoch 9902: train loss: 0.1780, test loss 0.8594\n",
      "Epoch 9903: train loss: 0.1780, test loss 0.8594\n",
      "Epoch 9904: train loss: 0.1780, test loss 0.8594\n",
      "Epoch 9905: train loss: 0.1780, test loss 0.8593\n",
      "Epoch 9906: train loss: 0.1780, test loss 0.8593\n",
      "Epoch 9907: train loss: 0.1780, test loss 0.8593\n",
      "Epoch 9908: train loss: 0.1780, test loss 0.8592\n",
      "Epoch 9909: train loss: 0.1780, test loss 0.8592\n",
      "Epoch 9910: train loss: 0.1780, test loss 0.8592\n",
      "Epoch 9911: train loss: 0.1780, test loss 0.8591\n",
      "Epoch 9912: train loss: 0.1780, test loss 0.8591\n",
      "Epoch 9913: train loss: 0.1780, test loss 0.8591\n",
      "Epoch 9914: train loss: 0.1780, test loss 0.8590\n",
      "Epoch 9915: train loss: 0.1780, test loss 0.8590\n",
      "Epoch 9916: train loss: 0.1780, test loss 0.8590\n",
      "Epoch 9917: train loss: 0.1780, test loss 0.8589\n",
      "Epoch 9918: train loss: 0.1780, test loss 0.8589\n",
      "Epoch 9919: train loss: 0.1780, test loss 0.8589\n",
      "Epoch 9920: train loss: 0.1780, test loss 0.8588\n",
      "Epoch 9921: train loss: 0.1780, test loss 0.8588\n",
      "Epoch 9922: train loss: 0.1780, test loss 0.8588\n",
      "Epoch 9923: train loss: 0.1780, test loss 0.8588\n",
      "Epoch 9924: train loss: 0.1780, test loss 0.8587\n",
      "Epoch 9925: train loss: 0.1780, test loss 0.8587\n",
      "Epoch 9926: train loss: 0.1780, test loss 0.8587\n",
      "Epoch 9927: train loss: 0.1780, test loss 0.8586\n",
      "Epoch 9928: train loss: 0.1780, test loss 0.8586\n",
      "Epoch 9929: train loss: 0.1780, test loss 0.8586\n",
      "Epoch 9930: train loss: 0.1780, test loss 0.8585\n",
      "Epoch 9931: train loss: 0.1780, test loss 0.8585\n",
      "Epoch 9932: train loss: 0.1780, test loss 0.8585\n",
      "Epoch 9933: train loss: 0.1780, test loss 0.8584\n",
      "Epoch 9934: train loss: 0.1780, test loss 0.8584\n",
      "Epoch 9935: train loss: 0.1780, test loss 0.8584\n",
      "Epoch 9936: train loss: 0.1780, test loss 0.8583\n",
      "Epoch 9937: train loss: 0.1780, test loss 0.8583\n",
      "Epoch 9938: train loss: 0.1780, test loss 0.8583\n",
      "Epoch 9939: train loss: 0.1780, test loss 0.8582\n",
      "Epoch 9940: train loss: 0.1780, test loss 0.8582\n",
      "Epoch 9941: train loss: 0.1780, test loss 0.8582\n",
      "Epoch 9942: train loss: 0.1780, test loss 0.8581\n",
      "Epoch 9943: train loss: 0.1780, test loss 0.8581\n",
      "Epoch 9944: train loss: 0.1780, test loss 0.8581\n",
      "Epoch 9945: train loss: 0.1780, test loss 0.8580\n",
      "Epoch 9946: train loss: 0.1779, test loss 0.8580\n",
      "Epoch 9947: train loss: 0.1779, test loss 0.8580\n",
      "Epoch 9948: train loss: 0.1779, test loss 0.8580\n",
      "Epoch 9949: train loss: 0.1779, test loss 0.8579\n",
      "Epoch 9950: train loss: 0.1779, test loss 0.8579\n",
      "Epoch 9951: train loss: 0.1779, test loss 0.8579\n",
      "Epoch 9952: train loss: 0.1779, test loss 0.8578\n",
      "Epoch 9953: train loss: 0.1779, test loss 0.8578\n",
      "Epoch 9954: train loss: 0.1779, test loss 0.8578\n",
      "Epoch 9955: train loss: 0.1779, test loss 0.8577\n",
      "Epoch 9956: train loss: 0.1779, test loss 0.8577\n",
      "Epoch 9957: train loss: 0.1779, test loss 0.8577\n",
      "Epoch 9958: train loss: 0.1779, test loss 0.8576\n",
      "Epoch 9959: train loss: 0.1779, test loss 0.8576\n",
      "Epoch 9960: train loss: 0.1779, test loss 0.8576\n",
      "Epoch 9961: train loss: 0.1779, test loss 0.8575\n",
      "Epoch 9962: train loss: 0.1779, test loss 0.8575\n",
      "Epoch 9963: train loss: 0.1779, test loss 0.8575\n",
      "Epoch 9964: train loss: 0.1779, test loss 0.8574\n",
      "Epoch 9965: train loss: 0.1779, test loss 0.8574\n",
      "Epoch 9966: train loss: 0.1779, test loss 0.8574\n",
      "Epoch 9967: train loss: 0.1779, test loss 0.8573\n",
      "Epoch 9968: train loss: 0.1779, test loss 0.8573\n",
      "Epoch 9969: train loss: 0.1779, test loss 0.8573\n",
      "Epoch 9970: train loss: 0.1779, test loss 0.8572\n",
      "Epoch 9971: train loss: 0.1779, test loss 0.8572\n",
      "Epoch 9972: train loss: 0.1779, test loss 0.8572\n",
      "Epoch 9973: train loss: 0.1779, test loss 0.8571\n",
      "Epoch 9974: train loss: 0.1779, test loss 0.8571\n",
      "Epoch 9975: train loss: 0.1779, test loss 0.8571\n",
      "Epoch 9976: train loss: 0.1779, test loss 0.8571\n",
      "Epoch 9977: train loss: 0.1779, test loss 0.8570\n",
      "Epoch 9978: train loss: 0.1779, test loss 0.8570\n",
      "Epoch 9979: train loss: 0.1779, test loss 0.8570\n",
      "Epoch 9980: train loss: 0.1779, test loss 0.8569\n",
      "Epoch 9981: train loss: 0.1779, test loss 0.8569\n",
      "Epoch 9982: train loss: 0.1779, test loss 0.8569\n",
      "Epoch 9983: train loss: 0.1779, test loss 0.8568\n",
      "Epoch 9984: train loss: 0.1779, test loss 0.8568\n",
      "Epoch 9985: train loss: 0.1779, test loss 0.8568\n",
      "Epoch 9986: train loss: 0.1779, test loss 0.8567\n",
      "Epoch 9987: train loss: 0.1779, test loss 0.8567\n",
      "Epoch 9988: train loss: 0.1779, test loss 0.8567\n",
      "Epoch 9989: train loss: 0.1779, test loss 0.8566\n",
      "Epoch 9990: train loss: 0.1779, test loss 0.8566\n",
      "Epoch 9991: train loss: 0.1779, test loss 0.8566\n",
      "Epoch 9992: train loss: 0.1779, test loss 0.8565\n",
      "Epoch 9993: train loss: 0.1779, test loss 0.8565\n",
      "Epoch 9994: train loss: 0.1779, test loss 0.8565\n",
      "Epoch 9995: train loss: 0.1779, test loss 0.8564\n",
      "Epoch 9996: train loss: 0.1779, test loss 0.8564\n",
      "Epoch 9997: train loss: 0.1779, test loss 0.8564\n",
      "Epoch 9998: train loss: 0.1779, test loss 0.8564\n",
      "Epoch 9999: train loss: 0.1779, test loss 0.8563\n",
      "Epoch 10000: train loss: 0.1779, test loss 0.8563\n",
      "Epoch 10001: train loss: 0.1779, test loss 0.8563\n",
      "Epoch 10002: train loss: 0.1779, test loss 0.8562\n",
      "Epoch 10003: train loss: 0.1779, test loss 0.8562\n",
      "Epoch 10004: train loss: 0.1779, test loss 0.8562\n",
      "Epoch 10005: train loss: 0.1779, test loss 0.8561\n",
      "Epoch 10006: train loss: 0.1779, test loss 0.8561\n",
      "Epoch 10007: train loss: 0.1779, test loss 0.8561\n",
      "Epoch 10008: train loss: 0.1779, test loss 0.8560\n",
      "Epoch 10009: train loss: 0.1779, test loss 0.8560\n",
      "Epoch 10010: train loss: 0.1779, test loss 0.8560\n",
      "Epoch 10011: train loss: 0.1779, test loss 0.8559\n",
      "Epoch 10012: train loss: 0.1779, test loss 0.8559\n",
      "Epoch 10013: train loss: 0.1779, test loss 0.8559\n",
      "Epoch 10014: train loss: 0.1779, test loss 0.8559\n",
      "Epoch 10015: train loss: 0.1779, test loss 0.8558\n",
      "Epoch 10016: train loss: 0.1779, test loss 0.8558\n",
      "Epoch 10017: train loss: 0.1779, test loss 0.8558\n",
      "Epoch 10018: train loss: 0.1779, test loss 0.8557\n",
      "Epoch 10019: train loss: 0.1779, test loss 0.8557\n",
      "Epoch 10020: train loss: 0.1779, test loss 0.8557\n",
      "Epoch 10021: train loss: 0.1779, test loss 0.8556\n",
      "Epoch 10022: train loss: 0.1779, test loss 0.8556\n",
      "Epoch 10023: train loss: 0.1779, test loss 0.8556\n",
      "Epoch 10024: train loss: 0.1779, test loss 0.8556\n",
      "Epoch 10025: train loss: 0.1779, test loss 0.8555\n",
      "Epoch 10026: train loss: 0.1779, test loss 0.8555\n",
      "Epoch 10027: train loss: 0.1779, test loss 0.8555\n",
      "Epoch 10028: train loss: 0.1779, test loss 0.8554\n",
      "Epoch 10029: train loss: 0.1779, test loss 0.8554\n",
      "Epoch 10030: train loss: 0.1779, test loss 0.8554\n",
      "Epoch 10031: train loss: 0.1779, test loss 0.8554\n",
      "Epoch 10032: train loss: 0.1778, test loss 0.8553\n",
      "Epoch 10033: train loss: 0.1778, test loss 0.8553\n",
      "Epoch 10034: train loss: 0.1778, test loss 0.8553\n",
      "Epoch 10035: train loss: 0.1778, test loss 0.8552\n",
      "Epoch 10036: train loss: 0.1778, test loss 0.8552\n",
      "Epoch 10037: train loss: 0.1778, test loss 0.8552\n",
      "Epoch 10038: train loss: 0.1778, test loss 0.8552\n",
      "Epoch 10039: train loss: 0.1778, test loss 0.8551\n",
      "Epoch 10040: train loss: 0.1778, test loss 0.8551\n",
      "Epoch 10041: train loss: 0.1778, test loss 0.8551\n",
      "Epoch 10042: train loss: 0.1778, test loss 0.8550\n",
      "Epoch 10043: train loss: 0.1778, test loss 0.8550\n",
      "Epoch 10044: train loss: 0.1778, test loss 0.8550\n",
      "Epoch 10045: train loss: 0.1778, test loss 0.8550\n",
      "Epoch 10046: train loss: 0.1778, test loss 0.8549\n",
      "Epoch 10047: train loss: 0.1778, test loss 0.8549\n",
      "Epoch 10048: train loss: 0.1778, test loss 0.8549\n",
      "Epoch 10049: train loss: 0.1778, test loss 0.8548\n",
      "Epoch 10050: train loss: 0.1778, test loss 0.8548\n",
      "Epoch 10051: train loss: 0.1778, test loss 0.8548\n",
      "Epoch 10052: train loss: 0.1778, test loss 0.8548\n",
      "Epoch 10053: train loss: 0.1778, test loss 0.8547\n",
      "Epoch 10054: train loss: 0.1778, test loss 0.8547\n",
      "Epoch 10055: train loss: 0.1778, test loss 0.8547\n",
      "Epoch 10056: train loss: 0.1778, test loss 0.8546\n",
      "Epoch 10057: train loss: 0.1778, test loss 0.8546\n",
      "Epoch 10058: train loss: 0.1778, test loss 0.8546\n",
      "Epoch 10059: train loss: 0.1778, test loss 0.8546\n",
      "Epoch 10060: train loss: 0.1778, test loss 0.8545\n",
      "Epoch 10061: train loss: 0.1778, test loss 0.8545\n",
      "Epoch 10062: train loss: 0.1778, test loss 0.8545\n",
      "Epoch 10063: train loss: 0.1778, test loss 0.8544\n",
      "Epoch 10064: train loss: 0.1778, test loss 0.8544\n",
      "Epoch 10065: train loss: 0.1778, test loss 0.8544\n",
      "Epoch 10066: train loss: 0.1778, test loss 0.8544\n",
      "Epoch 10067: train loss: 0.1778, test loss 0.8543\n",
      "Epoch 10068: train loss: 0.1778, test loss 0.8543\n",
      "Epoch 10069: train loss: 0.1778, test loss 0.8543\n",
      "Epoch 10070: train loss: 0.1778, test loss 0.8542\n",
      "Epoch 10071: train loss: 0.1778, test loss 0.8542\n",
      "Epoch 10072: train loss: 0.1778, test loss 0.8542\n",
      "Epoch 10073: train loss: 0.1778, test loss 0.8542\n",
      "Epoch 10074: train loss: 0.1778, test loss 0.8541\n",
      "Epoch 10075: train loss: 0.1778, test loss 0.8541\n",
      "Epoch 10076: train loss: 0.1778, test loss 0.8541\n",
      "Epoch 10077: train loss: 0.1778, test loss 0.8540\n",
      "Epoch 10078: train loss: 0.1778, test loss 0.8540\n",
      "Epoch 10079: train loss: 0.1778, test loss 0.8540\n",
      "Epoch 10080: train loss: 0.1778, test loss 0.8540\n",
      "Epoch 10081: train loss: 0.1778, test loss 0.8539\n",
      "Epoch 10082: train loss: 0.1778, test loss 0.8539\n",
      "Epoch 10083: train loss: 0.1778, test loss 0.8539\n",
      "Epoch 10084: train loss: 0.1778, test loss 0.8538\n",
      "Epoch 10085: train loss: 0.1778, test loss 0.8538\n",
      "Epoch 10086: train loss: 0.1778, test loss 0.8538\n",
      "Epoch 10087: train loss: 0.1778, test loss 0.8538\n",
      "Epoch 10088: train loss: 0.1778, test loss 0.8537\n",
      "Epoch 10089: train loss: 0.1778, test loss 0.8537\n",
      "Epoch 10090: train loss: 0.1778, test loss 0.8537\n",
      "Epoch 10091: train loss: 0.1778, test loss 0.8536\n",
      "Epoch 10092: train loss: 0.1778, test loss 0.8536\n",
      "Epoch 10093: train loss: 0.1778, test loss 0.8536\n",
      "Epoch 10094: train loss: 0.1778, test loss 0.8536\n",
      "Epoch 10095: train loss: 0.1778, test loss 0.8535\n",
      "Epoch 10096: train loss: 0.1778, test loss 0.8535\n",
      "Epoch 10097: train loss: 0.1778, test loss 0.8535\n",
      "Epoch 10098: train loss: 0.1778, test loss 0.8534\n",
      "Epoch 10099: train loss: 0.1778, test loss 0.8534\n",
      "Epoch 10100: train loss: 0.1778, test loss 0.8534\n",
      "Epoch 10101: train loss: 0.1778, test loss 0.8534\n",
      "Epoch 10102: train loss: 0.1778, test loss 0.8533\n",
      "Epoch 10103: train loss: 0.1778, test loss 0.8533\n",
      "Epoch 10104: train loss: 0.1778, test loss 0.8533\n",
      "Epoch 10105: train loss: 0.1778, test loss 0.8532\n",
      "Epoch 10106: train loss: 0.1778, test loss 0.8532\n",
      "Epoch 10107: train loss: 0.1778, test loss 0.8532\n",
      "Epoch 10108: train loss: 0.1778, test loss 0.8532\n",
      "Epoch 10109: train loss: 0.1778, test loss 0.8531\n",
      "Epoch 10110: train loss: 0.1778, test loss 0.8531\n",
      "Epoch 10111: train loss: 0.1778, test loss 0.8531\n",
      "Epoch 10112: train loss: 0.1778, test loss 0.8530\n",
      "Epoch 10113: train loss: 0.1778, test loss 0.8530\n",
      "Epoch 10114: train loss: 0.1778, test loss 0.8530\n",
      "Epoch 10115: train loss: 0.1778, test loss 0.8530\n",
      "Epoch 10116: train loss: 0.1778, test loss 0.8529\n",
      "Epoch 10117: train loss: 0.1778, test loss 0.8529\n",
      "Epoch 10118: train loss: 0.1778, test loss 0.8529\n",
      "Epoch 10119: train loss: 0.1778, test loss 0.8528\n",
      "Epoch 10120: train loss: 0.1778, test loss 0.8528\n",
      "Epoch 10121: train loss: 0.1778, test loss 0.8528\n",
      "Epoch 10122: train loss: 0.1777, test loss 0.8528\n",
      "Epoch 10123: train loss: 0.1777, test loss 0.8527\n",
      "Epoch 10124: train loss: 0.1777, test loss 0.8527\n",
      "Epoch 10125: train loss: 0.1777, test loss 0.8527\n",
      "Epoch 10126: train loss: 0.1777, test loss 0.8526\n",
      "Epoch 10127: train loss: 0.1777, test loss 0.8526\n",
      "Epoch 10128: train loss: 0.1777, test loss 0.8526\n",
      "Epoch 10129: train loss: 0.1777, test loss 0.8526\n",
      "Epoch 10130: train loss: 0.1777, test loss 0.8525\n",
      "Epoch 10131: train loss: 0.1777, test loss 0.8525\n",
      "Epoch 10132: train loss: 0.1777, test loss 0.8525\n",
      "Epoch 10133: train loss: 0.1777, test loss 0.8524\n",
      "Epoch 10134: train loss: 0.1777, test loss 0.8524\n",
      "Epoch 10135: train loss: 0.1777, test loss 0.8524\n",
      "Epoch 10136: train loss: 0.1777, test loss 0.8524\n",
      "Epoch 10137: train loss: 0.1777, test loss 0.8523\n",
      "Epoch 10138: train loss: 0.1777, test loss 0.8523\n",
      "Epoch 10139: train loss: 0.1777, test loss 0.8523\n",
      "Epoch 10140: train loss: 0.1777, test loss 0.8522\n",
      "Epoch 10141: train loss: 0.1777, test loss 0.8522\n",
      "Epoch 10142: train loss: 0.1777, test loss 0.8522\n",
      "Epoch 10143: train loss: 0.1777, test loss 0.8522\n",
      "Epoch 10144: train loss: 0.1777, test loss 0.8521\n",
      "Epoch 10145: train loss: 0.1777, test loss 0.8521\n",
      "Epoch 10146: train loss: 0.1777, test loss 0.8521\n",
      "Epoch 10147: train loss: 0.1777, test loss 0.8520\n",
      "Epoch 10148: train loss: 0.1777, test loss 0.8520\n",
      "Epoch 10149: train loss: 0.1777, test loss 0.8520\n",
      "Epoch 10150: train loss: 0.1777, test loss 0.8520\n",
      "Epoch 10151: train loss: 0.1777, test loss 0.8519\n",
      "Epoch 10152: train loss: 0.1777, test loss 0.8519\n",
      "Epoch 10153: train loss: 0.1777, test loss 0.8519\n",
      "Epoch 10154: train loss: 0.1777, test loss 0.8519\n",
      "Epoch 10155: train loss: 0.1777, test loss 0.8518\n",
      "Epoch 10156: train loss: 0.1777, test loss 0.8518\n",
      "Epoch 10157: train loss: 0.1777, test loss 0.8518\n",
      "Epoch 10158: train loss: 0.1777, test loss 0.8517\n",
      "Epoch 10159: train loss: 0.1777, test loss 0.8517\n",
      "Epoch 10160: train loss: 0.1777, test loss 0.8517\n",
      "Epoch 10161: train loss: 0.1777, test loss 0.8517\n",
      "Epoch 10162: train loss: 0.1777, test loss 0.8516\n",
      "Epoch 10163: train loss: 0.1777, test loss 0.8516\n",
      "Epoch 10164: train loss: 0.1777, test loss 0.8516\n",
      "Epoch 10165: train loss: 0.1777, test loss 0.8515\n",
      "Epoch 10166: train loss: 0.1777, test loss 0.8515\n",
      "Epoch 10167: train loss: 0.1777, test loss 0.8515\n",
      "Epoch 10168: train loss: 0.1777, test loss 0.8515\n",
      "Epoch 10169: train loss: 0.1777, test loss 0.8514\n",
      "Epoch 10170: train loss: 0.1777, test loss 0.8514\n",
      "Epoch 10171: train loss: 0.1777, test loss 0.8514\n",
      "Epoch 10172: train loss: 0.1777, test loss 0.8514\n",
      "Epoch 10173: train loss: 0.1777, test loss 0.8513\n",
      "Epoch 10174: train loss: 0.1777, test loss 0.8513\n",
      "Epoch 10175: train loss: 0.1777, test loss 0.8513\n",
      "Epoch 10176: train loss: 0.1777, test loss 0.8512\n",
      "Epoch 10177: train loss: 0.1777, test loss 0.8512\n",
      "Epoch 10178: train loss: 0.1777, test loss 0.8512\n",
      "Epoch 10179: train loss: 0.1777, test loss 0.8512\n",
      "Epoch 10180: train loss: 0.1777, test loss 0.8511\n",
      "Epoch 10181: train loss: 0.1777, test loss 0.8511\n",
      "Epoch 10182: train loss: 0.1777, test loss 0.8511\n",
      "Epoch 10183: train loss: 0.1777, test loss 0.8510\n",
      "Epoch 10184: train loss: 0.1777, test loss 0.8510\n",
      "Epoch 10185: train loss: 0.1777, test loss 0.8510\n",
      "Epoch 10186: train loss: 0.1777, test loss 0.8510\n",
      "Epoch 10187: train loss: 0.1777, test loss 0.8509\n",
      "Epoch 10188: train loss: 0.1777, test loss 0.8509\n",
      "Epoch 10189: train loss: 0.1777, test loss 0.8509\n",
      "Epoch 10190: train loss: 0.1777, test loss 0.8508\n",
      "Epoch 10191: train loss: 0.1777, test loss 0.8508\n",
      "Epoch 10192: train loss: 0.1777, test loss 0.8508\n",
      "Epoch 10193: train loss: 0.1777, test loss 0.8508\n",
      "Epoch 10194: train loss: 0.1777, test loss 0.8507\n",
      "Epoch 10195: train loss: 0.1777, test loss 0.8507\n",
      "Epoch 10196: train loss: 0.1777, test loss 0.8507\n",
      "Epoch 10197: train loss: 0.1777, test loss 0.8507\n",
      "Epoch 10198: train loss: 0.1777, test loss 0.8506\n",
      "Epoch 10199: train loss: 0.1777, test loss 0.8506\n",
      "Epoch 10200: train loss: 0.1777, test loss 0.8506\n",
      "Epoch 10201: train loss: 0.1777, test loss 0.8505\n",
      "Epoch 10202: train loss: 0.1777, test loss 0.8505\n",
      "Epoch 10203: train loss: 0.1777, test loss 0.8505\n",
      "Epoch 10204: train loss: 0.1777, test loss 0.8505\n",
      "Epoch 10205: train loss: 0.1777, test loss 0.8504\n",
      "Epoch 10206: train loss: 0.1777, test loss 0.8504\n",
      "Epoch 10207: train loss: 0.1777, test loss 0.8504\n",
      "Epoch 10208: train loss: 0.1777, test loss 0.8504\n",
      "Epoch 10209: train loss: 0.1777, test loss 0.8503\n",
      "Epoch 10210: train loss: 0.1777, test loss 0.8503\n",
      "Epoch 10211: train loss: 0.1777, test loss 0.8503\n",
      "Epoch 10212: train loss: 0.1777, test loss 0.8502\n",
      "Epoch 10213: train loss: 0.1777, test loss 0.8502\n",
      "Epoch 10214: train loss: 0.1776, test loss 0.8502\n",
      "Epoch 10215: train loss: 0.1776, test loss 0.8502\n",
      "Epoch 10216: train loss: 0.1776, test loss 0.8501\n",
      "Epoch 10217: train loss: 0.1776, test loss 0.8501\n",
      "Epoch 10218: train loss: 0.1776, test loss 0.8501\n",
      "Epoch 10219: train loss: 0.1776, test loss 0.8501\n",
      "Epoch 10220: train loss: 0.1776, test loss 0.8500\n",
      "Epoch 10221: train loss: 0.1776, test loss 0.8500\n",
      "Epoch 10222: train loss: 0.1776, test loss 0.8500\n",
      "Epoch 10223: train loss: 0.1776, test loss 0.8499\n",
      "Epoch 10224: train loss: 0.1776, test loss 0.8499\n",
      "Epoch 10225: train loss: 0.1776, test loss 0.8499\n",
      "Epoch 10226: train loss: 0.1776, test loss 0.8499\n",
      "Epoch 10227: train loss: 0.1776, test loss 0.8498\n",
      "Epoch 10228: train loss: 0.1776, test loss 0.8498\n",
      "Epoch 10229: train loss: 0.1776, test loss 0.8498\n",
      "Epoch 10230: train loss: 0.1776, test loss 0.8498\n",
      "Epoch 10231: train loss: 0.1776, test loss 0.8497\n",
      "Epoch 10232: train loss: 0.1776, test loss 0.8497\n",
      "Epoch 10233: train loss: 0.1776, test loss 0.8497\n",
      "Epoch 10234: train loss: 0.1776, test loss 0.8496\n",
      "Epoch 10235: train loss: 0.1776, test loss 0.8496\n",
      "Epoch 10236: train loss: 0.1776, test loss 0.8496\n",
      "Epoch 10237: train loss: 0.1776, test loss 0.8496\n",
      "Epoch 10238: train loss: 0.1776, test loss 0.8495\n",
      "Epoch 10239: train loss: 0.1776, test loss 0.8495\n",
      "Epoch 10240: train loss: 0.1776, test loss 0.8495\n",
      "Epoch 10241: train loss: 0.1776, test loss 0.8495\n",
      "Epoch 10242: train loss: 0.1776, test loss 0.8494\n",
      "Epoch 10243: train loss: 0.1776, test loss 0.8494\n",
      "Epoch 10244: train loss: 0.1776, test loss 0.8494\n",
      "Epoch 10245: train loss: 0.1776, test loss 0.8493\n",
      "Epoch 10246: train loss: 0.1776, test loss 0.8493\n",
      "Epoch 10247: train loss: 0.1776, test loss 0.8493\n",
      "Epoch 10248: train loss: 0.1776, test loss 0.8493\n",
      "Epoch 10249: train loss: 0.1776, test loss 0.8492\n",
      "Epoch 10250: train loss: 0.1776, test loss 0.8492\n",
      "Epoch 10251: train loss: 0.1776, test loss 0.8492\n",
      "Epoch 10252: train loss: 0.1776, test loss 0.8492\n",
      "Epoch 10253: train loss: 0.1776, test loss 0.8491\n",
      "Epoch 10254: train loss: 0.1776, test loss 0.8491\n",
      "Epoch 10255: train loss: 0.1776, test loss 0.8491\n",
      "Epoch 10256: train loss: 0.1776, test loss 0.8490\n",
      "Epoch 10257: train loss: 0.1776, test loss 0.8490\n",
      "Epoch 10258: train loss: 0.1776, test loss 0.8490\n",
      "Epoch 10259: train loss: 0.1776, test loss 0.8490\n",
      "Epoch 10260: train loss: 0.1776, test loss 0.8489\n",
      "Epoch 10261: train loss: 0.1776, test loss 0.8489\n",
      "Epoch 10262: train loss: 0.1776, test loss 0.8489\n",
      "Epoch 10263: train loss: 0.1776, test loss 0.8489\n",
      "Epoch 10264: train loss: 0.1776, test loss 0.8488\n",
      "Epoch 10265: train loss: 0.1776, test loss 0.8488\n",
      "Epoch 10266: train loss: 0.1776, test loss 0.8488\n",
      "Epoch 10267: train loss: 0.1776, test loss 0.8488\n",
      "Epoch 10268: train loss: 0.1776, test loss 0.8487\n",
      "Epoch 10269: train loss: 0.1776, test loss 0.8487\n",
      "Epoch 10270: train loss: 0.1776, test loss 0.8487\n",
      "Epoch 10271: train loss: 0.1776, test loss 0.8486\n",
      "Epoch 10272: train loss: 0.1776, test loss 0.8486\n",
      "Epoch 10273: train loss: 0.1776, test loss 0.8486\n",
      "Epoch 10274: train loss: 0.1776, test loss 0.8486\n",
      "Epoch 10275: train loss: 0.1776, test loss 0.8485\n",
      "Epoch 10276: train loss: 0.1776, test loss 0.8485\n",
      "Epoch 10277: train loss: 0.1776, test loss 0.8485\n",
      "Epoch 10278: train loss: 0.1776, test loss 0.8485\n",
      "Epoch 10279: train loss: 0.1776, test loss 0.8484\n",
      "Epoch 10280: train loss: 0.1776, test loss 0.8484\n",
      "Epoch 10281: train loss: 0.1776, test loss 0.8484\n",
      "Epoch 10282: train loss: 0.1776, test loss 0.8484\n",
      "Epoch 10283: train loss: 0.1776, test loss 0.8483\n",
      "Epoch 10284: train loss: 0.1776, test loss 0.8483\n",
      "Epoch 10285: train loss: 0.1776, test loss 0.8483\n",
      "Epoch 10286: train loss: 0.1776, test loss 0.8482\n",
      "Epoch 10287: train loss: 0.1776, test loss 0.8482\n",
      "Epoch 10288: train loss: 0.1776, test loss 0.8482\n",
      "Epoch 10289: train loss: 0.1776, test loss 0.8482\n",
      "Epoch 10290: train loss: 0.1776, test loss 0.8481\n",
      "Epoch 10291: train loss: 0.1776, test loss 0.8481\n",
      "Epoch 10292: train loss: 0.1776, test loss 0.8481\n",
      "Epoch 10293: train loss: 0.1776, test loss 0.8481\n",
      "Epoch 10294: train loss: 0.1776, test loss 0.8480\n",
      "Epoch 10295: train loss: 0.1776, test loss 0.8480\n",
      "Epoch 10296: train loss: 0.1776, test loss 0.8480\n",
      "Epoch 10297: train loss: 0.1776, test loss 0.8480\n",
      "Epoch 10298: train loss: 0.1776, test loss 0.8479\n",
      "Epoch 10299: train loss: 0.1776, test loss 0.8479\n",
      "Epoch 10300: train loss: 0.1776, test loss 0.8479\n",
      "Epoch 10301: train loss: 0.1776, test loss 0.8479\n",
      "Epoch 10302: train loss: 0.1776, test loss 0.8478\n",
      "Epoch 10303: train loss: 0.1776, test loss 0.8478\n",
      "Epoch 10304: train loss: 0.1776, test loss 0.8478\n",
      "Epoch 10305: train loss: 0.1776, test loss 0.8477\n",
      "Epoch 10306: train loss: 0.1776, test loss 0.8477\n",
      "Epoch 10307: train loss: 0.1776, test loss 0.8477\n",
      "Epoch 10308: train loss: 0.1776, test loss 0.8477\n",
      "Epoch 10309: train loss: 0.1775, test loss 0.8476\n",
      "Epoch 10310: train loss: 0.1775, test loss 0.8476\n",
      "Epoch 10311: train loss: 0.1775, test loss 0.8476\n",
      "Epoch 10312: train loss: 0.1775, test loss 0.8476\n",
      "Epoch 10313: train loss: 0.1775, test loss 0.8475\n",
      "Epoch 10314: train loss: 0.1775, test loss 0.8475\n",
      "Epoch 10315: train loss: 0.1775, test loss 0.8475\n",
      "Epoch 10316: train loss: 0.1775, test loss 0.8475\n",
      "Epoch 10317: train loss: 0.1775, test loss 0.8474\n",
      "Epoch 10318: train loss: 0.1775, test loss 0.8474\n",
      "Epoch 10319: train loss: 0.1775, test loss 0.8474\n",
      "Epoch 10320: train loss: 0.1775, test loss 0.8473\n",
      "Epoch 10321: train loss: 0.1775, test loss 0.8473\n",
      "Epoch 10322: train loss: 0.1775, test loss 0.8473\n",
      "Epoch 10323: train loss: 0.1775, test loss 0.8473\n",
      "Epoch 10324: train loss: 0.1775, test loss 0.8472\n",
      "Epoch 10325: train loss: 0.1775, test loss 0.8472\n",
      "Epoch 10326: train loss: 0.1775, test loss 0.8472\n",
      "Epoch 10327: train loss: 0.1775, test loss 0.8472\n",
      "Epoch 10328: train loss: 0.1775, test loss 0.8471\n",
      "Epoch 10329: train loss: 0.1775, test loss 0.8471\n",
      "Epoch 10330: train loss: 0.1775, test loss 0.8471\n",
      "Epoch 10331: train loss: 0.1775, test loss 0.8471\n",
      "Epoch 10332: train loss: 0.1775, test loss 0.8470\n",
      "Epoch 10333: train loss: 0.1775, test loss 0.8470\n",
      "Epoch 10334: train loss: 0.1775, test loss 0.8470\n",
      "Epoch 10335: train loss: 0.1775, test loss 0.8469\n",
      "Epoch 10336: train loss: 0.1775, test loss 0.8469\n",
      "Epoch 10337: train loss: 0.1775, test loss 0.8469\n",
      "Epoch 10338: train loss: 0.1775, test loss 0.8469\n",
      "Epoch 10339: train loss: 0.1775, test loss 0.8468\n",
      "Epoch 10340: train loss: 0.1775, test loss 0.8468\n",
      "Epoch 10341: train loss: 0.1775, test loss 0.8468\n",
      "Epoch 10342: train loss: 0.1775, test loss 0.8468\n",
      "Epoch 10343: train loss: 0.1775, test loss 0.8467\n",
      "Epoch 10344: train loss: 0.1775, test loss 0.8467\n",
      "Epoch 10345: train loss: 0.1775, test loss 0.8467\n",
      "Epoch 10346: train loss: 0.1775, test loss 0.8467\n",
      "Epoch 10347: train loss: 0.1775, test loss 0.8466\n",
      "Epoch 10348: train loss: 0.1775, test loss 0.8466\n",
      "Epoch 10349: train loss: 0.1775, test loss 0.8466\n",
      "Epoch 10350: train loss: 0.1775, test loss 0.8465\n",
      "Epoch 10351: train loss: 0.1775, test loss 0.8465\n",
      "Epoch 10352: train loss: 0.1775, test loss 0.8465\n",
      "Epoch 10353: train loss: 0.1775, test loss 0.8465\n",
      "Epoch 10354: train loss: 0.1775, test loss 0.8464\n",
      "Epoch 10355: train loss: 0.1775, test loss 0.8464\n",
      "Epoch 10356: train loss: 0.1775, test loss 0.8464\n",
      "Epoch 10357: train loss: 0.1775, test loss 0.8464\n",
      "Epoch 10358: train loss: 0.1775, test loss 0.8463\n",
      "Epoch 10359: train loss: 0.1775, test loss 0.8463\n",
      "Epoch 10360: train loss: 0.1775, test loss 0.8463\n",
      "Epoch 10361: train loss: 0.1775, test loss 0.8463\n",
      "Epoch 10362: train loss: 0.1775, test loss 0.8462\n",
      "Epoch 10363: train loss: 0.1775, test loss 0.8462\n",
      "Epoch 10364: train loss: 0.1775, test loss 0.8462\n",
      "Epoch 10365: train loss: 0.1775, test loss 0.8461\n",
      "Epoch 10366: train loss: 0.1775, test loss 0.8461\n",
      "Epoch 10367: train loss: 0.1775, test loss 0.8461\n",
      "Epoch 10368: train loss: 0.1775, test loss 0.8461\n",
      "Epoch 10369: train loss: 0.1775, test loss 0.8460\n",
      "Epoch 10370: train loss: 0.1775, test loss 0.8460\n",
      "Epoch 10371: train loss: 0.1775, test loss 0.8460\n",
      "Epoch 10372: train loss: 0.1775, test loss 0.8460\n",
      "Epoch 10373: train loss: 0.1775, test loss 0.8459\n",
      "Epoch 10374: train loss: 0.1775, test loss 0.8459\n",
      "Epoch 10375: train loss: 0.1775, test loss 0.8459\n",
      "Epoch 10376: train loss: 0.1775, test loss 0.8459\n",
      "Epoch 10377: train loss: 0.1775, test loss 0.8458\n",
      "Epoch 10378: train loss: 0.1775, test loss 0.8458\n",
      "Epoch 10379: train loss: 0.1775, test loss 0.8458\n",
      "Epoch 10380: train loss: 0.1775, test loss 0.8458\n",
      "Epoch 10381: train loss: 0.1775, test loss 0.8457\n",
      "Epoch 10382: train loss: 0.1775, test loss 0.8457\n",
      "Epoch 10383: train loss: 0.1775, test loss 0.8457\n",
      "Epoch 10384: train loss: 0.1775, test loss 0.8456\n",
      "Epoch 10385: train loss: 0.1775, test loss 0.8456\n",
      "Epoch 10386: train loss: 0.1775, test loss 0.8456\n",
      "Epoch 10387: train loss: 0.1775, test loss 0.8456\n",
      "Epoch 10388: train loss: 0.1775, test loss 0.8455\n",
      "Epoch 10389: train loss: 0.1775, test loss 0.8455\n",
      "Epoch 10390: train loss: 0.1775, test loss 0.8455\n",
      "Epoch 10391: train loss: 0.1775, test loss 0.8455\n",
      "Epoch 10392: train loss: 0.1775, test loss 0.8454\n",
      "Epoch 10393: train loss: 0.1775, test loss 0.8454\n",
      "Epoch 10394: train loss: 0.1775, test loss 0.8454\n",
      "Epoch 10395: train loss: 0.1775, test loss 0.8454\n",
      "Epoch 10396: train loss: 0.1775, test loss 0.8453\n",
      "Epoch 10397: train loss: 0.1775, test loss 0.8453\n",
      "Epoch 10398: train loss: 0.1775, test loss 0.8453\n",
      "Epoch 10399: train loss: 0.1775, test loss 0.8452\n",
      "Epoch 10400: train loss: 0.1775, test loss 0.8452\n",
      "Epoch 10401: train loss: 0.1775, test loss 0.8452\n",
      "Epoch 10402: train loss: 0.1775, test loss 0.8452\n",
      "Epoch 10403: train loss: 0.1775, test loss 0.8451\n",
      "Epoch 10404: train loss: 0.1775, test loss 0.8451\n",
      "Epoch 10405: train loss: 0.1774, test loss 0.8451\n",
      "Epoch 10406: train loss: 0.1774, test loss 0.8451\n",
      "Epoch 10407: train loss: 0.1774, test loss 0.8450\n",
      "Epoch 10408: train loss: 0.1774, test loss 0.8450\n",
      "Epoch 10409: train loss: 0.1774, test loss 0.8450\n",
      "Epoch 10410: train loss: 0.1774, test loss 0.8450\n",
      "Epoch 10411: train loss: 0.1774, test loss 0.8449\n",
      "Epoch 10412: train loss: 0.1774, test loss 0.8449\n",
      "Epoch 10413: train loss: 0.1774, test loss 0.8449\n",
      "Epoch 10414: train loss: 0.1774, test loss 0.8448\n",
      "Epoch 10415: train loss: 0.1774, test loss 0.8448\n",
      "Epoch 10416: train loss: 0.1774, test loss 0.8448\n",
      "Epoch 10417: train loss: 0.1774, test loss 0.8448\n",
      "Epoch 10418: train loss: 0.1774, test loss 0.8447\n",
      "Epoch 10419: train loss: 0.1774, test loss 0.8447\n",
      "Epoch 10420: train loss: 0.1774, test loss 0.8447\n",
      "Epoch 10421: train loss: 0.1774, test loss 0.8447\n",
      "Epoch 10422: train loss: 0.1774, test loss 0.8446\n",
      "Epoch 10423: train loss: 0.1774, test loss 0.8446\n",
      "Epoch 10424: train loss: 0.1774, test loss 0.8446\n",
      "Epoch 10425: train loss: 0.1774, test loss 0.8446\n",
      "Epoch 10426: train loss: 0.1774, test loss 0.8445\n",
      "Epoch 10427: train loss: 0.1774, test loss 0.8445\n",
      "Epoch 10428: train loss: 0.1774, test loss 0.8445\n",
      "Epoch 10429: train loss: 0.1774, test loss 0.8445\n",
      "Epoch 10430: train loss: 0.1774, test loss 0.8444\n",
      "Epoch 10431: train loss: 0.1774, test loss 0.8444\n",
      "Epoch 10432: train loss: 0.1774, test loss 0.8444\n",
      "Epoch 10433: train loss: 0.1774, test loss 0.8443\n",
      "Epoch 10434: train loss: 0.1774, test loss 0.8443\n",
      "Epoch 10435: train loss: 0.1774, test loss 0.8443\n",
      "Epoch 10436: train loss: 0.1774, test loss 0.8443\n",
      "Epoch 10437: train loss: 0.1774, test loss 0.8442\n",
      "Epoch 10438: train loss: 0.1774, test loss 0.8442\n",
      "Epoch 10439: train loss: 0.1774, test loss 0.8442\n",
      "Epoch 10440: train loss: 0.1774, test loss 0.8442\n",
      "Epoch 10441: train loss: 0.1774, test loss 0.8441\n",
      "Epoch 10442: train loss: 0.1774, test loss 0.8441\n",
      "Epoch 10443: train loss: 0.1774, test loss 0.8441\n",
      "Epoch 10444: train loss: 0.1774, test loss 0.8441\n",
      "Epoch 10445: train loss: 0.1774, test loss 0.8440\n",
      "Epoch 10446: train loss: 0.1774, test loss 0.8440\n",
      "Epoch 10447: train loss: 0.1774, test loss 0.8440\n",
      "Epoch 10448: train loss: 0.1774, test loss 0.8440\n",
      "Epoch 10449: train loss: 0.1774, test loss 0.8439\n",
      "Epoch 10450: train loss: 0.1774, test loss 0.8439\n",
      "Epoch 10451: train loss: 0.1774, test loss 0.8439\n",
      "Epoch 10452: train loss: 0.1774, test loss 0.8438\n",
      "Epoch 10453: train loss: 0.1774, test loss 0.8438\n",
      "Epoch 10454: train loss: 0.1774, test loss 0.8438\n",
      "Epoch 10455: train loss: 0.1774, test loss 0.8438\n",
      "Epoch 10456: train loss: 0.1774, test loss 0.8437\n",
      "Epoch 10457: train loss: 0.1774, test loss 0.8437\n",
      "Epoch 10458: train loss: 0.1774, test loss 0.8437\n",
      "Epoch 10459: train loss: 0.1774, test loss 0.8437\n",
      "Epoch 10460: train loss: 0.1774, test loss 0.8436\n",
      "Epoch 10461: train loss: 0.1774, test loss 0.8436\n",
      "Epoch 10462: train loss: 0.1774, test loss 0.8436\n",
      "Epoch 10463: train loss: 0.1774, test loss 0.8436\n",
      "Epoch 10464: train loss: 0.1774, test loss 0.8435\n",
      "Epoch 10465: train loss: 0.1774, test loss 0.8435\n",
      "Epoch 10466: train loss: 0.1774, test loss 0.8435\n",
      "Epoch 10467: train loss: 0.1774, test loss 0.8435\n",
      "Epoch 10468: train loss: 0.1774, test loss 0.8434\n",
      "Epoch 10469: train loss: 0.1774, test loss 0.8434\n",
      "Epoch 10470: train loss: 0.1774, test loss 0.8434\n",
      "Epoch 10471: train loss: 0.1774, test loss 0.8434\n",
      "Epoch 10472: train loss: 0.1774, test loss 0.8433\n",
      "Epoch 10473: train loss: 0.1774, test loss 0.8433\n",
      "Epoch 10474: train loss: 0.1774, test loss 0.8433\n",
      "Epoch 10475: train loss: 0.1774, test loss 0.8432\n",
      "Epoch 10476: train loss: 0.1774, test loss 0.8432\n",
      "Epoch 10477: train loss: 0.1774, test loss 0.8432\n",
      "Epoch 10478: train loss: 0.1774, test loss 0.8432\n",
      "Epoch 10479: train loss: 0.1774, test loss 0.8431\n",
      "Epoch 10480: train loss: 0.1774, test loss 0.8431\n",
      "Epoch 10481: train loss: 0.1774, test loss 0.8431\n",
      "Epoch 10482: train loss: 0.1774, test loss 0.8431\n",
      "Epoch 10483: train loss: 0.1774, test loss 0.8430\n",
      "Epoch 10484: train loss: 0.1774, test loss 0.8430\n",
      "Epoch 10485: train loss: 0.1774, test loss 0.8430\n",
      "Epoch 10486: train loss: 0.1774, test loss 0.8430\n",
      "Epoch 10487: train loss: 0.1774, test loss 0.8429\n",
      "Epoch 10488: train loss: 0.1774, test loss 0.8429\n",
      "Epoch 10489: train loss: 0.1774, test loss 0.8429\n",
      "Epoch 10490: train loss: 0.1774, test loss 0.8429\n",
      "Epoch 10491: train loss: 0.1774, test loss 0.8428\n",
      "Epoch 10492: train loss: 0.1774, test loss 0.8428\n",
      "Epoch 10493: train loss: 0.1774, test loss 0.8428\n",
      "Epoch 10494: train loss: 0.1774, test loss 0.8428\n",
      "Epoch 10495: train loss: 0.1774, test loss 0.8427\n",
      "Epoch 10496: train loss: 0.1774, test loss 0.8427\n",
      "Epoch 10497: train loss: 0.1774, test loss 0.8427\n",
      "Epoch 10498: train loss: 0.1774, test loss 0.8426\n",
      "Epoch 10499: train loss: 0.1774, test loss 0.8426\n",
      "Epoch 10500: train loss: 0.1774, test loss 0.8426\n",
      "Epoch 10501: train loss: 0.1774, test loss 0.8426\n",
      "Epoch 10502: train loss: 0.1774, test loss 0.8425\n",
      "Epoch 10503: train loss: 0.1773, test loss 0.8425\n",
      "Epoch 10504: train loss: 0.1773, test loss 0.8425\n",
      "Epoch 10505: train loss: 0.1773, test loss 0.8425\n",
      "Epoch 10506: train loss: 0.1773, test loss 0.8424\n",
      "Epoch 10507: train loss: 0.1773, test loss 0.8424\n",
      "Epoch 10508: train loss: 0.1773, test loss 0.8424\n",
      "Epoch 10509: train loss: 0.1773, test loss 0.8424\n",
      "Epoch 10510: train loss: 0.1773, test loss 0.8423\n",
      "Epoch 10511: train loss: 0.1773, test loss 0.8423\n",
      "Epoch 10512: train loss: 0.1773, test loss 0.8423\n",
      "Epoch 10513: train loss: 0.1773, test loss 0.8423\n",
      "Epoch 10514: train loss: 0.1773, test loss 0.8422\n",
      "Epoch 10515: train loss: 0.1773, test loss 0.8422\n",
      "Epoch 10516: train loss: 0.1773, test loss 0.8422\n",
      "Epoch 10517: train loss: 0.1773, test loss 0.8422\n",
      "Epoch 10518: train loss: 0.1773, test loss 0.8421\n",
      "Epoch 10519: train loss: 0.1773, test loss 0.8421\n",
      "Epoch 10520: train loss: 0.1773, test loss 0.8421\n",
      "Epoch 10521: train loss: 0.1773, test loss 0.8421\n",
      "Epoch 10522: train loss: 0.1773, test loss 0.8420\n",
      "Epoch 10523: train loss: 0.1773, test loss 0.8420\n",
      "Epoch 10524: train loss: 0.1773, test loss 0.8420\n",
      "Epoch 10525: train loss: 0.1773, test loss 0.8420\n",
      "Epoch 10526: train loss: 0.1773, test loss 0.8419\n",
      "Epoch 10527: train loss: 0.1773, test loss 0.8419\n",
      "Epoch 10528: train loss: 0.1773, test loss 0.8419\n",
      "Epoch 10529: train loss: 0.1773, test loss 0.8419\n",
      "Epoch 10530: train loss: 0.1773, test loss 0.8418\n",
      "Epoch 10531: train loss: 0.1773, test loss 0.8418\n",
      "Epoch 10532: train loss: 0.1773, test loss 0.8418\n",
      "Epoch 10533: train loss: 0.1773, test loss 0.8417\n",
      "Epoch 10534: train loss: 0.1773, test loss 0.8417\n",
      "Epoch 10535: train loss: 0.1773, test loss 0.8417\n",
      "Epoch 10536: train loss: 0.1773, test loss 0.8417\n",
      "Epoch 10537: train loss: 0.1773, test loss 0.8416\n",
      "Epoch 10538: train loss: 0.1773, test loss 0.8416\n",
      "Epoch 10539: train loss: 0.1773, test loss 0.8416\n",
      "Epoch 10540: train loss: 0.1773, test loss 0.8416\n",
      "Epoch 10541: train loss: 0.1773, test loss 0.8415\n",
      "Epoch 10542: train loss: 0.1773, test loss 0.8415\n",
      "Epoch 10543: train loss: 0.1773, test loss 0.8415\n",
      "Epoch 10544: train loss: 0.1773, test loss 0.8415\n",
      "Epoch 10545: train loss: 0.1773, test loss 0.8414\n",
      "Epoch 10546: train loss: 0.1773, test loss 0.8414\n",
      "Epoch 10547: train loss: 0.1773, test loss 0.8414\n",
      "Epoch 10548: train loss: 0.1773, test loss 0.8414\n",
      "Epoch 10549: train loss: 0.1773, test loss 0.8413\n",
      "Epoch 10550: train loss: 0.1773, test loss 0.8413\n",
      "Epoch 10551: train loss: 0.1773, test loss 0.8413\n",
      "Epoch 10552: train loss: 0.1773, test loss 0.8413\n",
      "Epoch 10553: train loss: 0.1773, test loss 0.8412\n",
      "Epoch 10554: train loss: 0.1773, test loss 0.8412\n",
      "Epoch 10555: train loss: 0.1773, test loss 0.8412\n",
      "Epoch 10556: train loss: 0.1773, test loss 0.8412\n",
      "Epoch 10557: train loss: 0.1773, test loss 0.8411\n",
      "Epoch 10558: train loss: 0.1773, test loss 0.8411\n",
      "Epoch 10559: train loss: 0.1773, test loss 0.8411\n",
      "Epoch 10560: train loss: 0.1773, test loss 0.8411\n",
      "Epoch 10561: train loss: 0.1773, test loss 0.8410\n",
      "Epoch 10562: train loss: 0.1773, test loss 0.8410\n",
      "Epoch 10563: train loss: 0.1773, test loss 0.8410\n",
      "Epoch 10564: train loss: 0.1773, test loss 0.8410\n",
      "Epoch 10565: train loss: 0.1773, test loss 0.8409\n",
      "Epoch 10566: train loss: 0.1773, test loss 0.8409\n",
      "Epoch 10567: train loss: 0.1773, test loss 0.8409\n",
      "Epoch 10568: train loss: 0.1773, test loss 0.8409\n",
      "Epoch 10569: train loss: 0.1773, test loss 0.8408\n",
      "Epoch 10570: train loss: 0.1773, test loss 0.8408\n",
      "Epoch 10571: train loss: 0.1773, test loss 0.8408\n",
      "Epoch 10572: train loss: 0.1773, test loss 0.8407\n",
      "Epoch 10573: train loss: 0.1773, test loss 0.8407\n",
      "Epoch 10574: train loss: 0.1773, test loss 0.8407\n",
      "Epoch 10575: train loss: 0.1773, test loss 0.8407\n",
      "Epoch 10576: train loss: 0.1773, test loss 0.8406\n",
      "Epoch 10577: train loss: 0.1773, test loss 0.8406\n",
      "Epoch 10578: train loss: 0.1773, test loss 0.8406\n",
      "Epoch 10579: train loss: 0.1773, test loss 0.8406\n",
      "Epoch 10580: train loss: 0.1773, test loss 0.8405\n",
      "Epoch 10581: train loss: 0.1773, test loss 0.8405\n",
      "Epoch 10582: train loss: 0.1773, test loss 0.8405\n",
      "Epoch 10583: train loss: 0.1773, test loss 0.8405\n",
      "Epoch 10584: train loss: 0.1773, test loss 0.8404\n",
      "Epoch 10585: train loss: 0.1773, test loss 0.8404\n",
      "Epoch 10586: train loss: 0.1773, test loss 0.8404\n",
      "Epoch 10587: train loss: 0.1773, test loss 0.8404\n",
      "Epoch 10588: train loss: 0.1773, test loss 0.8403\n",
      "Epoch 10589: train loss: 0.1773, test loss 0.8403\n",
      "Epoch 10590: train loss: 0.1773, test loss 0.8403\n",
      "Epoch 10591: train loss: 0.1773, test loss 0.8403\n",
      "Epoch 10592: train loss: 0.1773, test loss 0.8402\n",
      "Epoch 10593: train loss: 0.1773, test loss 0.8402\n",
      "Epoch 10594: train loss: 0.1773, test loss 0.8402\n",
      "Epoch 10595: train loss: 0.1773, test loss 0.8401\n",
      "Epoch 10596: train loss: 0.1773, test loss 0.8401\n",
      "Epoch 10597: train loss: 0.1773, test loss 0.8401\n",
      "Epoch 10598: train loss: 0.1773, test loss 0.8401\n",
      "Epoch 10599: train loss: 0.1773, test loss 0.8400\n",
      "Epoch 10600: train loss: 0.1773, test loss 0.8400\n",
      "Epoch 10601: train loss: 0.1773, test loss 0.8400\n",
      "Epoch 10602: train loss: 0.1773, test loss 0.8400\n",
      "Epoch 10603: train loss: 0.1772, test loss 0.8399\n",
      "Epoch 10604: train loss: 0.1772, test loss 0.8399\n",
      "Epoch 10605: train loss: 0.1772, test loss 0.8399\n",
      "Epoch 10606: train loss: 0.1772, test loss 0.8399\n",
      "Epoch 10607: train loss: 0.1772, test loss 0.8398\n",
      "Epoch 10608: train loss: 0.1772, test loss 0.8398\n",
      "Epoch 10609: train loss: 0.1772, test loss 0.8398\n",
      "Epoch 10610: train loss: 0.1772, test loss 0.8398\n",
      "Epoch 10611: train loss: 0.1772, test loss 0.8397\n",
      "Epoch 10612: train loss: 0.1772, test loss 0.8397\n",
      "Epoch 10613: train loss: 0.1772, test loss 0.8397\n",
      "Epoch 10614: train loss: 0.1772, test loss 0.8397\n",
      "Epoch 10615: train loss: 0.1772, test loss 0.8396\n",
      "Epoch 10616: train loss: 0.1772, test loss 0.8396\n",
      "Epoch 10617: train loss: 0.1772, test loss 0.8396\n",
      "Epoch 10618: train loss: 0.1772, test loss 0.8395\n",
      "Epoch 10619: train loss: 0.1772, test loss 0.8395\n",
      "Epoch 10620: train loss: 0.1772, test loss 0.8395\n",
      "Epoch 10621: train loss: 0.1772, test loss 0.8395\n",
      "Epoch 10622: train loss: 0.1772, test loss 0.8394\n",
      "Epoch 10623: train loss: 0.1772, test loss 0.8394\n",
      "Epoch 10624: train loss: 0.1772, test loss 0.8394\n",
      "Epoch 10625: train loss: 0.1772, test loss 0.8394\n",
      "Epoch 10626: train loss: 0.1772, test loss 0.8393\n",
      "Epoch 10627: train loss: 0.1772, test loss 0.8393\n",
      "Epoch 10628: train loss: 0.1772, test loss 0.8393\n",
      "Epoch 10629: train loss: 0.1772, test loss 0.8393\n",
      "Epoch 10630: train loss: 0.1772, test loss 0.8392\n",
      "Epoch 10631: train loss: 0.1772, test loss 0.8392\n",
      "Epoch 10632: train loss: 0.1772, test loss 0.8392\n",
      "Epoch 10633: train loss: 0.1772, test loss 0.8392\n",
      "Epoch 10634: train loss: 0.1772, test loss 0.8391\n",
      "Epoch 10635: train loss: 0.1772, test loss 0.8391\n",
      "Epoch 10636: train loss: 0.1772, test loss 0.8391\n",
      "Epoch 10637: train loss: 0.1772, test loss 0.8391\n",
      "Epoch 10638: train loss: 0.1772, test loss 0.8390\n",
      "Epoch 10639: train loss: 0.1772, test loss 0.8390\n",
      "Epoch 10640: train loss: 0.1772, test loss 0.8390\n",
      "Epoch 10641: train loss: 0.1772, test loss 0.8389\n",
      "Epoch 10642: train loss: 0.1772, test loss 0.8389\n",
      "Epoch 10643: train loss: 0.1772, test loss 0.8389\n",
      "Epoch 10644: train loss: 0.1772, test loss 0.8389\n",
      "Epoch 10645: train loss: 0.1772, test loss 0.8388\n",
      "Epoch 10646: train loss: 0.1772, test loss 0.8388\n",
      "Epoch 10647: train loss: 0.1772, test loss 0.8388\n",
      "Epoch 10648: train loss: 0.1772, test loss 0.8388\n",
      "Epoch 10649: train loss: 0.1772, test loss 0.8387\n",
      "Epoch 10650: train loss: 0.1772, test loss 0.8387\n",
      "Epoch 10651: train loss: 0.1772, test loss 0.8387\n",
      "Epoch 10652: train loss: 0.1772, test loss 0.8387\n",
      "Epoch 10653: train loss: 0.1772, test loss 0.8386\n",
      "Epoch 10654: train loss: 0.1772, test loss 0.8386\n",
      "Epoch 10655: train loss: 0.1772, test loss 0.8386\n",
      "Epoch 10656: train loss: 0.1772, test loss 0.8385\n",
      "Epoch 10657: train loss: 0.1772, test loss 0.8385\n",
      "Epoch 10658: train loss: 0.1772, test loss 0.8385\n",
      "Epoch 10659: train loss: 0.1772, test loss 0.8385\n",
      "Epoch 10660: train loss: 0.1772, test loss 0.8384\n",
      "Epoch 10661: train loss: 0.1772, test loss 0.8384\n",
      "Epoch 10662: train loss: 0.1772, test loss 0.8384\n",
      "Epoch 10663: train loss: 0.1772, test loss 0.8384\n",
      "Epoch 10664: train loss: 0.1772, test loss 0.8383\n",
      "Epoch 10665: train loss: 0.1772, test loss 0.8383\n",
      "Epoch 10666: train loss: 0.1772, test loss 0.8383\n",
      "Epoch 10667: train loss: 0.1772, test loss 0.8383\n",
      "Epoch 10668: train loss: 0.1772, test loss 0.8382\n",
      "Epoch 10669: train loss: 0.1772, test loss 0.8382\n",
      "Epoch 10670: train loss: 0.1772, test loss 0.8382\n",
      "Epoch 10671: train loss: 0.1772, test loss 0.8382\n",
      "Epoch 10672: train loss: 0.1772, test loss 0.8381\n",
      "Epoch 10673: train loss: 0.1772, test loss 0.8381\n",
      "Epoch 10674: train loss: 0.1772, test loss 0.8381\n",
      "Epoch 10675: train loss: 0.1772, test loss 0.8381\n",
      "Epoch 10676: train loss: 0.1772, test loss 0.8380\n",
      "Epoch 10677: train loss: 0.1772, test loss 0.8380\n",
      "Epoch 10678: train loss: 0.1772, test loss 0.8380\n",
      "Epoch 10679: train loss: 0.1772, test loss 0.8380\n",
      "Epoch 10680: train loss: 0.1772, test loss 0.8379\n",
      "Epoch 10681: train loss: 0.1772, test loss 0.8379\n",
      "Epoch 10682: train loss: 0.1772, test loss 0.8379\n",
      "Epoch 10683: train loss: 0.1772, test loss 0.8379\n",
      "Epoch 10684: train loss: 0.1772, test loss 0.8378\n",
      "Epoch 10685: train loss: 0.1772, test loss 0.8378\n",
      "Epoch 10686: train loss: 0.1772, test loss 0.8378\n",
      "Epoch 10687: train loss: 0.1772, test loss 0.8378\n",
      "Epoch 10688: train loss: 0.1772, test loss 0.8377\n",
      "Epoch 10689: train loss: 0.1772, test loss 0.8377\n",
      "Epoch 10690: train loss: 0.1772, test loss 0.8377\n",
      "Epoch 10691: train loss: 0.1772, test loss 0.8377\n",
      "Epoch 10692: train loss: 0.1772, test loss 0.8376\n",
      "Epoch 10693: train loss: 0.1772, test loss 0.8376\n",
      "Epoch 10694: train loss: 0.1772, test loss 0.8376\n",
      "Epoch 10695: train loss: 0.1772, test loss 0.8376\n",
      "Epoch 10696: train loss: 0.1772, test loss 0.8375\n",
      "Epoch 10697: train loss: 0.1772, test loss 0.8375\n",
      "Epoch 10698: train loss: 0.1772, test loss 0.8375\n",
      "Epoch 10699: train loss: 0.1772, test loss 0.8374\n",
      "Epoch 10700: train loss: 0.1772, test loss 0.8374\n",
      "Epoch 10701: train loss: 0.1772, test loss 0.8374\n",
      "Epoch 10702: train loss: 0.1772, test loss 0.8374\n",
      "Epoch 10703: train loss: 0.1772, test loss 0.8373\n",
      "Epoch 10704: train loss: 0.1771, test loss 0.8373\n",
      "Epoch 10705: train loss: 0.1771, test loss 0.8373\n",
      "Epoch 10706: train loss: 0.1771, test loss 0.8373\n",
      "Epoch 10707: train loss: 0.1771, test loss 0.8372\n",
      "Epoch 10708: train loss: 0.1771, test loss 0.8372\n",
      "Epoch 10709: train loss: 0.1771, test loss 0.8372\n",
      "Epoch 10710: train loss: 0.1771, test loss 0.8372\n",
      "Epoch 10711: train loss: 0.1771, test loss 0.8371\n",
      "Epoch 10712: train loss: 0.1771, test loss 0.8371\n",
      "Epoch 10713: train loss: 0.1771, test loss 0.8371\n",
      "Epoch 10714: train loss: 0.1771, test loss 0.8371\n",
      "Epoch 10715: train loss: 0.1771, test loss 0.8370\n",
      "Epoch 10716: train loss: 0.1771, test loss 0.8370\n",
      "Epoch 10717: train loss: 0.1771, test loss 0.8370\n",
      "Epoch 10718: train loss: 0.1771, test loss 0.8370\n",
      "Epoch 10719: train loss: 0.1771, test loss 0.8369\n",
      "Epoch 10720: train loss: 0.1771, test loss 0.8369\n",
      "Epoch 10721: train loss: 0.1771, test loss 0.8369\n",
      "Epoch 10722: train loss: 0.1771, test loss 0.8369\n",
      "Epoch 10723: train loss: 0.1771, test loss 0.8368\n",
      "Epoch 10724: train loss: 0.1771, test loss 0.8368\n",
      "Epoch 10725: train loss: 0.1771, test loss 0.8368\n",
      "Epoch 10726: train loss: 0.1771, test loss 0.8368\n",
      "Epoch 10727: train loss: 0.1771, test loss 0.8367\n",
      "Epoch 10728: train loss: 0.1771, test loss 0.8367\n",
      "Epoch 10729: train loss: 0.1771, test loss 0.8367\n",
      "Epoch 10730: train loss: 0.1771, test loss 0.8367\n",
      "Epoch 10731: train loss: 0.1771, test loss 0.8366\n",
      "Epoch 10732: train loss: 0.1771, test loss 0.8366\n",
      "Epoch 10733: train loss: 0.1771, test loss 0.8366\n",
      "Epoch 10734: train loss: 0.1771, test loss 0.8366\n",
      "Epoch 10735: train loss: 0.1771, test loss 0.8365\n",
      "Epoch 10736: train loss: 0.1771, test loss 0.8365\n",
      "Epoch 10737: train loss: 0.1771, test loss 0.8365\n",
      "Epoch 10738: train loss: 0.1771, test loss 0.8365\n",
      "Epoch 10739: train loss: 0.1771, test loss 0.8364\n",
      "Epoch 10740: train loss: 0.1771, test loss 0.8364\n",
      "Epoch 10741: train loss: 0.1771, test loss 0.8364\n",
      "Epoch 10742: train loss: 0.1771, test loss 0.8364\n",
      "Epoch 10743: train loss: 0.1771, test loss 0.8363\n",
      "Epoch 10744: train loss: 0.1771, test loss 0.8363\n",
      "Epoch 10745: train loss: 0.1771, test loss 0.8363\n",
      "Epoch 10746: train loss: 0.1771, test loss 0.8363\n",
      "Epoch 10747: train loss: 0.1771, test loss 0.8362\n",
      "Epoch 10748: train loss: 0.1771, test loss 0.8362\n",
      "Epoch 10749: train loss: 0.1771, test loss 0.8362\n",
      "Epoch 10750: train loss: 0.1771, test loss 0.8362\n",
      "Epoch 10751: train loss: 0.1771, test loss 0.8361\n",
      "Epoch 10752: train loss: 0.1771, test loss 0.8361\n",
      "Epoch 10753: train loss: 0.1771, test loss 0.8361\n",
      "Epoch 10754: train loss: 0.1771, test loss 0.8361\n",
      "Epoch 10755: train loss: 0.1771, test loss 0.8360\n",
      "Epoch 10756: train loss: 0.1771, test loss 0.8360\n",
      "Epoch 10757: train loss: 0.1771, test loss 0.8360\n",
      "Epoch 10758: train loss: 0.1771, test loss 0.8360\n",
      "Epoch 10759: train loss: 0.1771, test loss 0.8359\n",
      "Epoch 10760: train loss: 0.1771, test loss 0.8359\n",
      "Epoch 10761: train loss: 0.1771, test loss 0.8359\n",
      "Epoch 10762: train loss: 0.1771, test loss 0.8359\n",
      "Epoch 10763: train loss: 0.1771, test loss 0.8358\n",
      "Epoch 10764: train loss: 0.1771, test loss 0.8358\n",
      "Epoch 10765: train loss: 0.1771, test loss 0.8358\n",
      "Epoch 10766: train loss: 0.1771, test loss 0.8358\n",
      "Epoch 10767: train loss: 0.1771, test loss 0.8357\n",
      "Epoch 10768: train loss: 0.1771, test loss 0.8357\n",
      "Epoch 10769: train loss: 0.1771, test loss 0.8357\n",
      "Epoch 10770: train loss: 0.1771, test loss 0.8357\n",
      "Epoch 10771: train loss: 0.1771, test loss 0.8356\n",
      "Epoch 10772: train loss: 0.1771, test loss 0.8356\n",
      "Epoch 10773: train loss: 0.1771, test loss 0.8356\n",
      "Epoch 10774: train loss: 0.1771, test loss 0.8356\n",
      "Epoch 10775: train loss: 0.1771, test loss 0.8355\n",
      "Epoch 10776: train loss: 0.1771, test loss 0.8355\n",
      "Epoch 10777: train loss: 0.1771, test loss 0.8355\n",
      "Epoch 10778: train loss: 0.1771, test loss 0.8355\n",
      "Epoch 10779: train loss: 0.1771, test loss 0.8354\n",
      "Epoch 10780: train loss: 0.1771, test loss 0.8354\n",
      "Epoch 10781: train loss: 0.1771, test loss 0.8354\n",
      "Epoch 10782: train loss: 0.1771, test loss 0.8353\n",
      "Epoch 10783: train loss: 0.1771, test loss 0.8353\n",
      "Epoch 10784: train loss: 0.1771, test loss 0.8353\n",
      "Epoch 10785: train loss: 0.1771, test loss 0.8353\n",
      "Epoch 10786: train loss: 0.1771, test loss 0.8352\n",
      "Epoch 10787: train loss: 0.1771, test loss 0.8352\n",
      "Epoch 10788: train loss: 0.1771, test loss 0.8352\n",
      "Epoch 10789: train loss: 0.1771, test loss 0.8352\n",
      "Epoch 10790: train loss: 0.1771, test loss 0.8351\n",
      "Epoch 10791: train loss: 0.1771, test loss 0.8351\n",
      "Epoch 10792: train loss: 0.1771, test loss 0.8351\n",
      "Epoch 10793: train loss: 0.1771, test loss 0.8351\n",
      "Epoch 10794: train loss: 0.1771, test loss 0.8350\n",
      "Epoch 10795: train loss: 0.1771, test loss 0.8350\n",
      "Epoch 10796: train loss: 0.1771, test loss 0.8350\n",
      "Epoch 10797: train loss: 0.1771, test loss 0.8350\n",
      "Epoch 10798: train loss: 0.1771, test loss 0.8349\n",
      "Epoch 10799: train loss: 0.1771, test loss 0.8349\n",
      "Epoch 10800: train loss: 0.1771, test loss 0.8349\n",
      "Epoch 10801: train loss: 0.1771, test loss 0.8349\n",
      "Epoch 10802: train loss: 0.1771, test loss 0.8348\n",
      "Epoch 10803: train loss: 0.1771, test loss 0.8348\n",
      "Epoch 10804: train loss: 0.1771, test loss 0.8348\n",
      "Epoch 10805: train loss: 0.1771, test loss 0.8348\n",
      "Epoch 10806: train loss: 0.1771, test loss 0.8347\n",
      "Epoch 10807: train loss: 0.1770, test loss 0.8347\n",
      "Epoch 10808: train loss: 0.1770, test loss 0.8347\n",
      "Epoch 10809: train loss: 0.1770, test loss 0.8347\n",
      "Epoch 10810: train loss: 0.1770, test loss 0.8346\n",
      "Epoch 10811: train loss: 0.1770, test loss 0.8346\n",
      "Epoch 10812: train loss: 0.1770, test loss 0.8346\n",
      "Epoch 10813: train loss: 0.1770, test loss 0.8346\n",
      "Epoch 10814: train loss: 0.1770, test loss 0.8346\n",
      "Epoch 10815: train loss: 0.1770, test loss 0.8345\n",
      "Epoch 10816: train loss: 0.1770, test loss 0.8345\n",
      "Epoch 10817: train loss: 0.1770, test loss 0.8345\n",
      "Epoch 10818: train loss: 0.1770, test loss 0.8345\n",
      "Epoch 10819: train loss: 0.1770, test loss 0.8344\n",
      "Epoch 10820: train loss: 0.1770, test loss 0.8344\n",
      "Epoch 10821: train loss: 0.1770, test loss 0.8344\n",
      "Epoch 10822: train loss: 0.1770, test loss 0.8344\n",
      "Epoch 10823: train loss: 0.1770, test loss 0.8343\n",
      "Epoch 10824: train loss: 0.1770, test loss 0.8343\n",
      "Epoch 10825: train loss: 0.1770, test loss 0.8343\n",
      "Epoch 10826: train loss: 0.1770, test loss 0.8343\n",
      "Epoch 10827: train loss: 0.1770, test loss 0.8342\n",
      "Epoch 10828: train loss: 0.1770, test loss 0.8342\n",
      "Epoch 10829: train loss: 0.1770, test loss 0.8342\n",
      "Epoch 10830: train loss: 0.1770, test loss 0.8342\n",
      "Epoch 10831: train loss: 0.1770, test loss 0.8341\n",
      "Epoch 10832: train loss: 0.1770, test loss 0.8341\n",
      "Epoch 10833: train loss: 0.1770, test loss 0.8341\n",
      "Epoch 10834: train loss: 0.1770, test loss 0.8341\n",
      "Epoch 10835: train loss: 0.1770, test loss 0.8340\n",
      "Epoch 10836: train loss: 0.1770, test loss 0.8340\n",
      "Epoch 10837: train loss: 0.1770, test loss 0.8340\n",
      "Epoch 10838: train loss: 0.1770, test loss 0.8340\n",
      "Epoch 10839: train loss: 0.1770, test loss 0.8339\n",
      "Epoch 10840: train loss: 0.1770, test loss 0.8339\n",
      "Epoch 10841: train loss: 0.1770, test loss 0.8339\n",
      "Epoch 10842: train loss: 0.1770, test loss 0.8339\n",
      "Epoch 10843: train loss: 0.1770, test loss 0.8338\n",
      "Epoch 10844: train loss: 0.1770, test loss 0.8338\n",
      "Epoch 10845: train loss: 0.1770, test loss 0.8338\n",
      "Epoch 10846: train loss: 0.1770, test loss 0.8338\n",
      "Epoch 10847: train loss: 0.1770, test loss 0.8338\n",
      "Epoch 10848: train loss: 0.1770, test loss 0.8337\n",
      "Epoch 10849: train loss: 0.1770, test loss 0.8337\n",
      "Epoch 10850: train loss: 0.1770, test loss 0.8337\n",
      "Epoch 10851: train loss: 0.1770, test loss 0.8337\n",
      "Epoch 10852: train loss: 0.1770, test loss 0.8336\n",
      "Epoch 10853: train loss: 0.1770, test loss 0.8336\n",
      "Epoch 10854: train loss: 0.1770, test loss 0.8336\n",
      "Epoch 10855: train loss: 0.1770, test loss 0.8336\n",
      "Epoch 10856: train loss: 0.1770, test loss 0.8335\n",
      "Epoch 10857: train loss: 0.1770, test loss 0.8335\n",
      "Epoch 10858: train loss: 0.1770, test loss 0.8335\n",
      "Epoch 10859: train loss: 0.1770, test loss 0.8335\n",
      "Epoch 10860: train loss: 0.1770, test loss 0.8334\n",
      "Epoch 10861: train loss: 0.1770, test loss 0.8334\n",
      "Epoch 10862: train loss: 0.1770, test loss 0.8334\n",
      "Epoch 10863: train loss: 0.1770, test loss 0.8334\n",
      "Epoch 10864: train loss: 0.1770, test loss 0.8333\n",
      "Epoch 10865: train loss: 0.1770, test loss 0.8333\n",
      "Epoch 10866: train loss: 0.1770, test loss 0.8333\n",
      "Epoch 10867: train loss: 0.1770, test loss 0.8333\n",
      "Epoch 10868: train loss: 0.1770, test loss 0.8332\n",
      "Epoch 10869: train loss: 0.1770, test loss 0.8332\n",
      "Epoch 10870: train loss: 0.1770, test loss 0.8332\n",
      "Epoch 10871: train loss: 0.1770, test loss 0.8332\n",
      "Epoch 10872: train loss: 0.1770, test loss 0.8331\n",
      "Epoch 10873: train loss: 0.1770, test loss 0.8331\n",
      "Epoch 10874: train loss: 0.1770, test loss 0.8331\n",
      "Epoch 10875: train loss: 0.1770, test loss 0.8331\n",
      "Epoch 10876: train loss: 0.1770, test loss 0.8330\n",
      "Epoch 10877: train loss: 0.1770, test loss 0.8330\n",
      "Epoch 10878: train loss: 0.1770, test loss 0.8330\n",
      "Epoch 10879: train loss: 0.1770, test loss 0.8330\n",
      "Epoch 10880: train loss: 0.1770, test loss 0.8330\n",
      "Epoch 10881: train loss: 0.1770, test loss 0.8329\n",
      "Epoch 10882: train loss: 0.1770, test loss 0.8329\n",
      "Epoch 10883: train loss: 0.1770, test loss 0.8329\n",
      "Epoch 10884: train loss: 0.1770, test loss 0.8329\n",
      "Epoch 10885: train loss: 0.1770, test loss 0.8328\n",
      "Epoch 10886: train loss: 0.1770, test loss 0.8328\n",
      "Epoch 10887: train loss: 0.1770, test loss 0.8328\n",
      "Epoch 10888: train loss: 0.1770, test loss 0.8328\n",
      "Epoch 10889: train loss: 0.1770, test loss 0.8327\n",
      "Epoch 10890: train loss: 0.1770, test loss 0.8327\n",
      "Epoch 10891: train loss: 0.1770, test loss 0.8327\n",
      "Epoch 10892: train loss: 0.1770, test loss 0.8327\n",
      "Epoch 10893: train loss: 0.1770, test loss 0.8326\n",
      "Epoch 10894: train loss: 0.1770, test loss 0.8326\n",
      "Epoch 10895: train loss: 0.1770, test loss 0.8326\n",
      "Epoch 10896: train loss: 0.1770, test loss 0.8326\n",
      "Epoch 10897: train loss: 0.1770, test loss 0.8325\n",
      "Epoch 10898: train loss: 0.1770, test loss 0.8325\n",
      "Epoch 10899: train loss: 0.1770, test loss 0.8325\n",
      "Epoch 10900: train loss: 0.1770, test loss 0.8325\n",
      "Epoch 10901: train loss: 0.1770, test loss 0.8324\n",
      "Epoch 10902: train loss: 0.1770, test loss 0.8324\n",
      "Epoch 10903: train loss: 0.1770, test loss 0.8324\n",
      "Epoch 10904: train loss: 0.1770, test loss 0.8324\n",
      "Epoch 10905: train loss: 0.1770, test loss 0.8323\n",
      "Epoch 10906: train loss: 0.1770, test loss 0.8323\n",
      "Epoch 10907: train loss: 0.1770, test loss 0.8323\n",
      "Epoch 10908: train loss: 0.1770, test loss 0.8323\n",
      "Epoch 10909: train loss: 0.1770, test loss 0.8323\n",
      "Epoch 10910: train loss: 0.1770, test loss 0.8322\n",
      "Epoch 10911: train loss: 0.1769, test loss 0.8322\n",
      "Epoch 10912: train loss: 0.1769, test loss 0.8322\n",
      "Epoch 10913: train loss: 0.1769, test loss 0.8322\n",
      "Epoch 10914: train loss: 0.1769, test loss 0.8321\n",
      "Epoch 10915: train loss: 0.1769, test loss 0.8321\n",
      "Epoch 10916: train loss: 0.1769, test loss 0.8321\n",
      "Epoch 10917: train loss: 0.1769, test loss 0.8321\n",
      "Epoch 10918: train loss: 0.1769, test loss 0.8320\n",
      "Epoch 10919: train loss: 0.1769, test loss 0.8320\n",
      "Epoch 10920: train loss: 0.1769, test loss 0.8320\n",
      "Epoch 10921: train loss: 0.1769, test loss 0.8320\n",
      "Epoch 10922: train loss: 0.1769, test loss 0.8319\n",
      "Epoch 10923: train loss: 0.1769, test loss 0.8319\n",
      "Epoch 10924: train loss: 0.1769, test loss 0.8319\n",
      "Epoch 10925: train loss: 0.1769, test loss 0.8319\n",
      "Epoch 10926: train loss: 0.1769, test loss 0.8318\n",
      "Epoch 10927: train loss: 0.1769, test loss 0.8318\n",
      "Epoch 10928: train loss: 0.1769, test loss 0.8318\n",
      "Epoch 10929: train loss: 0.1769, test loss 0.8318\n",
      "Epoch 10930: train loss: 0.1769, test loss 0.8317\n",
      "Epoch 10931: train loss: 0.1769, test loss 0.8317\n",
      "Epoch 10932: train loss: 0.1769, test loss 0.8317\n",
      "Epoch 10933: train loss: 0.1769, test loss 0.8317\n",
      "Epoch 10934: train loss: 0.1769, test loss 0.8316\n",
      "Epoch 10935: train loss: 0.1769, test loss 0.8316\n",
      "Epoch 10936: train loss: 0.1769, test loss 0.8316\n",
      "Epoch 10937: train loss: 0.1769, test loss 0.8316\n",
      "Epoch 10938: train loss: 0.1769, test loss 0.8316\n",
      "Epoch 10939: train loss: 0.1769, test loss 0.8315\n",
      "Epoch 10940: train loss: 0.1769, test loss 0.8315\n",
      "Epoch 10941: train loss: 0.1769, test loss 0.8315\n",
      "Epoch 10942: train loss: 0.1769, test loss 0.8315\n",
      "Epoch 10943: train loss: 0.1769, test loss 0.8314\n",
      "Epoch 10944: train loss: 0.1769, test loss 0.8314\n",
      "Epoch 10945: train loss: 0.1769, test loss 0.8314\n",
      "Epoch 10946: train loss: 0.1769, test loss 0.8314\n",
      "Epoch 10947: train loss: 0.1769, test loss 0.8313\n",
      "Epoch 10948: train loss: 0.1769, test loss 0.8313\n",
      "Epoch 10949: train loss: 0.1769, test loss 0.8313\n",
      "Epoch 10950: train loss: 0.1769, test loss 0.8313\n",
      "Epoch 10951: train loss: 0.1769, test loss 0.8312\n",
      "Epoch 10952: train loss: 0.1769, test loss 0.8312\n",
      "Epoch 10953: train loss: 0.1769, test loss 0.8312\n",
      "Epoch 10954: train loss: 0.1769, test loss 0.8312\n",
      "Epoch 10955: train loss: 0.1769, test loss 0.8312\n",
      "Epoch 10956: train loss: 0.1769, test loss 0.8311\n",
      "Epoch 10957: train loss: 0.1769, test loss 0.8311\n",
      "Epoch 10958: train loss: 0.1769, test loss 0.8311\n",
      "Epoch 10959: train loss: 0.1769, test loss 0.8311\n",
      "Epoch 10960: train loss: 0.1769, test loss 0.8310\n",
      "Epoch 10961: train loss: 0.1769, test loss 0.8310\n",
      "Epoch 10962: train loss: 0.1769, test loss 0.8310\n",
      "Epoch 10963: train loss: 0.1769, test loss 0.8310\n",
      "Epoch 10964: train loss: 0.1769, test loss 0.8309\n",
      "Epoch 10965: train loss: 0.1769, test loss 0.8309\n",
      "Epoch 10966: train loss: 0.1769, test loss 0.8309\n",
      "Epoch 10967: train loss: 0.1769, test loss 0.8309\n",
      "Epoch 10968: train loss: 0.1769, test loss 0.8308\n",
      "Epoch 10969: train loss: 0.1769, test loss 0.8308\n",
      "Epoch 10970: train loss: 0.1769, test loss 0.8308\n",
      "Epoch 10971: train loss: 0.1769, test loss 0.8308\n",
      "Epoch 10972: train loss: 0.1769, test loss 0.8308\n",
      "Epoch 10973: train loss: 0.1769, test loss 0.8307\n",
      "Epoch 10974: train loss: 0.1769, test loss 0.8307\n",
      "Epoch 10975: train loss: 0.1769, test loss 0.8307\n",
      "Epoch 10976: train loss: 0.1769, test loss 0.8307\n",
      "Epoch 10977: train loss: 0.1769, test loss 0.8306\n",
      "Epoch 10978: train loss: 0.1769, test loss 0.8306\n",
      "Epoch 10979: train loss: 0.1769, test loss 0.8306\n",
      "Epoch 10980: train loss: 0.1769, test loss 0.8306\n",
      "Epoch 10981: train loss: 0.1769, test loss 0.8305\n",
      "Epoch 10982: train loss: 0.1769, test loss 0.8305\n",
      "Epoch 10983: train loss: 0.1769, test loss 0.8305\n",
      "Epoch 10984: train loss: 0.1769, test loss 0.8305\n",
      "Epoch 10985: train loss: 0.1769, test loss 0.8304\n",
      "Epoch 10986: train loss: 0.1769, test loss 0.8304\n",
      "Epoch 10987: train loss: 0.1769, test loss 0.8304\n",
      "Epoch 10988: train loss: 0.1769, test loss 0.8304\n",
      "Epoch 10989: train loss: 0.1769, test loss 0.8304\n",
      "Epoch 10990: train loss: 0.1769, test loss 0.8303\n",
      "Epoch 10991: train loss: 0.1769, test loss 0.8303\n",
      "Epoch 10992: train loss: 0.1769, test loss 0.8303\n",
      "Epoch 10993: train loss: 0.1769, test loss 0.8303\n",
      "Epoch 10994: train loss: 0.1769, test loss 0.8302\n",
      "Epoch 10995: train loss: 0.1769, test loss 0.8302\n",
      "Epoch 10996: train loss: 0.1769, test loss 0.8302\n",
      "Epoch 10997: train loss: 0.1769, test loss 0.8302\n",
      "Epoch 10998: train loss: 0.1769, test loss 0.8301\n",
      "Epoch 10999: train loss: 0.1769, test loss 0.8301\n",
      "Epoch 11000: train loss: 0.1769, test loss 0.8301\n",
      "Epoch 11001: train loss: 0.1769, test loss 0.8301\n",
      "Epoch 11002: train loss: 0.1769, test loss 0.8300\n",
      "Epoch 11003: train loss: 0.1769, test loss 0.8300\n",
      "Epoch 11004: train loss: 0.1769, test loss 0.8300\n",
      "Epoch 11005: train loss: 0.1769, test loss 0.8300\n",
      "Epoch 11006: train loss: 0.1769, test loss 0.8300\n",
      "Epoch 11007: train loss: 0.1769, test loss 0.8299\n",
      "Epoch 11008: train loss: 0.1769, test loss 0.8299\n",
      "Epoch 11009: train loss: 0.1769, test loss 0.8299\n",
      "Epoch 11010: train loss: 0.1769, test loss 0.8299\n",
      "Epoch 11011: train loss: 0.1769, test loss 0.8298\n",
      "Epoch 11012: train loss: 0.1769, test loss 0.8298\n",
      "Epoch 11013: train loss: 0.1769, test loss 0.8298\n",
      "Epoch 11014: train loss: 0.1769, test loss 0.8298\n",
      "Epoch 11015: train loss: 0.1769, test loss 0.8297\n",
      "Epoch 11016: train loss: 0.1769, test loss 0.8297\n",
      "Epoch 11017: train loss: 0.1768, test loss 0.8297\n",
      "Epoch 11018: train loss: 0.1768, test loss 0.8297\n",
      "Epoch 11019: train loss: 0.1768, test loss 0.8296\n",
      "Epoch 11020: train loss: 0.1768, test loss 0.8296\n",
      "Epoch 11021: train loss: 0.1768, test loss 0.8296\n",
      "Epoch 11022: train loss: 0.1768, test loss 0.8296\n",
      "Epoch 11023: train loss: 0.1768, test loss 0.8296\n",
      "Epoch 11024: train loss: 0.1768, test loss 0.8295\n",
      "Epoch 11025: train loss: 0.1768, test loss 0.8295\n",
      "Epoch 11026: train loss: 0.1768, test loss 0.8295\n",
      "Epoch 11027: train loss: 0.1768, test loss 0.8295\n",
      "Epoch 11028: train loss: 0.1768, test loss 0.8294\n",
      "Epoch 11029: train loss: 0.1768, test loss 0.8294\n",
      "Epoch 11030: train loss: 0.1768, test loss 0.8294\n",
      "Epoch 11031: train loss: 0.1768, test loss 0.8294\n",
      "Epoch 11032: train loss: 0.1768, test loss 0.8293\n",
      "Epoch 11033: train loss: 0.1768, test loss 0.8293\n",
      "Epoch 11034: train loss: 0.1768, test loss 0.8293\n",
      "Epoch 11035: train loss: 0.1768, test loss 0.8293\n",
      "Epoch 11036: train loss: 0.1768, test loss 0.8292\n",
      "Epoch 11037: train loss: 0.1768, test loss 0.8292\n",
      "Epoch 11038: train loss: 0.1768, test loss 0.8292\n",
      "Epoch 11039: train loss: 0.1768, test loss 0.8292\n",
      "Epoch 11040: train loss: 0.1768, test loss 0.8292\n",
      "Epoch 11041: train loss: 0.1768, test loss 0.8291\n",
      "Epoch 11042: train loss: 0.1768, test loss 0.8291\n",
      "Epoch 11043: train loss: 0.1768, test loss 0.8291\n",
      "Epoch 11044: train loss: 0.1768, test loss 0.8291\n",
      "Epoch 11045: train loss: 0.1768, test loss 0.8290\n",
      "Epoch 11046: train loss: 0.1768, test loss 0.8290\n",
      "Epoch 11047: train loss: 0.1768, test loss 0.8290\n",
      "Epoch 11048: train loss: 0.1768, test loss 0.8290\n",
      "Epoch 11049: train loss: 0.1768, test loss 0.8289\n",
      "Epoch 11050: train loss: 0.1768, test loss 0.8289\n",
      "Epoch 11051: train loss: 0.1768, test loss 0.8289\n",
      "Epoch 11052: train loss: 0.1768, test loss 0.8289\n",
      "Epoch 11053: train loss: 0.1768, test loss 0.8288\n",
      "Epoch 11054: train loss: 0.1768, test loss 0.8288\n",
      "Epoch 11055: train loss: 0.1768, test loss 0.8288\n",
      "Epoch 11056: train loss: 0.1768, test loss 0.8288\n",
      "Epoch 11057: train loss: 0.1768, test loss 0.8288\n",
      "Epoch 11058: train loss: 0.1768, test loss 0.8287\n",
      "Epoch 11059: train loss: 0.1768, test loss 0.8287\n",
      "Epoch 11060: train loss: 0.1768, test loss 0.8287\n",
      "Epoch 11061: train loss: 0.1768, test loss 0.8287\n",
      "Epoch 11062: train loss: 0.1768, test loss 0.8286\n",
      "Epoch 11063: train loss: 0.1768, test loss 0.8286\n",
      "Epoch 11064: train loss: 0.1768, test loss 0.8286\n",
      "Epoch 11065: train loss: 0.1768, test loss 0.8286\n",
      "Epoch 11066: train loss: 0.1768, test loss 0.8285\n",
      "Epoch 11067: train loss: 0.1768, test loss 0.8285\n",
      "Epoch 11068: train loss: 0.1768, test loss 0.8285\n",
      "Epoch 11069: train loss: 0.1768, test loss 0.8285\n",
      "Epoch 11070: train loss: 0.1768, test loss 0.8285\n",
      "Epoch 11071: train loss: 0.1768, test loss 0.8284\n",
      "Epoch 11072: train loss: 0.1768, test loss 0.8284\n",
      "Epoch 11073: train loss: 0.1768, test loss 0.8284\n",
      "Epoch 11074: train loss: 0.1768, test loss 0.8284\n",
      "Epoch 11075: train loss: 0.1768, test loss 0.8283\n",
      "Epoch 11076: train loss: 0.1768, test loss 0.8283\n",
      "Epoch 11077: train loss: 0.1768, test loss 0.8283\n",
      "Epoch 11078: train loss: 0.1768, test loss 0.8283\n",
      "Epoch 11079: train loss: 0.1768, test loss 0.8282\n",
      "Epoch 11080: train loss: 0.1768, test loss 0.8282\n",
      "Epoch 11081: train loss: 0.1768, test loss 0.8282\n",
      "Epoch 11082: train loss: 0.1768, test loss 0.8282\n",
      "Epoch 11083: train loss: 0.1768, test loss 0.8281\n",
      "Epoch 11084: train loss: 0.1768, test loss 0.8281\n",
      "Epoch 11085: train loss: 0.1768, test loss 0.8281\n",
      "Epoch 11086: train loss: 0.1768, test loss 0.8281\n",
      "Epoch 11087: train loss: 0.1768, test loss 0.8281\n",
      "Epoch 11088: train loss: 0.1768, test loss 0.8280\n",
      "Epoch 11089: train loss: 0.1768, test loss 0.8280\n",
      "Epoch 11090: train loss: 0.1768, test loss 0.8280\n",
      "Epoch 11091: train loss: 0.1768, test loss 0.8280\n",
      "Epoch 11092: train loss: 0.1768, test loss 0.8280\n",
      "Epoch 11093: train loss: 0.1768, test loss 0.8279\n",
      "Epoch 11094: train loss: 0.1768, test loss 0.8279\n",
      "Epoch 11095: train loss: 0.1768, test loss 0.8279\n",
      "Epoch 11096: train loss: 0.1768, test loss 0.8279\n",
      "Epoch 11097: train loss: 0.1768, test loss 0.8279\n",
      "Epoch 11098: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11099: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11100: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11101: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11102: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11103: train loss: 0.1768, test loss 0.8278\n",
      "Epoch 11104: train loss: 0.1768, test loss 0.8277\n",
      "Epoch 11105: train loss: 0.1768, test loss 0.8277\n",
      "Epoch 11106: train loss: 0.1768, test loss 0.8277\n",
      "Epoch 11107: train loss: 0.1768, test loss 0.8277\n",
      "Epoch 11108: train loss: 0.1768, test loss 0.8277\n",
      "Epoch 11109: train loss: 0.1768, test loss 0.8276\n",
      "Epoch 11110: train loss: 0.1768, test loss 0.8276\n",
      "Epoch 11111: train loss: 0.1768, test loss 0.8276\n",
      "Epoch 11112: train loss: 0.1768, test loss 0.8276\n",
      "Epoch 11113: train loss: 0.1768, test loss 0.8276\n",
      "Epoch 11114: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11115: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11116: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11117: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11118: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11119: train loss: 0.1768, test loss 0.8275\n",
      "Epoch 11120: train loss: 0.1768, test loss 0.8274\n",
      "Epoch 11121: train loss: 0.1768, test loss 0.8274\n",
      "Epoch 11122: train loss: 0.1768, test loss 0.8274\n",
      "Epoch 11123: train loss: 0.1768, test loss 0.8274\n",
      "Epoch 11124: train loss: 0.1768, test loss 0.8274\n",
      "Epoch 11125: train loss: 0.1767, test loss 0.8273\n",
      "Epoch 11126: train loss: 0.1767, test loss 0.8273\n",
      "Epoch 11127: train loss: 0.1767, test loss 0.8273\n",
      "Epoch 11128: train loss: 0.1767, test loss 0.8273\n",
      "Epoch 11129: train loss: 0.1767, test loss 0.8273\n",
      "Epoch 11130: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11131: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11132: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11133: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11134: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11135: train loss: 0.1767, test loss 0.8272\n",
      "Epoch 11136: train loss: 0.1767, test loss 0.8271\n",
      "Epoch 11137: train loss: 0.1767, test loss 0.8271\n",
      "Epoch 11138: train loss: 0.1767, test loss 0.8271\n",
      "Epoch 11139: train loss: 0.1767, test loss 0.8271\n",
      "Epoch 11140: train loss: 0.1767, test loss 0.8271\n",
      "Epoch 11141: train loss: 0.1767, test loss 0.8270\n",
      "Epoch 11142: train loss: 0.1767, test loss 0.8270\n",
      "Epoch 11143: train loss: 0.1767, test loss 0.8270\n",
      "Epoch 11144: train loss: 0.1767, test loss 0.8270\n",
      "Epoch 11145: train loss: 0.1767, test loss 0.8270\n",
      "Epoch 11146: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11147: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11148: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11149: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11150: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11151: train loss: 0.1767, test loss 0.8269\n",
      "Epoch 11152: train loss: 0.1767, test loss 0.8268\n",
      "Epoch 11153: train loss: 0.1767, test loss 0.8268\n",
      "Epoch 11154: train loss: 0.1767, test loss 0.8268\n",
      "Epoch 11155: train loss: 0.1767, test loss 0.8268\n",
      "Epoch 11156: train loss: 0.1767, test loss 0.8268\n",
      "Epoch 11157: train loss: 0.1767, test loss 0.8267\n",
      "Epoch 11158: train loss: 0.1767, test loss 0.8267\n",
      "Epoch 11159: train loss: 0.1767, test loss 0.8267\n",
      "Epoch 11160: train loss: 0.1767, test loss 0.8267\n",
      "Epoch 11161: train loss: 0.1767, test loss 0.8267\n",
      "Epoch 11162: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11163: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11164: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11165: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11166: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11167: train loss: 0.1767, test loss 0.8266\n",
      "Epoch 11168: train loss: 0.1767, test loss 0.8265\n",
      "Epoch 11169: train loss: 0.1767, test loss 0.8265\n",
      "Epoch 11170: train loss: 0.1767, test loss 0.8265\n",
      "Epoch 11171: train loss: 0.1767, test loss 0.8265\n",
      "Epoch 11172: train loss: 0.1767, test loss 0.8265\n",
      "Epoch 11173: train loss: 0.1767, test loss 0.8264\n",
      "Epoch 11174: train loss: 0.1767, test loss 0.8264\n",
      "Epoch 11175: train loss: 0.1767, test loss 0.8264\n",
      "Epoch 11176: train loss: 0.1767, test loss 0.8264\n",
      "Epoch 11177: train loss: 0.1767, test loss 0.8264\n",
      "Epoch 11178: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11179: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11180: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11181: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11182: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11183: train loss: 0.1767, test loss 0.8263\n",
      "Epoch 11184: train loss: 0.1767, test loss 0.8262\n",
      "Epoch 11185: train loss: 0.1767, test loss 0.8262\n",
      "Epoch 11186: train loss: 0.1767, test loss 0.8262\n",
      "Epoch 11187: train loss: 0.1767, test loss 0.8262\n",
      "Epoch 11188: train loss: 0.1767, test loss 0.8262\n",
      "Epoch 11189: train loss: 0.1767, test loss 0.8261\n",
      "Epoch 11190: train loss: 0.1767, test loss 0.8261\n",
      "Epoch 11191: train loss: 0.1767, test loss 0.8261\n",
      "Epoch 11192: train loss: 0.1767, test loss 0.8261\n",
      "Epoch 11193: train loss: 0.1767, test loss 0.8261\n",
      "Epoch 11194: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11195: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11196: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11197: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11198: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11199: train loss: 0.1767, test loss 0.8260\n",
      "Epoch 11200: train loss: 0.1767, test loss 0.8259\n",
      "Epoch 11201: train loss: 0.1767, test loss 0.8259\n",
      "Epoch 11202: train loss: 0.1767, test loss 0.8259\n",
      "Epoch 11203: train loss: 0.1767, test loss 0.8259\n",
      "Epoch 11204: train loss: 0.1767, test loss 0.8259\n",
      "Epoch 11205: train loss: 0.1767, test loss 0.8258\n",
      "Epoch 11206: train loss: 0.1767, test loss 0.8258\n",
      "Epoch 11207: train loss: 0.1767, test loss 0.8258\n",
      "Epoch 11208: train loss: 0.1767, test loss 0.8258\n",
      "Epoch 11209: train loss: 0.1767, test loss 0.8258\n",
      "Epoch 11210: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11211: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11212: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11213: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11214: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11215: train loss: 0.1767, test loss 0.8257\n",
      "Epoch 11216: train loss: 0.1767, test loss 0.8256\n",
      "Epoch 11217: train loss: 0.1767, test loss 0.8256\n",
      "Epoch 11218: train loss: 0.1767, test loss 0.8256\n",
      "Epoch 11219: train loss: 0.1767, test loss 0.8256\n",
      "Epoch 11220: train loss: 0.1767, test loss 0.8256\n",
      "Epoch 11221: train loss: 0.1767, test loss 0.8255\n",
      "Epoch 11222: train loss: 0.1767, test loss 0.8255\n",
      "Epoch 11223: train loss: 0.1767, test loss 0.8255\n",
      "Epoch 11224: train loss: 0.1767, test loss 0.8255\n",
      "Epoch 11225: train loss: 0.1767, test loss 0.8255\n",
      "Epoch 11226: train loss: 0.1767, test loss 0.8254\n",
      "Epoch 11227: train loss: 0.1767, test loss 0.8254\n",
      "Epoch 11228: train loss: 0.1767, test loss 0.8254\n",
      "Epoch 11229: train loss: 0.1767, test loss 0.8254\n",
      "Epoch 11230: train loss: 0.1767, test loss 0.8254\n",
      "Epoch 11231: train loss: 0.1766, test loss 0.8254\n",
      "Epoch 11232: train loss: 0.1766, test loss 0.8253\n",
      "Epoch 11233: train loss: 0.1766, test loss 0.8253\n",
      "Epoch 11234: train loss: 0.1766, test loss 0.8253\n",
      "Epoch 11235: train loss: 0.1766, test loss 0.8253\n",
      "Epoch 11236: train loss: 0.1766, test loss 0.8253\n",
      "Epoch 11237: train loss: 0.1766, test loss 0.8252\n",
      "Epoch 11238: train loss: 0.1766, test loss 0.8252\n",
      "Epoch 11239: train loss: 0.1766, test loss 0.8252\n",
      "Epoch 11240: train loss: 0.1766, test loss 0.8252\n",
      "Epoch 11241: train loss: 0.1766, test loss 0.8252\n",
      "Epoch 11242: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11243: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11244: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11245: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11246: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11247: train loss: 0.1766, test loss 0.8251\n",
      "Epoch 11248: train loss: 0.1766, test loss 0.8250\n",
      "Epoch 11249: train loss: 0.1766, test loss 0.8250\n",
      "Epoch 11250: train loss: 0.1766, test loss 0.8250\n",
      "Epoch 11251: train loss: 0.1766, test loss 0.8250\n",
      "Epoch 11252: train loss: 0.1766, test loss 0.8250\n",
      "Epoch 11253: train loss: 0.1766, test loss 0.8249\n",
      "Epoch 11254: train loss: 0.1766, test loss 0.8249\n",
      "Epoch 11255: train loss: 0.1766, test loss 0.8249\n",
      "Epoch 11256: train loss: 0.1766, test loss 0.8249\n",
      "Epoch 11257: train loss: 0.1766, test loss 0.8249\n",
      "Epoch 11258: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11259: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11260: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11261: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11262: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11263: train loss: 0.1766, test loss 0.8248\n",
      "Epoch 11264: train loss: 0.1766, test loss 0.8247\n",
      "Epoch 11265: train loss: 0.1766, test loss 0.8247\n",
      "Epoch 11266: train loss: 0.1766, test loss 0.8247\n",
      "Epoch 11267: train loss: 0.1766, test loss 0.8247\n",
      "Epoch 11268: train loss: 0.1766, test loss 0.8247\n",
      "Epoch 11269: train loss: 0.1766, test loss 0.8246\n",
      "Epoch 11270: train loss: 0.1766, test loss 0.8246\n",
      "Epoch 11271: train loss: 0.1766, test loss 0.8246\n",
      "Epoch 11272: train loss: 0.1766, test loss 0.8246\n",
      "Epoch 11273: train loss: 0.1766, test loss 0.8246\n",
      "Epoch 11274: train loss: 0.1766, test loss 0.8245\n",
      "Epoch 11275: train loss: 0.1766, test loss 0.8245\n",
      "Epoch 11276: train loss: 0.1766, test loss 0.8245\n",
      "Epoch 11277: train loss: 0.1766, test loss 0.8245\n",
      "Epoch 11278: train loss: 0.1766, test loss 0.8245\n",
      "Epoch 11279: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11280: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11281: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11282: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11283: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11284: train loss: 0.1766, test loss 0.8244\n",
      "Epoch 11285: train loss: 0.1766, test loss 0.8243\n",
      "Epoch 11286: train loss: 0.1766, test loss 0.8243\n",
      "Epoch 11287: train loss: 0.1766, test loss 0.8243\n",
      "Epoch 11288: train loss: 0.1766, test loss 0.8243\n",
      "Epoch 11289: train loss: 0.1766, test loss 0.8243\n",
      "Epoch 11290: train loss: 0.1766, test loss 0.8242\n",
      "Epoch 11291: train loss: 0.1766, test loss 0.8242\n",
      "Epoch 11292: train loss: 0.1766, test loss 0.8242\n",
      "Epoch 11293: train loss: 0.1766, test loss 0.8242\n",
      "Epoch 11294: train loss: 0.1766, test loss 0.8242\n",
      "Epoch 11295: train loss: 0.1766, test loss 0.8241\n",
      "Epoch 11296: train loss: 0.1766, test loss 0.8241\n",
      "Epoch 11297: train loss: 0.1766, test loss 0.8241\n",
      "Epoch 11298: train loss: 0.1766, test loss 0.8241\n",
      "Epoch 11299: train loss: 0.1766, test loss 0.8241\n",
      "Epoch 11300: train loss: 0.1766, test loss 0.8240\n",
      "Epoch 11301: train loss: 0.1766, test loss 0.8240\n",
      "Epoch 11302: train loss: 0.1766, test loss 0.8240\n",
      "Epoch 11303: train loss: 0.1766, test loss 0.8240\n",
      "Epoch 11304: train loss: 0.1766, test loss 0.8240\n",
      "Epoch 11305: train loss: 0.1766, test loss 0.8239\n",
      "Epoch 11306: train loss: 0.1766, test loss 0.8239\n",
      "Epoch 11307: train loss: 0.1766, test loss 0.8239\n",
      "Epoch 11308: train loss: 0.1766, test loss 0.8239\n",
      "Epoch 11309: train loss: 0.1766, test loss 0.8239\n",
      "Epoch 11310: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11311: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11312: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11313: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11314: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11315: train loss: 0.1766, test loss 0.8238\n",
      "Epoch 11316: train loss: 0.1766, test loss 0.8237\n",
      "Epoch 11317: train loss: 0.1766, test loss 0.8237\n",
      "Epoch 11318: train loss: 0.1766, test loss 0.8237\n",
      "Epoch 11319: train loss: 0.1766, test loss 0.8237\n",
      "Epoch 11320: train loss: 0.1766, test loss 0.8237\n",
      "Epoch 11321: train loss: 0.1766, test loss 0.8236\n",
      "Epoch 11322: train loss: 0.1766, test loss 0.8236\n",
      "Epoch 11323: train loss: 0.1766, test loss 0.8236\n",
      "Epoch 11324: train loss: 0.1766, test loss 0.8236\n",
      "Epoch 11325: train loss: 0.1766, test loss 0.8236\n",
      "Epoch 11326: train loss: 0.1766, test loss 0.8235\n",
      "Epoch 11327: train loss: 0.1766, test loss 0.8235\n",
      "Epoch 11328: train loss: 0.1766, test loss 0.8235\n",
      "Epoch 11329: train loss: 0.1766, test loss 0.8235\n",
      "Epoch 11330: train loss: 0.1766, test loss 0.8235\n",
      "Epoch 11331: train loss: 0.1766, test loss 0.8234\n",
      "Epoch 11332: train loss: 0.1766, test loss 0.8234\n",
      "Epoch 11333: train loss: 0.1766, test loss 0.8234\n",
      "Epoch 11334: train loss: 0.1766, test loss 0.8234\n",
      "Epoch 11335: train loss: 0.1766, test loss 0.8234\n",
      "Epoch 11336: train loss: 0.1766, test loss 0.8233\n",
      "Epoch 11337: train loss: 0.1766, test loss 0.8233\n",
      "Epoch 11338: train loss: 0.1766, test loss 0.8233\n",
      "Epoch 11339: train loss: 0.1766, test loss 0.8233\n",
      "Epoch 11340: train loss: 0.1765, test loss 0.8233\n",
      "Epoch 11341: train loss: 0.1765, test loss 0.8232\n",
      "Epoch 11342: train loss: 0.1765, test loss 0.8232\n",
      "Epoch 11343: train loss: 0.1765, test loss 0.8232\n",
      "Epoch 11344: train loss: 0.1765, test loss 0.8232\n",
      "Epoch 11345: train loss: 0.1765, test loss 0.8232\n",
      "Epoch 11346: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11347: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11348: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11349: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11350: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11351: train loss: 0.1765, test loss 0.8231\n",
      "Epoch 11352: train loss: 0.1765, test loss 0.8230\n",
      "Epoch 11353: train loss: 0.1765, test loss 0.8230\n",
      "Epoch 11354: train loss: 0.1765, test loss 0.8230\n",
      "Epoch 11355: train loss: 0.1765, test loss 0.8230\n",
      "Epoch 11356: train loss: 0.1765, test loss 0.8230\n",
      "Epoch 11357: train loss: 0.1765, test loss 0.8229\n",
      "Epoch 11358: train loss: 0.1765, test loss 0.8229\n",
      "Epoch 11359: train loss: 0.1765, test loss 0.8229\n",
      "Epoch 11360: train loss: 0.1765, test loss 0.8229\n",
      "Epoch 11361: train loss: 0.1765, test loss 0.8229\n",
      "Epoch 11362: train loss: 0.1765, test loss 0.8228\n",
      "Epoch 11363: train loss: 0.1765, test loss 0.8228\n",
      "Epoch 11364: train loss: 0.1765, test loss 0.8228\n",
      "Epoch 11365: train loss: 0.1765, test loss 0.8228\n",
      "Epoch 11366: train loss: 0.1765, test loss 0.8228\n",
      "Epoch 11367: train loss: 0.1765, test loss 0.8227\n",
      "Epoch 11368: train loss: 0.1765, test loss 0.8227\n",
      "Epoch 11369: train loss: 0.1765, test loss 0.8227\n",
      "Epoch 11370: train loss: 0.1765, test loss 0.8227\n",
      "Epoch 11371: train loss: 0.1765, test loss 0.8227\n",
      "Epoch 11372: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11373: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11374: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11375: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11376: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11377: train loss: 0.1765, test loss 0.8226\n",
      "Epoch 11378: train loss: 0.1765, test loss 0.8225\n",
      "Epoch 11379: train loss: 0.1765, test loss 0.8225\n",
      "Epoch 11380: train loss: 0.1765, test loss 0.8225\n",
      "Epoch 11381: train loss: 0.1765, test loss 0.8225\n",
      "Epoch 11382: train loss: 0.1765, test loss 0.8225\n",
      "Epoch 11383: train loss: 0.1765, test loss 0.8224\n",
      "Epoch 11384: train loss: 0.1765, test loss 0.8224\n",
      "Epoch 11385: train loss: 0.1765, test loss 0.8224\n",
      "Epoch 11386: train loss: 0.1765, test loss 0.8224\n",
      "Epoch 11387: train loss: 0.1765, test loss 0.8224\n",
      "Epoch 11388: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11389: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11390: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11391: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11392: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11393: train loss: 0.1765, test loss 0.8223\n",
      "Epoch 11394: train loss: 0.1765, test loss 0.8222\n",
      "Epoch 11395: train loss: 0.1765, test loss 0.8222\n",
      "Epoch 11396: train loss: 0.1765, test loss 0.8222\n",
      "Epoch 11397: train loss: 0.1765, test loss 0.8222\n",
      "Epoch 11398: train loss: 0.1765, test loss 0.8222\n",
      "Epoch 11399: train loss: 0.1765, test loss 0.8221\n",
      "Epoch 11400: train loss: 0.1765, test loss 0.8221\n",
      "Epoch 11401: train loss: 0.1765, test loss 0.8221\n",
      "Epoch 11402: train loss: 0.1765, test loss 0.8221\n",
      "Epoch 11403: train loss: 0.1765, test loss 0.8221\n",
      "Epoch 11404: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11405: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11406: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11407: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11408: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11409: train loss: 0.1765, test loss 0.8220\n",
      "Epoch 11410: train loss: 0.1765, test loss 0.8219\n",
      "Epoch 11411: train loss: 0.1765, test loss 0.8219\n",
      "Epoch 11412: train loss: 0.1765, test loss 0.8219\n",
      "Epoch 11413: train loss: 0.1765, test loss 0.8219\n",
      "Epoch 11414: train loss: 0.1765, test loss 0.8219\n",
      "Epoch 11415: train loss: 0.1765, test loss 0.8218\n",
      "Epoch 11416: train loss: 0.1765, test loss 0.8218\n",
      "Epoch 11417: train loss: 0.1765, test loss 0.8218\n",
      "Epoch 11418: train loss: 0.1765, test loss 0.8218\n",
      "Epoch 11419: train loss: 0.1765, test loss 0.8218\n",
      "Epoch 11420: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11421: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11422: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11423: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11424: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11425: train loss: 0.1765, test loss 0.8217\n",
      "Epoch 11426: train loss: 0.1765, test loss 0.8216\n",
      "Epoch 11427: train loss: 0.1765, test loss 0.8216\n",
      "Epoch 11428: train loss: 0.1765, test loss 0.8216\n",
      "Epoch 11429: train loss: 0.1765, test loss 0.8216\n",
      "Epoch 11430: train loss: 0.1765, test loss 0.8216\n",
      "Epoch 11431: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11432: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11433: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11434: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11435: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11436: train loss: 0.1765, test loss 0.8215\n",
      "Epoch 11437: train loss: 0.1765, test loss 0.8214\n",
      "Epoch 11438: train loss: 0.1765, test loss 0.8214\n",
      "Epoch 11439: train loss: 0.1765, test loss 0.8214\n",
      "Epoch 11440: train loss: 0.1765, test loss 0.8214\n",
      "Epoch 11441: train loss: 0.1765, test loss 0.8214\n",
      "Epoch 11442: train loss: 0.1765, test loss 0.8213\n",
      "Epoch 11443: train loss: 0.1765, test loss 0.8213\n",
      "Epoch 11444: train loss: 0.1765, test loss 0.8213\n",
      "Epoch 11445: train loss: 0.1765, test loss 0.8213\n",
      "Epoch 11446: train loss: 0.1765, test loss 0.8213\n",
      "Epoch 11447: train loss: 0.1765, test loss 0.8212\n",
      "Epoch 11448: train loss: 0.1765, test loss 0.8212\n",
      "Epoch 11449: train loss: 0.1765, test loss 0.8212\n",
      "Epoch 11450: train loss: 0.1764, test loss 0.8212\n",
      "Epoch 11451: train loss: 0.1764, test loss 0.8212\n",
      "Epoch 11452: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11453: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11454: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11455: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11456: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11457: train loss: 0.1764, test loss 0.8211\n",
      "Epoch 11458: train loss: 0.1764, test loss 0.8210\n",
      "Epoch 11459: train loss: 0.1764, test loss 0.8210\n",
      "Epoch 11460: train loss: 0.1764, test loss 0.8210\n",
      "Epoch 11461: train loss: 0.1764, test loss 0.8210\n",
      "Epoch 11462: train loss: 0.1764, test loss 0.8210\n",
      "Epoch 11463: train loss: 0.1764, test loss 0.8209\n",
      "Epoch 11464: train loss: 0.1764, test loss 0.8209\n",
      "Epoch 11465: train loss: 0.1764, test loss 0.8209\n",
      "Epoch 11466: train loss: 0.1764, test loss 0.8209\n",
      "Epoch 11467: train loss: 0.1764, test loss 0.8209\n",
      "Epoch 11468: train loss: 0.1764, test loss 0.8208\n",
      "Epoch 11469: train loss: 0.1764, test loss 0.8208\n",
      "Epoch 11470: train loss: 0.1764, test loss 0.8208\n",
      "Epoch 11471: train loss: 0.1764, test loss 0.8208\n",
      "Epoch 11472: train loss: 0.1764, test loss 0.8208\n",
      "Epoch 11473: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11474: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11475: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11476: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11477: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11478: train loss: 0.1764, test loss 0.8207\n",
      "Epoch 11479: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11480: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11481: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11482: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11483: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11484: train loss: 0.1764, test loss 0.8206\n",
      "Epoch 11485: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11486: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11487: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11488: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11489: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11490: train loss: 0.1764, test loss 0.8205\n",
      "Epoch 11491: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11492: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11493: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11494: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11495: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11496: train loss: 0.1764, test loss 0.8204\n",
      "Epoch 11497: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11498: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11499: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11500: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11501: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11502: train loss: 0.1764, test loss 0.8203\n",
      "Epoch 11503: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11504: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11505: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11506: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11507: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11508: train loss: 0.1764, test loss 0.8202\n",
      "Epoch 11509: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11510: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11511: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11512: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11513: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11514: train loss: 0.1764, test loss 0.8201\n",
      "Epoch 11515: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11516: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11517: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11518: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11519: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11520: train loss: 0.1764, test loss 0.8200\n",
      "Epoch 11521: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11522: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11523: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11524: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11525: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11526: train loss: 0.1764, test loss 0.8199\n",
      "Epoch 11527: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11528: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11529: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11530: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11531: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11532: train loss: 0.1764, test loss 0.8198\n",
      "Epoch 11533: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11534: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11535: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11536: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11537: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11538: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11539: train loss: 0.1764, test loss 0.8197\n",
      "Epoch 11540: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11541: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11542: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11543: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11544: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11545: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11546: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11547: train loss: 0.1764, test loss 0.8196\n",
      "Epoch 11548: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11549: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11550: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11551: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11552: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11553: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11554: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11555: train loss: 0.1764, test loss 0.8195\n",
      "Epoch 11556: train loss: 0.1764, test loss 0.8194\n",
      "Epoch 11557: train loss: 0.1764, test loss 0.8194\n",
      "Epoch 11558: train loss: 0.1763, test loss 0.8194\n",
      "Epoch 11559: train loss: 0.1763, test loss 0.8194\n",
      "Epoch 11560: train loss: 0.1763, test loss 0.8194\n",
      "Epoch 11561: train loss: 0.1763, test loss 0.8194\n",
      "Epoch 11562: train loss: 0.1763, test loss 0.8194\n",
      "Epoch 11563: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11564: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11565: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11566: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11567: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11568: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11569: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11570: train loss: 0.1763, test loss 0.8193\n",
      "Epoch 11571: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11572: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11573: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11574: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11575: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11576: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11577: train loss: 0.1763, test loss 0.8192\n",
      "Epoch 11578: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11579: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11580: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11581: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11582: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11583: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11584: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11585: train loss: 0.1763, test loss 0.8191\n",
      "Epoch 11586: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11587: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11588: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11589: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11590: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11591: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11592: train loss: 0.1763, test loss 0.8190\n",
      "Epoch 11593: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11594: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11595: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11596: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11597: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11598: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11599: train loss: 0.1763, test loss 0.8189\n",
      "Epoch 11600: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11601: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11602: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11603: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11604: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11605: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11606: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11607: train loss: 0.1763, test loss 0.8188\n",
      "Epoch 11608: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11609: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11610: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11611: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11612: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11613: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11614: train loss: 0.1763, test loss 0.8187\n",
      "Epoch 11615: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11616: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11617: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11618: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11619: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11620: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11621: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11622: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11623: train loss: 0.1763, test loss 0.8186\n",
      "Epoch 11624: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11625: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11626: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11627: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11628: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11629: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11630: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11631: train loss: 0.1763, test loss 0.8185\n",
      "Epoch 11632: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11633: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11634: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11635: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11636: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11637: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11638: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11639: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11640: train loss: 0.1763, test loss 0.8184\n",
      "Epoch 11641: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11642: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11643: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11644: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11645: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11646: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11647: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11648: train loss: 0.1763, test loss 0.8183\n",
      "Epoch 11649: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11650: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11651: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11652: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11653: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11654: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11655: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11656: train loss: 0.1763, test loss 0.8182\n",
      "Epoch 11657: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11658: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11659: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11660: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11661: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11662: train loss: 0.1763, test loss 0.8181\n",
      "Epoch 11663: train loss: 0.1762, test loss 0.8181\n",
      "Epoch 11664: train loss: 0.1762, test loss 0.8181\n",
      "Epoch 11665: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11666: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11667: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11668: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11669: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11670: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11671: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11672: train loss: 0.1762, test loss 0.8180\n",
      "Epoch 11673: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11674: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11675: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11676: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11677: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11678: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11679: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11680: train loss: 0.1762, test loss 0.8179\n",
      "Epoch 11681: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11682: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11683: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11684: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11685: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11686: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11687: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11688: train loss: 0.1762, test loss 0.8178\n",
      "Epoch 11689: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11690: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11691: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11692: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11693: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11694: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11695: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11696: train loss: 0.1762, test loss 0.8177\n",
      "Epoch 11697: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11698: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11699: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11700: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11701: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11702: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11703: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11704: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11705: train loss: 0.1762, test loss 0.8176\n",
      "Epoch 11706: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11707: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11708: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11709: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11710: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11711: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11712: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11713: train loss: 0.1762, test loss 0.8175\n",
      "Epoch 11714: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11715: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11716: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11717: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11718: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11719: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11720: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11721: train loss: 0.1762, test loss 0.8174\n",
      "Epoch 11722: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11723: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11724: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11725: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11726: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11727: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11728: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11729: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11730: train loss: 0.1762, test loss 0.8173\n",
      "Epoch 11731: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11732: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11733: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11734: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11735: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11736: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11737: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11738: train loss: 0.1762, test loss 0.8172\n",
      "Epoch 11739: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11740: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11741: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11742: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11743: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11744: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11745: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11746: train loss: 0.1762, test loss 0.8171\n",
      "Epoch 11747: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11748: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11749: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11750: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11751: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11752: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11753: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11754: train loss: 0.1762, test loss 0.8170\n",
      "Epoch 11755: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11756: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11757: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11758: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11759: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11760: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11761: train loss: 0.1762, test loss 0.8169\n",
      "Epoch 11762: train loss: 0.1762, test loss 0.8168\n",
      "Epoch 11763: train loss: 0.1762, test loss 0.8168\n",
      "Epoch 11764: train loss: 0.1762, test loss 0.8168\n",
      "Epoch 11765: train loss: 0.1762, test loss 0.8168\n",
      "Epoch 11766: train loss: 0.1761, test loss 0.8168\n",
      "Epoch 11767: train loss: 0.1761, test loss 0.8168\n",
      "Epoch 11768: train loss: 0.1761, test loss 0.8168\n",
      "Epoch 11769: train loss: 0.1761, test loss 0.8168\n",
      "Epoch 11770: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11771: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11772: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11773: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11774: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11775: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11776: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11777: train loss: 0.1761, test loss 0.8167\n",
      "Epoch 11778: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11779: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11780: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11781: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11782: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11783: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11784: train loss: 0.1761, test loss 0.8166\n",
      "Epoch 11785: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11786: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11787: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11788: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11789: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11790: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11791: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11792: train loss: 0.1761, test loss 0.8165\n",
      "Epoch 11793: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11794: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11795: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11796: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11797: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11798: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11799: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11800: train loss: 0.1761, test loss 0.8164\n",
      "Epoch 11801: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11802: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11803: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11804: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11805: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11806: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11807: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11808: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11809: train loss: 0.1761, test loss 0.8163\n",
      "Epoch 11810: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11811: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11812: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11813: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11814: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11815: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11816: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11817: train loss: 0.1761, test loss 0.8162\n",
      "Epoch 11818: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11819: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11820: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11821: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11822: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11823: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11824: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11825: train loss: 0.1761, test loss 0.8161\n",
      "Epoch 11826: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11827: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11828: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11829: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11830: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11831: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11832: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11833: train loss: 0.1761, test loss 0.8160\n",
      "Epoch 11834: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11835: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11836: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11837: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11838: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11839: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11840: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11841: train loss: 0.1761, test loss 0.8159\n",
      "Epoch 11842: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11843: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11844: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11845: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11846: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11847: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11848: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11849: train loss: 0.1761, test loss 0.8158\n",
      "Epoch 11850: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11851: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11852: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11853: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11854: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11855: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11856: train loss: 0.1761, test loss 0.8157\n",
      "Epoch 11857: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11858: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11859: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11860: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11861: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11862: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11863: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11864: train loss: 0.1761, test loss 0.8156\n",
      "Epoch 11865: train loss: 0.1761, test loss 0.8155\n",
      "Epoch 11866: train loss: 0.1761, test loss 0.8155\n",
      "Epoch 11867: train loss: 0.1761, test loss 0.8155\n",
      "Epoch 11868: train loss: 0.1761, test loss 0.8155\n",
      "Epoch 11869: train loss: 0.1760, test loss 0.8155\n",
      "Epoch 11870: train loss: 0.1760, test loss 0.8155\n",
      "Epoch 11871: train loss: 0.1760, test loss 0.8155\n",
      "Epoch 11872: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11873: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11874: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11875: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11876: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11877: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11878: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11879: train loss: 0.1760, test loss 0.8154\n",
      "Epoch 11880: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11881: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11882: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11883: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11884: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11885: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11886: train loss: 0.1760, test loss 0.8153\n",
      "Epoch 11887: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11888: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11889: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11890: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11891: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11892: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11893: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11894: train loss: 0.1760, test loss 0.8152\n",
      "Epoch 11895: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11896: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11897: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11898: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11899: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11900: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11901: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11902: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11903: train loss: 0.1760, test loss 0.8151\n",
      "Epoch 11904: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11905: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11906: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11907: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11908: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11909: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11910: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11911: train loss: 0.1760, test loss 0.8150\n",
      "Epoch 11912: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11913: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11914: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11915: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11916: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11917: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11918: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11919: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11920: train loss: 0.1760, test loss 0.8149\n",
      "Epoch 11921: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11922: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11923: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11924: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11925: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11926: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11927: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11928: train loss: 0.1760, test loss 0.8148\n",
      "Epoch 11929: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11930: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11931: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11932: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11933: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11934: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11935: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11936: train loss: 0.1760, test loss 0.8147\n",
      "Epoch 11937: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11938: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11939: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11940: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11941: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11942: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11943: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11944: train loss: 0.1760, test loss 0.8146\n",
      "Epoch 11945: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11946: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11947: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11948: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11949: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11950: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11951: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11952: train loss: 0.1760, test loss 0.8145\n",
      "Epoch 11953: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11954: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11955: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11956: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11957: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11958: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11959: train loss: 0.1760, test loss 0.8144\n",
      "Epoch 11960: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11961: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11962: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11963: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11964: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11965: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11966: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11967: train loss: 0.1760, test loss 0.8143\n",
      "Epoch 11968: train loss: 0.1760, test loss 0.8142\n",
      "Epoch 11969: train loss: 0.1760, test loss 0.8142\n",
      "Epoch 11970: train loss: 0.1760, test loss 0.8142\n",
      "Epoch 11971: train loss: 0.1760, test loss 0.8142\n",
      "Epoch 11972: train loss: 0.1760, test loss 0.8142\n",
      "Epoch 11973: train loss: 0.1759, test loss 0.8142\n",
      "Epoch 11974: train loss: 0.1759, test loss 0.8142\n",
      "Epoch 11975: train loss: 0.1759, test loss 0.8142\n",
      "Epoch 11976: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11977: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11978: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11979: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11980: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11981: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11982: train loss: 0.1759, test loss 0.8141\n",
      "Epoch 11983: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11984: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11985: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11986: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11987: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11988: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11989: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11990: train loss: 0.1759, test loss 0.8140\n",
      "Epoch 11991: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11992: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11993: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11994: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11995: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11996: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11997: train loss: 0.1759, test loss 0.8139\n",
      "Epoch 11998: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 11999: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12000: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12001: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12002: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12003: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12004: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12005: train loss: 0.1759, test loss 0.8138\n",
      "Epoch 12006: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12007: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12008: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12009: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12010: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12011: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12012: train loss: 0.1759, test loss 0.8137\n",
      "Epoch 12013: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12014: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12015: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12016: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12017: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12018: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12019: train loss: 0.1759, test loss 0.8136\n",
      "Epoch 12020: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12021: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12022: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12023: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12024: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12025: train loss: 0.1759, test loss 0.8135\n",
      "Epoch 12026: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12027: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12028: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12029: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12030: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12031: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12032: train loss: 0.1759, test loss 0.8134\n",
      "Epoch 12033: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12034: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12035: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12036: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12037: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12038: train loss: 0.1759, test loss 0.8133\n",
      "Epoch 12039: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12040: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12041: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12042: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12043: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12044: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12045: train loss: 0.1759, test loss 0.8132\n",
      "Epoch 12046: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12047: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12048: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12049: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12050: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12051: train loss: 0.1759, test loss 0.8131\n",
      "Epoch 12052: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12053: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12054: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12055: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12056: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12057: train loss: 0.1759, test loss 0.8130\n",
      "Epoch 12058: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12059: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12060: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12061: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12062: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12063: train loss: 0.1759, test loss 0.8129\n",
      "Epoch 12064: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12065: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12066: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12067: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12068: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12069: train loss: 0.1759, test loss 0.8128\n",
      "Epoch 12070: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12071: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12072: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12073: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12074: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12075: train loss: 0.1759, test loss 0.8127\n",
      "Epoch 12076: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12077: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12078: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12079: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12080: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12081: train loss: 0.1759, test loss 0.8126\n",
      "Epoch 12082: train loss: 0.1758, test loss 0.8125\n",
      "Epoch 12083: train loss: 0.1758, test loss 0.8125\n",
      "Epoch 12084: train loss: 0.1758, test loss 0.8125\n",
      "Epoch 12085: train loss: 0.1758, test loss 0.8125\n",
      "Epoch 12086: train loss: 0.1758, test loss 0.8125\n",
      "Epoch 12087: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12088: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12089: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12090: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12091: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12092: train loss: 0.1758, test loss 0.8124\n",
      "Epoch 12093: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12094: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12095: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12096: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12097: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12098: train loss: 0.1758, test loss 0.8123\n",
      "Epoch 12099: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12100: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12101: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12102: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12103: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12104: train loss: 0.1758, test loss 0.8122\n",
      "Epoch 12105: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12106: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12107: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12108: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12109: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12110: train loss: 0.1758, test loss 0.8121\n",
      "Epoch 12111: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12112: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12113: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12114: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12115: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12116: train loss: 0.1758, test loss 0.8120\n",
      "Epoch 12117: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12118: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12119: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12120: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12121: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12122: train loss: 0.1758, test loss 0.8119\n",
      "Epoch 12123: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12124: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12125: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12126: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12127: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12128: train loss: 0.1758, test loss 0.8118\n",
      "Epoch 12129: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12130: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12131: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12132: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12133: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12134: train loss: 0.1758, test loss 0.8117\n",
      "Epoch 12135: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12136: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12137: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12138: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12139: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12140: train loss: 0.1758, test loss 0.8116\n",
      "Epoch 12141: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12142: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12143: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12144: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12145: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12146: train loss: 0.1758, test loss 0.8115\n",
      "Epoch 12147: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12148: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12149: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12150: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12151: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12152: train loss: 0.1758, test loss 0.8114\n",
      "Epoch 12153: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12154: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12155: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12156: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12157: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12158: train loss: 0.1758, test loss 0.8113\n",
      "Epoch 12159: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12160: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12161: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12162: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12163: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12164: train loss: 0.1758, test loss 0.8112\n",
      "Epoch 12165: train loss: 0.1758, test loss 0.8111\n",
      "Epoch 12166: train loss: 0.1758, test loss 0.8111\n",
      "Epoch 12167: train loss: 0.1758, test loss 0.8111\n",
      "Epoch 12168: train loss: 0.1758, test loss 0.8111\n",
      "Epoch 12169: train loss: 0.1758, test loss 0.8111\n",
      "Epoch 12170: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12171: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12172: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12173: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12174: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12175: train loss: 0.1758, test loss 0.8110\n",
      "Epoch 12176: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12177: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12178: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12179: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12180: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12181: train loss: 0.1758, test loss 0.8109\n",
      "Epoch 12182: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12183: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12184: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12185: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12186: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12187: train loss: 0.1758, test loss 0.8108\n",
      "Epoch 12188: train loss: 0.1758, test loss 0.8107\n",
      "Epoch 12189: train loss: 0.1758, test loss 0.8107\n",
      "Epoch 12190: train loss: 0.1758, test loss 0.8107\n",
      "Epoch 12191: train loss: 0.1758, test loss 0.8107\n",
      "Epoch 12192: train loss: 0.1758, test loss 0.8107\n",
      "Epoch 12193: train loss: 0.1758, test loss 0.8106\n",
      "Epoch 12194: train loss: 0.1757, test loss 0.8106\n",
      "Epoch 12195: train loss: 0.1757, test loss 0.8106\n",
      "Epoch 12196: train loss: 0.1757, test loss 0.8106\n",
      "Epoch 12197: train loss: 0.1757, test loss 0.8106\n",
      "Epoch 12198: train loss: 0.1757, test loss 0.8106\n",
      "Epoch 12199: train loss: 0.1757, test loss 0.8105\n",
      "Epoch 12200: train loss: 0.1757, test loss 0.8105\n",
      "Epoch 12201: train loss: 0.1757, test loss 0.8105\n",
      "Epoch 12202: train loss: 0.1757, test loss 0.8105\n",
      "Epoch 12203: train loss: 0.1757, test loss 0.8105\n",
      "Epoch 12204: train loss: 0.1757, test loss 0.8104\n",
      "Epoch 12205: train loss: 0.1757, test loss 0.8104\n",
      "Epoch 12206: train loss: 0.1757, test loss 0.8104\n",
      "Epoch 12207: train loss: 0.1757, test loss 0.8104\n",
      "Epoch 12208: train loss: 0.1757, test loss 0.8104\n",
      "Epoch 12209: train loss: 0.1757, test loss 0.8103\n",
      "Epoch 12210: train loss: 0.1757, test loss 0.8103\n",
      "Epoch 12211: train loss: 0.1757, test loss 0.8103\n",
      "Epoch 12212: train loss: 0.1757, test loss 0.8103\n",
      "Epoch 12213: train loss: 0.1757, test loss 0.8103\n",
      "Epoch 12214: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12215: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12216: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12217: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12218: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12219: train loss: 0.1757, test loss 0.8102\n",
      "Epoch 12220: train loss: 0.1757, test loss 0.8101\n",
      "Epoch 12221: train loss: 0.1757, test loss 0.8101\n",
      "Epoch 12222: train loss: 0.1757, test loss 0.8101\n",
      "Epoch 12223: train loss: 0.1757, test loss 0.8101\n",
      "Epoch 12224: train loss: 0.1757, test loss 0.8101\n",
      "Epoch 12225: train loss: 0.1757, test loss 0.8100\n",
      "Epoch 12226: train loss: 0.1757, test loss 0.8100\n",
      "Epoch 12227: train loss: 0.1757, test loss 0.8100\n",
      "Epoch 12228: train loss: 0.1757, test loss 0.8100\n",
      "Epoch 12229: train loss: 0.1757, test loss 0.8100\n",
      "Epoch 12230: train loss: 0.1757, test loss 0.8099\n",
      "Epoch 12231: train loss: 0.1757, test loss 0.8099\n",
      "Epoch 12232: train loss: 0.1757, test loss 0.8099\n",
      "Epoch 12233: train loss: 0.1757, test loss 0.8099\n",
      "Epoch 12234: train loss: 0.1757, test loss 0.8099\n",
      "Epoch 12235: train loss: 0.1757, test loss 0.8098\n",
      "Epoch 12236: train loss: 0.1757, test loss 0.8098\n",
      "Epoch 12237: train loss: 0.1757, test loss 0.8098\n",
      "Epoch 12238: train loss: 0.1757, test loss 0.8098\n",
      "Epoch 12239: train loss: 0.1757, test loss 0.8098\n",
      "Epoch 12240: train loss: 0.1757, test loss 0.8097\n",
      "Epoch 12241: train loss: 0.1757, test loss 0.8097\n",
      "Epoch 12242: train loss: 0.1757, test loss 0.8097\n",
      "Epoch 12243: train loss: 0.1757, test loss 0.8097\n",
      "Epoch 12244: train loss: 0.1757, test loss 0.8097\n",
      "Epoch 12245: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12246: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12247: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12248: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12249: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12250: train loss: 0.1757, test loss 0.8096\n",
      "Epoch 12251: train loss: 0.1757, test loss 0.8095\n",
      "Epoch 12252: train loss: 0.1757, test loss 0.8095\n",
      "Epoch 12253: train loss: 0.1757, test loss 0.8095\n",
      "Epoch 12254: train loss: 0.1757, test loss 0.8095\n",
      "Epoch 12255: train loss: 0.1757, test loss 0.8095\n",
      "Epoch 12256: train loss: 0.1757, test loss 0.8094\n",
      "Epoch 12257: train loss: 0.1757, test loss 0.8094\n",
      "Epoch 12258: train loss: 0.1757, test loss 0.8094\n",
      "Epoch 12259: train loss: 0.1757, test loss 0.8094\n",
      "Epoch 12260: train loss: 0.1757, test loss 0.8094\n",
      "Epoch 12261: train loss: 0.1757, test loss 0.8093\n",
      "Epoch 12262: train loss: 0.1757, test loss 0.8093\n",
      "Epoch 12263: train loss: 0.1757, test loss 0.8093\n",
      "Epoch 12264: train loss: 0.1757, test loss 0.8093\n",
      "Epoch 12265: train loss: 0.1757, test loss 0.8093\n",
      "Epoch 12266: train loss: 0.1757, test loss 0.8092\n",
      "Epoch 12267: train loss: 0.1757, test loss 0.8092\n",
      "Epoch 12268: train loss: 0.1757, test loss 0.8092\n",
      "Epoch 12269: train loss: 0.1757, test loss 0.8092\n",
      "Epoch 12270: train loss: 0.1757, test loss 0.8092\n",
      "Epoch 12271: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12272: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12273: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12274: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12275: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12276: train loss: 0.1757, test loss 0.8091\n",
      "Epoch 12277: train loss: 0.1757, test loss 0.8090\n",
      "Epoch 12278: train loss: 0.1757, test loss 0.8090\n",
      "Epoch 12279: train loss: 0.1757, test loss 0.8090\n",
      "Epoch 12280: train loss: 0.1757, test loss 0.8090\n",
      "Epoch 12281: train loss: 0.1757, test loss 0.8090\n",
      "Epoch 12282: train loss: 0.1757, test loss 0.8089\n",
      "Epoch 12283: train loss: 0.1757, test loss 0.8089\n",
      "Epoch 12284: train loss: 0.1757, test loss 0.8089\n",
      "Epoch 12285: train loss: 0.1757, test loss 0.8089\n",
      "Epoch 12286: train loss: 0.1757, test loss 0.8089\n",
      "Epoch 12287: train loss: 0.1757, test loss 0.8088\n",
      "Epoch 12288: train loss: 0.1757, test loss 0.8088\n",
      "Epoch 12289: train loss: 0.1757, test loss 0.8088\n",
      "Epoch 12290: train loss: 0.1757, test loss 0.8088\n",
      "Epoch 12291: train loss: 0.1757, test loss 0.8088\n",
      "Epoch 12292: train loss: 0.1757, test loss 0.8087\n",
      "Epoch 12293: train loss: 0.1757, test loss 0.8087\n",
      "Epoch 12294: train loss: 0.1757, test loss 0.8087\n",
      "Epoch 12295: train loss: 0.1757, test loss 0.8087\n",
      "Epoch 12296: train loss: 0.1757, test loss 0.8087\n",
      "Epoch 12297: train loss: 0.1757, test loss 0.8086\n",
      "Epoch 12298: train loss: 0.1757, test loss 0.8086\n",
      "Epoch 12299: train loss: 0.1757, test loss 0.8086\n",
      "Epoch 12300: train loss: 0.1757, test loss 0.8086\n",
      "Epoch 12301: train loss: 0.1757, test loss 0.8086\n",
      "Epoch 12302: train loss: 0.1757, test loss 0.8085\n",
      "Epoch 12303: train loss: 0.1757, test loss 0.8085\n",
      "Epoch 12304: train loss: 0.1757, test loss 0.8085\n",
      "Epoch 12305: train loss: 0.1757, test loss 0.8085\n",
      "Epoch 12306: train loss: 0.1757, test loss 0.8085\n",
      "Epoch 12307: train loss: 0.1757, test loss 0.8084\n",
      "Epoch 12308: train loss: 0.1757, test loss 0.8084\n",
      "Epoch 12309: train loss: 0.1757, test loss 0.8084\n",
      "Epoch 12310: train loss: 0.1757, test loss 0.8084\n",
      "Epoch 12311: train loss: 0.1757, test loss 0.8084\n",
      "Epoch 12312: train loss: 0.1756, test loss 0.8084\n",
      "Epoch 12313: train loss: 0.1756, test loss 0.8083\n",
      "Epoch 12314: train loss: 0.1756, test loss 0.8083\n",
      "Epoch 12315: train loss: 0.1756, test loss 0.8083\n",
      "Epoch 12316: train loss: 0.1756, test loss 0.8083\n",
      "Epoch 12317: train loss: 0.1756, test loss 0.8083\n",
      "Epoch 12318: train loss: 0.1756, test loss 0.8082\n",
      "Epoch 12319: train loss: 0.1756, test loss 0.8082\n",
      "Epoch 12320: train loss: 0.1756, test loss 0.8082\n",
      "Epoch 12321: train loss: 0.1756, test loss 0.8082\n",
      "Epoch 12322: train loss: 0.1756, test loss 0.8082\n",
      "Epoch 12323: train loss: 0.1756, test loss 0.8081\n",
      "Epoch 12324: train loss: 0.1756, test loss 0.8081\n",
      "Epoch 12325: train loss: 0.1756, test loss 0.8081\n",
      "Epoch 12326: train loss: 0.1756, test loss 0.8081\n",
      "Epoch 12327: train loss: 0.1756, test loss 0.8081\n",
      "Epoch 12328: train loss: 0.1756, test loss 0.8080\n",
      "Epoch 12329: train loss: 0.1756, test loss 0.8080\n",
      "Epoch 12330: train loss: 0.1756, test loss 0.8080\n",
      "Epoch 12331: train loss: 0.1756, test loss 0.8080\n",
      "Epoch 12332: train loss: 0.1756, test loss 0.8080\n",
      "Epoch 12333: train loss: 0.1756, test loss 0.8079\n",
      "Epoch 12334: train loss: 0.1756, test loss 0.8079\n",
      "Epoch 12335: train loss: 0.1756, test loss 0.8079\n",
      "Epoch 12336: train loss: 0.1756, test loss 0.8079\n",
      "Epoch 12337: train loss: 0.1756, test loss 0.8079\n",
      "Epoch 12338: train loss: 0.1756, test loss 0.8078\n",
      "Epoch 12339: train loss: 0.1756, test loss 0.8078\n",
      "Epoch 12340: train loss: 0.1756, test loss 0.8078\n",
      "Epoch 12341: train loss: 0.1756, test loss 0.8078\n",
      "Epoch 12342: train loss: 0.1756, test loss 0.8078\n",
      "Epoch 12343: train loss: 0.1756, test loss 0.8077\n",
      "Epoch 12344: train loss: 0.1756, test loss 0.8077\n",
      "Epoch 12345: train loss: 0.1756, test loss 0.8077\n",
      "Epoch 12346: train loss: 0.1756, test loss 0.8077\n",
      "Epoch 12347: train loss: 0.1756, test loss 0.8077\n",
      "Epoch 12348: train loss: 0.1756, test loss 0.8076\n",
      "Epoch 12349: train loss: 0.1756, test loss 0.8076\n",
      "Epoch 12350: train loss: 0.1756, test loss 0.8076\n",
      "Epoch 12351: train loss: 0.1756, test loss 0.8076\n",
      "Epoch 12352: train loss: 0.1756, test loss 0.8076\n",
      "Epoch 12353: train loss: 0.1756, test loss 0.8075\n",
      "Epoch 12354: train loss: 0.1756, test loss 0.8075\n",
      "Epoch 12355: train loss: 0.1756, test loss 0.8075\n",
      "Epoch 12356: train loss: 0.1756, test loss 0.8075\n",
      "Epoch 12357: train loss: 0.1756, test loss 0.8075\n",
      "Epoch 12358: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12359: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12360: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12361: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12362: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12363: train loss: 0.1756, test loss 0.8074\n",
      "Epoch 12364: train loss: 0.1756, test loss 0.8073\n",
      "Epoch 12365: train loss: 0.1756, test loss 0.8073\n",
      "Epoch 12366: train loss: 0.1756, test loss 0.8073\n",
      "Epoch 12367: train loss: 0.1756, test loss 0.8073\n",
      "Epoch 12368: train loss: 0.1756, test loss 0.8073\n",
      "Epoch 12369: train loss: 0.1756, test loss 0.8072\n",
      "Epoch 12370: train loss: 0.1756, test loss 0.8072\n",
      "Epoch 12371: train loss: 0.1756, test loss 0.8072\n",
      "Epoch 12372: train loss: 0.1756, test loss 0.8072\n",
      "Epoch 12373: train loss: 0.1756, test loss 0.8072\n",
      "Epoch 12374: train loss: 0.1756, test loss 0.8071\n",
      "Epoch 12375: train loss: 0.1756, test loss 0.8071\n",
      "Epoch 12376: train loss: 0.1756, test loss 0.8071\n",
      "Epoch 12377: train loss: 0.1756, test loss 0.8071\n",
      "Epoch 12378: train loss: 0.1756, test loss 0.8071\n",
      "Epoch 12379: train loss: 0.1756, test loss 0.8070\n",
      "Epoch 12380: train loss: 0.1756, test loss 0.8070\n",
      "Epoch 12381: train loss: 0.1756, test loss 0.8070\n",
      "Epoch 12382: train loss: 0.1756, test loss 0.8070\n",
      "Epoch 12383: train loss: 0.1756, test loss 0.8070\n",
      "Epoch 12384: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12385: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12386: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12387: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12388: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12389: train loss: 0.1756, test loss 0.8069\n",
      "Epoch 12390: train loss: 0.1756, test loss 0.8068\n",
      "Epoch 12391: train loss: 0.1756, test loss 0.8068\n",
      "Epoch 12392: train loss: 0.1756, test loss 0.8068\n",
      "Epoch 12393: train loss: 0.1756, test loss 0.8068\n",
      "Epoch 12394: train loss: 0.1756, test loss 0.8068\n",
      "Epoch 12395: train loss: 0.1756, test loss 0.8067\n",
      "Epoch 12396: train loss: 0.1756, test loss 0.8067\n",
      "Epoch 12397: train loss: 0.1756, test loss 0.8067\n",
      "Epoch 12398: train loss: 0.1756, test loss 0.8067\n",
      "Epoch 12399: train loss: 0.1756, test loss 0.8067\n",
      "Epoch 12400: train loss: 0.1756, test loss 0.8066\n",
      "Epoch 12401: train loss: 0.1756, test loss 0.8066\n",
      "Epoch 12402: train loss: 0.1756, test loss 0.8066\n",
      "Epoch 12403: train loss: 0.1756, test loss 0.8066\n",
      "Epoch 12404: train loss: 0.1756, test loss 0.8066\n",
      "Epoch 12405: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12406: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12407: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12408: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12409: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12410: train loss: 0.1756, test loss 0.8065\n",
      "Epoch 12411: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12412: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12413: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12414: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12415: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12416: train loss: 0.1756, test loss 0.8064\n",
      "Epoch 12417: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12418: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12419: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12420: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12421: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12422: train loss: 0.1756, test loss 0.8063\n",
      "Epoch 12423: train loss: 0.1756, test loss 0.8062\n",
      "Epoch 12424: train loss: 0.1756, test loss 0.8062\n",
      "Epoch 12425: train loss: 0.1756, test loss 0.8062\n",
      "Epoch 12426: train loss: 0.1756, test loss 0.8062\n",
      "Epoch 12427: train loss: 0.1756, test loss 0.8062\n",
      "Epoch 12428: train loss: 0.1756, test loss 0.8061\n",
      "Epoch 12429: train loss: 0.1756, test loss 0.8061\n",
      "Epoch 12430: train loss: 0.1755, test loss 0.8061\n",
      "Epoch 12431: train loss: 0.1755, test loss 0.8061\n",
      "Epoch 12432: train loss: 0.1755, test loss 0.8061\n",
      "Epoch 12433: train loss: 0.1755, test loss 0.8061\n",
      "Epoch 12434: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12435: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12436: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12437: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12438: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12439: train loss: 0.1755, test loss 0.8060\n",
      "Epoch 12440: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12441: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12442: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12443: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12444: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12445: train loss: 0.1755, test loss 0.8059\n",
      "Epoch 12446: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12447: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12448: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12449: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12450: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12451: train loss: 0.1755, test loss 0.8058\n",
      "Epoch 12452: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12453: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12454: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12455: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12456: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12457: train loss: 0.1755, test loss 0.8057\n",
      "Epoch 12458: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12459: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12460: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12461: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12462: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12463: train loss: 0.1755, test loss 0.8056\n",
      "Epoch 12464: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12465: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12466: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12467: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12468: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12469: train loss: 0.1755, test loss 0.8055\n",
      "Epoch 12470: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12471: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12472: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12473: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12474: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12475: train loss: 0.1755, test loss 0.8054\n",
      "Epoch 12476: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12477: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12478: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12479: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12480: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12481: train loss: 0.1755, test loss 0.8053\n",
      "Epoch 12482: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12483: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12484: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12485: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12486: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12487: train loss: 0.1755, test loss 0.8052\n",
      "Epoch 12488: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12489: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12490: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12491: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12492: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12493: train loss: 0.1755, test loss 0.8051\n",
      "Epoch 12494: train loss: 0.1755, test loss 0.8050\n",
      "Epoch 12495: train loss: 0.1755, test loss 0.8050\n",
      "Epoch 12496: train loss: 0.1755, test loss 0.8050\n",
      "Epoch 12497: train loss: 0.1755, test loss 0.8050\n",
      "Epoch 12498: train loss: 0.1755, test loss 0.8050\n",
      "Epoch 12499: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12500: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12501: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12502: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12503: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12504: train loss: 0.1755, test loss 0.8049\n",
      "Epoch 12505: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12506: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12507: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12508: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12509: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12510: train loss: 0.1755, test loss 0.8048\n",
      "Epoch 12511: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12512: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12513: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12514: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12515: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12516: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12517: train loss: 0.1755, test loss 0.8047\n",
      "Epoch 12518: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12519: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12520: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12521: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12522: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12523: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12524: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12525: train loss: 0.1755, test loss 0.8046\n",
      "Epoch 12526: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12527: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12528: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12529: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12530: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12531: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12532: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12533: train loss: 0.1755, test loss 0.8045\n",
      "Epoch 12534: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12535: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12536: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12537: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12538: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12539: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12540: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12541: train loss: 0.1755, test loss 0.8044\n",
      "Epoch 12542: train loss: 0.1755, test loss 0.8043\n",
      "Epoch 12543: train loss: 0.1755, test loss 0.8043\n",
      "Epoch 12544: train loss: 0.1755, test loss 0.8043\n",
      "Epoch 12545: train loss: 0.1754, test loss 0.8043\n",
      "Epoch 12546: train loss: 0.1754, test loss 0.8043\n",
      "Epoch 12547: train loss: 0.1754, test loss 0.8043\n",
      "Epoch 12548: train loss: 0.1754, test loss 0.8043\n",
      "Epoch 12549: train loss: 0.1754, test loss 0.8043\n",
      "Epoch 12550: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12551: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12552: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12553: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12554: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12555: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12556: train loss: 0.1754, test loss 0.8042\n",
      "Epoch 12557: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12558: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12559: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12560: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12561: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12562: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12563: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12564: train loss: 0.1754, test loss 0.8041\n",
      "Epoch 12565: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12566: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12567: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12568: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12569: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12570: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12571: train loss: 0.1754, test loss 0.8040\n",
      "Epoch 12572: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12573: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12574: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12575: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12576: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12577: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12578: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12579: train loss: 0.1754, test loss 0.8039\n",
      "Epoch 12580: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12581: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12582: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12583: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12584: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12585: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12586: train loss: 0.1754, test loss 0.8038\n",
      "Epoch 12587: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12588: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12589: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12590: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12591: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12592: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12593: train loss: 0.1754, test loss 0.8037\n",
      "Epoch 12594: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12595: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12596: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12597: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12598: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12599: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12600: train loss: 0.1754, test loss 0.8036\n",
      "Epoch 12601: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12602: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12603: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12604: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12605: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12606: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12607: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12608: train loss: 0.1754, test loss 0.8035\n",
      "Epoch 12609: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12610: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12611: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12612: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12613: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12614: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12615: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12616: train loss: 0.1754, test loss 0.8034\n",
      "Epoch 12617: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12618: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12619: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12620: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12621: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12622: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12623: train loss: 0.1754, test loss 0.8033\n",
      "Epoch 12624: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12625: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12626: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12627: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12628: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12629: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12630: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12631: train loss: 0.1754, test loss 0.8032\n",
      "Epoch 12632: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12633: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12634: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12635: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12636: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12637: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12638: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12639: train loss: 0.1754, test loss 0.8031\n",
      "Epoch 12640: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12641: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12642: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12643: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12644: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12645: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12646: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12647: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12648: train loss: 0.1754, test loss 0.8030\n",
      "Epoch 12649: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12650: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12651: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12652: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12653: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12654: train loss: 0.1754, test loss 0.8029\n",
      "Epoch 12655: train loss: 0.1753, test loss 0.8029\n",
      "Epoch 12656: train loss: 0.1753, test loss 0.8029\n",
      "Epoch 12657: train loss: 0.1753, test loss 0.8029\n",
      "Epoch 12658: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12659: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12660: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12661: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12662: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12663: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12664: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12665: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12666: train loss: 0.1753, test loss 0.8028\n",
      "Epoch 12667: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12668: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12669: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12670: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12671: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12672: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12673: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12674: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12675: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12676: train loss: 0.1753, test loss 0.8027\n",
      "Epoch 12677: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12678: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12679: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12680: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12681: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12682: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12683: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12684: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12685: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12686: train loss: 0.1753, test loss 0.8026\n",
      "Epoch 12687: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12688: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12689: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12690: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12691: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12692: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12693: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12694: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12695: train loss: 0.1753, test loss 0.8025\n",
      "Epoch 12696: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12697: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12698: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12699: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12700: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12701: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12702: train loss: 0.1753, test loss 0.8024\n",
      "Epoch 12703: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12704: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12705: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12706: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12707: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12708: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12709: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12710: train loss: 0.1753, test loss 0.8023\n",
      "Epoch 12711: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12712: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12713: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12714: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12715: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12716: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12717: train loss: 0.1753, test loss 0.8022\n",
      "Epoch 12718: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12719: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12720: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12721: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12722: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12723: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12724: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12725: train loss: 0.1753, test loss 0.8021\n",
      "Epoch 12726: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12727: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12728: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12729: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12730: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12731: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12732: train loss: 0.1753, test loss 0.8020\n",
      "Epoch 12733: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12734: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12735: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12736: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12737: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12738: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12739: train loss: 0.1753, test loss 0.8019\n",
      "Epoch 12740: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12741: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12742: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12743: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12744: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12745: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12746: train loss: 0.1753, test loss 0.8018\n",
      "Epoch 12747: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12748: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12749: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12750: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12751: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12752: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12753: train loss: 0.1753, test loss 0.8017\n",
      "Epoch 12754: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12755: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12756: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12757: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12758: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12759: train loss: 0.1753, test loss 0.8016\n",
      "Epoch 12760: train loss: 0.1753, test loss 0.8015\n",
      "Epoch 12761: train loss: 0.1753, test loss 0.8015\n",
      "Epoch 12762: train loss: 0.1753, test loss 0.8015\n",
      "Epoch 12763: train loss: 0.1753, test loss 0.8015\n",
      "Epoch 12764: train loss: 0.1752, test loss 0.8015\n",
      "Epoch 12765: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12766: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12767: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12768: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12769: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12770: train loss: 0.1752, test loss 0.8014\n",
      "Epoch 12771: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12772: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12773: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12774: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12775: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12776: train loss: 0.1752, test loss 0.8013\n",
      "Epoch 12777: train loss: 0.1752, test loss 0.8012\n",
      "Epoch 12778: train loss: 0.1752, test loss 0.8012\n",
      "Epoch 12779: train loss: 0.1752, test loss 0.8012\n",
      "Epoch 12780: train loss: 0.1752, test loss 0.8012\n",
      "Epoch 12781: train loss: 0.1752, test loss 0.8012\n",
      "Epoch 12782: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12783: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12784: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12785: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12786: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12787: train loss: 0.1752, test loss 0.8011\n",
      "Epoch 12788: train loss: 0.1752, test loss 0.8010\n",
      "Epoch 12789: train loss: 0.1752, test loss 0.8010\n",
      "Epoch 12790: train loss: 0.1752, test loss 0.8010\n",
      "Epoch 12791: train loss: 0.1752, test loss 0.8010\n",
      "Epoch 12792: train loss: 0.1752, test loss 0.8010\n",
      "Epoch 12793: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12794: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12795: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12796: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12797: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12798: train loss: 0.1752, test loss 0.8009\n",
      "Epoch 12799: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12800: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12801: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12802: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12803: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12804: train loss: 0.1752, test loss 0.8008\n",
      "Epoch 12805: train loss: 0.1752, test loss 0.8007\n",
      "Epoch 12806: train loss: 0.1752, test loss 0.8007\n",
      "Epoch 12807: train loss: 0.1752, test loss 0.8007\n",
      "Epoch 12808: train loss: 0.1752, test loss 0.8007\n",
      "Epoch 12809: train loss: 0.1752, test loss 0.8007\n",
      "Epoch 12810: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12811: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12812: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12813: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12814: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12815: train loss: 0.1752, test loss 0.8006\n",
      "Epoch 12816: train loss: 0.1752, test loss 0.8005\n",
      "Epoch 12817: train loss: 0.1752, test loss 0.8005\n",
      "Epoch 12818: train loss: 0.1752, test loss 0.8005\n",
      "Epoch 12819: train loss: 0.1752, test loss 0.8005\n",
      "Epoch 12820: train loss: 0.1752, test loss 0.8005\n",
      "Epoch 12821: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12822: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12823: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12824: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12825: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12826: train loss: 0.1752, test loss 0.8004\n",
      "Epoch 12827: train loss: 0.1752, test loss 0.8003\n",
      "Epoch 12828: train loss: 0.1752, test loss 0.8003\n",
      "Epoch 12829: train loss: 0.1752, test loss 0.8003\n",
      "Epoch 12830: train loss: 0.1752, test loss 0.8003\n",
      "Epoch 12831: train loss: 0.1752, test loss 0.8003\n",
      "Epoch 12832: train loss: 0.1752, test loss 0.8002\n",
      "Epoch 12833: train loss: 0.1752, test loss 0.8002\n",
      "Epoch 12834: train loss: 0.1752, test loss 0.8002\n",
      "Epoch 12835: train loss: 0.1752, test loss 0.8002\n",
      "Epoch 12836: train loss: 0.1752, test loss 0.8002\n",
      "Epoch 12837: train loss: 0.1752, test loss 0.8001\n",
      "Epoch 12838: train loss: 0.1752, test loss 0.8001\n",
      "Epoch 12839: train loss: 0.1752, test loss 0.8001\n",
      "Epoch 12840: train loss: 0.1752, test loss 0.8001\n",
      "Epoch 12841: train loss: 0.1752, test loss 0.8001\n",
      "Epoch 12842: train loss: 0.1752, test loss 0.8000\n",
      "Epoch 12843: train loss: 0.1752, test loss 0.8000\n",
      "Epoch 12844: train loss: 0.1752, test loss 0.8000\n",
      "Epoch 12845: train loss: 0.1752, test loss 0.8000\n",
      "Epoch 12846: train loss: 0.1752, test loss 0.8000\n",
      "Epoch 12847: train loss: 0.1752, test loss 0.7999\n",
      "Epoch 12848: train loss: 0.1752, test loss 0.7999\n",
      "Epoch 12849: train loss: 0.1752, test loss 0.7999\n",
      "Epoch 12850: train loss: 0.1752, test loss 0.7999\n",
      "Epoch 12851: train loss: 0.1752, test loss 0.7999\n",
      "Epoch 12852: train loss: 0.1752, test loss 0.7998\n",
      "Epoch 12853: train loss: 0.1752, test loss 0.7998\n",
      "Epoch 12854: train loss: 0.1752, test loss 0.7998\n",
      "Epoch 12855: train loss: 0.1752, test loss 0.7998\n",
      "Epoch 12856: train loss: 0.1752, test loss 0.7998\n",
      "Epoch 12857: train loss: 0.1752, test loss 0.7997\n",
      "Epoch 12858: train loss: 0.1752, test loss 0.7997\n",
      "Epoch 12859: train loss: 0.1752, test loss 0.7997\n",
      "Epoch 12860: train loss: 0.1752, test loss 0.7997\n",
      "Epoch 12861: train loss: 0.1752, test loss 0.7997\n",
      "Epoch 12862: train loss: 0.1752, test loss 0.7996\n",
      "Epoch 12863: train loss: 0.1752, test loss 0.7996\n",
      "Epoch 12864: train loss: 0.1752, test loss 0.7996\n",
      "Epoch 12865: train loss: 0.1752, test loss 0.7996\n",
      "Epoch 12866: train loss: 0.1752, test loss 0.7996\n",
      "Epoch 12867: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12868: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12869: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12870: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12871: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12872: train loss: 0.1752, test loss 0.7995\n",
      "Epoch 12873: train loss: 0.1752, test loss 0.7994\n",
      "Epoch 12874: train loss: 0.1752, test loss 0.7994\n",
      "Epoch 12875: train loss: 0.1752, test loss 0.7994\n",
      "Epoch 12876: train loss: 0.1752, test loss 0.7994\n",
      "Epoch 12877: train loss: 0.1752, test loss 0.7994\n",
      "Epoch 12878: train loss: 0.1752, test loss 0.7993\n",
      "Epoch 12879: train loss: 0.1752, test loss 0.7993\n",
      "Epoch 12880: train loss: 0.1751, test loss 0.7993\n",
      "Epoch 12881: train loss: 0.1751, test loss 0.7993\n",
      "Epoch 12882: train loss: 0.1751, test loss 0.7993\n",
      "Epoch 12883: train loss: 0.1751, test loss 0.7992\n",
      "Epoch 12884: train loss: 0.1751, test loss 0.7992\n",
      "Epoch 12885: train loss: 0.1751, test loss 0.7992\n",
      "Epoch 12886: train loss: 0.1751, test loss 0.7992\n",
      "Epoch 12887: train loss: 0.1751, test loss 0.7992\n",
      "Epoch 12888: train loss: 0.1751, test loss 0.7991\n",
      "Epoch 12889: train loss: 0.1751, test loss 0.7991\n",
      "Epoch 12890: train loss: 0.1751, test loss 0.7991\n",
      "Epoch 12891: train loss: 0.1751, test loss 0.7991\n",
      "Epoch 12892: train loss: 0.1751, test loss 0.7991\n",
      "Epoch 12893: train loss: 0.1751, test loss 0.7990\n",
      "Epoch 12894: train loss: 0.1751, test loss 0.7990\n",
      "Epoch 12895: train loss: 0.1751, test loss 0.7990\n",
      "Epoch 12896: train loss: 0.1751, test loss 0.7990\n",
      "Epoch 12897: train loss: 0.1751, test loss 0.7990\n",
      "Epoch 12898: train loss: 0.1751, test loss 0.7989\n",
      "Epoch 12899: train loss: 0.1751, test loss 0.7989\n",
      "Epoch 12900: train loss: 0.1751, test loss 0.7989\n",
      "Epoch 12901: train loss: 0.1751, test loss 0.7989\n",
      "Epoch 12902: train loss: 0.1751, test loss 0.7988\n",
      "Epoch 12903: train loss: 0.1751, test loss 0.7988\n",
      "Epoch 12904: train loss: 0.1751, test loss 0.7988\n",
      "Epoch 12905: train loss: 0.1751, test loss 0.7988\n",
      "Epoch 12906: train loss: 0.1751, test loss 0.7988\n",
      "Epoch 12907: train loss: 0.1751, test loss 0.7987\n",
      "Epoch 12908: train loss: 0.1751, test loss 0.7987\n",
      "Epoch 12909: train loss: 0.1751, test loss 0.7987\n",
      "Epoch 12910: train loss: 0.1751, test loss 0.7987\n",
      "Epoch 12911: train loss: 0.1751, test loss 0.7987\n",
      "Epoch 12912: train loss: 0.1751, test loss 0.7986\n",
      "Epoch 12913: train loss: 0.1751, test loss 0.7986\n",
      "Epoch 12914: train loss: 0.1751, test loss 0.7986\n",
      "Epoch 12915: train loss: 0.1751, test loss 0.7986\n",
      "Epoch 12916: train loss: 0.1751, test loss 0.7986\n",
      "Epoch 12917: train loss: 0.1751, test loss 0.7985\n",
      "Epoch 12918: train loss: 0.1751, test loss 0.7985\n",
      "Epoch 12919: train loss: 0.1751, test loss 0.7985\n",
      "Epoch 12920: train loss: 0.1751, test loss 0.7985\n",
      "Epoch 12921: train loss: 0.1751, test loss 0.7985\n",
      "Epoch 12922: train loss: 0.1751, test loss 0.7984\n",
      "Epoch 12923: train loss: 0.1751, test loss 0.7984\n",
      "Epoch 12924: train loss: 0.1751, test loss 0.7984\n",
      "Epoch 12925: train loss: 0.1751, test loss 0.7984\n",
      "Epoch 12926: train loss: 0.1751, test loss 0.7984\n",
      "Epoch 12927: train loss: 0.1751, test loss 0.7983\n",
      "Epoch 12928: train loss: 0.1751, test loss 0.7983\n",
      "Epoch 12929: train loss: 0.1751, test loss 0.7983\n",
      "Epoch 12930: train loss: 0.1751, test loss 0.7983\n",
      "Epoch 12931: train loss: 0.1751, test loss 0.7983\n",
      "Epoch 12932: train loss: 0.1751, test loss 0.7982\n",
      "Epoch 12933: train loss: 0.1751, test loss 0.7982\n",
      "Epoch 12934: train loss: 0.1751, test loss 0.7982\n",
      "Epoch 12935: train loss: 0.1751, test loss 0.7982\n",
      "Epoch 12936: train loss: 0.1751, test loss 0.7982\n",
      "Epoch 12937: train loss: 0.1751, test loss 0.7981\n",
      "Epoch 12938: train loss: 0.1751, test loss 0.7981\n",
      "Epoch 12939: train loss: 0.1751, test loss 0.7981\n",
      "Epoch 12940: train loss: 0.1751, test loss 0.7981\n",
      "Epoch 12941: train loss: 0.1751, test loss 0.7981\n",
      "Epoch 12942: train loss: 0.1751, test loss 0.7980\n",
      "Epoch 12943: train loss: 0.1751, test loss 0.7980\n",
      "Epoch 12944: train loss: 0.1751, test loss 0.7980\n",
      "Epoch 12945: train loss: 0.1751, test loss 0.7980\n",
      "Epoch 12946: train loss: 0.1751, test loss 0.7980\n",
      "Epoch 12947: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12948: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12949: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12950: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12951: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12952: train loss: 0.1751, test loss 0.7979\n",
      "Epoch 12953: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12954: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12955: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12956: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12957: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12958: train loss: 0.1751, test loss 0.7978\n",
      "Epoch 12959: train loss: 0.1751, test loss 0.7977\n",
      "Epoch 12960: train loss: 0.1751, test loss 0.7977\n",
      "Epoch 12961: train loss: 0.1751, test loss 0.7977\n",
      "Epoch 12962: train loss: 0.1751, test loss 0.7977\n",
      "Epoch 12963: train loss: 0.1751, test loss 0.7977\n",
      "Epoch 12964: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12965: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12966: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12967: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12968: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12969: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12970: train loss: 0.1751, test loss 0.7976\n",
      "Epoch 12971: train loss: 0.1751, test loss 0.7975\n",
      "Epoch 12972: train loss: 0.1751, test loss 0.7975\n",
      "Epoch 12973: train loss: 0.1751, test loss 0.7975\n",
      "Epoch 12974: train loss: 0.1751, test loss 0.7975\n",
      "Epoch 12975: train loss: 0.1751, test loss 0.7975\n",
      "Epoch 12976: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12977: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12978: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12979: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12980: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12981: train loss: 0.1751, test loss 0.7974\n",
      "Epoch 12982: train loss: 0.1751, test loss 0.7973\n",
      "Epoch 12983: train loss: 0.1751, test loss 0.7973\n",
      "Epoch 12984: train loss: 0.1751, test loss 0.7973\n",
      "Epoch 12985: train loss: 0.1751, test loss 0.7973\n",
      "Epoch 12986: train loss: 0.1751, test loss 0.7973\n",
      "Epoch 12987: train loss: 0.1751, test loss 0.7972\n",
      "Epoch 12988: train loss: 0.1751, test loss 0.7972\n",
      "Epoch 12989: train loss: 0.1751, test loss 0.7972\n",
      "Epoch 12990: train loss: 0.1751, test loss 0.7972\n",
      "Epoch 12991: train loss: 0.1751, test loss 0.7972\n",
      "Epoch 12992: train loss: 0.1751, test loss 0.7971\n",
      "Epoch 12993: train loss: 0.1751, test loss 0.7971\n",
      "Epoch 12994: train loss: 0.1751, test loss 0.7971\n",
      "Epoch 12995: train loss: 0.1751, test loss 0.7971\n",
      "Epoch 12996: train loss: 0.1751, test loss 0.7971\n",
      "Epoch 12997: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 12998: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 12999: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 13000: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 13001: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 13002: train loss: 0.1750, test loss 0.7970\n",
      "Epoch 13003: train loss: 0.1750, test loss 0.7969\n",
      "Epoch 13004: train loss: 0.1750, test loss 0.7969\n",
      "Epoch 13005: train loss: 0.1750, test loss 0.7969\n",
      "Epoch 13006: train loss: 0.1750, test loss 0.7969\n",
      "Epoch 13007: train loss: 0.1750, test loss 0.7969\n",
      "Epoch 13008: train loss: 0.1750, test loss 0.7968\n",
      "Epoch 13009: train loss: 0.1750, test loss 0.7968\n",
      "Epoch 13010: train loss: 0.1750, test loss 0.7968\n",
      "Epoch 13011: train loss: 0.1750, test loss 0.7968\n",
      "Epoch 13012: train loss: 0.1750, test loss 0.7968\n",
      "Epoch 13013: train loss: 0.1750, test loss 0.7967\n",
      "Epoch 13014: train loss: 0.1750, test loss 0.7967\n",
      "Epoch 13015: train loss: 0.1750, test loss 0.7967\n",
      "Epoch 13016: train loss: 0.1750, test loss 0.7967\n",
      "Epoch 13017: train loss: 0.1750, test loss 0.7967\n",
      "Epoch 13018: train loss: 0.1750, test loss 0.7966\n",
      "Epoch 13019: train loss: 0.1750, test loss 0.7966\n",
      "Epoch 13020: train loss: 0.1750, test loss 0.7966\n",
      "Epoch 13021: train loss: 0.1750, test loss 0.7966\n",
      "Epoch 13022: train loss: 0.1750, test loss 0.7966\n",
      "Epoch 13023: train loss: 0.1750, test loss 0.7965\n",
      "Epoch 13024: train loss: 0.1750, test loss 0.7965\n",
      "Epoch 13025: train loss: 0.1750, test loss 0.7965\n",
      "Epoch 13026: train loss: 0.1750, test loss 0.7965\n",
      "Epoch 13027: train loss: 0.1750, test loss 0.7965\n",
      "Epoch 13028: train loss: 0.1750, test loss 0.7964\n",
      "Epoch 13029: train loss: 0.1750, test loss 0.7964\n",
      "Epoch 13030: train loss: 0.1750, test loss 0.7964\n",
      "Epoch 13031: train loss: 0.1750, test loss 0.7964\n",
      "Epoch 13032: train loss: 0.1750, test loss 0.7963\n",
      "Epoch 13033: train loss: 0.1750, test loss 0.7963\n",
      "Epoch 13034: train loss: 0.1750, test loss 0.7963\n",
      "Epoch 13035: train loss: 0.1750, test loss 0.7963\n",
      "Epoch 13036: train loss: 0.1750, test loss 0.7963\n",
      "Epoch 13037: train loss: 0.1750, test loss 0.7962\n",
      "Epoch 13038: train loss: 0.1750, test loss 0.7962\n",
      "Epoch 13039: train loss: 0.1750, test loss 0.7962\n",
      "Epoch 13040: train loss: 0.1750, test loss 0.7962\n",
      "Epoch 13041: train loss: 0.1750, test loss 0.7961\n",
      "Epoch 13042: train loss: 0.1750, test loss 0.7961\n",
      "Epoch 13043: train loss: 0.1750, test loss 0.7961\n",
      "Epoch 13044: train loss: 0.1750, test loss 0.7961\n",
      "Epoch 13045: train loss: 0.1750, test loss 0.7961\n",
      "Epoch 13046: train loss: 0.1750, test loss 0.7960\n",
      "Epoch 13047: train loss: 0.1750, test loss 0.7960\n",
      "Epoch 13048: train loss: 0.1750, test loss 0.7960\n",
      "Epoch 13049: train loss: 0.1750, test loss 0.7960\n",
      "Epoch 13050: train loss: 0.1750, test loss 0.7959\n",
      "Epoch 13051: train loss: 0.1750, test loss 0.7959\n",
      "Epoch 13052: train loss: 0.1750, test loss 0.7959\n",
      "Epoch 13053: train loss: 0.1750, test loss 0.7959\n",
      "Epoch 13054: train loss: 0.1750, test loss 0.7959\n",
      "Epoch 13055: train loss: 0.1750, test loss 0.7958\n",
      "Epoch 13056: train loss: 0.1750, test loss 0.7958\n",
      "Epoch 13057: train loss: 0.1750, test loss 0.7958\n",
      "Epoch 13058: train loss: 0.1750, test loss 0.7958\n",
      "Epoch 13059: train loss: 0.1750, test loss 0.7958\n",
      "Epoch 13060: train loss: 0.1750, test loss 0.7957\n",
      "Epoch 13061: train loss: 0.1750, test loss 0.7957\n",
      "Epoch 13062: train loss: 0.1750, test loss 0.7957\n",
      "Epoch 13063: train loss: 0.1750, test loss 0.7957\n",
      "Epoch 13064: train loss: 0.1750, test loss 0.7956\n",
      "Epoch 13065: train loss: 0.1750, test loss 0.7956\n",
      "Epoch 13066: train loss: 0.1750, test loss 0.7956\n",
      "Epoch 13067: train loss: 0.1750, test loss 0.7956\n",
      "Epoch 13068: train loss: 0.1750, test loss 0.7956\n",
      "Epoch 13069: train loss: 0.1750, test loss 0.7955\n",
      "Epoch 13070: train loss: 0.1750, test loss 0.7955\n",
      "Epoch 13071: train loss: 0.1750, test loss 0.7955\n",
      "Epoch 13072: train loss: 0.1750, test loss 0.7955\n",
      "Epoch 13073: train loss: 0.1750, test loss 0.7955\n",
      "Epoch 13074: train loss: 0.1750, test loss 0.7954\n",
      "Epoch 13075: train loss: 0.1750, test loss 0.7954\n",
      "Epoch 13076: train loss: 0.1750, test loss 0.7954\n",
      "Epoch 13077: train loss: 0.1750, test loss 0.7954\n",
      "Epoch 13078: train loss: 0.1750, test loss 0.7954\n",
      "Epoch 13079: train loss: 0.1750, test loss 0.7953\n",
      "Epoch 13080: train loss: 0.1750, test loss 0.7953\n",
      "Epoch 13081: train loss: 0.1750, test loss 0.7953\n",
      "Epoch 13082: train loss: 0.1750, test loss 0.7953\n",
      "Epoch 13083: train loss: 0.1750, test loss 0.7953\n",
      "Epoch 13084: train loss: 0.1750, test loss 0.7952\n",
      "Epoch 13085: train loss: 0.1750, test loss 0.7952\n",
      "Epoch 13086: train loss: 0.1750, test loss 0.7952\n",
      "Epoch 13087: train loss: 0.1750, test loss 0.7952\n",
      "Epoch 13088: train loss: 0.1750, test loss 0.7952\n",
      "Epoch 13089: train loss: 0.1750, test loss 0.7951\n",
      "Epoch 13090: train loss: 0.1750, test loss 0.7951\n",
      "Epoch 13091: train loss: 0.1750, test loss 0.7951\n",
      "Epoch 13092: train loss: 0.1750, test loss 0.7951\n",
      "Epoch 13093: train loss: 0.1750, test loss 0.7951\n",
      "Epoch 13094: train loss: 0.1750, test loss 0.7950\n",
      "Epoch 13095: train loss: 0.1750, test loss 0.7950\n",
      "Epoch 13096: train loss: 0.1750, test loss 0.7950\n",
      "Epoch 13097: train loss: 0.1750, test loss 0.7950\n",
      "Epoch 13098: train loss: 0.1750, test loss 0.7950\n",
      "Epoch 13099: train loss: 0.1750, test loss 0.7949\n",
      "Epoch 13100: train loss: 0.1750, test loss 0.7949\n",
      "Epoch 13101: train loss: 0.1750, test loss 0.7949\n",
      "Epoch 13102: train loss: 0.1750, test loss 0.7949\n",
      "Epoch 13103: train loss: 0.1750, test loss 0.7949\n",
      "Epoch 13104: train loss: 0.1750, test loss 0.7948\n",
      "Epoch 13105: train loss: 0.1749, test loss 0.7948\n",
      "Epoch 13106: train loss: 0.1749, test loss 0.7948\n",
      "Epoch 13107: train loss: 0.1749, test loss 0.7948\n",
      "Epoch 13108: train loss: 0.1749, test loss 0.7948\n",
      "Epoch 13109: train loss: 0.1749, test loss 0.7947\n",
      "Epoch 13110: train loss: 0.1749, test loss 0.7947\n",
      "Epoch 13111: train loss: 0.1749, test loss 0.7947\n",
      "Epoch 13112: train loss: 0.1749, test loss 0.7947\n",
      "Epoch 13113: train loss: 0.1749, test loss 0.7946\n",
      "Epoch 13114: train loss: 0.1749, test loss 0.7946\n",
      "Epoch 13115: train loss: 0.1749, test loss 0.7946\n",
      "Epoch 13116: train loss: 0.1749, test loss 0.7946\n",
      "Epoch 13117: train loss: 0.1749, test loss 0.7946\n",
      "Epoch 13118: train loss: 0.1749, test loss 0.7945\n",
      "Epoch 13119: train loss: 0.1749, test loss 0.7945\n",
      "Epoch 13120: train loss: 0.1749, test loss 0.7945\n",
      "Epoch 13121: train loss: 0.1749, test loss 0.7945\n",
      "Epoch 13122: train loss: 0.1749, test loss 0.7945\n",
      "Epoch 13123: train loss: 0.1749, test loss 0.7944\n",
      "Epoch 13124: train loss: 0.1749, test loss 0.7944\n",
      "Epoch 13125: train loss: 0.1749, test loss 0.7944\n",
      "Epoch 13126: train loss: 0.1749, test loss 0.7944\n",
      "Epoch 13127: train loss: 0.1749, test loss 0.7944\n",
      "Epoch 13128: train loss: 0.1749, test loss 0.7943\n",
      "Epoch 13129: train loss: 0.1749, test loss 0.7943\n",
      "Epoch 13130: train loss: 0.1749, test loss 0.7943\n",
      "Epoch 13131: train loss: 0.1749, test loss 0.7943\n",
      "Epoch 13132: train loss: 0.1749, test loss 0.7943\n",
      "Epoch 13133: train loss: 0.1749, test loss 0.7942\n",
      "Epoch 13134: train loss: 0.1749, test loss 0.7942\n",
      "Epoch 13135: train loss: 0.1749, test loss 0.7942\n",
      "Epoch 13136: train loss: 0.1749, test loss 0.7942\n",
      "Epoch 13137: train loss: 0.1749, test loss 0.7942\n",
      "Epoch 13138: train loss: 0.1749, test loss 0.7941\n",
      "Epoch 13139: train loss: 0.1749, test loss 0.7941\n",
      "Epoch 13140: train loss: 0.1749, test loss 0.7941\n",
      "Epoch 13141: train loss: 0.1749, test loss 0.7941\n",
      "Epoch 13142: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13143: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13144: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13145: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13146: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13147: train loss: 0.1749, test loss 0.7940\n",
      "Epoch 13148: train loss: 0.1749, test loss 0.7939\n",
      "Epoch 13149: train loss: 0.1749, test loss 0.7939\n",
      "Epoch 13150: train loss: 0.1749, test loss 0.7939\n",
      "Epoch 13151: train loss: 0.1749, test loss 0.7939\n",
      "Epoch 13152: train loss: 0.1749, test loss 0.7939\n",
      "Epoch 13153: train loss: 0.1749, test loss 0.7938\n",
      "Epoch 13154: train loss: 0.1749, test loss 0.7938\n",
      "Epoch 13155: train loss: 0.1749, test loss 0.7938\n",
      "Epoch 13156: train loss: 0.1749, test loss 0.7938\n",
      "Epoch 13157: train loss: 0.1749, test loss 0.7938\n",
      "Epoch 13158: train loss: 0.1749, test loss 0.7937\n",
      "Epoch 13159: train loss: 0.1749, test loss 0.7937\n",
      "Epoch 13160: train loss: 0.1749, test loss 0.7937\n",
      "Epoch 13161: train loss: 0.1749, test loss 0.7937\n",
      "Epoch 13162: train loss: 0.1749, test loss 0.7937\n",
      "Epoch 13163: train loss: 0.1749, test loss 0.7936\n",
      "Epoch 13164: train loss: 0.1749, test loss 0.7936\n",
      "Epoch 13165: train loss: 0.1749, test loss 0.7936\n",
      "Epoch 13166: train loss: 0.1749, test loss 0.7936\n",
      "Epoch 13167: train loss: 0.1749, test loss 0.7936\n",
      "Epoch 13168: train loss: 0.1749, test loss 0.7935\n",
      "Epoch 13169: train loss: 0.1749, test loss 0.7935\n",
      "Epoch 13170: train loss: 0.1749, test loss 0.7935\n",
      "Epoch 13171: train loss: 0.1749, test loss 0.7935\n",
      "Epoch 13172: train loss: 0.1749, test loss 0.7935\n",
      "Epoch 13173: train loss: 0.1749, test loss 0.7934\n",
      "Epoch 13174: train loss: 0.1749, test loss 0.7934\n",
      "Epoch 13175: train loss: 0.1749, test loss 0.7934\n",
      "Epoch 13176: train loss: 0.1749, test loss 0.7934\n",
      "Epoch 13177: train loss: 0.1749, test loss 0.7934\n",
      "Epoch 13178: train loss: 0.1749, test loss 0.7933\n",
      "Epoch 13179: train loss: 0.1749, test loss 0.7933\n",
      "Epoch 13180: train loss: 0.1749, test loss 0.7933\n",
      "Epoch 13181: train loss: 0.1749, test loss 0.7933\n",
      "Epoch 13182: train loss: 0.1749, test loss 0.7933\n",
      "Epoch 13183: train loss: 0.1749, test loss 0.7932\n",
      "Epoch 13184: train loss: 0.1749, test loss 0.7932\n",
      "Epoch 13185: train loss: 0.1749, test loss 0.7932\n",
      "Epoch 13186: train loss: 0.1749, test loss 0.7932\n",
      "Epoch 13187: train loss: 0.1749, test loss 0.7932\n",
      "Epoch 13188: train loss: 0.1749, test loss 0.7931\n",
      "Epoch 13189: train loss: 0.1749, test loss 0.7931\n",
      "Epoch 13190: train loss: 0.1749, test loss 0.7931\n",
      "Epoch 13191: train loss: 0.1749, test loss 0.7931\n",
      "Epoch 13192: train loss: 0.1749, test loss 0.7931\n",
      "Epoch 13193: train loss: 0.1749, test loss 0.7930\n",
      "Epoch 13194: train loss: 0.1749, test loss 0.7930\n",
      "Epoch 13195: train loss: 0.1749, test loss 0.7930\n",
      "Epoch 13196: train loss: 0.1749, test loss 0.7930\n",
      "Epoch 13197: train loss: 0.1749, test loss 0.7930\n",
      "Epoch 13198: train loss: 0.1749, test loss 0.7929\n",
      "Epoch 13199: train loss: 0.1749, test loss 0.7929\n",
      "Epoch 13200: train loss: 0.1749, test loss 0.7929\n",
      "Epoch 13201: train loss: 0.1749, test loss 0.7929\n",
      "Epoch 13202: train loss: 0.1749, test loss 0.7929\n",
      "Epoch 13203: train loss: 0.1749, test loss 0.7928\n",
      "Epoch 13204: train loss: 0.1749, test loss 0.7928\n",
      "Epoch 13205: train loss: 0.1749, test loss 0.7928\n",
      "Epoch 13206: train loss: 0.1749, test loss 0.7928\n",
      "Epoch 13207: train loss: 0.1749, test loss 0.7928\n",
      "Epoch 13208: train loss: 0.1749, test loss 0.7927\n",
      "Epoch 13209: train loss: 0.1749, test loss 0.7927\n",
      "Epoch 13210: train loss: 0.1749, test loss 0.7927\n",
      "Epoch 13211: train loss: 0.1749, test loss 0.7927\n",
      "Epoch 13212: train loss: 0.1749, test loss 0.7927\n",
      "Epoch 13213: train loss: 0.1749, test loss 0.7926\n",
      "Epoch 13214: train loss: 0.1749, test loss 0.7926\n",
      "Epoch 13215: train loss: 0.1748, test loss 0.7926\n",
      "Epoch 13216: train loss: 0.1748, test loss 0.7926\n",
      "Epoch 13217: train loss: 0.1748, test loss 0.7925\n",
      "Epoch 13218: train loss: 0.1748, test loss 0.7925\n",
      "Epoch 13219: train loss: 0.1748, test loss 0.7925\n",
      "Epoch 13220: train loss: 0.1748, test loss 0.7925\n",
      "Epoch 13221: train loss: 0.1748, test loss 0.7925\n",
      "Epoch 13222: train loss: 0.1748, test loss 0.7924\n",
      "Epoch 13223: train loss: 0.1748, test loss 0.7924\n",
      "Epoch 13224: train loss: 0.1748, test loss 0.7924\n",
      "Epoch 13225: train loss: 0.1748, test loss 0.7924\n",
      "Epoch 13226: train loss: 0.1748, test loss 0.7924\n",
      "Epoch 13227: train loss: 0.1748, test loss 0.7923\n",
      "Epoch 13228: train loss: 0.1748, test loss 0.7923\n",
      "Epoch 13229: train loss: 0.1748, test loss 0.7923\n",
      "Epoch 13230: train loss: 0.1748, test loss 0.7923\n",
      "Epoch 13231: train loss: 0.1748, test loss 0.7923\n",
      "Epoch 13232: train loss: 0.1748, test loss 0.7922\n",
      "Epoch 13233: train loss: 0.1748, test loss 0.7922\n",
      "Epoch 13234: train loss: 0.1748, test loss 0.7922\n",
      "Epoch 13235: train loss: 0.1748, test loss 0.7922\n",
      "Epoch 13236: train loss: 0.1748, test loss 0.7921\n",
      "Epoch 13237: train loss: 0.1748, test loss 0.7921\n",
      "Epoch 13238: train loss: 0.1748, test loss 0.7921\n",
      "Epoch 13239: train loss: 0.1748, test loss 0.7921\n",
      "Epoch 13240: train loss: 0.1748, test loss 0.7921\n",
      "Epoch 13241: train loss: 0.1748, test loss 0.7920\n",
      "Epoch 13242: train loss: 0.1748, test loss 0.7920\n",
      "Epoch 13243: train loss: 0.1748, test loss 0.7920\n",
      "Epoch 13244: train loss: 0.1748, test loss 0.7920\n",
      "Epoch 13245: train loss: 0.1748, test loss 0.7920\n",
      "Epoch 13246: train loss: 0.1748, test loss 0.7919\n",
      "Epoch 13247: train loss: 0.1748, test loss 0.7919\n",
      "Epoch 13248: train loss: 0.1748, test loss 0.7919\n",
      "Epoch 13249: train loss: 0.1748, test loss 0.7919\n",
      "Epoch 13250: train loss: 0.1748, test loss 0.7918\n",
      "Epoch 13251: train loss: 0.1748, test loss 0.7918\n",
      "Epoch 13252: train loss: 0.1748, test loss 0.7918\n",
      "Epoch 13253: train loss: 0.1748, test loss 0.7918\n",
      "Epoch 13254: train loss: 0.1748, test loss 0.7918\n",
      "Epoch 13255: train loss: 0.1748, test loss 0.7917\n",
      "Epoch 13256: train loss: 0.1748, test loss 0.7917\n",
      "Epoch 13257: train loss: 0.1748, test loss 0.7917\n",
      "Epoch 13258: train loss: 0.1748, test loss 0.7917\n",
      "Epoch 13259: train loss: 0.1748, test loss 0.7916\n",
      "Epoch 13260: train loss: 0.1748, test loss 0.7916\n",
      "Epoch 13261: train loss: 0.1748, test loss 0.7916\n",
      "Epoch 13262: train loss: 0.1748, test loss 0.7916\n",
      "Epoch 13263: train loss: 0.1748, test loss 0.7916\n",
      "Epoch 13264: train loss: 0.1748, test loss 0.7915\n",
      "Epoch 13265: train loss: 0.1748, test loss 0.7915\n",
      "Epoch 13266: train loss: 0.1748, test loss 0.7915\n",
      "Epoch 13267: train loss: 0.1748, test loss 0.7915\n",
      "Epoch 13268: train loss: 0.1748, test loss 0.7915\n",
      "Epoch 13269: train loss: 0.1748, test loss 0.7914\n",
      "Epoch 13270: train loss: 0.1748, test loss 0.7914\n",
      "Epoch 13271: train loss: 0.1748, test loss 0.7914\n",
      "Epoch 13272: train loss: 0.1748, test loss 0.7914\n",
      "Epoch 13273: train loss: 0.1748, test loss 0.7913\n",
      "Epoch 13274: train loss: 0.1748, test loss 0.7913\n",
      "Epoch 13275: train loss: 0.1748, test loss 0.7913\n",
      "Epoch 13276: train loss: 0.1748, test loss 0.7913\n",
      "Epoch 13277: train loss: 0.1748, test loss 0.7913\n",
      "Epoch 13278: train loss: 0.1748, test loss 0.7912\n",
      "Epoch 13279: train loss: 0.1748, test loss 0.7912\n",
      "Epoch 13280: train loss: 0.1748, test loss 0.7912\n",
      "Epoch 13281: train loss: 0.1748, test loss 0.7912\n",
      "Epoch 13282: train loss: 0.1748, test loss 0.7912\n",
      "Epoch 13283: train loss: 0.1748, test loss 0.7911\n",
      "Epoch 13284: train loss: 0.1748, test loss 0.7911\n",
      "Epoch 13285: train loss: 0.1748, test loss 0.7911\n",
      "Epoch 13286: train loss: 0.1748, test loss 0.7911\n",
      "Epoch 13287: train loss: 0.1748, test loss 0.7911\n",
      "Epoch 13288: train loss: 0.1748, test loss 0.7910\n",
      "Epoch 13289: train loss: 0.1748, test loss 0.7910\n",
      "Epoch 13290: train loss: 0.1748, test loss 0.7910\n",
      "Epoch 13291: train loss: 0.1748, test loss 0.7910\n",
      "Epoch 13292: train loss: 0.1748, test loss 0.7910\n",
      "Epoch 13293: train loss: 0.1748, test loss 0.7909\n",
      "Epoch 13294: train loss: 0.1748, test loss 0.7909\n",
      "Epoch 13295: train loss: 0.1748, test loss 0.7909\n",
      "Epoch 13296: train loss: 0.1748, test loss 0.7909\n",
      "Epoch 13297: train loss: 0.1748, test loss 0.7908\n",
      "Epoch 13298: train loss: 0.1748, test loss 0.7908\n",
      "Epoch 13299: train loss: 0.1748, test loss 0.7908\n",
      "Epoch 13300: train loss: 0.1748, test loss 0.7908\n",
      "Epoch 13301: train loss: 0.1748, test loss 0.7908\n",
      "Epoch 13302: train loss: 0.1748, test loss 0.7907\n",
      "Epoch 13303: train loss: 0.1748, test loss 0.7907\n",
      "Epoch 13304: train loss: 0.1748, test loss 0.7907\n",
      "Epoch 13305: train loss: 0.1748, test loss 0.7907\n",
      "Epoch 13306: train loss: 0.1748, test loss 0.7907\n",
      "Epoch 13307: train loss: 0.1748, test loss 0.7906\n",
      "Epoch 13308: train loss: 0.1748, test loss 0.7906\n",
      "Epoch 13309: train loss: 0.1748, test loss 0.7906\n",
      "Epoch 13310: train loss: 0.1748, test loss 0.7906\n",
      "Epoch 13311: train loss: 0.1748, test loss 0.7905\n",
      "Epoch 13312: train loss: 0.1748, test loss 0.7905\n",
      "Epoch 13313: train loss: 0.1748, test loss 0.7905\n",
      "Epoch 13314: train loss: 0.1748, test loss 0.7905\n",
      "Epoch 13315: train loss: 0.1748, test loss 0.7905\n",
      "Epoch 13316: train loss: 0.1748, test loss 0.7904\n",
      "Epoch 13317: train loss: 0.1748, test loss 0.7904\n",
      "Epoch 13318: train loss: 0.1748, test loss 0.7904\n",
      "Epoch 13319: train loss: 0.1748, test loss 0.7904\n",
      "Epoch 13320: train loss: 0.1748, test loss 0.7903\n",
      "Epoch 13321: train loss: 0.1748, test loss 0.7903\n",
      "Epoch 13322: train loss: 0.1748, test loss 0.7903\n",
      "Epoch 13323: train loss: 0.1748, test loss 0.7903\n",
      "Epoch 13324: train loss: 0.1748, test loss 0.7903\n",
      "Epoch 13325: train loss: 0.1748, test loss 0.7902\n",
      "Epoch 13326: train loss: 0.1748, test loss 0.7902\n",
      "Epoch 13327: train loss: 0.1748, test loss 0.7902\n",
      "Epoch 13328: train loss: 0.1747, test loss 0.7902\n",
      "Epoch 13329: train loss: 0.1747, test loss 0.7901\n",
      "Epoch 13330: train loss: 0.1747, test loss 0.7901\n",
      "Epoch 13331: train loss: 0.1747, test loss 0.7901\n",
      "Epoch 13332: train loss: 0.1747, test loss 0.7901\n",
      "Epoch 13333: train loss: 0.1747, test loss 0.7901\n",
      "Epoch 13334: train loss: 0.1747, test loss 0.7900\n",
      "Epoch 13335: train loss: 0.1747, test loss 0.7900\n",
      "Epoch 13336: train loss: 0.1747, test loss 0.7900\n",
      "Epoch 13337: train loss: 0.1747, test loss 0.7900\n",
      "Epoch 13338: train loss: 0.1747, test loss 0.7899\n",
      "Epoch 13339: train loss: 0.1747, test loss 0.7899\n",
      "Epoch 13340: train loss: 0.1747, test loss 0.7899\n",
      "Epoch 13341: train loss: 0.1747, test loss 0.7899\n",
      "Epoch 13342: train loss: 0.1747, test loss 0.7898\n",
      "Epoch 13343: train loss: 0.1747, test loss 0.7898\n",
      "Epoch 13344: train loss: 0.1747, test loss 0.7898\n",
      "Epoch 13345: train loss: 0.1747, test loss 0.7898\n",
      "Epoch 13346: train loss: 0.1747, test loss 0.7898\n",
      "Epoch 13347: train loss: 0.1747, test loss 0.7897\n",
      "Epoch 13348: train loss: 0.1747, test loss 0.7897\n",
      "Epoch 13349: train loss: 0.1747, test loss 0.7897\n",
      "Epoch 13350: train loss: 0.1747, test loss 0.7897\n",
      "Epoch 13351: train loss: 0.1747, test loss 0.7896\n",
      "Epoch 13352: train loss: 0.1747, test loss 0.7896\n",
      "Epoch 13353: train loss: 0.1747, test loss 0.7896\n",
      "Epoch 13354: train loss: 0.1747, test loss 0.7896\n",
      "Epoch 13355: train loss: 0.1747, test loss 0.7895\n",
      "Epoch 13356: train loss: 0.1747, test loss 0.7895\n",
      "Epoch 13357: train loss: 0.1747, test loss 0.7895\n",
      "Epoch 13358: train loss: 0.1747, test loss 0.7895\n",
      "Epoch 13359: train loss: 0.1747, test loss 0.7894\n",
      "Epoch 13360: train loss: 0.1747, test loss 0.7894\n",
      "Epoch 13361: train loss: 0.1747, test loss 0.7894\n",
      "Epoch 13362: train loss: 0.1747, test loss 0.7894\n",
      "Epoch 13363: train loss: 0.1747, test loss 0.7894\n",
      "Epoch 13364: train loss: 0.1747, test loss 0.7893\n",
      "Epoch 13365: train loss: 0.1747, test loss 0.7893\n",
      "Epoch 13366: train loss: 0.1747, test loss 0.7893\n",
      "Epoch 13367: train loss: 0.1747, test loss 0.7893\n",
      "Epoch 13368: train loss: 0.1747, test loss 0.7892\n",
      "Epoch 13369: train loss: 0.1747, test loss 0.7892\n",
      "Epoch 13370: train loss: 0.1747, test loss 0.7892\n",
      "Epoch 13371: train loss: 0.1747, test loss 0.7892\n",
      "Epoch 13372: train loss: 0.1747, test loss 0.7892\n",
      "Epoch 13373: train loss: 0.1747, test loss 0.7891\n",
      "Epoch 13374: train loss: 0.1747, test loss 0.7891\n",
      "Epoch 13375: train loss: 0.1747, test loss 0.7891\n",
      "Epoch 13376: train loss: 0.1747, test loss 0.7891\n",
      "Epoch 13377: train loss: 0.1747, test loss 0.7890\n",
      "Epoch 13378: train loss: 0.1747, test loss 0.7890\n",
      "Epoch 13379: train loss: 0.1747, test loss 0.7890\n",
      "Epoch 13380: train loss: 0.1747, test loss 0.7890\n",
      "Epoch 13381: train loss: 0.1747, test loss 0.7890\n",
      "Epoch 13382: train loss: 0.1747, test loss 0.7889\n",
      "Epoch 13383: train loss: 0.1747, test loss 0.7889\n",
      "Epoch 13384: train loss: 0.1747, test loss 0.7889\n",
      "Epoch 13385: train loss: 0.1747, test loss 0.7889\n",
      "Epoch 13386: train loss: 0.1747, test loss 0.7888\n",
      "Epoch 13387: train loss: 0.1747, test loss 0.7888\n",
      "Epoch 13388: train loss: 0.1747, test loss 0.7888\n",
      "Epoch 13389: train loss: 0.1747, test loss 0.7888\n",
      "Epoch 13390: train loss: 0.1747, test loss 0.7888\n",
      "Epoch 13391: train loss: 0.1747, test loss 0.7887\n",
      "Epoch 13392: train loss: 0.1747, test loss 0.7887\n",
      "Epoch 13393: train loss: 0.1747, test loss 0.7887\n",
      "Epoch 13394: train loss: 0.1747, test loss 0.7887\n",
      "Epoch 13395: train loss: 0.1747, test loss 0.7886\n",
      "Epoch 13396: train loss: 0.1747, test loss 0.7886\n",
      "Epoch 13397: train loss: 0.1747, test loss 0.7886\n",
      "Epoch 13398: train loss: 0.1747, test loss 0.7886\n",
      "Epoch 13399: train loss: 0.1747, test loss 0.7886\n",
      "Epoch 13400: train loss: 0.1747, test loss 0.7885\n",
      "Epoch 13401: train loss: 0.1747, test loss 0.7885\n",
      "Epoch 13402: train loss: 0.1747, test loss 0.7885\n",
      "Epoch 13403: train loss: 0.1747, test loss 0.7885\n",
      "Epoch 13404: train loss: 0.1747, test loss 0.7884\n",
      "Epoch 13405: train loss: 0.1747, test loss 0.7884\n",
      "Epoch 13406: train loss: 0.1747, test loss 0.7884\n",
      "Epoch 13407: train loss: 0.1747, test loss 0.7884\n",
      "Epoch 13408: train loss: 0.1747, test loss 0.7884\n",
      "Epoch 13409: train loss: 0.1747, test loss 0.7883\n",
      "Epoch 13410: train loss: 0.1747, test loss 0.7883\n",
      "Epoch 13411: train loss: 0.1747, test loss 0.7883\n",
      "Epoch 13412: train loss: 0.1747, test loss 0.7883\n",
      "Epoch 13413: train loss: 0.1747, test loss 0.7883\n",
      "Epoch 13414: train loss: 0.1747, test loss 0.7882\n",
      "Epoch 13415: train loss: 0.1747, test loss 0.7882\n",
      "Epoch 13416: train loss: 0.1747, test loss 0.7882\n",
      "Epoch 13417: train loss: 0.1747, test loss 0.7882\n",
      "Epoch 13418: train loss: 0.1747, test loss 0.7881\n",
      "Epoch 13419: train loss: 0.1747, test loss 0.7881\n",
      "Epoch 13420: train loss: 0.1747, test loss 0.7881\n",
      "Epoch 13421: train loss: 0.1747, test loss 0.7881\n",
      "Epoch 13422: train loss: 0.1747, test loss 0.7881\n",
      "Epoch 13423: train loss: 0.1747, test loss 0.7880\n",
      "Epoch 13424: train loss: 0.1747, test loss 0.7880\n",
      "Epoch 13425: train loss: 0.1747, test loss 0.7880\n",
      "Epoch 13426: train loss: 0.1747, test loss 0.7880\n",
      "Epoch 13427: train loss: 0.1747, test loss 0.7880\n",
      "Epoch 13428: train loss: 0.1747, test loss 0.7879\n",
      "Epoch 13429: train loss: 0.1747, test loss 0.7879\n",
      "Epoch 13430: train loss: 0.1747, test loss 0.7879\n",
      "Epoch 13431: train loss: 0.1747, test loss 0.7879\n",
      "Epoch 13432: train loss: 0.1747, test loss 0.7878\n",
      "Epoch 13433: train loss: 0.1747, test loss 0.7878\n",
      "Epoch 13434: train loss: 0.1747, test loss 0.7878\n",
      "Epoch 13435: train loss: 0.1747, test loss 0.7878\n",
      "Epoch 13436: train loss: 0.1747, test loss 0.7878\n",
      "Epoch 13437: train loss: 0.1747, test loss 0.7877\n",
      "Epoch 13438: train loss: 0.1747, test loss 0.7877\n",
      "Epoch 13439: train loss: 0.1747, test loss 0.7877\n",
      "Epoch 13440: train loss: 0.1747, test loss 0.7877\n",
      "Epoch 13441: train loss: 0.1747, test loss 0.7877\n",
      "Epoch 13442: train loss: 0.1747, test loss 0.7876\n",
      "Epoch 13443: train loss: 0.1747, test loss 0.7876\n",
      "Epoch 13444: train loss: 0.1747, test loss 0.7876\n",
      "Epoch 13445: train loss: 0.1747, test loss 0.7876\n",
      "Epoch 13446: train loss: 0.1746, test loss 0.7875\n",
      "Epoch 13447: train loss: 0.1746, test loss 0.7875\n",
      "Epoch 13448: train loss: 0.1746, test loss 0.7875\n",
      "Epoch 13449: train loss: 0.1746, test loss 0.7875\n",
      "Epoch 13450: train loss: 0.1746, test loss 0.7875\n",
      "Epoch 13451: train loss: 0.1746, test loss 0.7874\n",
      "Epoch 13452: train loss: 0.1746, test loss 0.7874\n",
      "Epoch 13453: train loss: 0.1746, test loss 0.7874\n",
      "Epoch 13454: train loss: 0.1746, test loss 0.7874\n",
      "Epoch 13455: train loss: 0.1746, test loss 0.7874\n",
      "Epoch 13456: train loss: 0.1746, test loss 0.7873\n",
      "Epoch 13457: train loss: 0.1746, test loss 0.7873\n",
      "Epoch 13458: train loss: 0.1746, test loss 0.7873\n",
      "Epoch 13459: train loss: 0.1746, test loss 0.7873\n",
      "Epoch 13460: train loss: 0.1746, test loss 0.7872\n",
      "Epoch 13461: train loss: 0.1746, test loss 0.7872\n",
      "Epoch 13462: train loss: 0.1746, test loss 0.7872\n",
      "Epoch 13463: train loss: 0.1746, test loss 0.7872\n",
      "Epoch 13464: train loss: 0.1746, test loss 0.7872\n",
      "Epoch 13465: train loss: 0.1746, test loss 0.7871\n",
      "Epoch 13466: train loss: 0.1746, test loss 0.7871\n",
      "Epoch 13467: train loss: 0.1746, test loss 0.7871\n",
      "Epoch 13468: train loss: 0.1746, test loss 0.7871\n",
      "Epoch 13469: train loss: 0.1746, test loss 0.7871\n",
      "Epoch 13470: train loss: 0.1746, test loss 0.7870\n",
      "Epoch 13471: train loss: 0.1746, test loss 0.7870\n",
      "Epoch 13472: train loss: 0.1746, test loss 0.7870\n",
      "Epoch 13473: train loss: 0.1746, test loss 0.7870\n",
      "Epoch 13474: train loss: 0.1746, test loss 0.7869\n",
      "Epoch 13475: train loss: 0.1746, test loss 0.7869\n",
      "Epoch 13476: train loss: 0.1746, test loss 0.7869\n",
      "Epoch 13477: train loss: 0.1746, test loss 0.7869\n",
      "Epoch 13478: train loss: 0.1746, test loss 0.7869\n",
      "Epoch 13479: train loss: 0.1746, test loss 0.7868\n",
      "Epoch 13480: train loss: 0.1746, test loss 0.7868\n",
      "Epoch 13481: train loss: 0.1746, test loss 0.7868\n",
      "Epoch 13482: train loss: 0.1746, test loss 0.7868\n",
      "Epoch 13483: train loss: 0.1746, test loss 0.7868\n",
      "Epoch 13484: train loss: 0.1746, test loss 0.7867\n",
      "Epoch 13485: train loss: 0.1746, test loss 0.7867\n",
      "Epoch 13486: train loss: 0.1746, test loss 0.7867\n",
      "Epoch 13487: train loss: 0.1746, test loss 0.7867\n",
      "Epoch 13488: train loss: 0.1746, test loss 0.7867\n",
      "Epoch 13489: train loss: 0.1746, test loss 0.7866\n",
      "Epoch 13490: train loss: 0.1746, test loss 0.7866\n",
      "Epoch 13491: train loss: 0.1746, test loss 0.7866\n",
      "Epoch 13492: train loss: 0.1746, test loss 0.7866\n",
      "Epoch 13493: train loss: 0.1746, test loss 0.7865\n",
      "Epoch 13494: train loss: 0.1746, test loss 0.7865\n",
      "Epoch 13495: train loss: 0.1746, test loss 0.7865\n",
      "Epoch 13496: train loss: 0.1746, test loss 0.7865\n",
      "Epoch 13497: train loss: 0.1746, test loss 0.7864\n",
      "Epoch 13498: train loss: 0.1746, test loss 0.7864\n",
      "Epoch 13499: train loss: 0.1746, test loss 0.7864\n",
      "Epoch 13500: train loss: 0.1746, test loss 0.7864\n",
      "Epoch 13501: train loss: 0.1746, test loss 0.7863\n",
      "Epoch 13502: train loss: 0.1746, test loss 0.7863\n",
      "Epoch 13503: train loss: 0.1746, test loss 0.7863\n",
      "Epoch 13504: train loss: 0.1746, test loss 0.7862\n",
      "Epoch 13505: train loss: 0.1746, test loss 0.7862\n",
      "Epoch 13506: train loss: 0.1746, test loss 0.7862\n",
      "Epoch 13507: train loss: 0.1746, test loss 0.7862\n",
      "Epoch 13508: train loss: 0.1746, test loss 0.7861\n",
      "Epoch 13509: train loss: 0.1746, test loss 0.7861\n",
      "Epoch 13510: train loss: 0.1746, test loss 0.7861\n",
      "Epoch 13511: train loss: 0.1746, test loss 0.7861\n",
      "Epoch 13512: train loss: 0.1746, test loss 0.7860\n",
      "Epoch 13513: train loss: 0.1746, test loss 0.7860\n",
      "Epoch 13514: train loss: 0.1746, test loss 0.7860\n",
      "Epoch 13515: train loss: 0.1746, test loss 0.7860\n",
      "Epoch 13516: train loss: 0.1746, test loss 0.7859\n",
      "Epoch 13517: train loss: 0.1746, test loss 0.7859\n",
      "Epoch 13518: train loss: 0.1746, test loss 0.7859\n",
      "Epoch 13519: train loss: 0.1746, test loss 0.7858\n",
      "Epoch 13520: train loss: 0.1746, test loss 0.7858\n",
      "Epoch 13521: train loss: 0.1746, test loss 0.7858\n",
      "Epoch 13522: train loss: 0.1746, test loss 0.7858\n",
      "Epoch 13523: train loss: 0.1746, test loss 0.7857\n",
      "Epoch 13524: train loss: 0.1746, test loss 0.7857\n",
      "Epoch 13525: train loss: 0.1746, test loss 0.7857\n",
      "Epoch 13526: train loss: 0.1746, test loss 0.7857\n",
      "Epoch 13527: train loss: 0.1746, test loss 0.7856\n",
      "Epoch 13528: train loss: 0.1746, test loss 0.7856\n",
      "Epoch 13529: train loss: 0.1746, test loss 0.7856\n",
      "Epoch 13530: train loss: 0.1746, test loss 0.7856\n",
      "Epoch 13531: train loss: 0.1746, test loss 0.7855\n",
      "Epoch 13532: train loss: 0.1746, test loss 0.7855\n",
      "Epoch 13533: train loss: 0.1746, test loss 0.7855\n",
      "Epoch 13534: train loss: 0.1746, test loss 0.7855\n",
      "Epoch 13535: train loss: 0.1746, test loss 0.7854\n",
      "Epoch 13536: train loss: 0.1746, test loss 0.7854\n",
      "Epoch 13537: train loss: 0.1746, test loss 0.7854\n",
      "Epoch 13538: train loss: 0.1746, test loss 0.7854\n",
      "Epoch 13539: train loss: 0.1746, test loss 0.7853\n",
      "Epoch 13540: train loss: 0.1746, test loss 0.7853\n",
      "Epoch 13541: train loss: 0.1746, test loss 0.7853\n",
      "Epoch 13542: train loss: 0.1746, test loss 0.7853\n",
      "Epoch 13543: train loss: 0.1746, test loss 0.7852\n",
      "Epoch 13544: train loss: 0.1746, test loss 0.7852\n",
      "Epoch 13545: train loss: 0.1746, test loss 0.7852\n",
      "Epoch 13546: train loss: 0.1746, test loss 0.7852\n",
      "Epoch 13547: train loss: 0.1746, test loss 0.7851\n",
      "Epoch 13548: train loss: 0.1746, test loss 0.7851\n",
      "Epoch 13549: train loss: 0.1746, test loss 0.7851\n",
      "Epoch 13550: train loss: 0.1746, test loss 0.7850\n",
      "Epoch 13551: train loss: 0.1746, test loss 0.7850\n",
      "Epoch 13552: train loss: 0.1746, test loss 0.7850\n",
      "Epoch 13553: train loss: 0.1746, test loss 0.7850\n",
      "Epoch 13554: train loss: 0.1746, test loss 0.7849\n",
      "Epoch 13555: train loss: 0.1746, test loss 0.7849\n",
      "Epoch 13556: train loss: 0.1746, test loss 0.7849\n",
      "Epoch 13557: train loss: 0.1746, test loss 0.7849\n",
      "Epoch 13558: train loss: 0.1746, test loss 0.7848\n",
      "Epoch 13559: train loss: 0.1746, test loss 0.7848\n",
      "Epoch 13560: train loss: 0.1746, test loss 0.7848\n",
      "Epoch 13561: train loss: 0.1746, test loss 0.7848\n",
      "Epoch 13562: train loss: 0.1746, test loss 0.7847\n",
      "Epoch 13563: train loss: 0.1746, test loss 0.7847\n",
      "Epoch 13564: train loss: 0.1746, test loss 0.7847\n",
      "Epoch 13565: train loss: 0.1746, test loss 0.7847\n",
      "Epoch 13566: train loss: 0.1746, test loss 0.7846\n",
      "Epoch 13567: train loss: 0.1746, test loss 0.7846\n",
      "Epoch 13568: train loss: 0.1746, test loss 0.7846\n",
      "Epoch 13569: train loss: 0.1746, test loss 0.7846\n",
      "Epoch 13570: train loss: 0.1746, test loss 0.7845\n",
      "Epoch 13571: train loss: 0.1746, test loss 0.7845\n",
      "Epoch 13572: train loss: 0.1745, test loss 0.7845\n",
      "Epoch 13573: train loss: 0.1745, test loss 0.7845\n",
      "Epoch 13574: train loss: 0.1745, test loss 0.7844\n",
      "Epoch 13575: train loss: 0.1745, test loss 0.7844\n",
      "Epoch 13576: train loss: 0.1745, test loss 0.7844\n",
      "Epoch 13577: train loss: 0.1745, test loss 0.7844\n",
      "Epoch 13578: train loss: 0.1745, test loss 0.7843\n",
      "Epoch 13579: train loss: 0.1745, test loss 0.7843\n",
      "Epoch 13580: train loss: 0.1745, test loss 0.7843\n",
      "Epoch 13581: train loss: 0.1745, test loss 0.7843\n",
      "Epoch 13582: train loss: 0.1745, test loss 0.7842\n",
      "Epoch 13583: train loss: 0.1745, test loss 0.7842\n",
      "Epoch 13584: train loss: 0.1745, test loss 0.7842\n",
      "Epoch 13585: train loss: 0.1745, test loss 0.7841\n",
      "Epoch 13586: train loss: 0.1745, test loss 0.7841\n",
      "Epoch 13587: train loss: 0.1745, test loss 0.7841\n",
      "Epoch 13588: train loss: 0.1745, test loss 0.7841\n",
      "Epoch 13589: train loss: 0.1745, test loss 0.7840\n",
      "Epoch 13590: train loss: 0.1745, test loss 0.7840\n",
      "Epoch 13591: train loss: 0.1745, test loss 0.7840\n",
      "Epoch 13592: train loss: 0.1745, test loss 0.7840\n",
      "Epoch 13593: train loss: 0.1745, test loss 0.7839\n",
      "Epoch 13594: train loss: 0.1745, test loss 0.7839\n",
      "Epoch 13595: train loss: 0.1745, test loss 0.7839\n",
      "Epoch 13596: train loss: 0.1745, test loss 0.7839\n",
      "Epoch 13597: train loss: 0.1745, test loss 0.7838\n",
      "Epoch 13598: train loss: 0.1745, test loss 0.7838\n",
      "Epoch 13599: train loss: 0.1745, test loss 0.7838\n",
      "Epoch 13600: train loss: 0.1745, test loss 0.7838\n",
      "Epoch 13601: train loss: 0.1745, test loss 0.7837\n",
      "Epoch 13602: train loss: 0.1745, test loss 0.7837\n",
      "Epoch 13603: train loss: 0.1745, test loss 0.7837\n",
      "Epoch 13604: train loss: 0.1745, test loss 0.7837\n",
      "Epoch 13605: train loss: 0.1745, test loss 0.7836\n",
      "Epoch 13606: train loss: 0.1745, test loss 0.7836\n",
      "Epoch 13607: train loss: 0.1745, test loss 0.7836\n",
      "Epoch 13608: train loss: 0.1745, test loss 0.7836\n",
      "Epoch 13609: train loss: 0.1745, test loss 0.7835\n",
      "Epoch 13610: train loss: 0.1745, test loss 0.7835\n",
      "Epoch 13611: train loss: 0.1745, test loss 0.7835\n",
      "Epoch 13612: train loss: 0.1745, test loss 0.7835\n",
      "Epoch 13613: train loss: 0.1745, test loss 0.7834\n",
      "Epoch 13614: train loss: 0.1745, test loss 0.7834\n",
      "Epoch 13615: train loss: 0.1745, test loss 0.7834\n",
      "Epoch 13616: train loss: 0.1745, test loss 0.7834\n",
      "Epoch 13617: train loss: 0.1745, test loss 0.7833\n",
      "Epoch 13618: train loss: 0.1745, test loss 0.7833\n",
      "Epoch 13619: train loss: 0.1745, test loss 0.7833\n",
      "Epoch 13620: train loss: 0.1745, test loss 0.7833\n",
      "Epoch 13621: train loss: 0.1745, test loss 0.7832\n",
      "Epoch 13622: train loss: 0.1745, test loss 0.7832\n",
      "Epoch 13623: train loss: 0.1745, test loss 0.7832\n",
      "Epoch 13624: train loss: 0.1745, test loss 0.7832\n",
      "Epoch 13625: train loss: 0.1745, test loss 0.7831\n",
      "Epoch 13626: train loss: 0.1745, test loss 0.7831\n",
      "Epoch 13627: train loss: 0.1745, test loss 0.7831\n",
      "Epoch 13628: train loss: 0.1745, test loss 0.7830\n",
      "Epoch 13629: train loss: 0.1745, test loss 0.7830\n",
      "Epoch 13630: train loss: 0.1745, test loss 0.7830\n",
      "Epoch 13631: train loss: 0.1745, test loss 0.7830\n",
      "Epoch 13632: train loss: 0.1745, test loss 0.7829\n",
      "Epoch 13633: train loss: 0.1745, test loss 0.7829\n",
      "Epoch 13634: train loss: 0.1745, test loss 0.7829\n",
      "Epoch 13635: train loss: 0.1745, test loss 0.7829\n",
      "Epoch 13636: train loss: 0.1745, test loss 0.7828\n",
      "Epoch 13637: train loss: 0.1745, test loss 0.7828\n",
      "Epoch 13638: train loss: 0.1745, test loss 0.7828\n",
      "Epoch 13639: train loss: 0.1745, test loss 0.7828\n",
      "Epoch 13640: train loss: 0.1745, test loss 0.7827\n",
      "Epoch 13641: train loss: 0.1745, test loss 0.7827\n",
      "Epoch 13642: train loss: 0.1745, test loss 0.7827\n",
      "Epoch 13643: train loss: 0.1745, test loss 0.7827\n",
      "Epoch 13644: train loss: 0.1745, test loss 0.7826\n",
      "Epoch 13645: train loss: 0.1745, test loss 0.7826\n",
      "Epoch 13646: train loss: 0.1745, test loss 0.7826\n",
      "Epoch 13647: train loss: 0.1745, test loss 0.7826\n",
      "Epoch 13648: train loss: 0.1745, test loss 0.7825\n",
      "Epoch 13649: train loss: 0.1745, test loss 0.7825\n",
      "Epoch 13650: train loss: 0.1745, test loss 0.7825\n",
      "Epoch 13651: train loss: 0.1745, test loss 0.7825\n",
      "Epoch 13652: train loss: 0.1745, test loss 0.7824\n",
      "Epoch 13653: train loss: 0.1745, test loss 0.7824\n",
      "Epoch 13654: train loss: 0.1745, test loss 0.7824\n",
      "Epoch 13655: train loss: 0.1745, test loss 0.7824\n",
      "Epoch 13656: train loss: 0.1745, test loss 0.7823\n",
      "Epoch 13657: train loss: 0.1745, test loss 0.7823\n",
      "Epoch 13658: train loss: 0.1745, test loss 0.7823\n",
      "Epoch 13659: train loss: 0.1745, test loss 0.7823\n",
      "Epoch 13660: train loss: 0.1745, test loss 0.7822\n",
      "Epoch 13661: train loss: 0.1745, test loss 0.7822\n",
      "Epoch 13662: train loss: 0.1745, test loss 0.7822\n",
      "Epoch 13663: train loss: 0.1745, test loss 0.7822\n",
      "Epoch 13664: train loss: 0.1745, test loss 0.7821\n",
      "Epoch 13665: train loss: 0.1745, test loss 0.7821\n",
      "Epoch 13666: train loss: 0.1745, test loss 0.7821\n",
      "Epoch 13667: train loss: 0.1745, test loss 0.7821\n",
      "Epoch 13668: train loss: 0.1745, test loss 0.7820\n",
      "Epoch 13669: train loss: 0.1745, test loss 0.7820\n",
      "Epoch 13670: train loss: 0.1745, test loss 0.7820\n",
      "Epoch 13671: train loss: 0.1745, test loss 0.7820\n",
      "Epoch 13672: train loss: 0.1745, test loss 0.7819\n",
      "Epoch 13673: train loss: 0.1745, test loss 0.7819\n",
      "Epoch 13674: train loss: 0.1745, test loss 0.7819\n",
      "Epoch 13675: train loss: 0.1745, test loss 0.7819\n",
      "Epoch 13676: train loss: 0.1745, test loss 0.7818\n",
      "Epoch 13677: train loss: 0.1745, test loss 0.7818\n",
      "Epoch 13678: train loss: 0.1745, test loss 0.7818\n",
      "Epoch 13679: train loss: 0.1745, test loss 0.7818\n",
      "Epoch 13680: train loss: 0.1745, test loss 0.7817\n",
      "Epoch 13681: train loss: 0.1745, test loss 0.7817\n",
      "Epoch 13682: train loss: 0.1745, test loss 0.7817\n",
      "Epoch 13683: train loss: 0.1745, test loss 0.7817\n",
      "Epoch 13684: train loss: 0.1745, test loss 0.7816\n",
      "Epoch 13685: train loss: 0.1745, test loss 0.7816\n",
      "Epoch 13686: train loss: 0.1745, test loss 0.7816\n",
      "Epoch 13687: train loss: 0.1745, test loss 0.7816\n",
      "Epoch 13688: train loss: 0.1745, test loss 0.7815\n",
      "Epoch 13689: train loss: 0.1745, test loss 0.7815\n",
      "Epoch 13690: train loss: 0.1745, test loss 0.7815\n",
      "Epoch 13691: train loss: 0.1745, test loss 0.7815\n",
      "Epoch 13692: train loss: 0.1745, test loss 0.7814\n",
      "Epoch 13693: train loss: 0.1745, test loss 0.7814\n",
      "Epoch 13694: train loss: 0.1745, test loss 0.7814\n",
      "Epoch 13695: train loss: 0.1745, test loss 0.7814\n",
      "Epoch 13696: train loss: 0.1745, test loss 0.7813\n",
      "Epoch 13697: train loss: 0.1745, test loss 0.7813\n",
      "Epoch 13698: train loss: 0.1745, test loss 0.7813\n",
      "Epoch 13699: train loss: 0.1745, test loss 0.7813\n",
      "Epoch 13700: train loss: 0.1745, test loss 0.7812\n",
      "Epoch 13701: train loss: 0.1745, test loss 0.7812\n",
      "Epoch 13702: train loss: 0.1745, test loss 0.7812\n",
      "Epoch 13703: train loss: 0.1744, test loss 0.7812\n",
      "Epoch 13704: train loss: 0.1744, test loss 0.7811\n",
      "Epoch 13705: train loss: 0.1744, test loss 0.7811\n",
      "Epoch 13706: train loss: 0.1744, test loss 0.7811\n",
      "Epoch 13707: train loss: 0.1744, test loss 0.7811\n",
      "Epoch 13708: train loss: 0.1744, test loss 0.7810\n",
      "Epoch 13709: train loss: 0.1744, test loss 0.7810\n",
      "Epoch 13710: train loss: 0.1744, test loss 0.7810\n",
      "Epoch 13711: train loss: 0.1744, test loss 0.7810\n",
      "Epoch 13712: train loss: 0.1744, test loss 0.7809\n",
      "Epoch 13713: train loss: 0.1744, test loss 0.7809\n",
      "Epoch 13714: train loss: 0.1744, test loss 0.7809\n",
      "Epoch 13715: train loss: 0.1744, test loss 0.7809\n",
      "Epoch 13716: train loss: 0.1744, test loss 0.7808\n",
      "Epoch 13717: train loss: 0.1744, test loss 0.7808\n",
      "Epoch 13718: train loss: 0.1744, test loss 0.7808\n",
      "Epoch 13719: train loss: 0.1744, test loss 0.7808\n",
      "Epoch 13720: train loss: 0.1744, test loss 0.7807\n",
      "Epoch 13721: train loss: 0.1744, test loss 0.7807\n",
      "Epoch 13722: train loss: 0.1744, test loss 0.7807\n",
      "Epoch 13723: train loss: 0.1744, test loss 0.7807\n",
      "Epoch 13724: train loss: 0.1744, test loss 0.7806\n",
      "Epoch 13725: train loss: 0.1744, test loss 0.7806\n",
      "Epoch 13726: train loss: 0.1744, test loss 0.7806\n",
      "Epoch 13727: train loss: 0.1744, test loss 0.7806\n",
      "Epoch 13728: train loss: 0.1744, test loss 0.7805\n",
      "Epoch 13729: train loss: 0.1744, test loss 0.7805\n",
      "Epoch 13730: train loss: 0.1744, test loss 0.7805\n",
      "Epoch 13731: train loss: 0.1744, test loss 0.7805\n",
      "Epoch 13732: train loss: 0.1744, test loss 0.7804\n",
      "Epoch 13733: train loss: 0.1744, test loss 0.7804\n",
      "Epoch 13734: train loss: 0.1744, test loss 0.7804\n",
      "Epoch 13735: train loss: 0.1744, test loss 0.7804\n",
      "Epoch 13736: train loss: 0.1744, test loss 0.7803\n",
      "Epoch 13737: train loss: 0.1744, test loss 0.7803\n",
      "Epoch 13738: train loss: 0.1744, test loss 0.7803\n",
      "Epoch 13739: train loss: 0.1744, test loss 0.7803\n",
      "Epoch 13740: train loss: 0.1744, test loss 0.7802\n",
      "Epoch 13741: train loss: 0.1744, test loss 0.7802\n",
      "Epoch 13742: train loss: 0.1744, test loss 0.7802\n",
      "Epoch 13743: train loss: 0.1744, test loss 0.7802\n",
      "Epoch 13744: train loss: 0.1744, test loss 0.7801\n",
      "Epoch 13745: train loss: 0.1744, test loss 0.7801\n",
      "Epoch 13746: train loss: 0.1744, test loss 0.7801\n",
      "Epoch 13747: train loss: 0.1744, test loss 0.7801\n",
      "Epoch 13748: train loss: 0.1744, test loss 0.7800\n",
      "Epoch 13749: train loss: 0.1744, test loss 0.7800\n",
      "Epoch 13750: train loss: 0.1744, test loss 0.7800\n",
      "Epoch 13751: train loss: 0.1744, test loss 0.7800\n",
      "Epoch 13752: train loss: 0.1744, test loss 0.7799\n",
      "Epoch 13753: train loss: 0.1744, test loss 0.7799\n",
      "Epoch 13754: train loss: 0.1744, test loss 0.7799\n",
      "Epoch 13755: train loss: 0.1744, test loss 0.7799\n",
      "Epoch 13756: train loss: 0.1744, test loss 0.7798\n",
      "Epoch 13757: train loss: 0.1744, test loss 0.7798\n",
      "Epoch 13758: train loss: 0.1744, test loss 0.7798\n",
      "Epoch 13759: train loss: 0.1744, test loss 0.7798\n",
      "Epoch 13760: train loss: 0.1744, test loss 0.7797\n",
      "Epoch 13761: train loss: 0.1744, test loss 0.7797\n",
      "Epoch 13762: train loss: 0.1744, test loss 0.7797\n",
      "Epoch 13763: train loss: 0.1744, test loss 0.7797\n",
      "Epoch 13764: train loss: 0.1744, test loss 0.7796\n",
      "Epoch 13765: train loss: 0.1744, test loss 0.7796\n",
      "Epoch 13766: train loss: 0.1744, test loss 0.7796\n",
      "Epoch 13767: train loss: 0.1744, test loss 0.7796\n",
      "Epoch 13768: train loss: 0.1744, test loss 0.7795\n",
      "Epoch 13769: train loss: 0.1744, test loss 0.7795\n",
      "Epoch 13770: train loss: 0.1744, test loss 0.7795\n",
      "Epoch 13771: train loss: 0.1744, test loss 0.7795\n",
      "Epoch 13772: train loss: 0.1744, test loss 0.7794\n",
      "Epoch 13773: train loss: 0.1744, test loss 0.7794\n",
      "Epoch 13774: train loss: 0.1744, test loss 0.7794\n",
      "Epoch 13775: train loss: 0.1744, test loss 0.7794\n",
      "Epoch 13776: train loss: 0.1744, test loss 0.7793\n",
      "Epoch 13777: train loss: 0.1744, test loss 0.7793\n",
      "Epoch 13778: train loss: 0.1744, test loss 0.7793\n",
      "Epoch 13779: train loss: 0.1744, test loss 0.7793\n",
      "Epoch 13780: train loss: 0.1744, test loss 0.7792\n",
      "Epoch 13781: train loss: 0.1744, test loss 0.7792\n",
      "Epoch 13782: train loss: 0.1744, test loss 0.7792\n",
      "Epoch 13783: train loss: 0.1744, test loss 0.7792\n",
      "Epoch 13784: train loss: 0.1744, test loss 0.7791\n",
      "Epoch 13785: train loss: 0.1744, test loss 0.7791\n",
      "Epoch 13786: train loss: 0.1744, test loss 0.7791\n",
      "Epoch 13787: train loss: 0.1744, test loss 0.7791\n",
      "Epoch 13788: train loss: 0.1744, test loss 0.7791\n",
      "Epoch 13789: train loss: 0.1744, test loss 0.7790\n",
      "Epoch 13790: train loss: 0.1744, test loss 0.7790\n",
      "Epoch 13791: train loss: 0.1744, test loss 0.7790\n",
      "Epoch 13792: train loss: 0.1744, test loss 0.7790\n",
      "Epoch 13793: train loss: 0.1744, test loss 0.7789\n",
      "Epoch 13794: train loss: 0.1744, test loss 0.7789\n",
      "Epoch 13795: train loss: 0.1744, test loss 0.7789\n",
      "Epoch 13796: train loss: 0.1744, test loss 0.7789\n",
      "Epoch 13797: train loss: 0.1744, test loss 0.7788\n",
      "Epoch 13798: train loss: 0.1744, test loss 0.7788\n",
      "Epoch 13799: train loss: 0.1744, test loss 0.7788\n",
      "Epoch 13800: train loss: 0.1744, test loss 0.7787\n",
      "Epoch 13801: train loss: 0.1744, test loss 0.7787\n",
      "Epoch 13802: train loss: 0.1744, test loss 0.7787\n",
      "Epoch 13803: train loss: 0.1744, test loss 0.7787\n",
      "Epoch 13804: train loss: 0.1744, test loss 0.7786\n",
      "Epoch 13805: train loss: 0.1744, test loss 0.7786\n",
      "Epoch 13806: train loss: 0.1744, test loss 0.7786\n",
      "Epoch 13807: train loss: 0.1744, test loss 0.7786\n",
      "Epoch 13808: train loss: 0.1744, test loss 0.7785\n",
      "Epoch 13809: train loss: 0.1744, test loss 0.7785\n",
      "Epoch 13810: train loss: 0.1744, test loss 0.7785\n",
      "Epoch 13811: train loss: 0.1744, test loss 0.7785\n",
      "Epoch 13812: train loss: 0.1744, test loss 0.7784\n",
      "Epoch 13813: train loss: 0.1744, test loss 0.7784\n",
      "Epoch 13814: train loss: 0.1744, test loss 0.7784\n",
      "Epoch 13815: train loss: 0.1744, test loss 0.7784\n",
      "Epoch 13816: train loss: 0.1744, test loss 0.7783\n",
      "Epoch 13817: train loss: 0.1744, test loss 0.7783\n",
      "Epoch 13818: train loss: 0.1744, test loss 0.7783\n",
      "Epoch 13819: train loss: 0.1744, test loss 0.7783\n",
      "Epoch 13820: train loss: 0.1744, test loss 0.7782\n",
      "Epoch 13821: train loss: 0.1744, test loss 0.7782\n",
      "Epoch 13822: train loss: 0.1744, test loss 0.7782\n",
      "Epoch 13823: train loss: 0.1744, test loss 0.7782\n",
      "Epoch 13824: train loss: 0.1744, test loss 0.7781\n",
      "Epoch 13825: train loss: 0.1744, test loss 0.7781\n",
      "Epoch 13826: train loss: 0.1744, test loss 0.7781\n",
      "Epoch 13827: train loss: 0.1744, test loss 0.7781\n",
      "Epoch 13828: train loss: 0.1744, test loss 0.7780\n",
      "Epoch 13829: train loss: 0.1744, test loss 0.7780\n",
      "Epoch 13830: train loss: 0.1744, test loss 0.7780\n",
      "Epoch 13831: train loss: 0.1744, test loss 0.7780\n",
      "Epoch 13832: train loss: 0.1744, test loss 0.7779\n",
      "Epoch 13833: train loss: 0.1744, test loss 0.7779\n",
      "Epoch 13834: train loss: 0.1744, test loss 0.7779\n",
      "Epoch 13835: train loss: 0.1744, test loss 0.7779\n",
      "Epoch 13836: train loss: 0.1744, test loss 0.7778\n",
      "Epoch 13837: train loss: 0.1744, test loss 0.7778\n",
      "Epoch 13838: train loss: 0.1743, test loss 0.7778\n",
      "Epoch 13839: train loss: 0.1743, test loss 0.7778\n",
      "Epoch 13840: train loss: 0.1743, test loss 0.7777\n",
      "Epoch 13841: train loss: 0.1743, test loss 0.7777\n",
      "Epoch 13842: train loss: 0.1743, test loss 0.7777\n",
      "Epoch 13843: train loss: 0.1743, test loss 0.7777\n",
      "Epoch 13844: train loss: 0.1743, test loss 0.7776\n",
      "Epoch 13845: train loss: 0.1743, test loss 0.7776\n",
      "Epoch 13846: train loss: 0.1743, test loss 0.7776\n",
      "Epoch 13847: train loss: 0.1743, test loss 0.7776\n",
      "Epoch 13848: train loss: 0.1743, test loss 0.7775\n",
      "Epoch 13849: train loss: 0.1743, test loss 0.7775\n",
      "Epoch 13850: train loss: 0.1743, test loss 0.7775\n",
      "Epoch 13851: train loss: 0.1743, test loss 0.7775\n",
      "Epoch 13852: train loss: 0.1743, test loss 0.7774\n",
      "Epoch 13853: train loss: 0.1743, test loss 0.7774\n",
      "Epoch 13854: train loss: 0.1743, test loss 0.7774\n",
      "Epoch 13855: train loss: 0.1743, test loss 0.7774\n",
      "Epoch 13856: train loss: 0.1743, test loss 0.7773\n",
      "Epoch 13857: train loss: 0.1743, test loss 0.7773\n",
      "Epoch 13858: train loss: 0.1743, test loss 0.7773\n",
      "Epoch 13859: train loss: 0.1743, test loss 0.7773\n",
      "Epoch 13860: train loss: 0.1743, test loss 0.7772\n",
      "Epoch 13861: train loss: 0.1743, test loss 0.7772\n",
      "Epoch 13862: train loss: 0.1743, test loss 0.7772\n",
      "Epoch 13863: train loss: 0.1743, test loss 0.7772\n",
      "Epoch 13864: train loss: 0.1743, test loss 0.7771\n",
      "Epoch 13865: train loss: 0.1743, test loss 0.7771\n",
      "Epoch 13866: train loss: 0.1743, test loss 0.7771\n",
      "Epoch 13867: train loss: 0.1743, test loss 0.7771\n",
      "Epoch 13868: train loss: 0.1743, test loss 0.7770\n",
      "Epoch 13869: train loss: 0.1743, test loss 0.7770\n",
      "Epoch 13870: train loss: 0.1743, test loss 0.7770\n",
      "Epoch 13871: train loss: 0.1743, test loss 0.7770\n",
      "Epoch 13872: train loss: 0.1743, test loss 0.7769\n",
      "Epoch 13873: train loss: 0.1743, test loss 0.7769\n",
      "Epoch 13874: train loss: 0.1743, test loss 0.7769\n",
      "Epoch 13875: train loss: 0.1743, test loss 0.7769\n",
      "Epoch 13876: train loss: 0.1743, test loss 0.7768\n",
      "Epoch 13877: train loss: 0.1743, test loss 0.7768\n",
      "Epoch 13878: train loss: 0.1743, test loss 0.7768\n",
      "Epoch 13879: train loss: 0.1743, test loss 0.7768\n",
      "Epoch 13880: train loss: 0.1743, test loss 0.7767\n",
      "Epoch 13881: train loss: 0.1743, test loss 0.7767\n",
      "Epoch 13882: train loss: 0.1743, test loss 0.7767\n",
      "Epoch 13883: train loss: 0.1743, test loss 0.7767\n",
      "Epoch 13884: train loss: 0.1743, test loss 0.7766\n",
      "Epoch 13885: train loss: 0.1743, test loss 0.7766\n",
      "Epoch 13886: train loss: 0.1743, test loss 0.7766\n",
      "Epoch 13887: train loss: 0.1743, test loss 0.7766\n",
      "Epoch 13888: train loss: 0.1743, test loss 0.7765\n",
      "Epoch 13889: train loss: 0.1743, test loss 0.7765\n",
      "Epoch 13890: train loss: 0.1743, test loss 0.7765\n",
      "Epoch 13891: train loss: 0.1743, test loss 0.7765\n",
      "Epoch 13892: train loss: 0.1743, test loss 0.7764\n",
      "Epoch 13893: train loss: 0.1743, test loss 0.7764\n",
      "Epoch 13894: train loss: 0.1743, test loss 0.7764\n",
      "Epoch 13895: train loss: 0.1743, test loss 0.7764\n",
      "Epoch 13896: train loss: 0.1743, test loss 0.7763\n",
      "Epoch 13897: train loss: 0.1743, test loss 0.7763\n",
      "Epoch 13898: train loss: 0.1743, test loss 0.7763\n",
      "Epoch 13899: train loss: 0.1743, test loss 0.7762\n",
      "Epoch 13900: train loss: 0.1743, test loss 0.7762\n",
      "Epoch 13901: train loss: 0.1743, test loss 0.7762\n",
      "Epoch 13902: train loss: 0.1743, test loss 0.7762\n",
      "Epoch 13903: train loss: 0.1743, test loss 0.7761\n",
      "Epoch 13904: train loss: 0.1743, test loss 0.7761\n",
      "Epoch 13905: train loss: 0.1743, test loss 0.7761\n",
      "Epoch 13906: train loss: 0.1743, test loss 0.7761\n",
      "Epoch 13907: train loss: 0.1743, test loss 0.7760\n",
      "Epoch 13908: train loss: 0.1743, test loss 0.7760\n",
      "Epoch 13909: train loss: 0.1743, test loss 0.7760\n",
      "Epoch 13910: train loss: 0.1743, test loss 0.7760\n",
      "Epoch 13911: train loss: 0.1743, test loss 0.7759\n",
      "Epoch 13912: train loss: 0.1743, test loss 0.7759\n",
      "Epoch 13913: train loss: 0.1743, test loss 0.7759\n",
      "Epoch 13914: train loss: 0.1743, test loss 0.7759\n",
      "Epoch 13915: train loss: 0.1743, test loss 0.7758\n",
      "Epoch 13916: train loss: 0.1743, test loss 0.7758\n",
      "Epoch 13917: train loss: 0.1743, test loss 0.7758\n",
      "Epoch 13918: train loss: 0.1743, test loss 0.7758\n",
      "Epoch 13919: train loss: 0.1743, test loss 0.7757\n",
      "Epoch 13920: train loss: 0.1743, test loss 0.7757\n",
      "Epoch 13921: train loss: 0.1743, test loss 0.7757\n",
      "Epoch 13922: train loss: 0.1743, test loss 0.7757\n",
      "Epoch 13923: train loss: 0.1743, test loss 0.7756\n",
      "Epoch 13924: train loss: 0.1743, test loss 0.7756\n",
      "Epoch 13925: train loss: 0.1743, test loss 0.7756\n",
      "Epoch 13926: train loss: 0.1743, test loss 0.7756\n",
      "Epoch 13927: train loss: 0.1743, test loss 0.7755\n",
      "Epoch 13928: train loss: 0.1743, test loss 0.7755\n",
      "Epoch 13929: train loss: 0.1743, test loss 0.7755\n",
      "Epoch 13930: train loss: 0.1743, test loss 0.7755\n",
      "Epoch 13931: train loss: 0.1743, test loss 0.7754\n",
      "Epoch 13932: train loss: 0.1743, test loss 0.7754\n",
      "Epoch 13933: train loss: 0.1743, test loss 0.7754\n",
      "Epoch 13934: train loss: 0.1743, test loss 0.7754\n",
      "Epoch 13935: train loss: 0.1743, test loss 0.7753\n",
      "Epoch 13936: train loss: 0.1743, test loss 0.7753\n",
      "Epoch 13937: train loss: 0.1743, test loss 0.7753\n",
      "Epoch 13938: train loss: 0.1743, test loss 0.7753\n",
      "Epoch 13939: train loss: 0.1743, test loss 0.7752\n",
      "Epoch 13940: train loss: 0.1743, test loss 0.7752\n",
      "Epoch 13941: train loss: 0.1743, test loss 0.7752\n",
      "Epoch 13942: train loss: 0.1743, test loss 0.7752\n",
      "Epoch 13943: train loss: 0.1743, test loss 0.7751\n",
      "Epoch 13944: train loss: 0.1743, test loss 0.7751\n",
      "Epoch 13945: train loss: 0.1743, test loss 0.7751\n",
      "Epoch 13946: train loss: 0.1743, test loss 0.7751\n",
      "Epoch 13947: train loss: 0.1743, test loss 0.7750\n",
      "Epoch 13948: train loss: 0.1743, test loss 0.7750\n",
      "Epoch 13949: train loss: 0.1743, test loss 0.7750\n",
      "Epoch 13950: train loss: 0.1743, test loss 0.7750\n",
      "Epoch 13951: train loss: 0.1743, test loss 0.7749\n",
      "Epoch 13952: train loss: 0.1743, test loss 0.7749\n",
      "Epoch 13953: train loss: 0.1743, test loss 0.7749\n",
      "Epoch 13954: train loss: 0.1743, test loss 0.7749\n",
      "Epoch 13955: train loss: 0.1743, test loss 0.7748\n",
      "Epoch 13956: train loss: 0.1743, test loss 0.7748\n",
      "Epoch 13957: train loss: 0.1743, test loss 0.7748\n",
      "Epoch 13958: train loss: 0.1743, test loss 0.7748\n",
      "Epoch 13959: train loss: 0.1743, test loss 0.7747\n",
      "Epoch 13960: train loss: 0.1743, test loss 0.7747\n",
      "Epoch 13961: train loss: 0.1743, test loss 0.7747\n",
      "Epoch 13962: train loss: 0.1743, test loss 0.7747\n",
      "Epoch 13963: train loss: 0.1743, test loss 0.7746\n",
      "Epoch 13964: train loss: 0.1743, test loss 0.7746\n",
      "Epoch 13965: train loss: 0.1743, test loss 0.7746\n",
      "Epoch 13966: train loss: 0.1743, test loss 0.7746\n",
      "Epoch 13967: train loss: 0.1743, test loss 0.7746\n",
      "Epoch 13968: train loss: 0.1743, test loss 0.7745\n",
      "Epoch 13969: train loss: 0.1743, test loss 0.7745\n",
      "Epoch 13970: train loss: 0.1743, test loss 0.7745\n",
      "Epoch 13971: train loss: 0.1743, test loss 0.7745\n",
      "Epoch 13972: train loss: 0.1743, test loss 0.7744\n",
      "Epoch 13973: train loss: 0.1743, test loss 0.7744\n",
      "Epoch 13974: train loss: 0.1743, test loss 0.7744\n",
      "Epoch 13975: train loss: 0.1743, test loss 0.7744\n",
      "Epoch 13976: train loss: 0.1742, test loss 0.7743\n",
      "Epoch 13977: train loss: 0.1742, test loss 0.7743\n",
      "Epoch 13978: train loss: 0.1742, test loss 0.7743\n",
      "Epoch 13979: train loss: 0.1742, test loss 0.7743\n",
      "Epoch 13980: train loss: 0.1742, test loss 0.7742\n",
      "Epoch 13981: train loss: 0.1742, test loss 0.7742\n",
      "Epoch 13982: train loss: 0.1742, test loss 0.7742\n",
      "Epoch 13983: train loss: 0.1742, test loss 0.7742\n",
      "Epoch 13984: train loss: 0.1742, test loss 0.7741\n",
      "Epoch 13985: train loss: 0.1742, test loss 0.7741\n",
      "Epoch 13986: train loss: 0.1742, test loss 0.7741\n",
      "Epoch 13987: train loss: 0.1742, test loss 0.7741\n",
      "Epoch 13988: train loss: 0.1742, test loss 0.7741\n",
      "Epoch 13989: train loss: 0.1742, test loss 0.7740\n",
      "Epoch 13990: train loss: 0.1742, test loss 0.7740\n",
      "Epoch 13991: train loss: 0.1742, test loss 0.7740\n",
      "Epoch 13992: train loss: 0.1742, test loss 0.7740\n",
      "Epoch 13993: train loss: 0.1742, test loss 0.7739\n",
      "Epoch 13994: train loss: 0.1742, test loss 0.7739\n",
      "Epoch 13995: train loss: 0.1742, test loss 0.7739\n",
      "Epoch 13996: train loss: 0.1742, test loss 0.7739\n",
      "Epoch 13997: train loss: 0.1742, test loss 0.7739\n",
      "Epoch 13998: train loss: 0.1742, test loss 0.7738\n",
      "Epoch 13999: train loss: 0.1742, test loss 0.7738\n",
      "Epoch 14000: train loss: 0.1742, test loss 0.7738\n",
      "Epoch 14001: train loss: 0.1742, test loss 0.7738\n",
      "Epoch 14002: train loss: 0.1742, test loss 0.7737\n",
      "Epoch 14003: train loss: 0.1742, test loss 0.7737\n",
      "Epoch 14004: train loss: 0.1742, test loss 0.7737\n",
      "Epoch 14005: train loss: 0.1742, test loss 0.7737\n",
      "Epoch 14006: train loss: 0.1742, test loss 0.7737\n",
      "Epoch 14007: train loss: 0.1742, test loss 0.7736\n",
      "Epoch 14008: train loss: 0.1742, test loss 0.7736\n",
      "Epoch 14009: train loss: 0.1742, test loss 0.7736\n",
      "Epoch 14010: train loss: 0.1742, test loss 0.7736\n",
      "Epoch 14011: train loss: 0.1742, test loss 0.7735\n",
      "Epoch 14012: train loss: 0.1742, test loss 0.7735\n",
      "Epoch 14013: train loss: 0.1742, test loss 0.7735\n",
      "Epoch 14014: train loss: 0.1742, test loss 0.7735\n",
      "Epoch 14015: train loss: 0.1742, test loss 0.7734\n",
      "Epoch 14016: train loss: 0.1742, test loss 0.7734\n",
      "Epoch 14017: train loss: 0.1742, test loss 0.7734\n",
      "Epoch 14018: train loss: 0.1742, test loss 0.7734\n",
      "Epoch 14019: train loss: 0.1742, test loss 0.7734\n",
      "Epoch 14020: train loss: 0.1742, test loss 0.7733\n",
      "Epoch 14021: train loss: 0.1742, test loss 0.7733\n",
      "Epoch 14022: train loss: 0.1742, test loss 0.7733\n",
      "Epoch 14023: train loss: 0.1742, test loss 0.7733\n",
      "Epoch 14024: train loss: 0.1742, test loss 0.7732\n",
      "Epoch 14025: train loss: 0.1742, test loss 0.7732\n",
      "Epoch 14026: train loss: 0.1742, test loss 0.7732\n",
      "Epoch 14027: train loss: 0.1742, test loss 0.7732\n",
      "Epoch 14028: train loss: 0.1742, test loss 0.7731\n",
      "Epoch 14029: train loss: 0.1742, test loss 0.7731\n",
      "Epoch 14030: train loss: 0.1742, test loss 0.7731\n",
      "Epoch 14031: train loss: 0.1742, test loss 0.7731\n",
      "Epoch 14032: train loss: 0.1742, test loss 0.7731\n",
      "Epoch 14033: train loss: 0.1742, test loss 0.7730\n",
      "Epoch 14034: train loss: 0.1742, test loss 0.7730\n",
      "Epoch 14035: train loss: 0.1742, test loss 0.7730\n",
      "Epoch 14036: train loss: 0.1742, test loss 0.7730\n",
      "Epoch 14037: train loss: 0.1742, test loss 0.7729\n",
      "Epoch 14038: train loss: 0.1742, test loss 0.7729\n",
      "Epoch 14039: train loss: 0.1742, test loss 0.7729\n",
      "Epoch 14040: train loss: 0.1742, test loss 0.7729\n",
      "Epoch 14041: train loss: 0.1742, test loss 0.7728\n",
      "Epoch 14042: train loss: 0.1742, test loss 0.7728\n",
      "Epoch 14043: train loss: 0.1742, test loss 0.7728\n",
      "Epoch 14044: train loss: 0.1742, test loss 0.7728\n",
      "Epoch 14045: train loss: 0.1742, test loss 0.7728\n",
      "Epoch 14046: train loss: 0.1742, test loss 0.7727\n",
      "Epoch 14047: train loss: 0.1742, test loss 0.7727\n",
      "Epoch 14048: train loss: 0.1742, test loss 0.7727\n",
      "Epoch 14049: train loss: 0.1742, test loss 0.7727\n",
      "Epoch 14050: train loss: 0.1742, test loss 0.7726\n",
      "Epoch 14051: train loss: 0.1742, test loss 0.7726\n",
      "Epoch 14052: train loss: 0.1742, test loss 0.7726\n",
      "Epoch 14053: train loss: 0.1742, test loss 0.7726\n",
      "Epoch 14054: train loss: 0.1742, test loss 0.7725\n",
      "Epoch 14055: train loss: 0.1742, test loss 0.7725\n",
      "Epoch 14056: train loss: 0.1742, test loss 0.7725\n",
      "Epoch 14057: train loss: 0.1742, test loss 0.7725\n",
      "Epoch 14058: train loss: 0.1742, test loss 0.7725\n",
      "Epoch 14059: train loss: 0.1742, test loss 0.7724\n",
      "Epoch 14060: train loss: 0.1742, test loss 0.7724\n",
      "Epoch 14061: train loss: 0.1742, test loss 0.7724\n",
      "Epoch 14062: train loss: 0.1742, test loss 0.7724\n",
      "Epoch 14063: train loss: 0.1742, test loss 0.7724\n",
      "Epoch 14064: train loss: 0.1742, test loss 0.7723\n",
      "Epoch 14065: train loss: 0.1742, test loss 0.7723\n",
      "Epoch 14066: train loss: 0.1742, test loss 0.7723\n",
      "Epoch 14067: train loss: 0.1742, test loss 0.7723\n",
      "Epoch 14068: train loss: 0.1742, test loss 0.7723\n",
      "Epoch 14069: train loss: 0.1742, test loss 0.7722\n",
      "Epoch 14070: train loss: 0.1742, test loss 0.7722\n",
      "Epoch 14071: train loss: 0.1742, test loss 0.7722\n",
      "Epoch 14072: train loss: 0.1742, test loss 0.7722\n",
      "Epoch 14073: train loss: 0.1742, test loss 0.7722\n",
      "Epoch 14074: train loss: 0.1742, test loss 0.7721\n",
      "Epoch 14075: train loss: 0.1742, test loss 0.7721\n",
      "Epoch 14076: train loss: 0.1742, test loss 0.7721\n",
      "Epoch 14077: train loss: 0.1742, test loss 0.7721\n",
      "Epoch 14078: train loss: 0.1742, test loss 0.7721\n",
      "Epoch 14079: train loss: 0.1742, test loss 0.7720\n",
      "Epoch 14080: train loss: 0.1742, test loss 0.7720\n",
      "Epoch 14081: train loss: 0.1742, test loss 0.7720\n",
      "Epoch 14082: train loss: 0.1742, test loss 0.7720\n",
      "Epoch 14083: train loss: 0.1742, test loss 0.7720\n",
      "Epoch 14084: train loss: 0.1742, test loss 0.7719\n",
      "Epoch 14085: train loss: 0.1742, test loss 0.7719\n",
      "Epoch 14086: train loss: 0.1742, test loss 0.7719\n",
      "Epoch 14087: train loss: 0.1742, test loss 0.7719\n",
      "Epoch 14088: train loss: 0.1742, test loss 0.7719\n",
      "Epoch 14089: train loss: 0.1742, test loss 0.7718\n",
      "Epoch 14090: train loss: 0.1742, test loss 0.7718\n",
      "Epoch 14091: train loss: 0.1742, test loss 0.7718\n",
      "Epoch 14092: train loss: 0.1742, test loss 0.7718\n",
      "Epoch 14093: train loss: 0.1742, test loss 0.7717\n",
      "Epoch 14094: train loss: 0.1742, test loss 0.7717\n",
      "Epoch 14095: train loss: 0.1742, test loss 0.7717\n",
      "Epoch 14096: train loss: 0.1742, test loss 0.7717\n",
      "Epoch 14097: train loss: 0.1742, test loss 0.7717\n",
      "Epoch 14098: train loss: 0.1742, test loss 0.7716\n",
      "Epoch 14099: train loss: 0.1742, test loss 0.7716\n",
      "Epoch 14100: train loss: 0.1742, test loss 0.7716\n",
      "Epoch 14101: train loss: 0.1742, test loss 0.7716\n",
      "Epoch 14102: train loss: 0.1742, test loss 0.7716\n",
      "Epoch 14103: train loss: 0.1742, test loss 0.7715\n",
      "Epoch 14104: train loss: 0.1742, test loss 0.7715\n",
      "Epoch 14105: train loss: 0.1742, test loss 0.7715\n",
      "Epoch 14106: train loss: 0.1742, test loss 0.7715\n",
      "Epoch 14107: train loss: 0.1742, test loss 0.7715\n",
      "Epoch 14108: train loss: 0.1742, test loss 0.7714\n",
      "Epoch 14109: train loss: 0.1742, test loss 0.7714\n",
      "Epoch 14110: train loss: 0.1742, test loss 0.7714\n",
      "Epoch 14111: train loss: 0.1742, test loss 0.7714\n",
      "Epoch 14112: train loss: 0.1742, test loss 0.7714\n",
      "Epoch 14113: train loss: 0.1742, test loss 0.7713\n",
      "Epoch 14114: train loss: 0.1741, test loss 0.7713\n",
      "Epoch 14115: train loss: 0.1741, test loss 0.7713\n",
      "Epoch 14116: train loss: 0.1741, test loss 0.7713\n",
      "Epoch 14117: train loss: 0.1741, test loss 0.7713\n",
      "Epoch 14118: train loss: 0.1741, test loss 0.7712\n",
      "Epoch 14119: train loss: 0.1741, test loss 0.7712\n",
      "Epoch 14120: train loss: 0.1741, test loss 0.7712\n",
      "Epoch 14121: train loss: 0.1741, test loss 0.7712\n",
      "Epoch 14122: train loss: 0.1741, test loss 0.7712\n",
      "Epoch 14123: train loss: 0.1741, test loss 0.7711\n",
      "Epoch 14124: train loss: 0.1741, test loss 0.7711\n",
      "Epoch 14125: train loss: 0.1741, test loss 0.7711\n",
      "Epoch 14126: train loss: 0.1741, test loss 0.7711\n",
      "Epoch 14127: train loss: 0.1741, test loss 0.7711\n",
      "Epoch 14128: train loss: 0.1741, test loss 0.7710\n",
      "Epoch 14129: train loss: 0.1741, test loss 0.7710\n",
      "Epoch 14130: train loss: 0.1741, test loss 0.7710\n",
      "Epoch 14131: train loss: 0.1741, test loss 0.7710\n",
      "Epoch 14132: train loss: 0.1741, test loss 0.7710\n",
      "Epoch 14133: train loss: 0.1741, test loss 0.7709\n",
      "Epoch 14134: train loss: 0.1741, test loss 0.7709\n",
      "Epoch 14135: train loss: 0.1741, test loss 0.7709\n",
      "Epoch 14136: train loss: 0.1741, test loss 0.7709\n",
      "Epoch 14137: train loss: 0.1741, test loss 0.7709\n",
      "Epoch 14138: train loss: 0.1741, test loss 0.7708\n",
      "Epoch 14139: train loss: 0.1741, test loss 0.7708\n",
      "Epoch 14140: train loss: 0.1741, test loss 0.7708\n",
      "Epoch 14141: train loss: 0.1741, test loss 0.7708\n",
      "Epoch 14142: train loss: 0.1741, test loss 0.7708\n",
      "Epoch 14143: train loss: 0.1741, test loss 0.7707\n",
      "Epoch 14144: train loss: 0.1741, test loss 0.7707\n",
      "Epoch 14145: train loss: 0.1741, test loss 0.7707\n",
      "Epoch 14146: train loss: 0.1741, test loss 0.7707\n",
      "Epoch 14147: train loss: 0.1741, test loss 0.7706\n",
      "Epoch 14148: train loss: 0.1741, test loss 0.7706\n",
      "Epoch 14149: train loss: 0.1741, test loss 0.7706\n",
      "Epoch 14150: train loss: 0.1741, test loss 0.7706\n",
      "Epoch 14151: train loss: 0.1741, test loss 0.7706\n",
      "Epoch 14152: train loss: 0.1741, test loss 0.7705\n",
      "Epoch 14153: train loss: 0.1741, test loss 0.7705\n",
      "Epoch 14154: train loss: 0.1741, test loss 0.7705\n",
      "Epoch 14155: train loss: 0.1741, test loss 0.7705\n",
      "Epoch 14156: train loss: 0.1741, test loss 0.7705\n",
      "Epoch 14157: train loss: 0.1741, test loss 0.7704\n",
      "Epoch 14158: train loss: 0.1741, test loss 0.7704\n",
      "Epoch 14159: train loss: 0.1741, test loss 0.7704\n",
      "Epoch 14160: train loss: 0.1741, test loss 0.7704\n",
      "Epoch 14161: train loss: 0.1741, test loss 0.7704\n",
      "Epoch 14162: train loss: 0.1741, test loss 0.7703\n",
      "Epoch 14163: train loss: 0.1741, test loss 0.7703\n",
      "Epoch 14164: train loss: 0.1741, test loss 0.7703\n",
      "Epoch 14165: train loss: 0.1741, test loss 0.7703\n",
      "Epoch 14166: train loss: 0.1741, test loss 0.7703\n",
      "Epoch 14167: train loss: 0.1741, test loss 0.7702\n",
      "Epoch 14168: train loss: 0.1741, test loss 0.7702\n",
      "Epoch 14169: train loss: 0.1741, test loss 0.7702\n",
      "Epoch 14170: train loss: 0.1741, test loss 0.7702\n",
      "Epoch 14171: train loss: 0.1741, test loss 0.7702\n",
      "Epoch 14172: train loss: 0.1741, test loss 0.7701\n",
      "Epoch 14173: train loss: 0.1741, test loss 0.7701\n",
      "Epoch 14174: train loss: 0.1741, test loss 0.7701\n",
      "Epoch 14175: train loss: 0.1741, test loss 0.7701\n",
      "Epoch 14176: train loss: 0.1741, test loss 0.7700\n",
      "Epoch 14177: train loss: 0.1741, test loss 0.7700\n",
      "Epoch 14178: train loss: 0.1741, test loss 0.7700\n",
      "Epoch 14179: train loss: 0.1741, test loss 0.7700\n",
      "Epoch 14180: train loss: 0.1741, test loss 0.7700\n",
      "Epoch 14181: train loss: 0.1741, test loss 0.7699\n",
      "Epoch 14182: train loss: 0.1741, test loss 0.7699\n",
      "Epoch 14183: train loss: 0.1741, test loss 0.7699\n",
      "Epoch 14184: train loss: 0.1741, test loss 0.7699\n",
      "Epoch 14185: train loss: 0.1741, test loss 0.7699\n",
      "Epoch 14186: train loss: 0.1741, test loss 0.7698\n",
      "Epoch 14187: train loss: 0.1741, test loss 0.7698\n",
      "Epoch 14188: train loss: 0.1741, test loss 0.7698\n",
      "Epoch 14189: train loss: 0.1741, test loss 0.7698\n",
      "Epoch 14190: train loss: 0.1741, test loss 0.7698\n",
      "Epoch 14191: train loss: 0.1741, test loss 0.7697\n",
      "Epoch 14192: train loss: 0.1741, test loss 0.7697\n",
      "Epoch 14193: train loss: 0.1741, test loss 0.7697\n",
      "Epoch 14194: train loss: 0.1741, test loss 0.7697\n",
      "Epoch 14195: train loss: 0.1741, test loss 0.7697\n",
      "Epoch 14196: train loss: 0.1741, test loss 0.7696\n",
      "Epoch 14197: train loss: 0.1741, test loss 0.7696\n",
      "Epoch 14198: train loss: 0.1741, test loss 0.7696\n",
      "Epoch 14199: train loss: 0.1741, test loss 0.7696\n",
      "Epoch 14200: train loss: 0.1741, test loss 0.7695\n",
      "Epoch 14201: train loss: 0.1741, test loss 0.7695\n",
      "Epoch 14202: train loss: 0.1741, test loss 0.7695\n",
      "Epoch 14203: train loss: 0.1741, test loss 0.7695\n",
      "Epoch 14204: train loss: 0.1741, test loss 0.7695\n",
      "Epoch 14205: train loss: 0.1741, test loss 0.7694\n",
      "Epoch 14206: train loss: 0.1741, test loss 0.7694\n",
      "Epoch 14207: train loss: 0.1741, test loss 0.7694\n",
      "Epoch 14208: train loss: 0.1741, test loss 0.7694\n",
      "Epoch 14209: train loss: 0.1741, test loss 0.7694\n",
      "Epoch 14210: train loss: 0.1741, test loss 0.7693\n",
      "Epoch 14211: train loss: 0.1741, test loss 0.7693\n",
      "Epoch 14212: train loss: 0.1741, test loss 0.7693\n",
      "Epoch 14213: train loss: 0.1741, test loss 0.7693\n",
      "Epoch 14214: train loss: 0.1741, test loss 0.7693\n",
      "Epoch 14215: train loss: 0.1741, test loss 0.7692\n",
      "Epoch 14216: train loss: 0.1741, test loss 0.7692\n",
      "Epoch 14217: train loss: 0.1741, test loss 0.7692\n",
      "Epoch 14218: train loss: 0.1741, test loss 0.7692\n",
      "Epoch 14219: train loss: 0.1741, test loss 0.7691\n",
      "Epoch 14220: train loss: 0.1741, test loss 0.7691\n",
      "Epoch 14221: train loss: 0.1741, test loss 0.7691\n",
      "Epoch 14222: train loss: 0.1741, test loss 0.7691\n",
      "Epoch 14223: train loss: 0.1741, test loss 0.7691\n",
      "Epoch 14224: train loss: 0.1741, test loss 0.7690\n",
      "Epoch 14225: train loss: 0.1741, test loss 0.7690\n",
      "Epoch 14226: train loss: 0.1741, test loss 0.7690\n",
      "Epoch 14227: train loss: 0.1741, test loss 0.7690\n",
      "Epoch 14228: train loss: 0.1741, test loss 0.7690\n",
      "Epoch 14229: train loss: 0.1741, test loss 0.7689\n",
      "Epoch 14230: train loss: 0.1741, test loss 0.7689\n",
      "Epoch 14231: train loss: 0.1741, test loss 0.7689\n",
      "Epoch 14232: train loss: 0.1741, test loss 0.7689\n",
      "Epoch 14233: train loss: 0.1741, test loss 0.7689\n",
      "Epoch 14234: train loss: 0.1741, test loss 0.7688\n",
      "Epoch 14235: train loss: 0.1741, test loss 0.7688\n",
      "Epoch 14236: train loss: 0.1741, test loss 0.7688\n",
      "Epoch 14237: train loss: 0.1741, test loss 0.7688\n",
      "Epoch 14238: train loss: 0.1741, test loss 0.7687\n",
      "Epoch 14239: train loss: 0.1741, test loss 0.7687\n",
      "Epoch 14240: train loss: 0.1741, test loss 0.7687\n",
      "Epoch 14241: train loss: 0.1741, test loss 0.7687\n",
      "Epoch 14242: train loss: 0.1741, test loss 0.7687\n",
      "Epoch 14243: train loss: 0.1741, test loss 0.7686\n",
      "Epoch 14244: train loss: 0.1741, test loss 0.7686\n",
      "Epoch 14245: train loss: 0.1741, test loss 0.7686\n",
      "Epoch 14246: train loss: 0.1741, test loss 0.7686\n",
      "Epoch 14247: train loss: 0.1741, test loss 0.7686\n",
      "Epoch 14248: train loss: 0.1741, test loss 0.7685\n",
      "Epoch 14249: train loss: 0.1741, test loss 0.7685\n",
      "Epoch 14250: train loss: 0.1741, test loss 0.7685\n",
      "Epoch 14251: train loss: 0.1741, test loss 0.7685\n",
      "Epoch 14252: train loss: 0.1741, test loss 0.7685\n",
      "Epoch 14253: train loss: 0.1741, test loss 0.7684\n",
      "Epoch 14254: train loss: 0.1740, test loss 0.7684\n",
      "Epoch 14255: train loss: 0.1740, test loss 0.7684\n",
      "Epoch 14256: train loss: 0.1740, test loss 0.7684\n",
      "Epoch 14257: train loss: 0.1740, test loss 0.7684\n",
      "Epoch 14258: train loss: 0.1740, test loss 0.7683\n",
      "Epoch 14259: train loss: 0.1740, test loss 0.7683\n",
      "Epoch 14260: train loss: 0.1740, test loss 0.7683\n",
      "Epoch 14261: train loss: 0.1740, test loss 0.7683\n",
      "Epoch 14262: train loss: 0.1740, test loss 0.7683\n",
      "Epoch 14263: train loss: 0.1740, test loss 0.7682\n",
      "Epoch 14264: train loss: 0.1740, test loss 0.7682\n",
      "Epoch 14265: train loss: 0.1740, test loss 0.7682\n",
      "Epoch 14266: train loss: 0.1740, test loss 0.7682\n",
      "Epoch 14267: train loss: 0.1740, test loss 0.7682\n",
      "Epoch 14268: train loss: 0.1740, test loss 0.7681\n",
      "Epoch 14269: train loss: 0.1740, test loss 0.7681\n",
      "Epoch 14270: train loss: 0.1740, test loss 0.7681\n",
      "Epoch 14271: train loss: 0.1740, test loss 0.7681\n",
      "Epoch 14272: train loss: 0.1740, test loss 0.7681\n",
      "Epoch 14273: train loss: 0.1740, test loss 0.7680\n",
      "Epoch 14274: train loss: 0.1740, test loss 0.7680\n",
      "Epoch 14275: train loss: 0.1740, test loss 0.7680\n",
      "Epoch 14276: train loss: 0.1740, test loss 0.7680\n",
      "Epoch 14277: train loss: 0.1740, test loss 0.7680\n",
      "Epoch 14278: train loss: 0.1740, test loss 0.7679\n",
      "Epoch 14279: train loss: 0.1740, test loss 0.7679\n",
      "Epoch 14280: train loss: 0.1740, test loss 0.7679\n",
      "Epoch 14281: train loss: 0.1740, test loss 0.7679\n",
      "Epoch 14282: train loss: 0.1740, test loss 0.7678\n",
      "Epoch 14283: train loss: 0.1740, test loss 0.7678\n",
      "Epoch 14284: train loss: 0.1740, test loss 0.7678\n",
      "Epoch 14285: train loss: 0.1740, test loss 0.7678\n",
      "Epoch 14286: train loss: 0.1740, test loss 0.7678\n",
      "Epoch 14287: train loss: 0.1740, test loss 0.7677\n",
      "Epoch 14288: train loss: 0.1740, test loss 0.7677\n",
      "Epoch 14289: train loss: 0.1740, test loss 0.7677\n",
      "Epoch 14290: train loss: 0.1740, test loss 0.7677\n",
      "Epoch 14291: train loss: 0.1740, test loss 0.7677\n",
      "Epoch 14292: train loss: 0.1740, test loss 0.7676\n",
      "Epoch 14293: train loss: 0.1740, test loss 0.7676\n",
      "Epoch 14294: train loss: 0.1740, test loss 0.7676\n",
      "Epoch 14295: train loss: 0.1740, test loss 0.7676\n",
      "Epoch 14296: train loss: 0.1740, test loss 0.7676\n",
      "Epoch 14297: train loss: 0.1740, test loss 0.7675\n",
      "Epoch 14298: train loss: 0.1740, test loss 0.7675\n",
      "Epoch 14299: train loss: 0.1740, test loss 0.7675\n",
      "Epoch 14300: train loss: 0.1740, test loss 0.7675\n",
      "Epoch 14301: train loss: 0.1740, test loss 0.7675\n",
      "Epoch 14302: train loss: 0.1740, test loss 0.7674\n",
      "Epoch 14303: train loss: 0.1740, test loss 0.7674\n",
      "Epoch 14304: train loss: 0.1740, test loss 0.7674\n",
      "Epoch 14305: train loss: 0.1740, test loss 0.7674\n",
      "Epoch 14306: train loss: 0.1740, test loss 0.7674\n",
      "Epoch 14307: train loss: 0.1740, test loss 0.7673\n",
      "Epoch 14308: train loss: 0.1740, test loss 0.7673\n",
      "Epoch 14309: train loss: 0.1740, test loss 0.7673\n",
      "Epoch 14310: train loss: 0.1740, test loss 0.7673\n",
      "Epoch 14311: train loss: 0.1740, test loss 0.7673\n",
      "Epoch 14312: train loss: 0.1740, test loss 0.7672\n",
      "Epoch 14313: train loss: 0.1740, test loss 0.7672\n",
      "Epoch 14314: train loss: 0.1740, test loss 0.7672\n",
      "Epoch 14315: train loss: 0.1740, test loss 0.7672\n",
      "Epoch 14316: train loss: 0.1740, test loss 0.7672\n",
      "Epoch 14317: train loss: 0.1740, test loss 0.7671\n",
      "Epoch 14318: train loss: 0.1740, test loss 0.7671\n",
      "Epoch 14319: train loss: 0.1740, test loss 0.7671\n",
      "Epoch 14320: train loss: 0.1740, test loss 0.7671\n",
      "Epoch 14321: train loss: 0.1740, test loss 0.7671\n",
      "Epoch 14322: train loss: 0.1740, test loss 0.7670\n",
      "Epoch 14323: train loss: 0.1740, test loss 0.7670\n",
      "Epoch 14324: train loss: 0.1740, test loss 0.7670\n",
      "Epoch 14325: train loss: 0.1740, test loss 0.7670\n",
      "Epoch 14326: train loss: 0.1740, test loss 0.7670\n",
      "Epoch 14327: train loss: 0.1740, test loss 0.7669\n",
      "Epoch 14328: train loss: 0.1740, test loss 0.7669\n",
      "Epoch 14329: train loss: 0.1740, test loss 0.7669\n",
      "Epoch 14330: train loss: 0.1740, test loss 0.7669\n",
      "Epoch 14331: train loss: 0.1740, test loss 0.7669\n",
      "Epoch 14332: train loss: 0.1740, test loss 0.7668\n",
      "Epoch 14333: train loss: 0.1740, test loss 0.7668\n",
      "Epoch 14334: train loss: 0.1740, test loss 0.7668\n",
      "Epoch 14335: train loss: 0.1740, test loss 0.7668\n",
      "Epoch 14336: train loss: 0.1740, test loss 0.7668\n",
      "Epoch 14337: train loss: 0.1740, test loss 0.7667\n",
      "Epoch 14338: train loss: 0.1740, test loss 0.7667\n",
      "Epoch 14339: train loss: 0.1740, test loss 0.7667\n",
      "Epoch 14340: train loss: 0.1740, test loss 0.7667\n",
      "Epoch 14341: train loss: 0.1740, test loss 0.7667\n",
      "Epoch 14342: train loss: 0.1740, test loss 0.7666\n",
      "Epoch 14343: train loss: 0.1740, test loss 0.7666\n",
      "Epoch 14344: train loss: 0.1740, test loss 0.7666\n",
      "Epoch 14345: train loss: 0.1740, test loss 0.7666\n",
      "Epoch 14346: train loss: 0.1740, test loss 0.7665\n",
      "Epoch 14347: train loss: 0.1740, test loss 0.7665\n",
      "Epoch 14348: train loss: 0.1740, test loss 0.7665\n",
      "Epoch 14349: train loss: 0.1740, test loss 0.7665\n",
      "Epoch 14350: train loss: 0.1740, test loss 0.7665\n",
      "Epoch 14351: train loss: 0.1740, test loss 0.7664\n",
      "Epoch 14352: train loss: 0.1740, test loss 0.7664\n",
      "Epoch 14353: train loss: 0.1740, test loss 0.7664\n",
      "Epoch 14354: train loss: 0.1740, test loss 0.7664\n",
      "Epoch 14355: train loss: 0.1740, test loss 0.7664\n",
      "Epoch 14356: train loss: 0.1740, test loss 0.7663\n",
      "Epoch 14357: train loss: 0.1740, test loss 0.7663\n",
      "Epoch 14358: train loss: 0.1740, test loss 0.7663\n",
      "Epoch 14359: train loss: 0.1740, test loss 0.7663\n",
      "Epoch 14360: train loss: 0.1740, test loss 0.7663\n",
      "Epoch 14361: train loss: 0.1740, test loss 0.7662\n",
      "Epoch 14362: train loss: 0.1740, test loss 0.7662\n",
      "Epoch 14363: train loss: 0.1740, test loss 0.7662\n",
      "Epoch 14364: train loss: 0.1740, test loss 0.7662\n",
      "Epoch 14365: train loss: 0.1740, test loss 0.7662\n",
      "Epoch 14366: train loss: 0.1740, test loss 0.7661\n",
      "Epoch 14367: train loss: 0.1740, test loss 0.7661\n",
      "Epoch 14368: train loss: 0.1740, test loss 0.7661\n",
      "Epoch 14369: train loss: 0.1740, test loss 0.7661\n",
      "Epoch 14370: train loss: 0.1740, test loss 0.7661\n",
      "Epoch 14371: train loss: 0.1740, test loss 0.7660\n",
      "Epoch 14372: train loss: 0.1740, test loss 0.7660\n",
      "Epoch 14373: train loss: 0.1740, test loss 0.7660\n",
      "Epoch 14374: train loss: 0.1740, test loss 0.7660\n",
      "Epoch 14375: train loss: 0.1740, test loss 0.7660\n",
      "Epoch 14376: train loss: 0.1740, test loss 0.7659\n",
      "Epoch 14377: train loss: 0.1740, test loss 0.7659\n",
      "Epoch 14378: train loss: 0.1740, test loss 0.7659\n",
      "Epoch 14379: train loss: 0.1740, test loss 0.7659\n",
      "Epoch 14380: train loss: 0.1740, test loss 0.7659\n",
      "Epoch 14381: train loss: 0.1740, test loss 0.7658\n",
      "Epoch 14382: train loss: 0.1740, test loss 0.7658\n",
      "Epoch 14383: train loss: 0.1740, test loss 0.7658\n",
      "Epoch 14384: train loss: 0.1740, test loss 0.7658\n",
      "Epoch 14385: train loss: 0.1740, test loss 0.7658\n",
      "Epoch 14386: train loss: 0.1740, test loss 0.7657\n",
      "Epoch 14387: train loss: 0.1740, test loss 0.7657\n",
      "Epoch 14388: train loss: 0.1740, test loss 0.7657\n",
      "Epoch 14389: train loss: 0.1740, test loss 0.7657\n",
      "Epoch 14390: train loss: 0.1740, test loss 0.7657\n",
      "Epoch 14391: train loss: 0.1740, test loss 0.7656\n",
      "Epoch 14392: train loss: 0.1740, test loss 0.7656\n",
      "Epoch 14393: train loss: 0.1740, test loss 0.7656\n",
      "Epoch 14394: train loss: 0.1740, test loss 0.7656\n",
      "Epoch 14395: train loss: 0.1740, test loss 0.7656\n",
      "Epoch 14396: train loss: 0.1740, test loss 0.7655\n",
      "Epoch 14397: train loss: 0.1739, test loss 0.7655\n",
      "Epoch 14398: train loss: 0.1739, test loss 0.7655\n",
      "Epoch 14399: train loss: 0.1739, test loss 0.7655\n",
      "Epoch 14400: train loss: 0.1739, test loss 0.7654\n",
      "Epoch 14401: train loss: 0.1739, test loss 0.7654\n",
      "Epoch 14402: train loss: 0.1739, test loss 0.7654\n",
      "Epoch 14403: train loss: 0.1739, test loss 0.7654\n",
      "Epoch 14404: train loss: 0.1739, test loss 0.7654\n",
      "Epoch 14405: train loss: 0.1739, test loss 0.7653\n",
      "Epoch 14406: train loss: 0.1739, test loss 0.7653\n",
      "Epoch 14407: train loss: 0.1739, test loss 0.7653\n",
      "Epoch 14408: train loss: 0.1739, test loss 0.7653\n",
      "Epoch 14409: train loss: 0.1739, test loss 0.7653\n",
      "Epoch 14410: train loss: 0.1739, test loss 0.7652\n",
      "Epoch 14411: train loss: 0.1739, test loss 0.7652\n",
      "Epoch 14412: train loss: 0.1739, test loss 0.7652\n",
      "Epoch 14413: train loss: 0.1739, test loss 0.7652\n",
      "Epoch 14414: train loss: 0.1739, test loss 0.7652\n",
      "Epoch 14415: train loss: 0.1739, test loss 0.7651\n",
      "Epoch 14416: train loss: 0.1739, test loss 0.7651\n",
      "Epoch 14417: train loss: 0.1739, test loss 0.7651\n",
      "Epoch 14418: train loss: 0.1739, test loss 0.7651\n",
      "Epoch 14419: train loss: 0.1739, test loss 0.7651\n",
      "Epoch 14420: train loss: 0.1739, test loss 0.7650\n",
      "Epoch 14421: train loss: 0.1739, test loss 0.7650\n",
      "Epoch 14422: train loss: 0.1739, test loss 0.7650\n",
      "Epoch 14423: train loss: 0.1739, test loss 0.7650\n",
      "Epoch 14424: train loss: 0.1739, test loss 0.7650\n",
      "Epoch 14425: train loss: 0.1739, test loss 0.7649\n",
      "Epoch 14426: train loss: 0.1739, test loss 0.7649\n",
      "Epoch 14427: train loss: 0.1739, test loss 0.7649\n",
      "Epoch 14428: train loss: 0.1739, test loss 0.7649\n",
      "Epoch 14429: train loss: 0.1739, test loss 0.7649\n",
      "Epoch 14430: train loss: 0.1739, test loss 0.7648\n",
      "Epoch 14431: train loss: 0.1739, test loss 0.7648\n",
      "Epoch 14432: train loss: 0.1739, test loss 0.7648\n",
      "Epoch 14433: train loss: 0.1739, test loss 0.7648\n",
      "Epoch 14434: train loss: 0.1739, test loss 0.7648\n",
      "Epoch 14435: train loss: 0.1739, test loss 0.7647\n",
      "Epoch 14436: train loss: 0.1739, test loss 0.7647\n",
      "Epoch 14437: train loss: 0.1739, test loss 0.7647\n",
      "Epoch 14438: train loss: 0.1739, test loss 0.7647\n",
      "Epoch 14439: train loss: 0.1739, test loss 0.7647\n",
      "Epoch 14440: train loss: 0.1739, test loss 0.7646\n",
      "Epoch 14441: train loss: 0.1739, test loss 0.7646\n",
      "Epoch 14442: train loss: 0.1739, test loss 0.7646\n",
      "Epoch 14443: train loss: 0.1739, test loss 0.7646\n",
      "Epoch 14444: train loss: 0.1739, test loss 0.7646\n",
      "Epoch 14445: train loss: 0.1739, test loss 0.7645\n",
      "Epoch 14446: train loss: 0.1739, test loss 0.7645\n",
      "Epoch 14447: train loss: 0.1739, test loss 0.7645\n",
      "Epoch 14448: train loss: 0.1739, test loss 0.7645\n",
      "Epoch 14449: train loss: 0.1739, test loss 0.7644\n",
      "Epoch 14450: train loss: 0.1739, test loss 0.7644\n",
      "Epoch 14451: train loss: 0.1739, test loss 0.7644\n",
      "Epoch 14452: train loss: 0.1739, test loss 0.7644\n",
      "Epoch 14453: train loss: 0.1739, test loss 0.7644\n",
      "Epoch 14454: train loss: 0.1739, test loss 0.7643\n",
      "Epoch 14455: train loss: 0.1739, test loss 0.7643\n",
      "Epoch 14456: train loss: 0.1739, test loss 0.7643\n",
      "Epoch 14457: train loss: 0.1739, test loss 0.7643\n",
      "Epoch 14458: train loss: 0.1739, test loss 0.7643\n",
      "Epoch 14459: train loss: 0.1739, test loss 0.7642\n",
      "Epoch 14460: train loss: 0.1739, test loss 0.7642\n",
      "Epoch 14461: train loss: 0.1739, test loss 0.7642\n",
      "Epoch 14462: train loss: 0.1739, test loss 0.7642\n",
      "Epoch 14463: train loss: 0.1739, test loss 0.7642\n",
      "Epoch 14464: train loss: 0.1739, test loss 0.7641\n",
      "Epoch 14465: train loss: 0.1739, test loss 0.7641\n",
      "Epoch 14466: train loss: 0.1739, test loss 0.7641\n",
      "Epoch 14467: train loss: 0.1739, test loss 0.7641\n",
      "Epoch 14468: train loss: 0.1739, test loss 0.7641\n",
      "Epoch 14469: train loss: 0.1739, test loss 0.7640\n",
      "Epoch 14470: train loss: 0.1739, test loss 0.7640\n",
      "Epoch 14471: train loss: 0.1739, test loss 0.7640\n",
      "Epoch 14472: train loss: 0.1739, test loss 0.7640\n",
      "Epoch 14473: train loss: 0.1739, test loss 0.7640\n",
      "Epoch 14474: train loss: 0.1739, test loss 0.7639\n",
      "Epoch 14475: train loss: 0.1739, test loss 0.7639\n",
      "Epoch 14476: train loss: 0.1739, test loss 0.7639\n",
      "Epoch 14477: train loss: 0.1739, test loss 0.7639\n",
      "Epoch 14478: train loss: 0.1739, test loss 0.7639\n",
      "Epoch 14479: train loss: 0.1739, test loss 0.7638\n",
      "Epoch 14480: train loss: 0.1739, test loss 0.7638\n",
      "Epoch 14481: train loss: 0.1739, test loss 0.7638\n",
      "Epoch 14482: train loss: 0.1739, test loss 0.7638\n",
      "Epoch 14483: train loss: 0.1739, test loss 0.7638\n",
      "Epoch 14484: train loss: 0.1739, test loss 0.7637\n",
      "Epoch 14485: train loss: 0.1739, test loss 0.7637\n",
      "Epoch 14486: train loss: 0.1739, test loss 0.7637\n",
      "Epoch 14487: train loss: 0.1739, test loss 0.7637\n",
      "Epoch 14488: train loss: 0.1739, test loss 0.7637\n",
      "Epoch 14489: train loss: 0.1739, test loss 0.7636\n",
      "Epoch 14490: train loss: 0.1739, test loss 0.7636\n",
      "Epoch 14491: train loss: 0.1739, test loss 0.7636\n",
      "Epoch 14492: train loss: 0.1739, test loss 0.7636\n",
      "Epoch 14493: train loss: 0.1739, test loss 0.7636\n",
      "Epoch 14494: train loss: 0.1739, test loss 0.7635\n",
      "Epoch 14495: train loss: 0.1739, test loss 0.7635\n",
      "Epoch 14496: train loss: 0.1739, test loss 0.7635\n",
      "Epoch 14497: train loss: 0.1739, test loss 0.7635\n",
      "Epoch 14498: train loss: 0.1739, test loss 0.7635\n",
      "Epoch 14499: train loss: 0.1739, test loss 0.7634\n",
      "Epoch 14500: train loss: 0.1739, test loss 0.7634\n",
      "Epoch 14501: train loss: 0.1739, test loss 0.7634\n",
      "Epoch 14502: train loss: 0.1739, test loss 0.7634\n",
      "Epoch 14503: train loss: 0.1739, test loss 0.7634\n",
      "Epoch 14504: train loss: 0.1739, test loss 0.7633\n",
      "Epoch 14505: train loss: 0.1739, test loss 0.7633\n",
      "Epoch 14506: train loss: 0.1739, test loss 0.7633\n",
      "Epoch 14507: train loss: 0.1739, test loss 0.7633\n",
      "Epoch 14508: train loss: 0.1739, test loss 0.7633\n",
      "Epoch 14509: train loss: 0.1739, test loss 0.7632\n",
      "Epoch 14510: train loss: 0.1739, test loss 0.7632\n",
      "Epoch 14511: train loss: 0.1739, test loss 0.7632\n",
      "Epoch 14512: train loss: 0.1739, test loss 0.7632\n",
      "Epoch 14513: train loss: 0.1739, test loss 0.7632\n",
      "Epoch 14514: train loss: 0.1739, test loss 0.7631\n",
      "Epoch 14515: train loss: 0.1739, test loss 0.7631\n",
      "Epoch 14516: train loss: 0.1739, test loss 0.7631\n",
      "Epoch 14517: train loss: 0.1739, test loss 0.7631\n",
      "Epoch 14518: train loss: 0.1739, test loss 0.7631\n",
      "Epoch 14519: train loss: 0.1739, test loss 0.7630\n",
      "Epoch 14520: train loss: 0.1739, test loss 0.7630\n",
      "Epoch 14521: train loss: 0.1739, test loss 0.7630\n",
      "Epoch 14522: train loss: 0.1739, test loss 0.7630\n",
      "Epoch 14523: train loss: 0.1739, test loss 0.7630\n",
      "Epoch 14524: train loss: 0.1739, test loss 0.7629\n",
      "Epoch 14525: train loss: 0.1739, test loss 0.7629\n",
      "Epoch 14526: train loss: 0.1739, test loss 0.7629\n",
      "Epoch 14527: train loss: 0.1739, test loss 0.7629\n",
      "Epoch 14528: train loss: 0.1739, test loss 0.7629\n",
      "Epoch 14529: train loss: 0.1739, test loss 0.7628\n",
      "Epoch 14530: train loss: 0.1739, test loss 0.7628\n",
      "Epoch 14531: train loss: 0.1739, test loss 0.7628\n",
      "Epoch 14532: train loss: 0.1739, test loss 0.7628\n",
      "Epoch 14533: train loss: 0.1739, test loss 0.7628\n",
      "Epoch 14534: train loss: 0.1739, test loss 0.7627\n",
      "Epoch 14535: train loss: 0.1739, test loss 0.7627\n",
      "Epoch 14536: train loss: 0.1739, test loss 0.7627\n",
      "Epoch 14537: train loss: 0.1739, test loss 0.7627\n",
      "Epoch 14538: train loss: 0.1739, test loss 0.7627\n",
      "Epoch 14539: train loss: 0.1739, test loss 0.7626\n",
      "Epoch 14540: train loss: 0.1739, test loss 0.7626\n",
      "Epoch 14541: train loss: 0.1739, test loss 0.7626\n",
      "Epoch 14542: train loss: 0.1739, test loss 0.7626\n",
      "Epoch 14543: train loss: 0.1738, test loss 0.7626\n",
      "Epoch 14544: train loss: 0.1738, test loss 0.7625\n",
      "Epoch 14545: train loss: 0.1738, test loss 0.7625\n",
      "Epoch 14546: train loss: 0.1738, test loss 0.7625\n",
      "Epoch 14547: train loss: 0.1738, test loss 0.7625\n",
      "Epoch 14548: train loss: 0.1738, test loss 0.7625\n",
      "Epoch 14549: train loss: 0.1738, test loss 0.7624\n",
      "Epoch 14550: train loss: 0.1738, test loss 0.7624\n",
      "Epoch 14551: train loss: 0.1738, test loss 0.7624\n",
      "Epoch 14552: train loss: 0.1738, test loss 0.7624\n",
      "Epoch 14553: train loss: 0.1738, test loss 0.7624\n",
      "Epoch 14554: train loss: 0.1738, test loss 0.7623\n",
      "Epoch 14555: train loss: 0.1738, test loss 0.7623\n",
      "Epoch 14556: train loss: 0.1738, test loss 0.7623\n",
      "Epoch 14557: train loss: 0.1738, test loss 0.7623\n",
      "Epoch 14558: train loss: 0.1738, test loss 0.7622\n",
      "Epoch 14559: train loss: 0.1738, test loss 0.7622\n",
      "Epoch 14560: train loss: 0.1738, test loss 0.7622\n",
      "Epoch 14561: train loss: 0.1738, test loss 0.7622\n",
      "Epoch 14562: train loss: 0.1738, test loss 0.7622\n",
      "Epoch 14563: train loss: 0.1738, test loss 0.7621\n",
      "Epoch 14564: train loss: 0.1738, test loss 0.7621\n",
      "Epoch 14565: train loss: 0.1738, test loss 0.7621\n",
      "Epoch 14566: train loss: 0.1738, test loss 0.7621\n",
      "Epoch 14567: train loss: 0.1738, test loss 0.7621\n",
      "Epoch 14568: train loss: 0.1738, test loss 0.7620\n",
      "Epoch 14569: train loss: 0.1738, test loss 0.7620\n",
      "Epoch 14570: train loss: 0.1738, test loss 0.7620\n",
      "Epoch 14571: train loss: 0.1738, test loss 0.7620\n",
      "Epoch 14572: train loss: 0.1738, test loss 0.7620\n",
      "Epoch 14573: train loss: 0.1738, test loss 0.7619\n",
      "Epoch 14574: train loss: 0.1738, test loss 0.7619\n",
      "Epoch 14575: train loss: 0.1738, test loss 0.7619\n",
      "Epoch 14576: train loss: 0.1738, test loss 0.7619\n",
      "Epoch 14577: train loss: 0.1738, test loss 0.7619\n",
      "Epoch 14578: train loss: 0.1738, test loss 0.7618\n",
      "Epoch 14579: train loss: 0.1738, test loss 0.7618\n",
      "Epoch 14580: train loss: 0.1738, test loss 0.7618\n",
      "Epoch 14581: train loss: 0.1738, test loss 0.7618\n",
      "Epoch 14582: train loss: 0.1738, test loss 0.7617\n",
      "Epoch 14583: train loss: 0.1738, test loss 0.7617\n",
      "Epoch 14584: train loss: 0.1738, test loss 0.7617\n",
      "Epoch 14585: train loss: 0.1738, test loss 0.7617\n",
      "Epoch 14586: train loss: 0.1738, test loss 0.7617\n",
      "Epoch 14587: train loss: 0.1738, test loss 0.7616\n",
      "Epoch 14588: train loss: 0.1738, test loss 0.7616\n",
      "Epoch 14589: train loss: 0.1738, test loss 0.7616\n",
      "Epoch 14590: train loss: 0.1738, test loss 0.7616\n",
      "Epoch 14591: train loss: 0.1738, test loss 0.7616\n",
      "Epoch 14592: train loss: 0.1738, test loss 0.7615\n",
      "Epoch 14593: train loss: 0.1738, test loss 0.7615\n",
      "Epoch 14594: train loss: 0.1738, test loss 0.7615\n",
      "Epoch 14595: train loss: 0.1738, test loss 0.7615\n",
      "Epoch 14596: train loss: 0.1738, test loss 0.7615\n",
      "Epoch 14597: train loss: 0.1738, test loss 0.7614\n",
      "Epoch 14598: train loss: 0.1738, test loss 0.7614\n",
      "Epoch 14599: train loss: 0.1738, test loss 0.7614\n",
      "Epoch 14600: train loss: 0.1738, test loss 0.7614\n",
      "Epoch 14601: train loss: 0.1738, test loss 0.7614\n",
      "Epoch 14602: train loss: 0.1738, test loss 0.7613\n",
      "Epoch 14603: train loss: 0.1738, test loss 0.7613\n",
      "Epoch 14604: train loss: 0.1738, test loss 0.7613\n",
      "Epoch 14605: train loss: 0.1738, test loss 0.7613\n",
      "Epoch 14606: train loss: 0.1738, test loss 0.7613\n",
      "Epoch 14607: train loss: 0.1738, test loss 0.7612\n",
      "Epoch 14608: train loss: 0.1738, test loss 0.7612\n",
      "Epoch 14609: train loss: 0.1738, test loss 0.7612\n",
      "Epoch 14610: train loss: 0.1738, test loss 0.7612\n",
      "Epoch 14611: train loss: 0.1738, test loss 0.7612\n",
      "Epoch 14612: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14613: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14614: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14615: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14616: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14617: train loss: 0.1738, test loss 0.7611\n",
      "Epoch 14618: train loss: 0.1738, test loss 0.7610\n",
      "Epoch 14619: train loss: 0.1738, test loss 0.7610\n",
      "Epoch 14620: train loss: 0.1738, test loss 0.7610\n",
      "Epoch 14621: train loss: 0.1738, test loss 0.7610\n",
      "Epoch 14622: train loss: 0.1738, test loss 0.7610\n",
      "Epoch 14623: train loss: 0.1738, test loss 0.7609\n",
      "Epoch 14624: train loss: 0.1738, test loss 0.7609\n",
      "Epoch 14625: train loss: 0.1738, test loss 0.7609\n",
      "Epoch 14626: train loss: 0.1738, test loss 0.7609\n",
      "Epoch 14627: train loss: 0.1738, test loss 0.7609\n",
      "Epoch 14628: train loss: 0.1738, test loss 0.7608\n",
      "Epoch 14629: train loss: 0.1738, test loss 0.7608\n",
      "Epoch 14630: train loss: 0.1738, test loss 0.7608\n",
      "Epoch 14631: train loss: 0.1738, test loss 0.7608\n",
      "Epoch 14632: train loss: 0.1738, test loss 0.7608\n",
      "Epoch 14633: train loss: 0.1738, test loss 0.7607\n",
      "Epoch 14634: train loss: 0.1738, test loss 0.7607\n",
      "Epoch 14635: train loss: 0.1738, test loss 0.7607\n",
      "Epoch 14636: train loss: 0.1738, test loss 0.7607\n",
      "Epoch 14637: train loss: 0.1738, test loss 0.7607\n",
      "Epoch 14638: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14639: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14640: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14641: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14642: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14643: train loss: 0.1738, test loss 0.7606\n",
      "Epoch 14644: train loss: 0.1738, test loss 0.7605\n",
      "Epoch 14645: train loss: 0.1738, test loss 0.7605\n",
      "Epoch 14646: train loss: 0.1738, test loss 0.7605\n",
      "Epoch 14647: train loss: 0.1738, test loss 0.7605\n",
      "Epoch 14648: train loss: 0.1738, test loss 0.7605\n",
      "Epoch 14649: train loss: 0.1738, test loss 0.7604\n",
      "Epoch 14650: train loss: 0.1738, test loss 0.7604\n",
      "Epoch 14651: train loss: 0.1738, test loss 0.7604\n",
      "Epoch 14652: train loss: 0.1738, test loss 0.7604\n",
      "Epoch 14653: train loss: 0.1738, test loss 0.7604\n",
      "Epoch 14654: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14655: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14656: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14657: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14658: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14659: train loss: 0.1738, test loss 0.7603\n",
      "Epoch 14660: train loss: 0.1738, test loss 0.7602\n",
      "Epoch 14661: train loss: 0.1738, test loss 0.7602\n",
      "Epoch 14662: train loss: 0.1738, test loss 0.7602\n",
      "Epoch 14663: train loss: 0.1738, test loss 0.7602\n",
      "Epoch 14664: train loss: 0.1738, test loss 0.7602\n",
      "Epoch 14665: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14666: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14667: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14668: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14669: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14670: train loss: 0.1738, test loss 0.7601\n",
      "Epoch 14671: train loss: 0.1738, test loss 0.7600\n",
      "Epoch 14672: train loss: 0.1738, test loss 0.7600\n",
      "Epoch 14673: train loss: 0.1738, test loss 0.7600\n",
      "Epoch 14674: train loss: 0.1738, test loss 0.7600\n",
      "Epoch 14675: train loss: 0.1738, test loss 0.7600\n",
      "Epoch 14676: train loss: 0.1738, test loss 0.7599\n",
      "Epoch 14677: train loss: 0.1738, test loss 0.7599\n",
      "Epoch 14678: train loss: 0.1738, test loss 0.7599\n",
      "Epoch 14679: train loss: 0.1738, test loss 0.7599\n",
      "Epoch 14680: train loss: 0.1738, test loss 0.7599\n",
      "Epoch 14681: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14682: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14683: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14684: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14685: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14686: train loss: 0.1738, test loss 0.7598\n",
      "Epoch 14687: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14688: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14689: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14690: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14691: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14692: train loss: 0.1738, test loss 0.7597\n",
      "Epoch 14693: train loss: 0.1738, test loss 0.7596\n",
      "Epoch 14694: train loss: 0.1738, test loss 0.7596\n",
      "Epoch 14695: train loss: 0.1737, test loss 0.7596\n",
      "Epoch 14696: train loss: 0.1737, test loss 0.7596\n",
      "Epoch 14697: train loss: 0.1737, test loss 0.7596\n",
      "Epoch 14698: train loss: 0.1737, test loss 0.7596\n",
      "Epoch 14699: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14700: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14701: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14702: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14703: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14704: train loss: 0.1737, test loss 0.7595\n",
      "Epoch 14705: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14706: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14707: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14708: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14709: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14710: train loss: 0.1737, test loss 0.7594\n",
      "Epoch 14711: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14712: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14713: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14714: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14715: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14716: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14717: train loss: 0.1737, test loss 0.7593\n",
      "Epoch 14718: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14719: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14720: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14721: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14722: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14723: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14724: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14725: train loss: 0.1737, test loss 0.7592\n",
      "Epoch 14726: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14727: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14728: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14729: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14730: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14731: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14732: train loss: 0.1737, test loss 0.7591\n",
      "Epoch 14733: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14734: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14735: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14736: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14737: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14738: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14739: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14740: train loss: 0.1737, test loss 0.7590\n",
      "Epoch 14741: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14742: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14743: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14744: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14745: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14746: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14747: train loss: 0.1737, test loss 0.7589\n",
      "Epoch 14748: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14749: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14750: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14751: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14752: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14753: train loss: 0.1737, test loss 0.7588\n",
      "Epoch 14754: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14755: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14756: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14757: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14758: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14759: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14760: train loss: 0.1737, test loss 0.7587\n",
      "Epoch 14761: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14762: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14763: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14764: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14765: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14766: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14767: train loss: 0.1737, test loss 0.7586\n",
      "Epoch 14768: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14769: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14770: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14771: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14772: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14773: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14774: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14775: train loss: 0.1737, test loss 0.7585\n",
      "Epoch 14776: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14777: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14778: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14779: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14780: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14781: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14782: train loss: 0.1737, test loss 0.7584\n",
      "Epoch 14783: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14784: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14785: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14786: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14787: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14788: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14789: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14790: train loss: 0.1737, test loss 0.7583\n",
      "Epoch 14791: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14792: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14793: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14794: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14795: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14796: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14797: train loss: 0.1737, test loss 0.7582\n",
      "Epoch 14798: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14799: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14800: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14801: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14802: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14803: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14804: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14805: train loss: 0.1737, test loss 0.7581\n",
      "Epoch 14806: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14807: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14808: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14809: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14810: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14811: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14812: train loss: 0.1737, test loss 0.7580\n",
      "Epoch 14813: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14814: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14815: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14816: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14817: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14818: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14819: train loss: 0.1737, test loss 0.7579\n",
      "Epoch 14820: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14821: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14822: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14823: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14824: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14825: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14826: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14827: train loss: 0.1737, test loss 0.7578\n",
      "Epoch 14828: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14829: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14830: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14831: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14832: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14833: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14834: train loss: 0.1737, test loss 0.7577\n",
      "Epoch 14835: train loss: 0.1736, test loss 0.7577\n",
      "Epoch 14836: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14837: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14838: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14839: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14840: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14841: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14842: train loss: 0.1736, test loss 0.7576\n",
      "Epoch 14843: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14844: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14845: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14846: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14847: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14848: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14849: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14850: train loss: 0.1736, test loss 0.7575\n",
      "Epoch 14851: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14852: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14853: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14854: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14855: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14856: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14857: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14858: train loss: 0.1736, test loss 0.7574\n",
      "Epoch 14859: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14860: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14861: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14862: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14863: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14864: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14865: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14866: train loss: 0.1736, test loss 0.7573\n",
      "Epoch 14867: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14868: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14869: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14870: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14871: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14872: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14873: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14874: train loss: 0.1736, test loss 0.7572\n",
      "Epoch 14875: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14876: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14877: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14878: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14879: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14880: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14881: train loss: 0.1736, test loss 0.7571\n",
      "Epoch 14882: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14883: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14884: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14885: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14886: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14887: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14888: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14889: train loss: 0.1736, test loss 0.7570\n",
      "Epoch 14890: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14891: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14892: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14893: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14894: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14895: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14896: train loss: 0.1736, test loss 0.7569\n",
      "Epoch 14897: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14898: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14899: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14900: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14901: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14902: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14903: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14904: train loss: 0.1736, test loss 0.7568\n",
      "Epoch 14905: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14906: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14907: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14908: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14909: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14910: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14911: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14912: train loss: 0.1736, test loss 0.7567\n",
      "Epoch 14913: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14914: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14915: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14916: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14917: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14918: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14919: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14920: train loss: 0.1736, test loss 0.7566\n",
      "Epoch 14921: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14922: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14923: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14924: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14925: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14926: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14927: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14928: train loss: 0.1736, test loss 0.7565\n",
      "Epoch 14929: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14930: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14931: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14932: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14933: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14934: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14935: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14936: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14937: train loss: 0.1736, test loss 0.7564\n",
      "Epoch 14938: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14939: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14940: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14941: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14942: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14943: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14944: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14945: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14946: train loss: 0.1736, test loss 0.7563\n",
      "Epoch 14947: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14948: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14949: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14950: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14951: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14952: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14953: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14954: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14955: train loss: 0.1736, test loss 0.7562\n",
      "Epoch 14956: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14957: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14958: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14959: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14960: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14961: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14962: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14963: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14964: train loss: 0.1736, test loss 0.7561\n",
      "Epoch 14965: train loss: 0.1736, test loss 0.7560\n",
      "Epoch 14966: train loss: 0.1736, test loss 0.7560\n",
      "Epoch 14967: train loss: 0.1736, test loss 0.7560\n",
      "Epoch 14968: train loss: 0.1736, test loss 0.7560\n",
      "Epoch 14969: train loss: 0.1735, test loss 0.7560\n",
      "Epoch 14970: train loss: 0.1735, test loss 0.7560\n",
      "Epoch 14971: train loss: 0.1735, test loss 0.7560\n",
      "Epoch 14972: train loss: 0.1735, test loss 0.7560\n",
      "Epoch 14973: train loss: 0.1735, test loss 0.7560\n",
      "Epoch 14974: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14975: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14976: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14977: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14978: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14979: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14980: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14981: train loss: 0.1735, test loss 0.7559\n",
      "Epoch 14982: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14983: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14984: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14985: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14986: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14987: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14988: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14989: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14990: train loss: 0.1735, test loss 0.7558\n",
      "Epoch 14991: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14992: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14993: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14994: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14995: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14996: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14997: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14998: train loss: 0.1735, test loss 0.7557\n",
      "Epoch 14999: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15000: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15001: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15002: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15003: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15004: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15005: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15006: train loss: 0.1735, test loss 0.7556\n",
      "Epoch 15007: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15008: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15009: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15010: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15011: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15012: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15013: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15014: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15015: train loss: 0.1735, test loss 0.7555\n",
      "Epoch 15016: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15017: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15018: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15019: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15020: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15021: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15022: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15023: train loss: 0.1735, test loss 0.7554\n",
      "Epoch 15024: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15025: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15026: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15027: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15028: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15029: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15030: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15031: train loss: 0.1735, test loss 0.7553\n",
      "Epoch 15032: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15033: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15034: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15035: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15036: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15037: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15038: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15039: train loss: 0.1735, test loss 0.7552\n",
      "Epoch 15040: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15041: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15042: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15043: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15044: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15045: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15046: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15047: train loss: 0.1735, test loss 0.7551\n",
      "Epoch 15048: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15049: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15050: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15051: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15052: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15053: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15054: train loss: 0.1735, test loss 0.7550\n",
      "Epoch 15055: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15056: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15057: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15058: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15059: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15060: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15061: train loss: 0.1735, test loss 0.7549\n",
      "Epoch 15062: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15063: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15064: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15065: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15066: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15067: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15068: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15069: train loss: 0.1735, test loss 0.7548\n",
      "Epoch 15070: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15071: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15072: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15073: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15074: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15075: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15076: train loss: 0.1735, test loss 0.7547\n",
      "Epoch 15077: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15078: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15079: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15080: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15081: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15082: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15083: train loss: 0.1735, test loss 0.7546\n",
      "Epoch 15084: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15085: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15086: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15087: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15088: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15089: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15090: train loss: 0.1735, test loss 0.7545\n",
      "Epoch 15091: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15092: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15093: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15094: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15095: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15096: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15097: train loss: 0.1735, test loss 0.7544\n",
      "Epoch 15098: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15099: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15100: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15101: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15102: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15103: train loss: 0.1735, test loss 0.7543\n",
      "Epoch 15104: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15105: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15106: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15107: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15108: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15109: train loss: 0.1734, test loss 0.7542\n",
      "Epoch 15110: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15111: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15112: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15113: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15114: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15115: train loss: 0.1734, test loss 0.7541\n",
      "Epoch 15116: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15117: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15118: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15119: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15120: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15121: train loss: 0.1734, test loss 0.7540\n",
      "Epoch 15122: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15123: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15124: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15125: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15126: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15127: train loss: 0.1734, test loss 0.7539\n",
      "Epoch 15128: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15129: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15130: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15131: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15132: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15133: train loss: 0.1734, test loss 0.7538\n",
      "Epoch 15134: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15135: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15136: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15137: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15138: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15139: train loss: 0.1734, test loss 0.7537\n",
      "Epoch 15140: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15141: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15142: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15143: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15144: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15145: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15146: train loss: 0.1734, test loss 0.7536\n",
      "Epoch 15147: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15148: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15149: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15150: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15151: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15152: train loss: 0.1734, test loss 0.7535\n",
      "Epoch 15153: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15154: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15155: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15156: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15157: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15158: train loss: 0.1734, test loss 0.7534\n",
      "Epoch 15159: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15160: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15161: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15162: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15163: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15164: train loss: 0.1734, test loss 0.7533\n",
      "Epoch 15165: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15166: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15167: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15168: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15169: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15170: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15171: train loss: 0.1734, test loss 0.7532\n",
      "Epoch 15172: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15173: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15174: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15175: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15176: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15177: train loss: 0.1734, test loss 0.7531\n",
      "Epoch 15178: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15179: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15180: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15181: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15182: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15183: train loss: 0.1734, test loss 0.7530\n",
      "Epoch 15184: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15185: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15186: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15187: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15188: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15189: train loss: 0.1734, test loss 0.7529\n",
      "Epoch 15190: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15191: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15192: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15193: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15194: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15195: train loss: 0.1734, test loss 0.7528\n",
      "Epoch 15196: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15197: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15198: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15199: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15200: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15201: train loss: 0.1734, test loss 0.7527\n",
      "Epoch 15202: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15203: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15204: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15205: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15206: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15207: train loss: 0.1734, test loss 0.7526\n",
      "Epoch 15208: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15209: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15210: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15211: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15212: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15213: train loss: 0.1734, test loss 0.7525\n",
      "Epoch 15214: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15215: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15216: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15217: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15218: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15219: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15220: train loss: 0.1734, test loss 0.7524\n",
      "Epoch 15221: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15222: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15223: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15224: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15225: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15226: train loss: 0.1734, test loss 0.7523\n",
      "Epoch 15227: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15228: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15229: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15230: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15231: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15232: train loss: 0.1734, test loss 0.7522\n",
      "Epoch 15233: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15234: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15235: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15236: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15237: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15238: train loss: 0.1734, test loss 0.7521\n",
      "Epoch 15239: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15240: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15241: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15242: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15243: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15244: train loss: 0.1734, test loss 0.7520\n",
      "Epoch 15245: train loss: 0.1734, test loss 0.7519\n",
      "Epoch 15246: train loss: 0.1734, test loss 0.7519\n",
      "Epoch 15247: train loss: 0.1734, test loss 0.7519\n",
      "Epoch 15248: train loss: 0.1734, test loss 0.7519\n",
      "Epoch 15249: train loss: 0.1734, test loss 0.7519\n",
      "Epoch 15250: train loss: 0.1734, test loss 0.7518\n",
      "Epoch 15251: train loss: 0.1733, test loss 0.7518\n",
      "Epoch 15252: train loss: 0.1733, test loss 0.7518\n",
      "Epoch 15253: train loss: 0.1733, test loss 0.7518\n",
      "Epoch 15254: train loss: 0.1733, test loss 0.7518\n",
      "Epoch 15255: train loss: 0.1733, test loss 0.7518\n",
      "Epoch 15256: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15257: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15258: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15259: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15260: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15261: train loss: 0.1733, test loss 0.7517\n",
      "Epoch 15262: train loss: 0.1733, test loss 0.7516\n",
      "Epoch 15263: train loss: 0.1733, test loss 0.7516\n",
      "Epoch 15264: train loss: 0.1733, test loss 0.7516\n",
      "Epoch 15265: train loss: 0.1733, test loss 0.7516\n",
      "Epoch 15266: train loss: 0.1733, test loss 0.7515\n",
      "Epoch 15267: train loss: 0.1733, test loss 0.7515\n",
      "Epoch 15268: train loss: 0.1733, test loss 0.7515\n",
      "Epoch 15269: train loss: 0.1733, test loss 0.7515\n",
      "Epoch 15270: train loss: 0.1733, test loss 0.7515\n",
      "Epoch 15271: train loss: 0.1733, test loss 0.7514\n",
      "Epoch 15272: train loss: 0.1733, test loss 0.7514\n",
      "Epoch 15273: train loss: 0.1733, test loss 0.7514\n",
      "Epoch 15274: train loss: 0.1733, test loss 0.7514\n",
      "Epoch 15275: train loss: 0.1733, test loss 0.7514\n",
      "Epoch 15276: train loss: 0.1733, test loss 0.7513\n",
      "Epoch 15277: train loss: 0.1733, test loss 0.7513\n",
      "Epoch 15278: train loss: 0.1733, test loss 0.7513\n",
      "Epoch 15279: train loss: 0.1733, test loss 0.7513\n",
      "Epoch 15280: train loss: 0.1733, test loss 0.7512\n",
      "Epoch 15281: train loss: 0.1733, test loss 0.7512\n",
      "Epoch 15282: train loss: 0.1733, test loss 0.7512\n",
      "Epoch 15283: train loss: 0.1733, test loss 0.7512\n",
      "Epoch 15284: train loss: 0.1733, test loss 0.7512\n",
      "Epoch 15285: train loss: 0.1733, test loss 0.7511\n",
      "Epoch 15286: train loss: 0.1733, test loss 0.7511\n",
      "Epoch 15287: train loss: 0.1733, test loss 0.7511\n",
      "Epoch 15288: train loss: 0.1733, test loss 0.7511\n",
      "Epoch 15289: train loss: 0.1733, test loss 0.7511\n",
      "Epoch 15290: train loss: 0.1733, test loss 0.7510\n",
      "Epoch 15291: train loss: 0.1733, test loss 0.7510\n",
      "Epoch 15292: train loss: 0.1733, test loss 0.7510\n",
      "Epoch 15293: train loss: 0.1733, test loss 0.7510\n",
      "Epoch 15294: train loss: 0.1733, test loss 0.7510\n",
      "Epoch 15295: train loss: 0.1733, test loss 0.7509\n",
      "Epoch 15296: train loss: 0.1733, test loss 0.7509\n",
      "Epoch 15297: train loss: 0.1733, test loss 0.7509\n",
      "Epoch 15298: train loss: 0.1733, test loss 0.7509\n",
      "Epoch 15299: train loss: 0.1733, test loss 0.7508\n",
      "Epoch 15300: train loss: 0.1733, test loss 0.7508\n",
      "Epoch 15301: train loss: 0.1733, test loss 0.7508\n",
      "Epoch 15302: train loss: 0.1733, test loss 0.7508\n",
      "Epoch 15303: train loss: 0.1733, test loss 0.7508\n",
      "Epoch 15304: train loss: 0.1733, test loss 0.7507\n",
      "Epoch 15305: train loss: 0.1733, test loss 0.7507\n",
      "Epoch 15306: train loss: 0.1733, test loss 0.7507\n",
      "Epoch 15307: train loss: 0.1733, test loss 0.7507\n",
      "Epoch 15308: train loss: 0.1733, test loss 0.7507\n",
      "Epoch 15309: train loss: 0.1733, test loss 0.7506\n",
      "Epoch 15310: train loss: 0.1733, test loss 0.7506\n",
      "Epoch 15311: train loss: 0.1733, test loss 0.7506\n",
      "Epoch 15312: train loss: 0.1733, test loss 0.7506\n",
      "Epoch 15313: train loss: 0.1733, test loss 0.7505\n",
      "Epoch 15314: train loss: 0.1733, test loss 0.7505\n",
      "Epoch 15315: train loss: 0.1733, test loss 0.7505\n",
      "Epoch 15316: train loss: 0.1733, test loss 0.7505\n",
      "Epoch 15317: train loss: 0.1733, test loss 0.7505\n",
      "Epoch 15318: train loss: 0.1733, test loss 0.7504\n",
      "Epoch 15319: train loss: 0.1733, test loss 0.7504\n",
      "Epoch 15320: train loss: 0.1733, test loss 0.7504\n",
      "Epoch 15321: train loss: 0.1733, test loss 0.7504\n",
      "Epoch 15322: train loss: 0.1733, test loss 0.7503\n",
      "Epoch 15323: train loss: 0.1733, test loss 0.7503\n",
      "Epoch 15324: train loss: 0.1733, test loss 0.7503\n",
      "Epoch 15325: train loss: 0.1733, test loss 0.7503\n",
      "Epoch 15326: train loss: 0.1733, test loss 0.7502\n",
      "Epoch 15327: train loss: 0.1733, test loss 0.7502\n",
      "Epoch 15328: train loss: 0.1733, test loss 0.7502\n",
      "Epoch 15329: train loss: 0.1733, test loss 0.7502\n",
      "Epoch 15330: train loss: 0.1733, test loss 0.7502\n",
      "Epoch 15331: train loss: 0.1733, test loss 0.7501\n",
      "Epoch 15332: train loss: 0.1733, test loss 0.7501\n",
      "Epoch 15333: train loss: 0.1733, test loss 0.7501\n",
      "Epoch 15334: train loss: 0.1733, test loss 0.7501\n",
      "Epoch 15335: train loss: 0.1733, test loss 0.7500\n",
      "Epoch 15336: train loss: 0.1733, test loss 0.7500\n",
      "Epoch 15337: train loss: 0.1733, test loss 0.7500\n",
      "Epoch 15338: train loss: 0.1733, test loss 0.7500\n",
      "Epoch 15339: train loss: 0.1733, test loss 0.7499\n",
      "Epoch 15340: train loss: 0.1733, test loss 0.7499\n",
      "Epoch 15341: train loss: 0.1733, test loss 0.7499\n",
      "Epoch 15342: train loss: 0.1733, test loss 0.7499\n",
      "Epoch 15343: train loss: 0.1733, test loss 0.7498\n",
      "Epoch 15344: train loss: 0.1733, test loss 0.7498\n",
      "Epoch 15345: train loss: 0.1733, test loss 0.7498\n",
      "Epoch 15346: train loss: 0.1733, test loss 0.7498\n",
      "Epoch 15347: train loss: 0.1733, test loss 0.7497\n",
      "Epoch 15348: train loss: 0.1733, test loss 0.7497\n",
      "Epoch 15349: train loss: 0.1733, test loss 0.7497\n",
      "Epoch 15350: train loss: 0.1733, test loss 0.7497\n",
      "Epoch 15351: train loss: 0.1733, test loss 0.7496\n",
      "Epoch 15352: train loss: 0.1733, test loss 0.7496\n",
      "Epoch 15353: train loss: 0.1733, test loss 0.7496\n",
      "Epoch 15354: train loss: 0.1733, test loss 0.7496\n",
      "Epoch 15355: train loss: 0.1733, test loss 0.7495\n",
      "Epoch 15356: train loss: 0.1733, test loss 0.7495\n",
      "Epoch 15357: train loss: 0.1733, test loss 0.7495\n",
      "Epoch 15358: train loss: 0.1733, test loss 0.7494\n",
      "Epoch 15359: train loss: 0.1733, test loss 0.7494\n",
      "Epoch 15360: train loss: 0.1733, test loss 0.7494\n",
      "Epoch 15361: train loss: 0.1733, test loss 0.7494\n",
      "Epoch 15362: train loss: 0.1733, test loss 0.7493\n",
      "Epoch 15363: train loss: 0.1733, test loss 0.7493\n",
      "Epoch 15364: train loss: 0.1733, test loss 0.7493\n",
      "Epoch 15365: train loss: 0.1733, test loss 0.7493\n",
      "Epoch 15366: train loss: 0.1733, test loss 0.7492\n",
      "Epoch 15367: train loss: 0.1733, test loss 0.7492\n",
      "Epoch 15368: train loss: 0.1733, test loss 0.7492\n",
      "Epoch 15369: train loss: 0.1733, test loss 0.7492\n",
      "Epoch 15370: train loss: 0.1733, test loss 0.7491\n",
      "Epoch 15371: train loss: 0.1733, test loss 0.7491\n",
      "Epoch 15372: train loss: 0.1733, test loss 0.7491\n",
      "Epoch 15373: train loss: 0.1733, test loss 0.7491\n",
      "Epoch 15374: train loss: 0.1733, test loss 0.7490\n",
      "Epoch 15375: train loss: 0.1733, test loss 0.7490\n",
      "Epoch 15376: train loss: 0.1733, test loss 0.7490\n",
      "Epoch 15377: train loss: 0.1733, test loss 0.7490\n",
      "Epoch 15378: train loss: 0.1733, test loss 0.7489\n",
      "Epoch 15379: train loss: 0.1733, test loss 0.7489\n",
      "Epoch 15380: train loss: 0.1733, test loss 0.7489\n",
      "Epoch 15381: train loss: 0.1733, test loss 0.7489\n",
      "Epoch 15382: train loss: 0.1733, test loss 0.7488\n",
      "Epoch 15383: train loss: 0.1733, test loss 0.7488\n",
      "Epoch 15384: train loss: 0.1733, test loss 0.7488\n",
      "Epoch 15385: train loss: 0.1733, test loss 0.7488\n",
      "Epoch 15386: train loss: 0.1733, test loss 0.7487\n",
      "Epoch 15387: train loss: 0.1733, test loss 0.7487\n",
      "Epoch 15388: train loss: 0.1733, test loss 0.7487\n",
      "Epoch 15389: train loss: 0.1733, test loss 0.7487\n",
      "Epoch 15390: train loss: 0.1733, test loss 0.7486\n",
      "Epoch 15391: train loss: 0.1733, test loss 0.7486\n",
      "Epoch 15392: train loss: 0.1733, test loss 0.7486\n",
      "Epoch 15393: train loss: 0.1733, test loss 0.7486\n",
      "Epoch 15394: train loss: 0.1733, test loss 0.7485\n",
      "Epoch 15395: train loss: 0.1733, test loss 0.7485\n",
      "Epoch 15396: train loss: 0.1733, test loss 0.7485\n",
      "Epoch 15397: train loss: 0.1733, test loss 0.7485\n",
      "Epoch 15398: train loss: 0.1733, test loss 0.7484\n",
      "Epoch 15399: train loss: 0.1733, test loss 0.7484\n",
      "Epoch 15400: train loss: 0.1733, test loss 0.7484\n",
      "Epoch 15401: train loss: 0.1733, test loss 0.7484\n",
      "Epoch 15402: train loss: 0.1733, test loss 0.7483\n",
      "Epoch 15403: train loss: 0.1733, test loss 0.7483\n",
      "Epoch 15404: train loss: 0.1733, test loss 0.7483\n",
      "Epoch 15405: train loss: 0.1733, test loss 0.7483\n",
      "Epoch 15406: train loss: 0.1733, test loss 0.7483\n",
      "Epoch 15407: train loss: 0.1733, test loss 0.7482\n",
      "Epoch 15408: train loss: 0.1733, test loss 0.7482\n",
      "Epoch 15409: train loss: 0.1733, test loss 0.7482\n",
      "Epoch 15410: train loss: 0.1733, test loss 0.7482\n",
      "Epoch 15411: train loss: 0.1733, test loss 0.7481\n",
      "Epoch 15412: train loss: 0.1733, test loss 0.7481\n",
      "Epoch 15413: train loss: 0.1733, test loss 0.7481\n",
      "Epoch 15414: train loss: 0.1732, test loss 0.7481\n",
      "Epoch 15415: train loss: 0.1732, test loss 0.7480\n",
      "Epoch 15416: train loss: 0.1732, test loss 0.7480\n",
      "Epoch 15417: train loss: 0.1732, test loss 0.7480\n",
      "Epoch 15418: train loss: 0.1732, test loss 0.7480\n",
      "Epoch 15419: train loss: 0.1732, test loss 0.7479\n",
      "Epoch 15420: train loss: 0.1732, test loss 0.7479\n",
      "Epoch 15421: train loss: 0.1732, test loss 0.7479\n",
      "Epoch 15422: train loss: 0.1732, test loss 0.7479\n",
      "Epoch 15423: train loss: 0.1732, test loss 0.7478\n",
      "Epoch 15424: train loss: 0.1732, test loss 0.7478\n",
      "Epoch 15425: train loss: 0.1732, test loss 0.7478\n",
      "Epoch 15426: train loss: 0.1732, test loss 0.7478\n",
      "Epoch 15427: train loss: 0.1732, test loss 0.7477\n",
      "Epoch 15428: train loss: 0.1732, test loss 0.7477\n",
      "Epoch 15429: train loss: 0.1732, test loss 0.7477\n",
      "Epoch 15430: train loss: 0.1732, test loss 0.7477\n",
      "Epoch 15431: train loss: 0.1732, test loss 0.7476\n",
      "Epoch 15432: train loss: 0.1732, test loss 0.7476\n",
      "Epoch 15433: train loss: 0.1732, test loss 0.7476\n",
      "Epoch 15434: train loss: 0.1732, test loss 0.7476\n",
      "Epoch 15435: train loss: 0.1732, test loss 0.7476\n",
      "Epoch 15436: train loss: 0.1732, test loss 0.7475\n",
      "Epoch 15437: train loss: 0.1732, test loss 0.7475\n",
      "Epoch 15438: train loss: 0.1732, test loss 0.7475\n",
      "Epoch 15439: train loss: 0.1732, test loss 0.7475\n",
      "Epoch 15440: train loss: 0.1732, test loss 0.7474\n",
      "Epoch 15441: train loss: 0.1732, test loss 0.7474\n",
      "Epoch 15442: train loss: 0.1732, test loss 0.7474\n",
      "Epoch 15443: train loss: 0.1732, test loss 0.7474\n",
      "Epoch 15444: train loss: 0.1732, test loss 0.7473\n",
      "Epoch 15445: train loss: 0.1732, test loss 0.7473\n",
      "Epoch 15446: train loss: 0.1732, test loss 0.7473\n",
      "Epoch 15447: train loss: 0.1732, test loss 0.7473\n",
      "Epoch 15448: train loss: 0.1732, test loss 0.7472\n",
      "Epoch 15449: train loss: 0.1732, test loss 0.7472\n",
      "Epoch 15450: train loss: 0.1732, test loss 0.7472\n",
      "Epoch 15451: train loss: 0.1732, test loss 0.7472\n",
      "Epoch 15452: train loss: 0.1732, test loss 0.7471\n",
      "Epoch 15453: train loss: 0.1732, test loss 0.7471\n",
      "Epoch 15454: train loss: 0.1732, test loss 0.7471\n",
      "Epoch 15455: train loss: 0.1732, test loss 0.7471\n",
      "Epoch 15456: train loss: 0.1732, test loss 0.7471\n",
      "Epoch 15457: train loss: 0.1732, test loss 0.7470\n",
      "Epoch 15458: train loss: 0.1732, test loss 0.7470\n",
      "Epoch 15459: train loss: 0.1732, test loss 0.7470\n",
      "Epoch 15460: train loss: 0.1732, test loss 0.7470\n",
      "Epoch 15461: train loss: 0.1732, test loss 0.7469\n",
      "Epoch 15462: train loss: 0.1732, test loss 0.7469\n",
      "Epoch 15463: train loss: 0.1732, test loss 0.7469\n",
      "Epoch 15464: train loss: 0.1732, test loss 0.7469\n",
      "Epoch 15465: train loss: 0.1732, test loss 0.7468\n",
      "Epoch 15466: train loss: 0.1732, test loss 0.7468\n",
      "Epoch 15467: train loss: 0.1732, test loss 0.7468\n",
      "Epoch 15468: train loss: 0.1732, test loss 0.7468\n",
      "Epoch 15469: train loss: 0.1732, test loss 0.7467\n",
      "Epoch 15470: train loss: 0.1732, test loss 0.7467\n",
      "Epoch 15471: train loss: 0.1732, test loss 0.7467\n",
      "Epoch 15472: train loss: 0.1732, test loss 0.7467\n",
      "Epoch 15473: train loss: 0.1732, test loss 0.7466\n",
      "Epoch 15474: train loss: 0.1732, test loss 0.7466\n",
      "Epoch 15475: train loss: 0.1732, test loss 0.7466\n",
      "Epoch 15476: train loss: 0.1732, test loss 0.7466\n",
      "Epoch 15477: train loss: 0.1732, test loss 0.7466\n",
      "Epoch 15478: train loss: 0.1732, test loss 0.7465\n",
      "Epoch 15479: train loss: 0.1732, test loss 0.7465\n",
      "Epoch 15480: train loss: 0.1732, test loss 0.7465\n",
      "Epoch 15481: train loss: 0.1732, test loss 0.7465\n",
      "Epoch 15482: train loss: 0.1732, test loss 0.7464\n",
      "Epoch 15483: train loss: 0.1732, test loss 0.7464\n",
      "Epoch 15484: train loss: 0.1732, test loss 0.7464\n",
      "Epoch 15485: train loss: 0.1732, test loss 0.7464\n",
      "Epoch 15486: train loss: 0.1732, test loss 0.7463\n",
      "Epoch 15487: train loss: 0.1732, test loss 0.7463\n",
      "Epoch 15488: train loss: 0.1732, test loss 0.7463\n",
      "Epoch 15489: train loss: 0.1732, test loss 0.7463\n",
      "Epoch 15490: train loss: 0.1732, test loss 0.7462\n",
      "Epoch 15491: train loss: 0.1732, test loss 0.7462\n",
      "Epoch 15492: train loss: 0.1732, test loss 0.7462\n",
      "Epoch 15493: train loss: 0.1732, test loss 0.7462\n",
      "Epoch 15494: train loss: 0.1732, test loss 0.7461\n",
      "Epoch 15495: train loss: 0.1732, test loss 0.7461\n",
      "Epoch 15496: train loss: 0.1732, test loss 0.7461\n",
      "Epoch 15497: train loss: 0.1732, test loss 0.7461\n",
      "Epoch 15498: train loss: 0.1732, test loss 0.7460\n",
      "Epoch 15499: train loss: 0.1732, test loss 0.7460\n",
      "Epoch 15500: train loss: 0.1732, test loss 0.7460\n",
      "Epoch 15501: train loss: 0.1732, test loss 0.7460\n",
      "Epoch 15502: train loss: 0.1732, test loss 0.7460\n",
      "Epoch 15503: train loss: 0.1732, test loss 0.7459\n",
      "Epoch 15504: train loss: 0.1732, test loss 0.7459\n",
      "Epoch 15505: train loss: 0.1732, test loss 0.7459\n",
      "Epoch 15506: train loss: 0.1732, test loss 0.7459\n",
      "Epoch 15507: train loss: 0.1732, test loss 0.7458\n",
      "Epoch 15508: train loss: 0.1732, test loss 0.7458\n",
      "Epoch 15509: train loss: 0.1732, test loss 0.7458\n",
      "Epoch 15510: train loss: 0.1732, test loss 0.7458\n",
      "Epoch 15511: train loss: 0.1732, test loss 0.7457\n",
      "Epoch 15512: train loss: 0.1732, test loss 0.7457\n",
      "Epoch 15513: train loss: 0.1732, test loss 0.7457\n",
      "Epoch 15514: train loss: 0.1732, test loss 0.7457\n",
      "Epoch 15515: train loss: 0.1732, test loss 0.7456\n",
      "Epoch 15516: train loss: 0.1732, test loss 0.7456\n",
      "Epoch 15517: train loss: 0.1732, test loss 0.7456\n",
      "Epoch 15518: train loss: 0.1732, test loss 0.7456\n",
      "Epoch 15519: train loss: 0.1732, test loss 0.7455\n",
      "Epoch 15520: train loss: 0.1732, test loss 0.7455\n",
      "Epoch 15521: train loss: 0.1732, test loss 0.7455\n",
      "Epoch 15522: train loss: 0.1732, test loss 0.7455\n",
      "Epoch 15523: train loss: 0.1732, test loss 0.7454\n",
      "Epoch 15524: train loss: 0.1732, test loss 0.7454\n",
      "Epoch 15525: train loss: 0.1732, test loss 0.7454\n",
      "Epoch 15526: train loss: 0.1732, test loss 0.7454\n",
      "Epoch 15527: train loss: 0.1732, test loss 0.7454\n",
      "Epoch 15528: train loss: 0.1732, test loss 0.7453\n",
      "Epoch 15529: train loss: 0.1732, test loss 0.7453\n",
      "Epoch 15530: train loss: 0.1732, test loss 0.7453\n",
      "Epoch 15531: train loss: 0.1732, test loss 0.7453\n",
      "Epoch 15532: train loss: 0.1732, test loss 0.7452\n",
      "Epoch 15533: train loss: 0.1732, test loss 0.7452\n",
      "Epoch 15534: train loss: 0.1732, test loss 0.7452\n",
      "Epoch 15535: train loss: 0.1732, test loss 0.7452\n",
      "Epoch 15536: train loss: 0.1732, test loss 0.7451\n",
      "Epoch 15537: train loss: 0.1732, test loss 0.7451\n",
      "Epoch 15538: train loss: 0.1732, test loss 0.7451\n",
      "Epoch 15539: train loss: 0.1732, test loss 0.7451\n",
      "Epoch 15540: train loss: 0.1732, test loss 0.7450\n",
      "Epoch 15541: train loss: 0.1732, test loss 0.7450\n",
      "Epoch 15542: train loss: 0.1732, test loss 0.7450\n",
      "Epoch 15543: train loss: 0.1732, test loss 0.7450\n",
      "Epoch 15544: train loss: 0.1732, test loss 0.7450\n",
      "Epoch 15545: train loss: 0.1732, test loss 0.7449\n",
      "Epoch 15546: train loss: 0.1732, test loss 0.7449\n",
      "Epoch 15547: train loss: 0.1732, test loss 0.7449\n",
      "Epoch 15548: train loss: 0.1732, test loss 0.7449\n",
      "Epoch 15549: train loss: 0.1732, test loss 0.7448\n",
      "Epoch 15550: train loss: 0.1732, test loss 0.7448\n",
      "Epoch 15551: train loss: 0.1732, test loss 0.7448\n",
      "Epoch 15552: train loss: 0.1732, test loss 0.7448\n",
      "Epoch 15553: train loss: 0.1732, test loss 0.7447\n",
      "Epoch 15554: train loss: 0.1732, test loss 0.7447\n",
      "Epoch 15555: train loss: 0.1732, test loss 0.7447\n",
      "Epoch 15556: train loss: 0.1732, test loss 0.7447\n",
      "Epoch 15557: train loss: 0.1732, test loss 0.7446\n",
      "Epoch 15558: train loss: 0.1732, test loss 0.7446\n",
      "Epoch 15559: train loss: 0.1732, test loss 0.7446\n",
      "Epoch 15560: train loss: 0.1732, test loss 0.7446\n",
      "Epoch 15561: train loss: 0.1732, test loss 0.7446\n",
      "Epoch 15562: train loss: 0.1732, test loss 0.7445\n",
      "Epoch 15563: train loss: 0.1732, test loss 0.7445\n",
      "Epoch 15564: train loss: 0.1732, test loss 0.7445\n",
      "Epoch 15565: train loss: 0.1732, test loss 0.7445\n",
      "Epoch 15566: train loss: 0.1732, test loss 0.7444\n",
      "Epoch 15567: train loss: 0.1732, test loss 0.7444\n",
      "Epoch 15568: train loss: 0.1732, test loss 0.7444\n",
      "Epoch 15569: train loss: 0.1732, test loss 0.7444\n",
      "Epoch 15570: train loss: 0.1732, test loss 0.7444\n",
      "Epoch 15571: train loss: 0.1732, test loss 0.7443\n",
      "Epoch 15572: train loss: 0.1732, test loss 0.7443\n",
      "Epoch 15573: train loss: 0.1732, test loss 0.7443\n",
      "Epoch 15574: train loss: 0.1732, test loss 0.7443\n",
      "Epoch 15575: train loss: 0.1732, test loss 0.7442\n",
      "Epoch 15576: train loss: 0.1732, test loss 0.7442\n",
      "Epoch 15577: train loss: 0.1732, test loss 0.7442\n",
      "Epoch 15578: train loss: 0.1732, test loss 0.7442\n",
      "Epoch 15579: train loss: 0.1732, test loss 0.7442\n",
      "Epoch 15580: train loss: 0.1732, test loss 0.7441\n",
      "Epoch 15581: train loss: 0.1732, test loss 0.7441\n",
      "Epoch 15582: train loss: 0.1731, test loss 0.7441\n",
      "Epoch 15583: train loss: 0.1731, test loss 0.7441\n",
      "Epoch 15584: train loss: 0.1731, test loss 0.7440\n",
      "Epoch 15585: train loss: 0.1731, test loss 0.7440\n",
      "Epoch 15586: train loss: 0.1731, test loss 0.7440\n",
      "Epoch 15587: train loss: 0.1731, test loss 0.7440\n",
      "Epoch 15588: train loss: 0.1731, test loss 0.7440\n",
      "Epoch 15589: train loss: 0.1731, test loss 0.7439\n",
      "Epoch 15590: train loss: 0.1731, test loss 0.7439\n",
      "Epoch 15591: train loss: 0.1731, test loss 0.7439\n",
      "Epoch 15592: train loss: 0.1731, test loss 0.7439\n",
      "Epoch 15593: train loss: 0.1731, test loss 0.7438\n",
      "Epoch 15594: train loss: 0.1731, test loss 0.7438\n",
      "Epoch 15595: train loss: 0.1731, test loss 0.7438\n",
      "Epoch 15596: train loss: 0.1731, test loss 0.7438\n",
      "Epoch 15597: train loss: 0.1731, test loss 0.7438\n",
      "Epoch 15598: train loss: 0.1731, test loss 0.7437\n",
      "Epoch 15599: train loss: 0.1731, test loss 0.7437\n",
      "Epoch 15600: train loss: 0.1731, test loss 0.7437\n",
      "Epoch 15601: train loss: 0.1731, test loss 0.7437\n",
      "Epoch 15602: train loss: 0.1731, test loss 0.7436\n",
      "Epoch 15603: train loss: 0.1731, test loss 0.7436\n",
      "Epoch 15604: train loss: 0.1731, test loss 0.7436\n",
      "Epoch 15605: train loss: 0.1731, test loss 0.7436\n",
      "Epoch 15606: train loss: 0.1731, test loss 0.7436\n",
      "Epoch 15607: train loss: 0.1731, test loss 0.7435\n",
      "Epoch 15608: train loss: 0.1731, test loss 0.7435\n",
      "Epoch 15609: train loss: 0.1731, test loss 0.7435\n",
      "Epoch 15610: train loss: 0.1731, test loss 0.7435\n",
      "Epoch 15611: train loss: 0.1731, test loss 0.7434\n",
      "Epoch 15612: train loss: 0.1731, test loss 0.7434\n",
      "Epoch 15613: train loss: 0.1731, test loss 0.7434\n",
      "Epoch 15614: train loss: 0.1731, test loss 0.7434\n",
      "Epoch 15615: train loss: 0.1731, test loss 0.7434\n",
      "Epoch 15616: train loss: 0.1731, test loss 0.7433\n",
      "Epoch 15617: train loss: 0.1731, test loss 0.7433\n",
      "Epoch 15618: train loss: 0.1731, test loss 0.7433\n",
      "Epoch 15619: train loss: 0.1731, test loss 0.7433\n",
      "Epoch 15620: train loss: 0.1731, test loss 0.7433\n",
      "Epoch 15621: train loss: 0.1731, test loss 0.7432\n",
      "Epoch 15622: train loss: 0.1731, test loss 0.7432\n",
      "Epoch 15623: train loss: 0.1731, test loss 0.7432\n",
      "Epoch 15624: train loss: 0.1731, test loss 0.7432\n",
      "Epoch 15625: train loss: 0.1731, test loss 0.7431\n",
      "Epoch 15626: train loss: 0.1731, test loss 0.7431\n",
      "Epoch 15627: train loss: 0.1731, test loss 0.7431\n",
      "Epoch 15628: train loss: 0.1731, test loss 0.7431\n",
      "Epoch 15629: train loss: 0.1731, test loss 0.7431\n",
      "Epoch 15630: train loss: 0.1731, test loss 0.7430\n",
      "Epoch 15631: train loss: 0.1731, test loss 0.7430\n",
      "Epoch 15632: train loss: 0.1731, test loss 0.7430\n",
      "Epoch 15633: train loss: 0.1731, test loss 0.7430\n",
      "Epoch 15634: train loss: 0.1731, test loss 0.7430\n",
      "Epoch 15635: train loss: 0.1731, test loss 0.7429\n",
      "Epoch 15636: train loss: 0.1731, test loss 0.7429\n",
      "Epoch 15637: train loss: 0.1731, test loss 0.7429\n",
      "Epoch 15638: train loss: 0.1731, test loss 0.7429\n",
      "Epoch 15639: train loss: 0.1731, test loss 0.7428\n",
      "Epoch 15640: train loss: 0.1731, test loss 0.7428\n",
      "Epoch 15641: train loss: 0.1731, test loss 0.7428\n",
      "Epoch 15642: train loss: 0.1731, test loss 0.7428\n",
      "Epoch 15643: train loss: 0.1731, test loss 0.7428\n",
      "Epoch 15644: train loss: 0.1731, test loss 0.7427\n",
      "Epoch 15645: train loss: 0.1731, test loss 0.7427\n",
      "Epoch 15646: train loss: 0.1731, test loss 0.7427\n",
      "Epoch 15647: train loss: 0.1731, test loss 0.7427\n",
      "Epoch 15648: train loss: 0.1731, test loss 0.7426\n",
      "Epoch 15649: train loss: 0.1731, test loss 0.7426\n",
      "Epoch 15650: train loss: 0.1731, test loss 0.7426\n",
      "Epoch 15651: train loss: 0.1731, test loss 0.7426\n",
      "Epoch 15652: train loss: 0.1731, test loss 0.7426\n",
      "Epoch 15653: train loss: 0.1731, test loss 0.7425\n",
      "Epoch 15654: train loss: 0.1731, test loss 0.7425\n",
      "Epoch 15655: train loss: 0.1731, test loss 0.7425\n",
      "Epoch 15656: train loss: 0.1731, test loss 0.7425\n",
      "Epoch 15657: train loss: 0.1731, test loss 0.7425\n",
      "Epoch 15658: train loss: 0.1731, test loss 0.7424\n",
      "Epoch 15659: train loss: 0.1731, test loss 0.7424\n",
      "Epoch 15660: train loss: 0.1731, test loss 0.7424\n",
      "Epoch 15661: train loss: 0.1731, test loss 0.7424\n",
      "Epoch 15662: train loss: 0.1731, test loss 0.7423\n",
      "Epoch 15663: train loss: 0.1731, test loss 0.7423\n",
      "Epoch 15664: train loss: 0.1731, test loss 0.7423\n",
      "Epoch 15665: train loss: 0.1731, test loss 0.7423\n",
      "Epoch 15666: train loss: 0.1731, test loss 0.7423\n",
      "Epoch 15667: train loss: 0.1731, test loss 0.7422\n",
      "Epoch 15668: train loss: 0.1731, test loss 0.7422\n",
      "Epoch 15669: train loss: 0.1731, test loss 0.7422\n",
      "Epoch 15670: train loss: 0.1731, test loss 0.7422\n",
      "Epoch 15671: train loss: 0.1731, test loss 0.7422\n",
      "Epoch 15672: train loss: 0.1731, test loss 0.7421\n",
      "Epoch 15673: train loss: 0.1731, test loss 0.7421\n",
      "Epoch 15674: train loss: 0.1731, test loss 0.7421\n",
      "Epoch 15675: train loss: 0.1731, test loss 0.7421\n",
      "Epoch 15676: train loss: 0.1731, test loss 0.7420\n",
      "Epoch 15677: train loss: 0.1731, test loss 0.7420\n",
      "Epoch 15678: train loss: 0.1731, test loss 0.7420\n",
      "Epoch 15679: train loss: 0.1731, test loss 0.7420\n",
      "Epoch 15680: train loss: 0.1731, test loss 0.7420\n",
      "Epoch 15681: train loss: 0.1731, test loss 0.7419\n",
      "Epoch 15682: train loss: 0.1731, test loss 0.7419\n",
      "Epoch 15683: train loss: 0.1731, test loss 0.7419\n",
      "Epoch 15684: train loss: 0.1731, test loss 0.7419\n",
      "Epoch 15685: train loss: 0.1731, test loss 0.7418\n",
      "Epoch 15686: train loss: 0.1731, test loss 0.7418\n",
      "Epoch 15687: train loss: 0.1731, test loss 0.7418\n",
      "Epoch 15688: train loss: 0.1731, test loss 0.7418\n",
      "Epoch 15689: train loss: 0.1731, test loss 0.7418\n",
      "Epoch 15690: train loss: 0.1731, test loss 0.7417\n",
      "Epoch 15691: train loss: 0.1731, test loss 0.7417\n",
      "Epoch 15692: train loss: 0.1731, test loss 0.7417\n",
      "Epoch 15693: train loss: 0.1731, test loss 0.7417\n",
      "Epoch 15694: train loss: 0.1731, test loss 0.7417\n",
      "Epoch 15695: train loss: 0.1731, test loss 0.7416\n",
      "Epoch 15696: train loss: 0.1731, test loss 0.7416\n",
      "Epoch 15697: train loss: 0.1731, test loss 0.7416\n",
      "Epoch 15698: train loss: 0.1731, test loss 0.7416\n",
      "Epoch 15699: train loss: 0.1731, test loss 0.7415\n",
      "Epoch 15700: train loss: 0.1731, test loss 0.7415\n",
      "Epoch 15701: train loss: 0.1731, test loss 0.7415\n",
      "Epoch 15702: train loss: 0.1731, test loss 0.7415\n",
      "Epoch 15703: train loss: 0.1731, test loss 0.7415\n",
      "Epoch 15704: train loss: 0.1731, test loss 0.7414\n",
      "Epoch 15705: train loss: 0.1731, test loss 0.7414\n",
      "Epoch 15706: train loss: 0.1731, test loss 0.7414\n",
      "Epoch 15707: train loss: 0.1731, test loss 0.7414\n",
      "Epoch 15708: train loss: 0.1731, test loss 0.7414\n",
      "Epoch 15709: train loss: 0.1731, test loss 0.7413\n",
      "Epoch 15710: train loss: 0.1731, test loss 0.7413\n",
      "Epoch 15711: train loss: 0.1731, test loss 0.7413\n",
      "Epoch 15712: train loss: 0.1731, test loss 0.7413\n",
      "Epoch 15713: train loss: 0.1731, test loss 0.7412\n",
      "Epoch 15714: train loss: 0.1731, test loss 0.7412\n",
      "Epoch 15715: train loss: 0.1731, test loss 0.7412\n",
      "Epoch 15716: train loss: 0.1731, test loss 0.7412\n",
      "Epoch 15717: train loss: 0.1731, test loss 0.7412\n",
      "Epoch 15718: train loss: 0.1731, test loss 0.7411\n",
      "Epoch 15719: train loss: 0.1731, test loss 0.7411\n",
      "Epoch 15720: train loss: 0.1731, test loss 0.7411\n",
      "Epoch 15721: train loss: 0.1731, test loss 0.7411\n",
      "Epoch 15722: train loss: 0.1731, test loss 0.7411\n",
      "Epoch 15723: train loss: 0.1731, test loss 0.7410\n",
      "Epoch 15724: train loss: 0.1731, test loss 0.7410\n",
      "Epoch 15725: train loss: 0.1731, test loss 0.7410\n",
      "Epoch 15726: train loss: 0.1731, test loss 0.7410\n",
      "Epoch 15727: train loss: 0.1731, test loss 0.7410\n",
      "Epoch 15728: train loss: 0.1731, test loss 0.7409\n",
      "Epoch 15729: train loss: 0.1731, test loss 0.7409\n",
      "Epoch 15730: train loss: 0.1731, test loss 0.7409\n",
      "Epoch 15731: train loss: 0.1731, test loss 0.7409\n",
      "Epoch 15732: train loss: 0.1731, test loss 0.7409\n",
      "Epoch 15733: train loss: 0.1731, test loss 0.7408\n",
      "Epoch 15734: train loss: 0.1731, test loss 0.7408\n",
      "Epoch 15735: train loss: 0.1731, test loss 0.7408\n",
      "Epoch 15736: train loss: 0.1731, test loss 0.7408\n",
      "Epoch 15737: train loss: 0.1731, test loss 0.7407\n",
      "Epoch 15738: train loss: 0.1731, test loss 0.7407\n",
      "Epoch 15739: train loss: 0.1731, test loss 0.7407\n",
      "Epoch 15740: train loss: 0.1731, test loss 0.7407\n",
      "Epoch 15741: train loss: 0.1731, test loss 0.7407\n",
      "Epoch 15742: train loss: 0.1731, test loss 0.7406\n",
      "Epoch 15743: train loss: 0.1731, test loss 0.7406\n",
      "Epoch 15744: train loss: 0.1731, test loss 0.7406\n",
      "Epoch 15745: train loss: 0.1731, test loss 0.7406\n",
      "Epoch 15746: train loss: 0.1731, test loss 0.7406\n",
      "Epoch 15747: train loss: 0.1731, test loss 0.7405\n",
      "Epoch 15748: train loss: 0.1731, test loss 0.7405\n",
      "Epoch 15749: train loss: 0.1731, test loss 0.7405\n",
      "Epoch 15750: train loss: 0.1731, test loss 0.7405\n",
      "Epoch 15751: train loss: 0.1730, test loss 0.7405\n",
      "Epoch 15752: train loss: 0.1730, test loss 0.7404\n",
      "Epoch 15753: train loss: 0.1730, test loss 0.7404\n",
      "Epoch 15754: train loss: 0.1730, test loss 0.7404\n",
      "Epoch 15755: train loss: 0.1730, test loss 0.7404\n",
      "Epoch 15756: train loss: 0.1730, test loss 0.7404\n",
      "Epoch 15757: train loss: 0.1730, test loss 0.7403\n",
      "Epoch 15758: train loss: 0.1730, test loss 0.7403\n",
      "Epoch 15759: train loss: 0.1730, test loss 0.7403\n",
      "Epoch 15760: train loss: 0.1730, test loss 0.7403\n",
      "Epoch 15761: train loss: 0.1730, test loss 0.7402\n",
      "Epoch 15762: train loss: 0.1730, test loss 0.7402\n",
      "Epoch 15763: train loss: 0.1730, test loss 0.7402\n",
      "Epoch 15764: train loss: 0.1730, test loss 0.7402\n",
      "Epoch 15765: train loss: 0.1730, test loss 0.7402\n",
      "Epoch 15766: train loss: 0.1730, test loss 0.7401\n",
      "Epoch 15767: train loss: 0.1730, test loss 0.7401\n",
      "Epoch 15768: train loss: 0.1730, test loss 0.7401\n",
      "Epoch 15769: train loss: 0.1730, test loss 0.7401\n",
      "Epoch 15770: train loss: 0.1730, test loss 0.7401\n",
      "Epoch 15771: train loss: 0.1730, test loss 0.7400\n",
      "Epoch 15772: train loss: 0.1730, test loss 0.7400\n",
      "Epoch 15773: train loss: 0.1730, test loss 0.7400\n",
      "Epoch 15774: train loss: 0.1730, test loss 0.7400\n",
      "Epoch 15775: train loss: 0.1730, test loss 0.7400\n",
      "Epoch 15776: train loss: 0.1730, test loss 0.7399\n",
      "Epoch 15777: train loss: 0.1730, test loss 0.7399\n",
      "Epoch 15778: train loss: 0.1730, test loss 0.7399\n",
      "Epoch 15779: train loss: 0.1730, test loss 0.7399\n",
      "Epoch 15780: train loss: 0.1730, test loss 0.7399\n",
      "Epoch 15781: train loss: 0.1730, test loss 0.7398\n",
      "Epoch 15782: train loss: 0.1730, test loss 0.7398\n",
      "Epoch 15783: train loss: 0.1730, test loss 0.7398\n",
      "Epoch 15784: train loss: 0.1730, test loss 0.7398\n",
      "Epoch 15785: train loss: 0.1730, test loss 0.7397\n",
      "Epoch 15786: train loss: 0.1730, test loss 0.7397\n",
      "Epoch 15787: train loss: 0.1730, test loss 0.7397\n",
      "Epoch 15788: train loss: 0.1730, test loss 0.7397\n",
      "Epoch 15789: train loss: 0.1730, test loss 0.7397\n",
      "Epoch 15790: train loss: 0.1730, test loss 0.7396\n",
      "Epoch 15791: train loss: 0.1730, test loss 0.7396\n",
      "Epoch 15792: train loss: 0.1730, test loss 0.7396\n",
      "Epoch 15793: train loss: 0.1730, test loss 0.7396\n",
      "Epoch 15794: train loss: 0.1730, test loss 0.7396\n",
      "Epoch 15795: train loss: 0.1730, test loss 0.7395\n",
      "Epoch 15796: train loss: 0.1730, test loss 0.7395\n",
      "Epoch 15797: train loss: 0.1730, test loss 0.7395\n",
      "Epoch 15798: train loss: 0.1730, test loss 0.7395\n",
      "Epoch 15799: train loss: 0.1730, test loss 0.7395\n",
      "Epoch 15800: train loss: 0.1730, test loss 0.7394\n",
      "Epoch 15801: train loss: 0.1730, test loss 0.7394\n",
      "Epoch 15802: train loss: 0.1730, test loss 0.7394\n",
      "Epoch 15803: train loss: 0.1730, test loss 0.7394\n",
      "Epoch 15804: train loss: 0.1730, test loss 0.7394\n",
      "Epoch 15805: train loss: 0.1730, test loss 0.7393\n",
      "Epoch 15806: train loss: 0.1730, test loss 0.7393\n",
      "Epoch 15807: train loss: 0.1730, test loss 0.7393\n",
      "Epoch 15808: train loss: 0.1730, test loss 0.7393\n",
      "Epoch 15809: train loss: 0.1730, test loss 0.7392\n",
      "Epoch 15810: train loss: 0.1730, test loss 0.7392\n",
      "Epoch 15811: train loss: 0.1730, test loss 0.7392\n",
      "Epoch 15812: train loss: 0.1730, test loss 0.7392\n",
      "Epoch 15813: train loss: 0.1730, test loss 0.7392\n",
      "Epoch 15814: train loss: 0.1730, test loss 0.7391\n",
      "Epoch 15815: train loss: 0.1730, test loss 0.7391\n",
      "Epoch 15816: train loss: 0.1730, test loss 0.7391\n",
      "Epoch 15817: train loss: 0.1730, test loss 0.7391\n",
      "Epoch 15818: train loss: 0.1730, test loss 0.7391\n",
      "Epoch 15819: train loss: 0.1730, test loss 0.7390\n",
      "Epoch 15820: train loss: 0.1730, test loss 0.7390\n",
      "Epoch 15821: train loss: 0.1730, test loss 0.7390\n",
      "Epoch 15822: train loss: 0.1730, test loss 0.7390\n",
      "Epoch 15823: train loss: 0.1730, test loss 0.7390\n",
      "Epoch 15824: train loss: 0.1730, test loss 0.7389\n",
      "Epoch 15825: train loss: 0.1730, test loss 0.7389\n",
      "Epoch 15826: train loss: 0.1730, test loss 0.7389\n",
      "Epoch 15827: train loss: 0.1730, test loss 0.7389\n",
      "Epoch 15828: train loss: 0.1730, test loss 0.7389\n",
      "Epoch 15829: train loss: 0.1730, test loss 0.7388\n",
      "Epoch 15830: train loss: 0.1730, test loss 0.7388\n",
      "Epoch 15831: train loss: 0.1730, test loss 0.7388\n",
      "Epoch 15832: train loss: 0.1730, test loss 0.7388\n",
      "Epoch 15833: train loss: 0.1730, test loss 0.7387\n",
      "Epoch 15834: train loss: 0.1730, test loss 0.7387\n",
      "Epoch 15835: train loss: 0.1730, test loss 0.7387\n",
      "Epoch 15836: train loss: 0.1730, test loss 0.7387\n",
      "Epoch 15837: train loss: 0.1730, test loss 0.7387\n",
      "Epoch 15838: train loss: 0.1730, test loss 0.7386\n",
      "Epoch 15839: train loss: 0.1730, test loss 0.7386\n",
      "Epoch 15840: train loss: 0.1730, test loss 0.7386\n",
      "Epoch 15841: train loss: 0.1730, test loss 0.7386\n",
      "Epoch 15842: train loss: 0.1730, test loss 0.7386\n",
      "Epoch 15843: train loss: 0.1730, test loss 0.7385\n",
      "Epoch 15844: train loss: 0.1730, test loss 0.7385\n",
      "Epoch 15845: train loss: 0.1730, test loss 0.7385\n",
      "Epoch 15846: train loss: 0.1730, test loss 0.7385\n",
      "Epoch 15847: train loss: 0.1730, test loss 0.7385\n",
      "Epoch 15848: train loss: 0.1730, test loss 0.7384\n",
      "Epoch 15849: train loss: 0.1730, test loss 0.7384\n",
      "Epoch 15850: train loss: 0.1730, test loss 0.7384\n",
      "Epoch 15851: train loss: 0.1730, test loss 0.7384\n",
      "Epoch 15852: train loss: 0.1730, test loss 0.7383\n",
      "Epoch 15853: train loss: 0.1730, test loss 0.7383\n",
      "Epoch 15854: train loss: 0.1730, test loss 0.7383\n",
      "Epoch 15855: train loss: 0.1730, test loss 0.7383\n",
      "Epoch 15856: train loss: 0.1730, test loss 0.7383\n",
      "Epoch 15857: train loss: 0.1730, test loss 0.7382\n",
      "Epoch 15858: train loss: 0.1730, test loss 0.7382\n",
      "Epoch 15859: train loss: 0.1730, test loss 0.7382\n",
      "Epoch 15860: train loss: 0.1730, test loss 0.7382\n",
      "Epoch 15861: train loss: 0.1730, test loss 0.7382\n",
      "Epoch 15862: train loss: 0.1730, test loss 0.7381\n",
      "Epoch 15863: train loss: 0.1730, test loss 0.7381\n",
      "Epoch 15864: train loss: 0.1730, test loss 0.7381\n",
      "Epoch 15865: train loss: 0.1730, test loss 0.7381\n",
      "Epoch 15866: train loss: 0.1730, test loss 0.7380\n",
      "Epoch 15867: train loss: 0.1730, test loss 0.7380\n",
      "Epoch 15868: train loss: 0.1730, test loss 0.7380\n",
      "Epoch 15869: train loss: 0.1730, test loss 0.7380\n",
      "Epoch 15870: train loss: 0.1730, test loss 0.7379\n",
      "Epoch 15871: train loss: 0.1730, test loss 0.7379\n",
      "Epoch 15872: train loss: 0.1730, test loss 0.7379\n",
      "Epoch 15873: train loss: 0.1730, test loss 0.7379\n",
      "Epoch 15874: train loss: 0.1730, test loss 0.7378\n",
      "Epoch 15875: train loss: 0.1730, test loss 0.7378\n",
      "Epoch 15876: train loss: 0.1730, test loss 0.7378\n",
      "Epoch 15877: train loss: 0.1730, test loss 0.7378\n",
      "Epoch 15878: train loss: 0.1730, test loss 0.7377\n",
      "Epoch 15879: train loss: 0.1730, test loss 0.7377\n",
      "Epoch 15880: train loss: 0.1730, test loss 0.7377\n",
      "Epoch 15881: train loss: 0.1730, test loss 0.7377\n",
      "Epoch 15882: train loss: 0.1730, test loss 0.7376\n",
      "Epoch 15883: train loss: 0.1730, test loss 0.7376\n",
      "Epoch 15884: train loss: 0.1730, test loss 0.7376\n",
      "Epoch 15885: train loss: 0.1730, test loss 0.7376\n",
      "Epoch 15886: train loss: 0.1730, test loss 0.7376\n",
      "Epoch 15887: train loss: 0.1730, test loss 0.7375\n",
      "Epoch 15888: train loss: 0.1730, test loss 0.7375\n",
      "Epoch 15889: train loss: 0.1730, test loss 0.7375\n",
      "Epoch 15890: train loss: 0.1730, test loss 0.7375\n",
      "Epoch 15891: train loss: 0.1730, test loss 0.7374\n",
      "Epoch 15892: train loss: 0.1730, test loss 0.7374\n",
      "Epoch 15893: train loss: 0.1730, test loss 0.7374\n",
      "Epoch 15894: train loss: 0.1730, test loss 0.7374\n",
      "Epoch 15895: train loss: 0.1730, test loss 0.7373\n",
      "Epoch 15896: train loss: 0.1730, test loss 0.7373\n",
      "Epoch 15897: train loss: 0.1730, test loss 0.7373\n",
      "Epoch 15898: train loss: 0.1730, test loss 0.7373\n",
      "Epoch 15899: train loss: 0.1730, test loss 0.7372\n",
      "Epoch 15900: train loss: 0.1730, test loss 0.7372\n",
      "Epoch 15901: train loss: 0.1730, test loss 0.7372\n",
      "Epoch 15902: train loss: 0.1730, test loss 0.7372\n",
      "Epoch 15903: train loss: 0.1730, test loss 0.7371\n",
      "Epoch 15904: train loss: 0.1730, test loss 0.7371\n",
      "Epoch 15905: train loss: 0.1730, test loss 0.7371\n",
      "Epoch 15906: train loss: 0.1730, test loss 0.7371\n",
      "Epoch 15907: train loss: 0.1730, test loss 0.7371\n",
      "Epoch 15908: train loss: 0.1730, test loss 0.7370\n",
      "Epoch 15909: train loss: 0.1730, test loss 0.7370\n",
      "Epoch 15910: train loss: 0.1730, test loss 0.7370\n",
      "Epoch 15911: train loss: 0.1730, test loss 0.7370\n",
      "Epoch 15912: train loss: 0.1730, test loss 0.7369\n",
      "Epoch 15913: train loss: 0.1730, test loss 0.7369\n",
      "Epoch 15914: train loss: 0.1730, test loss 0.7369\n",
      "Epoch 15915: train loss: 0.1730, test loss 0.7369\n",
      "Epoch 15916: train loss: 0.1730, test loss 0.7368\n",
      "Epoch 15917: train loss: 0.1730, test loss 0.7368\n",
      "Epoch 15918: train loss: 0.1730, test loss 0.7368\n",
      "Epoch 15919: train loss: 0.1730, test loss 0.7368\n",
      "Epoch 15920: train loss: 0.1730, test loss 0.7367\n",
      "Epoch 15921: train loss: 0.1730, test loss 0.7367\n",
      "Epoch 15922: train loss: 0.1730, test loss 0.7367\n",
      "Epoch 15923: train loss: 0.1730, test loss 0.7367\n",
      "Epoch 15924: train loss: 0.1729, test loss 0.7367\n",
      "Epoch 15925: train loss: 0.1729, test loss 0.7366\n",
      "Epoch 15926: train loss: 0.1729, test loss 0.7366\n",
      "Epoch 15927: train loss: 0.1729, test loss 0.7366\n",
      "Epoch 15928: train loss: 0.1729, test loss 0.7366\n",
      "Epoch 15929: train loss: 0.1729, test loss 0.7365\n",
      "Epoch 15930: train loss: 0.1729, test loss 0.7365\n",
      "Epoch 15931: train loss: 0.1729, test loss 0.7365\n",
      "Epoch 15932: train loss: 0.1729, test loss 0.7365\n",
      "Epoch 15933: train loss: 0.1729, test loss 0.7364\n",
      "Epoch 15934: train loss: 0.1729, test loss 0.7364\n",
      "Epoch 15935: train loss: 0.1729, test loss 0.7364\n",
      "Epoch 15936: train loss: 0.1729, test loss 0.7364\n",
      "Epoch 15937: train loss: 0.1729, test loss 0.7364\n",
      "Epoch 15938: train loss: 0.1729, test loss 0.7363\n",
      "Epoch 15939: train loss: 0.1729, test loss 0.7363\n",
      "Epoch 15940: train loss: 0.1729, test loss 0.7363\n",
      "Epoch 15941: train loss: 0.1729, test loss 0.7363\n",
      "Epoch 15942: train loss: 0.1729, test loss 0.7362\n",
      "Epoch 15943: train loss: 0.1729, test loss 0.7362\n",
      "Epoch 15944: train loss: 0.1729, test loss 0.7362\n",
      "Epoch 15945: train loss: 0.1729, test loss 0.7362\n",
      "Epoch 15946: train loss: 0.1729, test loss 0.7361\n",
      "Epoch 15947: train loss: 0.1729, test loss 0.7361\n",
      "Epoch 15948: train loss: 0.1729, test loss 0.7361\n",
      "Epoch 15949: train loss: 0.1729, test loss 0.7361\n",
      "Epoch 15950: train loss: 0.1729, test loss 0.7360\n",
      "Epoch 15951: train loss: 0.1729, test loss 0.7360\n",
      "Epoch 15952: train loss: 0.1729, test loss 0.7360\n",
      "Epoch 15953: train loss: 0.1729, test loss 0.7360\n",
      "Epoch 15954: train loss: 0.1729, test loss 0.7360\n",
      "Epoch 15955: train loss: 0.1729, test loss 0.7359\n",
      "Epoch 15956: train loss: 0.1729, test loss 0.7359\n",
      "Epoch 15957: train loss: 0.1729, test loss 0.7359\n",
      "Epoch 15958: train loss: 0.1729, test loss 0.7359\n",
      "Epoch 15959: train loss: 0.1729, test loss 0.7358\n",
      "Epoch 15960: train loss: 0.1729, test loss 0.7358\n",
      "Epoch 15961: train loss: 0.1729, test loss 0.7358\n",
      "Epoch 15962: train loss: 0.1729, test loss 0.7358\n",
      "Epoch 15963: train loss: 0.1729, test loss 0.7357\n",
      "Epoch 15964: train loss: 0.1729, test loss 0.7357\n",
      "Epoch 15965: train loss: 0.1729, test loss 0.7357\n",
      "Epoch 15966: train loss: 0.1729, test loss 0.7357\n",
      "Epoch 15967: train loss: 0.1729, test loss 0.7357\n",
      "Epoch 15968: train loss: 0.1729, test loss 0.7356\n",
      "Epoch 15969: train loss: 0.1729, test loss 0.7356\n",
      "Epoch 15970: train loss: 0.1729, test loss 0.7356\n",
      "Epoch 15971: train loss: 0.1729, test loss 0.7356\n",
      "Epoch 15972: train loss: 0.1729, test loss 0.7355\n",
      "Epoch 15973: train loss: 0.1729, test loss 0.7355\n",
      "Epoch 15974: train loss: 0.1729, test loss 0.7355\n",
      "Epoch 15975: train loss: 0.1729, test loss 0.7355\n",
      "Epoch 15976: train loss: 0.1729, test loss 0.7354\n",
      "Epoch 15977: train loss: 0.1729, test loss 0.7354\n",
      "Epoch 15978: train loss: 0.1729, test loss 0.7354\n",
      "Epoch 15979: train loss: 0.1729, test loss 0.7354\n",
      "Epoch 15980: train loss: 0.1729, test loss 0.7354\n",
      "Epoch 15981: train loss: 0.1729, test loss 0.7353\n",
      "Epoch 15982: train loss: 0.1729, test loss 0.7353\n",
      "Epoch 15983: train loss: 0.1729, test loss 0.7353\n",
      "Epoch 15984: train loss: 0.1729, test loss 0.7353\n",
      "Epoch 15985: train loss: 0.1729, test loss 0.7352\n",
      "Epoch 15986: train loss: 0.1729, test loss 0.7352\n",
      "Epoch 15987: train loss: 0.1729, test loss 0.7352\n",
      "Epoch 15988: train loss: 0.1729, test loss 0.7352\n",
      "Epoch 15989: train loss: 0.1729, test loss 0.7351\n",
      "Epoch 15990: train loss: 0.1729, test loss 0.7351\n",
      "Epoch 15991: train loss: 0.1729, test loss 0.7351\n",
      "Epoch 15992: train loss: 0.1729, test loss 0.7351\n",
      "Epoch 15993: train loss: 0.1729, test loss 0.7351\n",
      "Epoch 15994: train loss: 0.1729, test loss 0.7350\n",
      "Epoch 15995: train loss: 0.1729, test loss 0.7350\n",
      "Epoch 15996: train loss: 0.1729, test loss 0.7350\n",
      "Epoch 15997: train loss: 0.1729, test loss 0.7350\n",
      "Epoch 15998: train loss: 0.1729, test loss 0.7349\n",
      "Epoch 15999: train loss: 0.1729, test loss 0.7349\n",
      "Epoch 16000: train loss: 0.1729, test loss 0.7349\n",
      "Epoch 16001: train loss: 0.1729, test loss 0.7349\n",
      "Epoch 16002: train loss: 0.1729, test loss 0.7348\n",
      "Epoch 16003: train loss: 0.1729, test loss 0.7348\n",
      "Epoch 16004: train loss: 0.1729, test loss 0.7348\n",
      "Epoch 16005: train loss: 0.1729, test loss 0.7348\n",
      "Epoch 16006: train loss: 0.1729, test loss 0.7348\n",
      "Epoch 16007: train loss: 0.1729, test loss 0.7347\n",
      "Epoch 16008: train loss: 0.1729, test loss 0.7347\n",
      "Epoch 16009: train loss: 0.1729, test loss 0.7347\n",
      "Epoch 16010: train loss: 0.1729, test loss 0.7347\n",
      "Epoch 16011: train loss: 0.1729, test loss 0.7346\n",
      "Epoch 16012: train loss: 0.1729, test loss 0.7346\n",
      "Epoch 16013: train loss: 0.1729, test loss 0.7346\n",
      "Epoch 16014: train loss: 0.1729, test loss 0.7346\n",
      "Epoch 16015: train loss: 0.1729, test loss 0.7345\n",
      "Epoch 16016: train loss: 0.1729, test loss 0.7345\n",
      "Epoch 16017: train loss: 0.1729, test loss 0.7345\n",
      "Epoch 16018: train loss: 0.1729, test loss 0.7345\n",
      "Epoch 16019: train loss: 0.1729, test loss 0.7345\n",
      "Epoch 16020: train loss: 0.1729, test loss 0.7344\n",
      "Epoch 16021: train loss: 0.1729, test loss 0.7344\n",
      "Epoch 16022: train loss: 0.1729, test loss 0.7344\n",
      "Epoch 16023: train loss: 0.1729, test loss 0.7344\n",
      "Epoch 16024: train loss: 0.1729, test loss 0.7343\n",
      "Epoch 16025: train loss: 0.1729, test loss 0.7343\n",
      "Epoch 16026: train loss: 0.1729, test loss 0.7343\n",
      "Epoch 16027: train loss: 0.1729, test loss 0.7343\n",
      "Epoch 16028: train loss: 0.1729, test loss 0.7342\n",
      "Epoch 16029: train loss: 0.1729, test loss 0.7342\n",
      "Epoch 16030: train loss: 0.1729, test loss 0.7342\n",
      "Epoch 16031: train loss: 0.1729, test loss 0.7342\n",
      "Epoch 16032: train loss: 0.1729, test loss 0.7342\n",
      "Epoch 16033: train loss: 0.1729, test loss 0.7341\n",
      "Epoch 16034: train loss: 0.1729, test loss 0.7341\n",
      "Epoch 16035: train loss: 0.1729, test loss 0.7341\n",
      "Epoch 16036: train loss: 0.1729, test loss 0.7341\n",
      "Epoch 16037: train loss: 0.1729, test loss 0.7340\n",
      "Epoch 16038: train loss: 0.1729, test loss 0.7340\n",
      "Epoch 16039: train loss: 0.1729, test loss 0.7340\n",
      "Epoch 16040: train loss: 0.1729, test loss 0.7340\n",
      "Epoch 16041: train loss: 0.1729, test loss 0.7339\n",
      "Epoch 16042: train loss: 0.1729, test loss 0.7339\n",
      "Epoch 16043: train loss: 0.1729, test loss 0.7339\n",
      "Epoch 16044: train loss: 0.1729, test loss 0.7339\n",
      "Epoch 16045: train loss: 0.1729, test loss 0.7339\n",
      "Epoch 16046: train loss: 0.1729, test loss 0.7338\n",
      "Epoch 16047: train loss: 0.1729, test loss 0.7338\n",
      "Epoch 16048: train loss: 0.1729, test loss 0.7338\n",
      "Epoch 16049: train loss: 0.1729, test loss 0.7338\n",
      "Epoch 16050: train loss: 0.1729, test loss 0.7337\n",
      "Epoch 16051: train loss: 0.1729, test loss 0.7337\n",
      "Epoch 16052: train loss: 0.1729, test loss 0.7337\n",
      "Epoch 16053: train loss: 0.1729, test loss 0.7337\n",
      "Epoch 16054: train loss: 0.1729, test loss 0.7337\n",
      "Epoch 16055: train loss: 0.1729, test loss 0.7336\n",
      "Epoch 16056: train loss: 0.1729, test loss 0.7336\n",
      "Epoch 16057: train loss: 0.1729, test loss 0.7336\n",
      "Epoch 16058: train loss: 0.1729, test loss 0.7336\n",
      "Epoch 16059: train loss: 0.1729, test loss 0.7335\n",
      "Epoch 16060: train loss: 0.1729, test loss 0.7335\n",
      "Epoch 16061: train loss: 0.1729, test loss 0.7335\n",
      "Epoch 16062: train loss: 0.1729, test loss 0.7335\n",
      "Epoch 16063: train loss: 0.1729, test loss 0.7334\n",
      "Epoch 16064: train loss: 0.1729, test loss 0.7334\n",
      "Epoch 16065: train loss: 0.1729, test loss 0.7334\n",
      "Epoch 16066: train loss: 0.1729, test loss 0.7334\n",
      "Epoch 16067: train loss: 0.1729, test loss 0.7334\n",
      "Epoch 16068: train loss: 0.1729, test loss 0.7333\n",
      "Epoch 16069: train loss: 0.1729, test loss 0.7333\n",
      "Epoch 16070: train loss: 0.1729, test loss 0.7333\n",
      "Epoch 16071: train loss: 0.1729, test loss 0.7333\n",
      "Epoch 16072: train loss: 0.1729, test loss 0.7332\n",
      "Epoch 16073: train loss: 0.1729, test loss 0.7332\n",
      "Epoch 16074: train loss: 0.1729, test loss 0.7332\n",
      "Epoch 16075: train loss: 0.1729, test loss 0.7332\n",
      "Epoch 16076: train loss: 0.1729, test loss 0.7331\n",
      "Epoch 16077: train loss: 0.1729, test loss 0.7331\n",
      "Epoch 16078: train loss: 0.1729, test loss 0.7331\n",
      "Epoch 16079: train loss: 0.1729, test loss 0.7331\n",
      "Epoch 16080: train loss: 0.1729, test loss 0.7331\n",
      "Epoch 16081: train loss: 0.1729, test loss 0.7330\n",
      "Epoch 16082: train loss: 0.1729, test loss 0.7330\n",
      "Epoch 16083: train loss: 0.1729, test loss 0.7330\n",
      "Epoch 16084: train loss: 0.1729, test loss 0.7330\n",
      "Epoch 16085: train loss: 0.1729, test loss 0.7329\n",
      "Epoch 16086: train loss: 0.1729, test loss 0.7329\n",
      "Epoch 16087: train loss: 0.1729, test loss 0.7329\n",
      "Epoch 16088: train loss: 0.1729, test loss 0.7329\n",
      "Epoch 16089: train loss: 0.1729, test loss 0.7329\n",
      "Epoch 16090: train loss: 0.1729, test loss 0.7328\n",
      "Epoch 16091: train loss: 0.1729, test loss 0.7328\n",
      "Epoch 16092: train loss: 0.1729, test loss 0.7328\n",
      "Epoch 16093: train loss: 0.1729, test loss 0.7328\n",
      "Epoch 16094: train loss: 0.1729, test loss 0.7327\n",
      "Epoch 16095: train loss: 0.1729, test loss 0.7327\n",
      "Epoch 16096: train loss: 0.1729, test loss 0.7327\n",
      "Epoch 16097: train loss: 0.1729, test loss 0.7327\n",
      "Epoch 16098: train loss: 0.1729, test loss 0.7326\n",
      "Epoch 16099: train loss: 0.1729, test loss 0.7326\n",
      "Epoch 16100: train loss: 0.1729, test loss 0.7326\n",
      "Epoch 16101: train loss: 0.1729, test loss 0.7326\n",
      "Epoch 16102: train loss: 0.1729, test loss 0.7326\n",
      "Epoch 16103: train loss: 0.1729, test loss 0.7325\n",
      "Epoch 16104: train loss: 0.1728, test loss 0.7325\n",
      "Epoch 16105: train loss: 0.1728, test loss 0.7325\n",
      "Epoch 16106: train loss: 0.1728, test loss 0.7325\n",
      "Epoch 16107: train loss: 0.1728, test loss 0.7324\n",
      "Epoch 16108: train loss: 0.1728, test loss 0.7324\n",
      "Epoch 16109: train loss: 0.1728, test loss 0.7324\n",
      "Epoch 16110: train loss: 0.1728, test loss 0.7324\n",
      "Epoch 16111: train loss: 0.1728, test loss 0.7324\n",
      "Epoch 16112: train loss: 0.1728, test loss 0.7323\n",
      "Epoch 16113: train loss: 0.1728, test loss 0.7323\n",
      "Epoch 16114: train loss: 0.1728, test loss 0.7323\n",
      "Epoch 16115: train loss: 0.1728, test loss 0.7323\n",
      "Epoch 16116: train loss: 0.1728, test loss 0.7322\n",
      "Epoch 16117: train loss: 0.1728, test loss 0.7322\n",
      "Epoch 16118: train loss: 0.1728, test loss 0.7322\n",
      "Epoch 16119: train loss: 0.1728, test loss 0.7322\n",
      "Epoch 16120: train loss: 0.1728, test loss 0.7322\n",
      "Epoch 16121: train loss: 0.1728, test loss 0.7321\n",
      "Epoch 16122: train loss: 0.1728, test loss 0.7321\n",
      "Epoch 16123: train loss: 0.1728, test loss 0.7321\n",
      "Epoch 16124: train loss: 0.1728, test loss 0.7321\n",
      "Epoch 16125: train loss: 0.1728, test loss 0.7320\n",
      "Epoch 16126: train loss: 0.1728, test loss 0.7320\n",
      "Epoch 16127: train loss: 0.1728, test loss 0.7320\n",
      "Epoch 16128: train loss: 0.1728, test loss 0.7320\n",
      "Epoch 16129: train loss: 0.1728, test loss 0.7319\n",
      "Epoch 16130: train loss: 0.1728, test loss 0.7319\n",
      "Epoch 16131: train loss: 0.1728, test loss 0.7319\n",
      "Epoch 16132: train loss: 0.1728, test loss 0.7319\n",
      "Epoch 16133: train loss: 0.1728, test loss 0.7319\n",
      "Epoch 16134: train loss: 0.1728, test loss 0.7318\n",
      "Epoch 16135: train loss: 0.1728, test loss 0.7318\n",
      "Epoch 16136: train loss: 0.1728, test loss 0.7318\n",
      "Epoch 16137: train loss: 0.1728, test loss 0.7318\n",
      "Epoch 16138: train loss: 0.1728, test loss 0.7317\n",
      "Epoch 16139: train loss: 0.1728, test loss 0.7317\n",
      "Epoch 16140: train loss: 0.1728, test loss 0.7317\n",
      "Epoch 16141: train loss: 0.1728, test loss 0.7317\n",
      "Epoch 16142: train loss: 0.1728, test loss 0.7317\n",
      "Epoch 16143: train loss: 0.1728, test loss 0.7316\n",
      "Epoch 16144: train loss: 0.1728, test loss 0.7316\n",
      "Epoch 16145: train loss: 0.1728, test loss 0.7316\n",
      "Epoch 16146: train loss: 0.1728, test loss 0.7316\n",
      "Epoch 16147: train loss: 0.1728, test loss 0.7315\n",
      "Epoch 16148: train loss: 0.1728, test loss 0.7315\n",
      "Epoch 16149: train loss: 0.1728, test loss 0.7315\n",
      "Epoch 16150: train loss: 0.1728, test loss 0.7315\n",
      "Epoch 16151: train loss: 0.1728, test loss 0.7315\n",
      "Epoch 16152: train loss: 0.1728, test loss 0.7314\n",
      "Epoch 16153: train loss: 0.1728, test loss 0.7314\n",
      "Epoch 16154: train loss: 0.1728, test loss 0.7314\n",
      "Epoch 16155: train loss: 0.1728, test loss 0.7314\n",
      "Epoch 16156: train loss: 0.1728, test loss 0.7313\n",
      "Epoch 16157: train loss: 0.1728, test loss 0.7313\n",
      "Epoch 16158: train loss: 0.1728, test loss 0.7313\n",
      "Epoch 16159: train loss: 0.1728, test loss 0.7313\n",
      "Epoch 16160: train loss: 0.1728, test loss 0.7313\n",
      "Epoch 16161: train loss: 0.1728, test loss 0.7312\n",
      "Epoch 16162: train loss: 0.1728, test loss 0.7312\n",
      "Epoch 16163: train loss: 0.1728, test loss 0.7312\n",
      "Epoch 16164: train loss: 0.1728, test loss 0.7312\n",
      "Epoch 16165: train loss: 0.1728, test loss 0.7311\n",
      "Epoch 16166: train loss: 0.1728, test loss 0.7311\n",
      "Epoch 16167: train loss: 0.1728, test loss 0.7311\n",
      "Epoch 16168: train loss: 0.1728, test loss 0.7311\n",
      "Epoch 16169: train loss: 0.1728, test loss 0.7310\n",
      "Epoch 16170: train loss: 0.1728, test loss 0.7310\n",
      "Epoch 16171: train loss: 0.1728, test loss 0.7310\n",
      "Epoch 16172: train loss: 0.1728, test loss 0.7310\n",
      "Epoch 16173: train loss: 0.1728, test loss 0.7310\n",
      "Epoch 16174: train loss: 0.1728, test loss 0.7309\n",
      "Epoch 16175: train loss: 0.1728, test loss 0.7309\n",
      "Epoch 16176: train loss: 0.1728, test loss 0.7309\n",
      "Epoch 16177: train loss: 0.1728, test loss 0.7309\n",
      "Epoch 16178: train loss: 0.1728, test loss 0.7308\n",
      "Epoch 16179: train loss: 0.1728, test loss 0.7308\n",
      "Epoch 16180: train loss: 0.1728, test loss 0.7308\n",
      "Epoch 16181: train loss: 0.1728, test loss 0.7308\n",
      "Epoch 16182: train loss: 0.1728, test loss 0.7308\n",
      "Epoch 16183: train loss: 0.1728, test loss 0.7307\n",
      "Epoch 16184: train loss: 0.1728, test loss 0.7307\n",
      "Epoch 16185: train loss: 0.1728, test loss 0.7307\n",
      "Epoch 16186: train loss: 0.1728, test loss 0.7307\n",
      "Epoch 16187: train loss: 0.1728, test loss 0.7306\n",
      "Epoch 16188: train loss: 0.1728, test loss 0.7306\n",
      "Epoch 16189: train loss: 0.1728, test loss 0.7306\n",
      "Epoch 16190: train loss: 0.1728, test loss 0.7306\n",
      "Epoch 16191: train loss: 0.1728, test loss 0.7306\n",
      "Epoch 16192: train loss: 0.1728, test loss 0.7305\n",
      "Epoch 16193: train loss: 0.1728, test loss 0.7305\n",
      "Epoch 16194: train loss: 0.1728, test loss 0.7305\n",
      "Epoch 16195: train loss: 0.1728, test loss 0.7305\n",
      "Epoch 16196: train loss: 0.1728, test loss 0.7304\n",
      "Epoch 16197: train loss: 0.1728, test loss 0.7304\n",
      "Epoch 16198: train loss: 0.1728, test loss 0.7304\n",
      "Epoch 16199: train loss: 0.1728, test loss 0.7304\n",
      "Epoch 16200: train loss: 0.1728, test loss 0.7304\n",
      "Epoch 16201: train loss: 0.1728, test loss 0.7303\n",
      "Epoch 16202: train loss: 0.1728, test loss 0.7303\n",
      "Epoch 16203: train loss: 0.1728, test loss 0.7303\n",
      "Epoch 16204: train loss: 0.1728, test loss 0.7303\n",
      "Epoch 16205: train loss: 0.1728, test loss 0.7302\n",
      "Epoch 16206: train loss: 0.1728, test loss 0.7302\n",
      "Epoch 16207: train loss: 0.1728, test loss 0.7302\n",
      "Epoch 16208: train loss: 0.1728, test loss 0.7302\n",
      "Epoch 16209: train loss: 0.1728, test loss 0.7302\n",
      "Epoch 16210: train loss: 0.1728, test loss 0.7301\n",
      "Epoch 16211: train loss: 0.1728, test loss 0.7301\n",
      "Epoch 16212: train loss: 0.1728, test loss 0.7301\n",
      "Epoch 16213: train loss: 0.1728, test loss 0.7301\n",
      "Epoch 16214: train loss: 0.1728, test loss 0.7301\n",
      "Epoch 16215: train loss: 0.1728, test loss 0.7300\n",
      "Epoch 16216: train loss: 0.1728, test loss 0.7300\n",
      "Epoch 16217: train loss: 0.1728, test loss 0.7300\n",
      "Epoch 16218: train loss: 0.1728, test loss 0.7300\n",
      "Epoch 16219: train loss: 0.1728, test loss 0.7299\n",
      "Epoch 16220: train loss: 0.1728, test loss 0.7299\n",
      "Epoch 16221: train loss: 0.1728, test loss 0.7299\n",
      "Epoch 16222: train loss: 0.1728, test loss 0.7299\n",
      "Epoch 16223: train loss: 0.1728, test loss 0.7299\n",
      "Epoch 16224: train loss: 0.1728, test loss 0.7298\n",
      "Epoch 16225: train loss: 0.1728, test loss 0.7298\n",
      "Epoch 16226: train loss: 0.1728, test loss 0.7298\n",
      "Epoch 16227: train loss: 0.1728, test loss 0.7298\n",
      "Epoch 16228: train loss: 0.1728, test loss 0.7297\n",
      "Epoch 16229: train loss: 0.1728, test loss 0.7297\n",
      "Epoch 16230: train loss: 0.1728, test loss 0.7297\n",
      "Epoch 16231: train loss: 0.1728, test loss 0.7297\n",
      "Epoch 16232: train loss: 0.1728, test loss 0.7297\n",
      "Epoch 16233: train loss: 0.1728, test loss 0.7296\n",
      "Epoch 16234: train loss: 0.1728, test loss 0.7296\n",
      "Epoch 16235: train loss: 0.1728, test loss 0.7296\n",
      "Epoch 16236: train loss: 0.1728, test loss 0.7296\n",
      "Epoch 16237: train loss: 0.1728, test loss 0.7295\n",
      "Epoch 16238: train loss: 0.1728, test loss 0.7295\n",
      "Epoch 16239: train loss: 0.1728, test loss 0.7295\n",
      "Epoch 16240: train loss: 0.1728, test loss 0.7295\n",
      "Epoch 16241: train loss: 0.1728, test loss 0.7295\n",
      "Epoch 16242: train loss: 0.1728, test loss 0.7294\n",
      "Epoch 16243: train loss: 0.1728, test loss 0.7294\n",
      "Epoch 16244: train loss: 0.1728, test loss 0.7294\n",
      "Epoch 16245: train loss: 0.1728, test loss 0.7294\n",
      "Epoch 16246: train loss: 0.1728, test loss 0.7293\n",
      "Epoch 16247: train loss: 0.1728, test loss 0.7293\n",
      "Epoch 16248: train loss: 0.1728, test loss 0.7293\n",
      "Epoch 16249: train loss: 0.1728, test loss 0.7293\n",
      "Epoch 16250: train loss: 0.1728, test loss 0.7293\n",
      "Epoch 16251: train loss: 0.1728, test loss 0.7292\n",
      "Epoch 16252: train loss: 0.1728, test loss 0.7292\n",
      "Epoch 16253: train loss: 0.1728, test loss 0.7292\n",
      "Epoch 16254: train loss: 0.1728, test loss 0.7292\n",
      "Epoch 16255: train loss: 0.1728, test loss 0.7292\n",
      "Epoch 16256: train loss: 0.1728, test loss 0.7291\n",
      "Epoch 16257: train loss: 0.1728, test loss 0.7291\n",
      "Epoch 16258: train loss: 0.1728, test loss 0.7291\n",
      "Epoch 16259: train loss: 0.1728, test loss 0.7291\n",
      "Epoch 16260: train loss: 0.1728, test loss 0.7290\n",
      "Epoch 16261: train loss: 0.1728, test loss 0.7290\n",
      "Epoch 16262: train loss: 0.1728, test loss 0.7290\n",
      "Epoch 16263: train loss: 0.1728, test loss 0.7290\n",
      "Epoch 16264: train loss: 0.1728, test loss 0.7290\n",
      "Epoch 16265: train loss: 0.1728, test loss 0.7289\n",
      "Epoch 16266: train loss: 0.1728, test loss 0.7289\n",
      "Epoch 16267: train loss: 0.1728, test loss 0.7289\n",
      "Epoch 16268: train loss: 0.1728, test loss 0.7289\n",
      "Epoch 16269: train loss: 0.1728, test loss 0.7288\n",
      "Epoch 16270: train loss: 0.1728, test loss 0.7288\n",
      "Epoch 16271: train loss: 0.1728, test loss 0.7288\n",
      "Epoch 16272: train loss: 0.1728, test loss 0.7288\n",
      "Epoch 16273: train loss: 0.1728, test loss 0.7288\n",
      "Epoch 16274: train loss: 0.1728, test loss 0.7287\n",
      "Epoch 16275: train loss: 0.1728, test loss 0.7287\n",
      "Epoch 16276: train loss: 0.1728, test loss 0.7287\n",
      "Epoch 16277: train loss: 0.1728, test loss 0.7287\n",
      "Epoch 16278: train loss: 0.1728, test loss 0.7286\n",
      "Epoch 16279: train loss: 0.1728, test loss 0.7286\n",
      "Epoch 16280: train loss: 0.1728, test loss 0.7286\n",
      "Epoch 16281: train loss: 0.1728, test loss 0.7286\n",
      "Epoch 16282: train loss: 0.1728, test loss 0.7286\n",
      "Epoch 16283: train loss: 0.1728, test loss 0.7285\n",
      "Epoch 16284: train loss: 0.1728, test loss 0.7285\n",
      "Epoch 16285: train loss: 0.1728, test loss 0.7285\n",
      "Epoch 16286: train loss: 0.1728, test loss 0.7285\n",
      "Epoch 16287: train loss: 0.1728, test loss 0.7284\n",
      "Epoch 16288: train loss: 0.1728, test loss 0.7284\n",
      "Epoch 16289: train loss: 0.1727, test loss 0.7284\n",
      "Epoch 16290: train loss: 0.1727, test loss 0.7284\n",
      "Epoch 16291: train loss: 0.1727, test loss 0.7284\n",
      "Epoch 16292: train loss: 0.1727, test loss 0.7283\n",
      "Epoch 16293: train loss: 0.1727, test loss 0.7283\n",
      "Epoch 16294: train loss: 0.1727, test loss 0.7283\n",
      "Epoch 16295: train loss: 0.1727, test loss 0.7283\n",
      "Epoch 16296: train loss: 0.1727, test loss 0.7282\n",
      "Epoch 16297: train loss: 0.1727, test loss 0.7282\n",
      "Epoch 16298: train loss: 0.1727, test loss 0.7282\n",
      "Epoch 16299: train loss: 0.1727, test loss 0.7282\n",
      "Epoch 16300: train loss: 0.1727, test loss 0.7282\n",
      "Epoch 16301: train loss: 0.1727, test loss 0.7281\n",
      "Epoch 16302: train loss: 0.1727, test loss 0.7281\n",
      "Epoch 16303: train loss: 0.1727, test loss 0.7281\n",
      "Epoch 16304: train loss: 0.1727, test loss 0.7281\n",
      "Epoch 16305: train loss: 0.1727, test loss 0.7280\n",
      "Epoch 16306: train loss: 0.1727, test loss 0.7280\n",
      "Epoch 16307: train loss: 0.1727, test loss 0.7280\n",
      "Epoch 16308: train loss: 0.1727, test loss 0.7280\n",
      "Epoch 16309: train loss: 0.1727, test loss 0.7280\n",
      "Epoch 16310: train loss: 0.1727, test loss 0.7279\n",
      "Epoch 16311: train loss: 0.1727, test loss 0.7279\n",
      "Epoch 16312: train loss: 0.1727, test loss 0.7279\n",
      "Epoch 16313: train loss: 0.1727, test loss 0.7279\n",
      "Epoch 16314: train loss: 0.1727, test loss 0.7278\n",
      "Epoch 16315: train loss: 0.1727, test loss 0.7278\n",
      "Epoch 16316: train loss: 0.1727, test loss 0.7278\n",
      "Epoch 16317: train loss: 0.1727, test loss 0.7278\n",
      "Epoch 16318: train loss: 0.1727, test loss 0.7278\n",
      "Epoch 16319: train loss: 0.1727, test loss 0.7277\n",
      "Epoch 16320: train loss: 0.1727, test loss 0.7277\n",
      "Epoch 16321: train loss: 0.1727, test loss 0.7277\n",
      "Epoch 16322: train loss: 0.1727, test loss 0.7277\n",
      "Epoch 16323: train loss: 0.1727, test loss 0.7276\n",
      "Epoch 16324: train loss: 0.1727, test loss 0.7276\n",
      "Epoch 16325: train loss: 0.1727, test loss 0.7276\n",
      "Epoch 16326: train loss: 0.1727, test loss 0.7276\n",
      "Epoch 16327: train loss: 0.1727, test loss 0.7276\n",
      "Epoch 16328: train loss: 0.1727, test loss 0.7275\n",
      "Epoch 16329: train loss: 0.1727, test loss 0.7275\n",
      "Epoch 16330: train loss: 0.1727, test loss 0.7275\n",
      "Epoch 16331: train loss: 0.1727, test loss 0.7275\n",
      "Epoch 16332: train loss: 0.1727, test loss 0.7275\n",
      "Epoch 16333: train loss: 0.1727, test loss 0.7274\n",
      "Epoch 16334: train loss: 0.1727, test loss 0.7274\n",
      "Epoch 16335: train loss: 0.1727, test loss 0.7274\n",
      "Epoch 16336: train loss: 0.1727, test loss 0.7274\n",
      "Epoch 16337: train loss: 0.1727, test loss 0.7273\n",
      "Epoch 16338: train loss: 0.1727, test loss 0.7273\n",
      "Epoch 16339: train loss: 0.1727, test loss 0.7273\n",
      "Epoch 16340: train loss: 0.1727, test loss 0.7273\n",
      "Epoch 16341: train loss: 0.1727, test loss 0.7273\n",
      "Epoch 16342: train loss: 0.1727, test loss 0.7272\n",
      "Epoch 16343: train loss: 0.1727, test loss 0.7272\n",
      "Epoch 16344: train loss: 0.1727, test loss 0.7272\n",
      "Epoch 16345: train loss: 0.1727, test loss 0.7272\n",
      "Epoch 16346: train loss: 0.1727, test loss 0.7271\n",
      "Epoch 16347: train loss: 0.1727, test loss 0.7271\n",
      "Epoch 16348: train loss: 0.1727, test loss 0.7271\n",
      "Epoch 16349: train loss: 0.1727, test loss 0.7271\n",
      "Epoch 16350: train loss: 0.1727, test loss 0.7271\n",
      "Epoch 16351: train loss: 0.1727, test loss 0.7270\n",
      "Epoch 16352: train loss: 0.1727, test loss 0.7270\n",
      "Epoch 16353: train loss: 0.1727, test loss 0.7270\n",
      "Epoch 16354: train loss: 0.1727, test loss 0.7270\n",
      "Epoch 16355: train loss: 0.1727, test loss 0.7270\n",
      "Epoch 16356: train loss: 0.1727, test loss 0.7269\n",
      "Epoch 16357: train loss: 0.1727, test loss 0.7269\n",
      "Epoch 16358: train loss: 0.1727, test loss 0.7269\n",
      "Epoch 16359: train loss: 0.1727, test loss 0.7269\n",
      "Epoch 16360: train loss: 0.1727, test loss 0.7269\n",
      "Epoch 16361: train loss: 0.1727, test loss 0.7268\n",
      "Epoch 16362: train loss: 0.1727, test loss 0.7268\n",
      "Epoch 16363: train loss: 0.1727, test loss 0.7268\n",
      "Epoch 16364: train loss: 0.1727, test loss 0.7268\n",
      "Epoch 16365: train loss: 0.1727, test loss 0.7268\n",
      "Epoch 16366: train loss: 0.1727, test loss 0.7267\n",
      "Epoch 16367: train loss: 0.1727, test loss 0.7267\n",
      "Epoch 16368: train loss: 0.1727, test loss 0.7267\n",
      "Epoch 16369: train loss: 0.1727, test loss 0.7267\n",
      "Epoch 16370: train loss: 0.1727, test loss 0.7267\n",
      "Epoch 16371: train loss: 0.1727, test loss 0.7266\n",
      "Epoch 16372: train loss: 0.1727, test loss 0.7266\n",
      "Epoch 16373: train loss: 0.1727, test loss 0.7266\n",
      "Epoch 16374: train loss: 0.1727, test loss 0.7266\n",
      "Epoch 16375: train loss: 0.1727, test loss 0.7266\n",
      "Epoch 16376: train loss: 0.1727, test loss 0.7265\n",
      "Epoch 16377: train loss: 0.1727, test loss 0.7265\n",
      "Epoch 16378: train loss: 0.1727, test loss 0.7265\n",
      "Epoch 16379: train loss: 0.1727, test loss 0.7265\n",
      "Epoch 16380: train loss: 0.1727, test loss 0.7264\n",
      "Epoch 16381: train loss: 0.1727, test loss 0.7264\n",
      "Epoch 16382: train loss: 0.1727, test loss 0.7264\n",
      "Epoch 16383: train loss: 0.1727, test loss 0.7264\n",
      "Epoch 16384: train loss: 0.1727, test loss 0.7264\n",
      "Epoch 16385: train loss: 0.1727, test loss 0.7263\n",
      "Epoch 16386: train loss: 0.1727, test loss 0.7263\n",
      "Epoch 16387: train loss: 0.1727, test loss 0.7263\n",
      "Epoch 16388: train loss: 0.1727, test loss 0.7263\n",
      "Epoch 16389: train loss: 0.1727, test loss 0.7263\n",
      "Epoch 16390: train loss: 0.1727, test loss 0.7262\n",
      "Epoch 16391: train loss: 0.1727, test loss 0.7262\n",
      "Epoch 16392: train loss: 0.1727, test loss 0.7262\n",
      "Epoch 16393: train loss: 0.1727, test loss 0.7262\n",
      "Epoch 16394: train loss: 0.1727, test loss 0.7262\n",
      "Epoch 16395: train loss: 0.1727, test loss 0.7261\n",
      "Epoch 16396: train loss: 0.1727, test loss 0.7261\n",
      "Epoch 16397: train loss: 0.1727, test loss 0.7261\n",
      "Epoch 16398: train loss: 0.1727, test loss 0.7261\n",
      "Epoch 16399: train loss: 0.1727, test loss 0.7261\n",
      "Epoch 16400: train loss: 0.1727, test loss 0.7260\n",
      "Epoch 16401: train loss: 0.1727, test loss 0.7260\n",
      "Epoch 16402: train loss: 0.1727, test loss 0.7260\n",
      "Epoch 16403: train loss: 0.1727, test loss 0.7260\n",
      "Epoch 16404: train loss: 0.1727, test loss 0.7260\n",
      "Epoch 16405: train loss: 0.1727, test loss 0.7259\n",
      "Epoch 16406: train loss: 0.1727, test loss 0.7259\n",
      "Epoch 16407: train loss: 0.1727, test loss 0.7259\n",
      "Epoch 16408: train loss: 0.1727, test loss 0.7259\n",
      "Epoch 16409: train loss: 0.1727, test loss 0.7259\n",
      "Epoch 16410: train loss: 0.1727, test loss 0.7258\n",
      "Epoch 16411: train loss: 0.1727, test loss 0.7258\n",
      "Epoch 16412: train loss: 0.1727, test loss 0.7258\n",
      "Epoch 16413: train loss: 0.1727, test loss 0.7258\n",
      "Epoch 16414: train loss: 0.1727, test loss 0.7258\n",
      "Epoch 16415: train loss: 0.1727, test loss 0.7257\n",
      "Epoch 16416: train loss: 0.1727, test loss 0.7257\n",
      "Epoch 16417: train loss: 0.1727, test loss 0.7257\n",
      "Epoch 16418: train loss: 0.1727, test loss 0.7257\n",
      "Epoch 16419: train loss: 0.1727, test loss 0.7257\n",
      "Epoch 16420: train loss: 0.1727, test loss 0.7256\n",
      "Epoch 16421: train loss: 0.1727, test loss 0.7256\n",
      "Epoch 16422: train loss: 0.1727, test loss 0.7256\n",
      "Epoch 16423: train loss: 0.1727, test loss 0.7256\n",
      "Epoch 16424: train loss: 0.1727, test loss 0.7256\n",
      "Epoch 16425: train loss: 0.1727, test loss 0.7255\n",
      "Epoch 16426: train loss: 0.1727, test loss 0.7255\n",
      "Epoch 16427: train loss: 0.1727, test loss 0.7255\n",
      "Epoch 16428: train loss: 0.1727, test loss 0.7255\n",
      "Epoch 16429: train loss: 0.1727, test loss 0.7255\n",
      "Epoch 16430: train loss: 0.1727, test loss 0.7254\n",
      "Epoch 16431: train loss: 0.1727, test loss 0.7254\n",
      "Epoch 16432: train loss: 0.1727, test loss 0.7254\n",
      "Epoch 16433: train loss: 0.1727, test loss 0.7254\n",
      "Epoch 16434: train loss: 0.1727, test loss 0.7254\n",
      "Epoch 16435: train loss: 0.1727, test loss 0.7253\n",
      "Epoch 16436: train loss: 0.1727, test loss 0.7253\n",
      "Epoch 16437: train loss: 0.1727, test loss 0.7253\n",
      "Epoch 16438: train loss: 0.1727, test loss 0.7253\n",
      "Epoch 16439: train loss: 0.1727, test loss 0.7253\n",
      "Epoch 16440: train loss: 0.1727, test loss 0.7252\n",
      "Epoch 16441: train loss: 0.1727, test loss 0.7252\n",
      "Epoch 16442: train loss: 0.1727, test loss 0.7252\n",
      "Epoch 16443: train loss: 0.1727, test loss 0.7252\n",
      "Epoch 16444: train loss: 0.1727, test loss 0.7252\n",
      "Epoch 16445: train loss: 0.1727, test loss 0.7251\n",
      "Epoch 16446: train loss: 0.1727, test loss 0.7251\n",
      "Epoch 16447: train loss: 0.1727, test loss 0.7251\n",
      "Epoch 16448: train loss: 0.1727, test loss 0.7251\n",
      "Epoch 16449: train loss: 0.1727, test loss 0.7251\n",
      "Epoch 16450: train loss: 0.1727, test loss 0.7250\n",
      "Epoch 16451: train loss: 0.1727, test loss 0.7250\n",
      "Epoch 16452: train loss: 0.1727, test loss 0.7250\n",
      "Epoch 16453: train loss: 0.1727, test loss 0.7250\n",
      "Epoch 16454: train loss: 0.1727, test loss 0.7250\n",
      "Epoch 16455: train loss: 0.1727, test loss 0.7249\n",
      "Epoch 16456: train loss: 0.1727, test loss 0.7249\n",
      "Epoch 16457: train loss: 0.1727, test loss 0.7249\n",
      "Epoch 16458: train loss: 0.1727, test loss 0.7249\n",
      "Epoch 16459: train loss: 0.1727, test loss 0.7249\n",
      "Epoch 16460: train loss: 0.1727, test loss 0.7248\n",
      "Epoch 16461: train loss: 0.1727, test loss 0.7248\n",
      "Epoch 16462: train loss: 0.1727, test loss 0.7248\n",
      "Epoch 16463: train loss: 0.1727, test loss 0.7248\n",
      "Epoch 16464: train loss: 0.1727, test loss 0.7248\n",
      "Epoch 16465: train loss: 0.1727, test loss 0.7247\n",
      "Epoch 16466: train loss: 0.1727, test loss 0.7247\n",
      "Epoch 16467: train loss: 0.1727, test loss 0.7247\n",
      "Epoch 16468: train loss: 0.1727, test loss 0.7247\n",
      "Epoch 16469: train loss: 0.1727, test loss 0.7247\n",
      "Epoch 16470: train loss: 0.1727, test loss 0.7246\n",
      "Epoch 16471: train loss: 0.1727, test loss 0.7246\n",
      "Epoch 16472: train loss: 0.1727, test loss 0.7246\n",
      "Epoch 16473: train loss: 0.1727, test loss 0.7246\n",
      "Epoch 16474: train loss: 0.1727, test loss 0.7246\n",
      "Epoch 16475: train loss: 0.1727, test loss 0.7245\n",
      "Epoch 16476: train loss: 0.1727, test loss 0.7245\n",
      "Epoch 16477: train loss: 0.1727, test loss 0.7245\n",
      "Epoch 16478: train loss: 0.1726, test loss 0.7245\n",
      "Epoch 16479: train loss: 0.1726, test loss 0.7245\n",
      "Epoch 16480: train loss: 0.1726, test loss 0.7244\n",
      "Epoch 16481: train loss: 0.1726, test loss 0.7244\n",
      "Epoch 16482: train loss: 0.1726, test loss 0.7244\n",
      "Epoch 16483: train loss: 0.1726, test loss 0.7244\n",
      "Epoch 16484: train loss: 0.1726, test loss 0.7244\n",
      "Epoch 16485: train loss: 0.1726, test loss 0.7243\n",
      "Epoch 16486: train loss: 0.1726, test loss 0.7243\n",
      "Epoch 16487: train loss: 0.1726, test loss 0.7243\n",
      "Epoch 16488: train loss: 0.1726, test loss 0.7243\n",
      "Epoch 16489: train loss: 0.1726, test loss 0.7243\n",
      "Epoch 16490: train loss: 0.1726, test loss 0.7242\n",
      "Epoch 16491: train loss: 0.1726, test loss 0.7242\n",
      "Epoch 16492: train loss: 0.1726, test loss 0.7242\n",
      "Epoch 16493: train loss: 0.1726, test loss 0.7242\n",
      "Epoch 16494: train loss: 0.1726, test loss 0.7242\n",
      "Epoch 16495: train loss: 0.1726, test loss 0.7241\n",
      "Epoch 16496: train loss: 0.1726, test loss 0.7241\n",
      "Epoch 16497: train loss: 0.1726, test loss 0.7241\n",
      "Epoch 16498: train loss: 0.1726, test loss 0.7241\n",
      "Epoch 16499: train loss: 0.1726, test loss 0.7241\n",
      "Epoch 16500: train loss: 0.1726, test loss 0.7240\n",
      "Epoch 16501: train loss: 0.1726, test loss 0.7240\n",
      "Epoch 16502: train loss: 0.1726, test loss 0.7240\n",
      "Epoch 16503: train loss: 0.1726, test loss 0.7240\n",
      "Epoch 16504: train loss: 0.1726, test loss 0.7240\n",
      "Epoch 16505: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16506: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16507: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16508: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16509: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16510: train loss: 0.1726, test loss 0.7239\n",
      "Epoch 16511: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16512: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16513: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16514: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16515: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16516: train loss: 0.1726, test loss 0.7238\n",
      "Epoch 16517: train loss: 0.1726, test loss 0.7237\n",
      "Epoch 16518: train loss: 0.1726, test loss 0.7237\n",
      "Epoch 16519: train loss: 0.1726, test loss 0.7237\n",
      "Epoch 16520: train loss: 0.1726, test loss 0.7237\n",
      "Epoch 16521: train loss: 0.1726, test loss 0.7237\n",
      "Epoch 16522: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16523: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16524: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16525: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16526: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16527: train loss: 0.1726, test loss 0.7236\n",
      "Epoch 16528: train loss: 0.1726, test loss 0.7235\n",
      "Epoch 16529: train loss: 0.1726, test loss 0.7235\n",
      "Epoch 16530: train loss: 0.1726, test loss 0.7235\n",
      "Epoch 16531: train loss: 0.1726, test loss 0.7235\n",
      "Epoch 16532: train loss: 0.1726, test loss 0.7235\n",
      "Epoch 16533: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16534: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16535: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16536: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16537: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16538: train loss: 0.1726, test loss 0.7234\n",
      "Epoch 16539: train loss: 0.1726, test loss 0.7233\n",
      "Epoch 16540: train loss: 0.1726, test loss 0.7233\n",
      "Epoch 16541: train loss: 0.1726, test loss 0.7233\n",
      "Epoch 16542: train loss: 0.1726, test loss 0.7233\n",
      "Epoch 16543: train loss: 0.1726, test loss 0.7233\n",
      "Epoch 16544: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16545: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16546: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16547: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16548: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16549: train loss: 0.1726, test loss 0.7232\n",
      "Epoch 16550: train loss: 0.1726, test loss 0.7231\n",
      "Epoch 16551: train loss: 0.1726, test loss 0.7231\n",
      "Epoch 16552: train loss: 0.1726, test loss 0.7231\n",
      "Epoch 16553: train loss: 0.1726, test loss 0.7231\n",
      "Epoch 16554: train loss: 0.1726, test loss 0.7231\n",
      "Epoch 16555: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16556: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16557: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16558: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16559: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16560: train loss: 0.1726, test loss 0.7230\n",
      "Epoch 16561: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16562: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16563: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16564: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16565: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16566: train loss: 0.1726, test loss 0.7229\n",
      "Epoch 16567: train loss: 0.1726, test loss 0.7228\n",
      "Epoch 16568: train loss: 0.1726, test loss 0.7228\n",
      "Epoch 16569: train loss: 0.1726, test loss 0.7228\n",
      "Epoch 16570: train loss: 0.1726, test loss 0.7228\n",
      "Epoch 16571: train loss: 0.1726, test loss 0.7228\n",
      "Epoch 16572: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16573: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16574: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16575: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16576: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16577: train loss: 0.1726, test loss 0.7227\n",
      "Epoch 16578: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16579: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16580: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16581: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16582: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16583: train loss: 0.1726, test loss 0.7226\n",
      "Epoch 16584: train loss: 0.1726, test loss 0.7225\n",
      "Epoch 16585: train loss: 0.1726, test loss 0.7225\n",
      "Epoch 16586: train loss: 0.1726, test loss 0.7225\n",
      "Epoch 16587: train loss: 0.1726, test loss 0.7225\n",
      "Epoch 16588: train loss: 0.1726, test loss 0.7225\n",
      "Epoch 16589: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16590: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16591: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16592: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16593: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16594: train loss: 0.1726, test loss 0.7224\n",
      "Epoch 16595: train loss: 0.1726, test loss 0.7223\n",
      "Epoch 16596: train loss: 0.1726, test loss 0.7223\n",
      "Epoch 16597: train loss: 0.1726, test loss 0.7223\n",
      "Epoch 16598: train loss: 0.1726, test loss 0.7223\n",
      "Epoch 16599: train loss: 0.1726, test loss 0.7223\n",
      "Epoch 16600: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16601: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16602: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16603: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16604: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16605: train loss: 0.1726, test loss 0.7222\n",
      "Epoch 16606: train loss: 0.1726, test loss 0.7221\n",
      "Epoch 16607: train loss: 0.1726, test loss 0.7221\n",
      "Epoch 16608: train loss: 0.1726, test loss 0.7221\n",
      "Epoch 16609: train loss: 0.1726, test loss 0.7221\n",
      "Epoch 16610: train loss: 0.1726, test loss 0.7221\n",
      "Epoch 16611: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16612: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16613: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16614: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16615: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16616: train loss: 0.1726, test loss 0.7220\n",
      "Epoch 16617: train loss: 0.1726, test loss 0.7219\n",
      "Epoch 16618: train loss: 0.1726, test loss 0.7219\n",
      "Epoch 16619: train loss: 0.1726, test loss 0.7219\n",
      "Epoch 16620: train loss: 0.1726, test loss 0.7219\n",
      "Epoch 16621: train loss: 0.1726, test loss 0.7219\n",
      "Epoch 16622: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16623: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16624: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16625: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16626: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16627: train loss: 0.1726, test loss 0.7218\n",
      "Epoch 16628: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16629: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16630: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16631: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16632: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16633: train loss: 0.1726, test loss 0.7217\n",
      "Epoch 16634: train loss: 0.1726, test loss 0.7216\n",
      "Epoch 16635: train loss: 0.1726, test loss 0.7216\n",
      "Epoch 16636: train loss: 0.1726, test loss 0.7216\n",
      "Epoch 16637: train loss: 0.1726, test loss 0.7216\n",
      "Epoch 16638: train loss: 0.1726, test loss 0.7216\n",
      "Epoch 16639: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16640: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16641: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16642: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16643: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16644: train loss: 0.1726, test loss 0.7215\n",
      "Epoch 16645: train loss: 0.1726, test loss 0.7214\n",
      "Epoch 16646: train loss: 0.1726, test loss 0.7214\n",
      "Epoch 16647: train loss: 0.1726, test loss 0.7214\n",
      "Epoch 16648: train loss: 0.1726, test loss 0.7214\n",
      "Epoch 16649: train loss: 0.1726, test loss 0.7214\n",
      "Epoch 16650: train loss: 0.1726, test loss 0.7213\n",
      "Epoch 16651: train loss: 0.1726, test loss 0.7213\n",
      "Epoch 16652: train loss: 0.1726, test loss 0.7213\n",
      "Epoch 16653: train loss: 0.1726, test loss 0.7213\n",
      "Epoch 16654: train loss: 0.1726, test loss 0.7213\n",
      "Epoch 16655: train loss: 0.1726, test loss 0.7212\n",
      "Epoch 16656: train loss: 0.1726, test loss 0.7212\n",
      "Epoch 16657: train loss: 0.1726, test loss 0.7212\n",
      "Epoch 16658: train loss: 0.1726, test loss 0.7212\n",
      "Epoch 16659: train loss: 0.1726, test loss 0.7212\n",
      "Epoch 16660: train loss: 0.1726, test loss 0.7211\n",
      "Epoch 16661: train loss: 0.1726, test loss 0.7211\n",
      "Epoch 16662: train loss: 0.1726, test loss 0.7211\n",
      "Epoch 16663: train loss: 0.1726, test loss 0.7211\n",
      "Epoch 16664: train loss: 0.1726, test loss 0.7211\n",
      "Epoch 16665: train loss: 0.1726, test loss 0.7210\n",
      "Epoch 16666: train loss: 0.1725, test loss 0.7210\n",
      "Epoch 16667: train loss: 0.1725, test loss 0.7210\n",
      "Epoch 16668: train loss: 0.1725, test loss 0.7210\n",
      "Epoch 16669: train loss: 0.1725, test loss 0.7210\n",
      "Epoch 16670: train loss: 0.1725, test loss 0.7210\n",
      "Epoch 16671: train loss: 0.1725, test loss 0.7209\n",
      "Epoch 16672: train loss: 0.1725, test loss 0.7209\n",
      "Epoch 16673: train loss: 0.1725, test loss 0.7209\n",
      "Epoch 16674: train loss: 0.1725, test loss 0.7209\n",
      "Epoch 16675: train loss: 0.1725, test loss 0.7209\n",
      "Epoch 16676: train loss: 0.1725, test loss 0.7208\n",
      "Epoch 16677: train loss: 0.1725, test loss 0.7208\n",
      "Epoch 16678: train loss: 0.1725, test loss 0.7208\n",
      "Epoch 16679: train loss: 0.1725, test loss 0.7208\n",
      "Epoch 16680: train loss: 0.1725, test loss 0.7208\n",
      "Epoch 16681: train loss: 0.1725, test loss 0.7207\n",
      "Epoch 16682: train loss: 0.1725, test loss 0.7207\n",
      "Epoch 16683: train loss: 0.1725, test loss 0.7207\n",
      "Epoch 16684: train loss: 0.1725, test loss 0.7207\n",
      "Epoch 16685: train loss: 0.1725, test loss 0.7207\n",
      "Epoch 16686: train loss: 0.1725, test loss 0.7206\n",
      "Epoch 16687: train loss: 0.1725, test loss 0.7206\n",
      "Epoch 16688: train loss: 0.1725, test loss 0.7206\n",
      "Epoch 16689: train loss: 0.1725, test loss 0.7206\n",
      "Epoch 16690: train loss: 0.1725, test loss 0.7206\n",
      "Epoch 16691: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16692: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16693: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16694: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16695: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16696: train loss: 0.1725, test loss 0.7205\n",
      "Epoch 16697: train loss: 0.1725, test loss 0.7204\n",
      "Epoch 16698: train loss: 0.1725, test loss 0.7204\n",
      "Epoch 16699: train loss: 0.1725, test loss 0.7204\n",
      "Epoch 16700: train loss: 0.1725, test loss 0.7204\n",
      "Epoch 16701: train loss: 0.1725, test loss 0.7204\n",
      "Epoch 16702: train loss: 0.1725, test loss 0.7203\n",
      "Epoch 16703: train loss: 0.1725, test loss 0.7203\n",
      "Epoch 16704: train loss: 0.1725, test loss 0.7203\n",
      "Epoch 16705: train loss: 0.1725, test loss 0.7203\n",
      "Epoch 16706: train loss: 0.1725, test loss 0.7203\n",
      "Epoch 16707: train loss: 0.1725, test loss 0.7202\n",
      "Epoch 16708: train loss: 0.1725, test loss 0.7202\n",
      "Epoch 16709: train loss: 0.1725, test loss 0.7202\n",
      "Epoch 16710: train loss: 0.1725, test loss 0.7202\n",
      "Epoch 16711: train loss: 0.1725, test loss 0.7202\n",
      "Epoch 16712: train loss: 0.1725, test loss 0.7201\n",
      "Epoch 16713: train loss: 0.1725, test loss 0.7201\n",
      "Epoch 16714: train loss: 0.1725, test loss 0.7201\n",
      "Epoch 16715: train loss: 0.1725, test loss 0.7201\n",
      "Epoch 16716: train loss: 0.1725, test loss 0.7201\n",
      "Epoch 16717: train loss: 0.1725, test loss 0.7200\n",
      "Epoch 16718: train loss: 0.1725, test loss 0.7200\n",
      "Epoch 16719: train loss: 0.1725, test loss 0.7200\n",
      "Epoch 16720: train loss: 0.1725, test loss 0.7200\n",
      "Epoch 16721: train loss: 0.1725, test loss 0.7200\n",
      "Epoch 16722: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16723: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16724: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16725: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16726: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16727: train loss: 0.1725, test loss 0.7199\n",
      "Epoch 16728: train loss: 0.1725, test loss 0.7198\n",
      "Epoch 16729: train loss: 0.1725, test loss 0.7198\n",
      "Epoch 16730: train loss: 0.1725, test loss 0.7198\n",
      "Epoch 16731: train loss: 0.1725, test loss 0.7198\n",
      "Epoch 16732: train loss: 0.1725, test loss 0.7198\n",
      "Epoch 16733: train loss: 0.1725, test loss 0.7197\n",
      "Epoch 16734: train loss: 0.1725, test loss 0.7197\n",
      "Epoch 16735: train loss: 0.1725, test loss 0.7197\n",
      "Epoch 16736: train loss: 0.1725, test loss 0.7197\n",
      "Epoch 16737: train loss: 0.1725, test loss 0.7197\n",
      "Epoch 16738: train loss: 0.1725, test loss 0.7196\n",
      "Epoch 16739: train loss: 0.1725, test loss 0.7196\n",
      "Epoch 16740: train loss: 0.1725, test loss 0.7196\n",
      "Epoch 16741: train loss: 0.1725, test loss 0.7196\n",
      "Epoch 16742: train loss: 0.1725, test loss 0.7196\n",
      "Epoch 16743: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16744: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16745: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16746: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16747: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16748: train loss: 0.1725, test loss 0.7195\n",
      "Epoch 16749: train loss: 0.1725, test loss 0.7194\n",
      "Epoch 16750: train loss: 0.1725, test loss 0.7194\n",
      "Epoch 16751: train loss: 0.1725, test loss 0.7194\n",
      "Epoch 16752: train loss: 0.1725, test loss 0.7194\n",
      "Epoch 16753: train loss: 0.1725, test loss 0.7194\n",
      "Epoch 16754: train loss: 0.1725, test loss 0.7193\n",
      "Epoch 16755: train loss: 0.1725, test loss 0.7193\n",
      "Epoch 16756: train loss: 0.1725, test loss 0.7193\n",
      "Epoch 16757: train loss: 0.1725, test loss 0.7193\n",
      "Epoch 16758: train loss: 0.1725, test loss 0.7193\n",
      "Epoch 16759: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16760: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16761: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16762: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16763: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16764: train loss: 0.1725, test loss 0.7192\n",
      "Epoch 16765: train loss: 0.1725, test loss 0.7191\n",
      "Epoch 16766: train loss: 0.1725, test loss 0.7191\n",
      "Epoch 16767: train loss: 0.1725, test loss 0.7191\n",
      "Epoch 16768: train loss: 0.1725, test loss 0.7191\n",
      "Epoch 16769: train loss: 0.1725, test loss 0.7191\n",
      "Epoch 16770: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16771: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16772: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16773: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16774: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16775: train loss: 0.1725, test loss 0.7190\n",
      "Epoch 16776: train loss: 0.1725, test loss 0.7189\n",
      "Epoch 16777: train loss: 0.1725, test loss 0.7189\n",
      "Epoch 16778: train loss: 0.1725, test loss 0.7189\n",
      "Epoch 16779: train loss: 0.1725, test loss 0.7189\n",
      "Epoch 16780: train loss: 0.1725, test loss 0.7189\n",
      "Epoch 16781: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16782: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16783: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16784: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16785: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16786: train loss: 0.1725, test loss 0.7188\n",
      "Epoch 16787: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16788: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16789: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16790: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16791: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16792: train loss: 0.1725, test loss 0.7187\n",
      "Epoch 16793: train loss: 0.1725, test loss 0.7186\n",
      "Epoch 16794: train loss: 0.1725, test loss 0.7186\n",
      "Epoch 16795: train loss: 0.1725, test loss 0.7186\n",
      "Epoch 16796: train loss: 0.1725, test loss 0.7186\n",
      "Epoch 16797: train loss: 0.1725, test loss 0.7186\n",
      "Epoch 16798: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16799: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16800: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16801: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16802: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16803: train loss: 0.1725, test loss 0.7185\n",
      "Epoch 16804: train loss: 0.1725, test loss 0.7184\n",
      "Epoch 16805: train loss: 0.1725, test loss 0.7184\n",
      "Epoch 16806: train loss: 0.1725, test loss 0.7184\n",
      "Epoch 16807: train loss: 0.1725, test loss 0.7184\n",
      "Epoch 16808: train loss: 0.1725, test loss 0.7184\n",
      "Epoch 16809: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16810: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16811: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16812: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16813: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16814: train loss: 0.1725, test loss 0.7183\n",
      "Epoch 16815: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16816: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16817: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16818: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16819: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16820: train loss: 0.1725, test loss 0.7182\n",
      "Epoch 16821: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16822: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16823: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16824: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16825: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16826: train loss: 0.1725, test loss 0.7181\n",
      "Epoch 16827: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16828: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16829: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16830: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16831: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16832: train loss: 0.1725, test loss 0.7180\n",
      "Epoch 16833: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16834: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16835: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16836: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16837: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16838: train loss: 0.1725, test loss 0.7179\n",
      "Epoch 16839: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16840: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16841: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16842: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16843: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16844: train loss: 0.1725, test loss 0.7178\n",
      "Epoch 16845: train loss: 0.1725, test loss 0.7177\n",
      "Epoch 16846: train loss: 0.1725, test loss 0.7177\n",
      "Epoch 16847: train loss: 0.1725, test loss 0.7177\n",
      "Epoch 16848: train loss: 0.1725, test loss 0.7177\n",
      "Epoch 16849: train loss: 0.1725, test loss 0.7177\n",
      "Epoch 16850: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16851: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16852: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16853: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16854: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16855: train loss: 0.1725, test loss 0.7176\n",
      "Epoch 16856: train loss: 0.1725, test loss 0.7175\n",
      "Epoch 16857: train loss: 0.1725, test loss 0.7175\n",
      "Epoch 16858: train loss: 0.1724, test loss 0.7175\n",
      "Epoch 16859: train loss: 0.1724, test loss 0.7175\n",
      "Epoch 16860: train loss: 0.1724, test loss 0.7175\n",
      "Epoch 16861: train loss: 0.1724, test loss 0.7175\n",
      "Epoch 16862: train loss: 0.1724, test loss 0.7174\n",
      "Epoch 16863: train loss: 0.1724, test loss 0.7174\n",
      "Epoch 16864: train loss: 0.1724, test loss 0.7174\n",
      "Epoch 16865: train loss: 0.1724, test loss 0.7174\n",
      "Epoch 16866: train loss: 0.1724, test loss 0.7174\n",
      "Epoch 16867: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16868: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16869: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16870: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16871: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16872: train loss: 0.1724, test loss 0.7173\n",
      "Epoch 16873: train loss: 0.1724, test loss 0.7172\n",
      "Epoch 16874: train loss: 0.1724, test loss 0.7172\n",
      "Epoch 16875: train loss: 0.1724, test loss 0.7172\n",
      "Epoch 16876: train loss: 0.1724, test loss 0.7172\n",
      "Epoch 16877: train loss: 0.1724, test loss 0.7172\n",
      "Epoch 16878: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16879: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16880: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16881: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16882: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16883: train loss: 0.1724, test loss 0.7171\n",
      "Epoch 16884: train loss: 0.1724, test loss 0.7170\n",
      "Epoch 16885: train loss: 0.1724, test loss 0.7170\n",
      "Epoch 16886: train loss: 0.1724, test loss 0.7170\n",
      "Epoch 16887: train loss: 0.1724, test loss 0.7170\n",
      "Epoch 16888: train loss: 0.1724, test loss 0.7170\n",
      "Epoch 16889: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16890: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16891: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16892: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16893: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16894: train loss: 0.1724, test loss 0.7169\n",
      "Epoch 16895: train loss: 0.1724, test loss 0.7168\n",
      "Epoch 16896: train loss: 0.1724, test loss 0.7168\n",
      "Epoch 16897: train loss: 0.1724, test loss 0.7168\n",
      "Epoch 16898: train loss: 0.1724, test loss 0.7168\n",
      "Epoch 16899: train loss: 0.1724, test loss 0.7168\n",
      "Epoch 16900: train loss: 0.1724, test loss 0.7167\n",
      "Epoch 16901: train loss: 0.1724, test loss 0.7167\n",
      "Epoch 16902: train loss: 0.1724, test loss 0.7167\n",
      "Epoch 16903: train loss: 0.1724, test loss 0.7167\n",
      "Epoch 16904: train loss: 0.1724, test loss 0.7166\n",
      "Epoch 16905: train loss: 0.1724, test loss 0.7166\n",
      "Epoch 16906: train loss: 0.1724, test loss 0.7166\n",
      "Epoch 16907: train loss: 0.1724, test loss 0.7166\n",
      "Epoch 16908: train loss: 0.1724, test loss 0.7166\n",
      "Epoch 16909: train loss: 0.1724, test loss 0.7165\n",
      "Epoch 16910: train loss: 0.1724, test loss 0.7165\n",
      "Epoch 16911: train loss: 0.1724, test loss 0.7165\n",
      "Epoch 16912: train loss: 0.1724, test loss 0.7165\n",
      "Epoch 16913: train loss: 0.1724, test loss 0.7165\n",
      "Epoch 16914: train loss: 0.1724, test loss 0.7164\n",
      "Epoch 16915: train loss: 0.1724, test loss 0.7164\n",
      "Epoch 16916: train loss: 0.1724, test loss 0.7164\n",
      "Epoch 16917: train loss: 0.1724, test loss 0.7164\n",
      "Epoch 16918: train loss: 0.1724, test loss 0.7164\n",
      "Epoch 16919: train loss: 0.1724, test loss 0.7163\n",
      "Epoch 16920: train loss: 0.1724, test loss 0.7163\n",
      "Epoch 16921: train loss: 0.1724, test loss 0.7163\n",
      "Epoch 16922: train loss: 0.1724, test loss 0.7163\n",
      "Epoch 16923: train loss: 0.1724, test loss 0.7163\n",
      "Epoch 16924: train loss: 0.1724, test loss 0.7162\n",
      "Epoch 16925: train loss: 0.1724, test loss 0.7162\n",
      "Epoch 16926: train loss: 0.1724, test loss 0.7162\n",
      "Epoch 16927: train loss: 0.1724, test loss 0.7162\n",
      "Epoch 16928: train loss: 0.1724, test loss 0.7162\n",
      "Epoch 16929: train loss: 0.1724, test loss 0.7161\n",
      "Epoch 16930: train loss: 0.1724, test loss 0.7161\n",
      "Epoch 16931: train loss: 0.1724, test loss 0.7161\n",
      "Epoch 16932: train loss: 0.1724, test loss 0.7161\n",
      "Epoch 16933: train loss: 0.1724, test loss 0.7161\n",
      "Epoch 16934: train loss: 0.1724, test loss 0.7160\n",
      "Epoch 16935: train loss: 0.1724, test loss 0.7160\n",
      "Epoch 16936: train loss: 0.1724, test loss 0.7160\n",
      "Epoch 16937: train loss: 0.1724, test loss 0.7160\n",
      "Epoch 16938: train loss: 0.1724, test loss 0.7160\n",
      "Epoch 16939: train loss: 0.1724, test loss 0.7159\n",
      "Epoch 16940: train loss: 0.1724, test loss 0.7159\n",
      "Epoch 16941: train loss: 0.1724, test loss 0.7159\n",
      "Epoch 16942: train loss: 0.1724, test loss 0.7159\n",
      "Epoch 16943: train loss: 0.1724, test loss 0.7159\n",
      "Epoch 16944: train loss: 0.1724, test loss 0.7158\n",
      "Epoch 16945: train loss: 0.1724, test loss 0.7158\n",
      "Epoch 16946: train loss: 0.1724, test loss 0.7158\n",
      "Epoch 16947: train loss: 0.1724, test loss 0.7158\n",
      "Epoch 16948: train loss: 0.1724, test loss 0.7158\n",
      "Epoch 16949: train loss: 0.1724, test loss 0.7157\n",
      "Epoch 16950: train loss: 0.1724, test loss 0.7157\n",
      "Epoch 16951: train loss: 0.1724, test loss 0.7157\n",
      "Epoch 16952: train loss: 0.1724, test loss 0.7157\n",
      "Epoch 16953: train loss: 0.1724, test loss 0.7157\n",
      "Epoch 16954: train loss: 0.1724, test loss 0.7156\n",
      "Epoch 16955: train loss: 0.1724, test loss 0.7156\n",
      "Epoch 16956: train loss: 0.1724, test loss 0.7156\n",
      "Epoch 16957: train loss: 0.1724, test loss 0.7156\n",
      "Epoch 16958: train loss: 0.1724, test loss 0.7156\n",
      "Epoch 16959: train loss: 0.1724, test loss 0.7155\n",
      "Epoch 16960: train loss: 0.1724, test loss 0.7155\n",
      "Epoch 16961: train loss: 0.1724, test loss 0.7155\n",
      "Epoch 16962: train loss: 0.1724, test loss 0.7155\n",
      "Epoch 16963: train loss: 0.1724, test loss 0.7155\n",
      "Epoch 16964: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16965: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16966: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16967: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16968: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16969: train loss: 0.1724, test loss 0.7154\n",
      "Epoch 16970: train loss: 0.1724, test loss 0.7153\n",
      "Epoch 16971: train loss: 0.1724, test loss 0.7153\n",
      "Epoch 16972: train loss: 0.1724, test loss 0.7153\n",
      "Epoch 16973: train loss: 0.1724, test loss 0.7153\n",
      "Epoch 16974: train loss: 0.1724, test loss 0.7153\n",
      "Epoch 16975: train loss: 0.1724, test loss 0.7152\n",
      "Epoch 16976: train loss: 0.1724, test loss 0.7152\n",
      "Epoch 16977: train loss: 0.1724, test loss 0.7152\n",
      "Epoch 16978: train loss: 0.1724, test loss 0.7152\n",
      "Epoch 16979: train loss: 0.1724, test loss 0.7152\n",
      "Epoch 16980: train loss: 0.1724, test loss 0.7151\n",
      "Epoch 16981: train loss: 0.1724, test loss 0.7151\n",
      "Epoch 16982: train loss: 0.1724, test loss 0.7151\n",
      "Epoch 16983: train loss: 0.1724, test loss 0.7151\n",
      "Epoch 16984: train loss: 0.1724, test loss 0.7151\n",
      "Epoch 16985: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16986: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16987: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16988: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16989: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16990: train loss: 0.1724, test loss 0.7150\n",
      "Epoch 16991: train loss: 0.1724, test loss 0.7149\n",
      "Epoch 16992: train loss: 0.1724, test loss 0.7149\n",
      "Epoch 16993: train loss: 0.1724, test loss 0.7149\n",
      "Epoch 16994: train loss: 0.1724, test loss 0.7149\n",
      "Epoch 16995: train loss: 0.1724, test loss 0.7149\n",
      "Epoch 16996: train loss: 0.1724, test loss 0.7148\n",
      "Epoch 16997: train loss: 0.1724, test loss 0.7148\n",
      "Epoch 16998: train loss: 0.1724, test loss 0.7148\n",
      "Epoch 16999: train loss: 0.1724, test loss 0.7148\n",
      "Epoch 17000: train loss: 0.1724, test loss 0.7148\n",
      "Epoch 17001: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17002: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17003: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17004: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17005: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17006: train loss: 0.1724, test loss 0.7147\n",
      "Epoch 17007: train loss: 0.1724, test loss 0.7146\n",
      "Epoch 17008: train loss: 0.1724, test loss 0.7146\n",
      "Epoch 17009: train loss: 0.1724, test loss 0.7146\n",
      "Epoch 17010: train loss: 0.1724, test loss 0.7146\n",
      "Epoch 17011: train loss: 0.1724, test loss 0.7146\n",
      "Epoch 17012: train loss: 0.1724, test loss 0.7145\n",
      "Epoch 17013: train loss: 0.1724, test loss 0.7145\n",
      "Epoch 17014: train loss: 0.1724, test loss 0.7145\n",
      "Epoch 17015: train loss: 0.1724, test loss 0.7145\n",
      "Epoch 17016: train loss: 0.1724, test loss 0.7145\n",
      "Epoch 17017: train loss: 0.1724, test loss 0.7144\n",
      "Epoch 17018: train loss: 0.1724, test loss 0.7144\n",
      "Epoch 17019: train loss: 0.1724, test loss 0.7144\n",
      "Epoch 17020: train loss: 0.1724, test loss 0.7144\n",
      "Epoch 17021: train loss: 0.1724, test loss 0.7143\n",
      "Epoch 17022: train loss: 0.1724, test loss 0.7143\n",
      "Epoch 17023: train loss: 0.1724, test loss 0.7143\n",
      "Epoch 17024: train loss: 0.1724, test loss 0.7143\n",
      "Epoch 17025: train loss: 0.1724, test loss 0.7143\n",
      "Epoch 17026: train loss: 0.1724, test loss 0.7142\n",
      "Epoch 17027: train loss: 0.1724, test loss 0.7142\n",
      "Epoch 17028: train loss: 0.1724, test loss 0.7142\n",
      "Epoch 17029: train loss: 0.1724, test loss 0.7142\n",
      "Epoch 17030: train loss: 0.1724, test loss 0.7142\n",
      "Epoch 17031: train loss: 0.1724, test loss 0.7141\n",
      "Epoch 17032: train loss: 0.1724, test loss 0.7141\n",
      "Epoch 17033: train loss: 0.1724, test loss 0.7141\n",
      "Epoch 17034: train loss: 0.1724, test loss 0.7141\n",
      "Epoch 17035: train loss: 0.1724, test loss 0.7141\n",
      "Epoch 17036: train loss: 0.1724, test loss 0.7140\n",
      "Epoch 17037: train loss: 0.1724, test loss 0.7140\n",
      "Epoch 17038: train loss: 0.1724, test loss 0.7140\n",
      "Epoch 17039: train loss: 0.1724, test loss 0.7140\n",
      "Epoch 17040: train loss: 0.1724, test loss 0.7139\n",
      "Epoch 17041: train loss: 0.1724, test loss 0.7139\n",
      "Epoch 17042: train loss: 0.1724, test loss 0.7139\n",
      "Epoch 17043: train loss: 0.1724, test loss 0.7139\n",
      "Epoch 17044: train loss: 0.1724, test loss 0.7139\n",
      "Epoch 17045: train loss: 0.1724, test loss 0.7138\n",
      "Epoch 17046: train loss: 0.1724, test loss 0.7138\n",
      "Epoch 17047: train loss: 0.1724, test loss 0.7138\n",
      "Epoch 17048: train loss: 0.1724, test loss 0.7138\n",
      "Epoch 17049: train loss: 0.1724, test loss 0.7138\n",
      "Epoch 17050: train loss: 0.1724, test loss 0.7137\n",
      "Epoch 17051: train loss: 0.1724, test loss 0.7137\n",
      "Epoch 17052: train loss: 0.1724, test loss 0.7137\n",
      "Epoch 17053: train loss: 0.1724, test loss 0.7137\n",
      "Epoch 17054: train loss: 0.1724, test loss 0.7137\n",
      "Epoch 17055: train loss: 0.1724, test loss 0.7136\n",
      "Epoch 17056: train loss: 0.1723, test loss 0.7136\n",
      "Epoch 17057: train loss: 0.1723, test loss 0.7136\n",
      "Epoch 17058: train loss: 0.1723, test loss 0.7136\n",
      "Epoch 17059: train loss: 0.1723, test loss 0.7136\n",
      "Epoch 17060: train loss: 0.1723, test loss 0.7135\n",
      "Epoch 17061: train loss: 0.1723, test loss 0.7135\n",
      "Epoch 17062: train loss: 0.1723, test loss 0.7135\n",
      "Epoch 17063: train loss: 0.1723, test loss 0.7135\n",
      "Epoch 17064: train loss: 0.1723, test loss 0.7135\n",
      "Epoch 17065: train loss: 0.1723, test loss 0.7134\n",
      "Epoch 17066: train loss: 0.1723, test loss 0.7134\n",
      "Epoch 17067: train loss: 0.1723, test loss 0.7134\n",
      "Epoch 17068: train loss: 0.1723, test loss 0.7134\n",
      "Epoch 17069: train loss: 0.1723, test loss 0.7133\n",
      "Epoch 17070: train loss: 0.1723, test loss 0.7133\n",
      "Epoch 17071: train loss: 0.1723, test loss 0.7133\n",
      "Epoch 17072: train loss: 0.1723, test loss 0.7133\n",
      "Epoch 17073: train loss: 0.1723, test loss 0.7133\n",
      "Epoch 17074: train loss: 0.1723, test loss 0.7132\n",
      "Epoch 17075: train loss: 0.1723, test loss 0.7132\n",
      "Epoch 17076: train loss: 0.1723, test loss 0.7132\n",
      "Epoch 17077: train loss: 0.1723, test loss 0.7132\n",
      "Epoch 17078: train loss: 0.1723, test loss 0.7132\n",
      "Epoch 17079: train loss: 0.1723, test loss 0.7131\n",
      "Epoch 17080: train loss: 0.1723, test loss 0.7131\n",
      "Epoch 17081: train loss: 0.1723, test loss 0.7131\n",
      "Epoch 17082: train loss: 0.1723, test loss 0.7131\n",
      "Epoch 17083: train loss: 0.1723, test loss 0.7131\n",
      "Epoch 17084: train loss: 0.1723, test loss 0.7130\n",
      "Epoch 17085: train loss: 0.1723, test loss 0.7130\n",
      "Epoch 17086: train loss: 0.1723, test loss 0.7130\n",
      "Epoch 17087: train loss: 0.1723, test loss 0.7130\n",
      "Epoch 17088: train loss: 0.1723, test loss 0.7130\n",
      "Epoch 17089: train loss: 0.1723, test loss 0.7129\n",
      "Epoch 17090: train loss: 0.1723, test loss 0.7129\n",
      "Epoch 17091: train loss: 0.1723, test loss 0.7129\n",
      "Epoch 17092: train loss: 0.1723, test loss 0.7129\n",
      "Epoch 17093: train loss: 0.1723, test loss 0.7129\n",
      "Epoch 17094: train loss: 0.1723, test loss 0.7128\n",
      "Epoch 17095: train loss: 0.1723, test loss 0.7128\n",
      "Epoch 17096: train loss: 0.1723, test loss 0.7128\n",
      "Epoch 17097: train loss: 0.1723, test loss 0.7128\n",
      "Epoch 17098: train loss: 0.1723, test loss 0.7128\n",
      "Epoch 17099: train loss: 0.1723, test loss 0.7127\n",
      "Epoch 17100: train loss: 0.1723, test loss 0.7127\n",
      "Epoch 17101: train loss: 0.1723, test loss 0.7127\n",
      "Epoch 17102: train loss: 0.1723, test loss 0.7127\n",
      "Epoch 17103: train loss: 0.1723, test loss 0.7127\n",
      "Epoch 17104: train loss: 0.1723, test loss 0.7126\n",
      "Epoch 17105: train loss: 0.1723, test loss 0.7126\n",
      "Epoch 17106: train loss: 0.1723, test loss 0.7126\n",
      "Epoch 17107: train loss: 0.1723, test loss 0.7126\n",
      "Epoch 17108: train loss: 0.1723, test loss 0.7126\n",
      "Epoch 17109: train loss: 0.1723, test loss 0.7125\n",
      "Epoch 17110: train loss: 0.1723, test loss 0.7125\n",
      "Epoch 17111: train loss: 0.1723, test loss 0.7125\n",
      "Epoch 17112: train loss: 0.1723, test loss 0.7125\n",
      "Epoch 17113: train loss: 0.1723, test loss 0.7125\n",
      "Epoch 17114: train loss: 0.1723, test loss 0.7124\n",
      "Epoch 17115: train loss: 0.1723, test loss 0.7124\n",
      "Epoch 17116: train loss: 0.1723, test loss 0.7124\n",
      "Epoch 17117: train loss: 0.1723, test loss 0.7124\n",
      "Epoch 17118: train loss: 0.1723, test loss 0.7124\n",
      "Epoch 17119: train loss: 0.1723, test loss 0.7123\n",
      "Epoch 17120: train loss: 0.1723, test loss 0.7123\n",
      "Epoch 17121: train loss: 0.1723, test loss 0.7123\n",
      "Epoch 17122: train loss: 0.1723, test loss 0.7123\n",
      "Epoch 17123: train loss: 0.1723, test loss 0.7123\n",
      "Epoch 17124: train loss: 0.1723, test loss 0.7122\n",
      "Epoch 17125: train loss: 0.1723, test loss 0.7122\n",
      "Epoch 17126: train loss: 0.1723, test loss 0.7122\n",
      "Epoch 17127: train loss: 0.1723, test loss 0.7122\n",
      "Epoch 17128: train loss: 0.1723, test loss 0.7122\n",
      "Epoch 17129: train loss: 0.1723, test loss 0.7121\n",
      "Epoch 17130: train loss: 0.1723, test loss 0.7121\n",
      "Epoch 17131: train loss: 0.1723, test loss 0.7121\n",
      "Epoch 17132: train loss: 0.1723, test loss 0.7121\n",
      "Epoch 17133: train loss: 0.1723, test loss 0.7121\n",
      "Epoch 17134: train loss: 0.1723, test loss 0.7120\n",
      "Epoch 17135: train loss: 0.1723, test loss 0.7120\n",
      "Epoch 17136: train loss: 0.1723, test loss 0.7120\n",
      "Epoch 17137: train loss: 0.1723, test loss 0.7120\n",
      "Epoch 17138: train loss: 0.1723, test loss 0.7119\n",
      "Epoch 17139: train loss: 0.1723, test loss 0.7119\n",
      "Epoch 17140: train loss: 0.1723, test loss 0.7119\n",
      "Epoch 17141: train loss: 0.1723, test loss 0.7119\n",
      "Epoch 17142: train loss: 0.1723, test loss 0.7119\n",
      "Epoch 17143: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17144: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17145: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17146: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17147: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17148: train loss: 0.1723, test loss 0.7118\n",
      "Epoch 17149: train loss: 0.1723, test loss 0.7117\n",
      "Epoch 17150: train loss: 0.1723, test loss 0.7117\n",
      "Epoch 17151: train loss: 0.1723, test loss 0.7117\n",
      "Epoch 17152: train loss: 0.1723, test loss 0.7117\n",
      "Epoch 17153: train loss: 0.1723, test loss 0.7117\n",
      "Epoch 17154: train loss: 0.1723, test loss 0.7116\n",
      "Epoch 17155: train loss: 0.1723, test loss 0.7116\n",
      "Epoch 17156: train loss: 0.1723, test loss 0.7116\n",
      "Epoch 17157: train loss: 0.1723, test loss 0.7116\n",
      "Epoch 17158: train loss: 0.1723, test loss 0.7116\n",
      "Epoch 17159: train loss: 0.1723, test loss 0.7115\n",
      "Epoch 17160: train loss: 0.1723, test loss 0.7115\n",
      "Epoch 17161: train loss: 0.1723, test loss 0.7115\n",
      "Epoch 17162: train loss: 0.1723, test loss 0.7115\n",
      "Epoch 17163: train loss: 0.1723, test loss 0.7115\n",
      "Epoch 17164: train loss: 0.1723, test loss 0.7114\n",
      "Epoch 17165: train loss: 0.1723, test loss 0.7114\n",
      "Epoch 17166: train loss: 0.1723, test loss 0.7114\n",
      "Epoch 17167: train loss: 0.1723, test loss 0.7114\n",
      "Epoch 17168: train loss: 0.1723, test loss 0.7114\n",
      "Epoch 17169: train loss: 0.1723, test loss 0.7113\n",
      "Epoch 17170: train loss: 0.1723, test loss 0.7113\n",
      "Epoch 17171: train loss: 0.1723, test loss 0.7113\n",
      "Epoch 17172: train loss: 0.1723, test loss 0.7113\n",
      "Epoch 17173: train loss: 0.1723, test loss 0.7113\n",
      "Epoch 17174: train loss: 0.1723, test loss 0.7112\n",
      "Epoch 17175: train loss: 0.1723, test loss 0.7112\n",
      "Epoch 17176: train loss: 0.1723, test loss 0.7112\n",
      "Epoch 17177: train loss: 0.1723, test loss 0.7112\n",
      "Epoch 17178: train loss: 0.1723, test loss 0.7112\n",
      "Epoch 17179: train loss: 0.1723, test loss 0.7111\n",
      "Epoch 17180: train loss: 0.1723, test loss 0.7111\n",
      "Epoch 17181: train loss: 0.1723, test loss 0.7111\n",
      "Epoch 17182: train loss: 0.1723, test loss 0.7111\n",
      "Epoch 17183: train loss: 0.1723, test loss 0.7111\n",
      "Epoch 17184: train loss: 0.1723, test loss 0.7110\n",
      "Epoch 17185: train loss: 0.1723, test loss 0.7110\n",
      "Epoch 17186: train loss: 0.1723, test loss 0.7110\n",
      "Epoch 17187: train loss: 0.1723, test loss 0.7110\n",
      "Epoch 17188: train loss: 0.1723, test loss 0.7110\n",
      "Epoch 17189: train loss: 0.1723, test loss 0.7109\n",
      "Epoch 17190: train loss: 0.1723, test loss 0.7109\n",
      "Epoch 17191: train loss: 0.1723, test loss 0.7109\n",
      "Epoch 17192: train loss: 0.1723, test loss 0.7109\n",
      "Epoch 17193: train loss: 0.1723, test loss 0.7109\n",
      "Epoch 17194: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17195: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17196: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17197: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17198: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17199: train loss: 0.1723, test loss 0.7108\n",
      "Epoch 17200: train loss: 0.1723, test loss 0.7107\n",
      "Epoch 17201: train loss: 0.1723, test loss 0.7107\n",
      "Epoch 17202: train loss: 0.1723, test loss 0.7107\n",
      "Epoch 17203: train loss: 0.1723, test loss 0.7107\n",
      "Epoch 17204: train loss: 0.1723, test loss 0.7107\n",
      "Epoch 17205: train loss: 0.1723, test loss 0.7106\n",
      "Epoch 17206: train loss: 0.1723, test loss 0.7106\n",
      "Epoch 17207: train loss: 0.1723, test loss 0.7106\n",
      "Epoch 17208: train loss: 0.1723, test loss 0.7106\n",
      "Epoch 17209: train loss: 0.1723, test loss 0.7106\n",
      "Epoch 17210: train loss: 0.1723, test loss 0.7105\n",
      "Epoch 17211: train loss: 0.1723, test loss 0.7105\n",
      "Epoch 17212: train loss: 0.1723, test loss 0.7105\n",
      "Epoch 17213: train loss: 0.1723, test loss 0.7105\n",
      "Epoch 17214: train loss: 0.1723, test loss 0.7105\n",
      "Epoch 17215: train loss: 0.1723, test loss 0.7104\n",
      "Epoch 17216: train loss: 0.1723, test loss 0.7104\n",
      "Epoch 17217: train loss: 0.1723, test loss 0.7104\n",
      "Epoch 17218: train loss: 0.1723, test loss 0.7104\n",
      "Epoch 17219: train loss: 0.1723, test loss 0.7104\n",
      "Epoch 17220: train loss: 0.1723, test loss 0.7103\n",
      "Epoch 17221: train loss: 0.1723, test loss 0.7103\n",
      "Epoch 17222: train loss: 0.1723, test loss 0.7103\n",
      "Epoch 17223: train loss: 0.1723, test loss 0.7103\n",
      "Epoch 17224: train loss: 0.1723, test loss 0.7103\n",
      "Epoch 17225: train loss: 0.1723, test loss 0.7102\n",
      "Epoch 17226: train loss: 0.1723, test loss 0.7102\n",
      "Epoch 17227: train loss: 0.1723, test loss 0.7102\n",
      "Epoch 17228: train loss: 0.1723, test loss 0.7102\n",
      "Epoch 17229: train loss: 0.1723, test loss 0.7102\n",
      "Epoch 17230: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17231: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17232: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17233: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17234: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17235: train loss: 0.1723, test loss 0.7101\n",
      "Epoch 17236: train loss: 0.1723, test loss 0.7100\n",
      "Epoch 17237: train loss: 0.1723, test loss 0.7100\n",
      "Epoch 17238: train loss: 0.1723, test loss 0.7100\n",
      "Epoch 17239: train loss: 0.1723, test loss 0.7100\n",
      "Epoch 17240: train loss: 0.1723, test loss 0.7100\n",
      "Epoch 17241: train loss: 0.1723, test loss 0.7099\n",
      "Epoch 17242: train loss: 0.1723, test loss 0.7099\n",
      "Epoch 17243: train loss: 0.1723, test loss 0.7099\n",
      "Epoch 17244: train loss: 0.1723, test loss 0.7099\n",
      "Epoch 17245: train loss: 0.1723, test loss 0.7099\n",
      "Epoch 17246: train loss: 0.1723, test loss 0.7098\n",
      "Epoch 17247: train loss: 0.1723, test loss 0.7098\n",
      "Epoch 17248: train loss: 0.1723, test loss 0.7098\n",
      "Epoch 17249: train loss: 0.1723, test loss 0.7098\n",
      "Epoch 17250: train loss: 0.1723, test loss 0.7098\n",
      "Epoch 17251: train loss: 0.1723, test loss 0.7097\n",
      "Epoch 17252: train loss: 0.1723, test loss 0.7097\n",
      "Epoch 17253: train loss: 0.1723, test loss 0.7097\n",
      "Epoch 17254: train loss: 0.1723, test loss 0.7097\n",
      "Epoch 17255: train loss: 0.1723, test loss 0.7097\n",
      "Epoch 17256: train loss: 0.1723, test loss 0.7096\n",
      "Epoch 17257: train loss: 0.1723, test loss 0.7096\n",
      "Epoch 17258: train loss: 0.1723, test loss 0.7096\n",
      "Epoch 17259: train loss: 0.1723, test loss 0.7096\n",
      "Epoch 17260: train loss: 0.1723, test loss 0.7096\n",
      "Epoch 17261: train loss: 0.1722, test loss 0.7096\n",
      "Epoch 17262: train loss: 0.1722, test loss 0.7095\n",
      "Epoch 17263: train loss: 0.1722, test loss 0.7095\n",
      "Epoch 17264: train loss: 0.1722, test loss 0.7095\n",
      "Epoch 17265: train loss: 0.1722, test loss 0.7095\n",
      "Epoch 17266: train loss: 0.1722, test loss 0.7095\n",
      "Epoch 17267: train loss: 0.1722, test loss 0.7094\n",
      "Epoch 17268: train loss: 0.1722, test loss 0.7094\n",
      "Epoch 17269: train loss: 0.1722, test loss 0.7094\n",
      "Epoch 17270: train loss: 0.1722, test loss 0.7094\n",
      "Epoch 17271: train loss: 0.1722, test loss 0.7094\n",
      "Epoch 17272: train loss: 0.1722, test loss 0.7093\n",
      "Epoch 17273: train loss: 0.1722, test loss 0.7093\n",
      "Epoch 17274: train loss: 0.1722, test loss 0.7093\n",
      "Epoch 17275: train loss: 0.1722, test loss 0.7093\n",
      "Epoch 17276: train loss: 0.1722, test loss 0.7093\n",
      "Epoch 17277: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17278: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17279: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17280: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17281: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17282: train loss: 0.1722, test loss 0.7092\n",
      "Epoch 17283: train loss: 0.1722, test loss 0.7091\n",
      "Epoch 17284: train loss: 0.1722, test loss 0.7091\n",
      "Epoch 17285: train loss: 0.1722, test loss 0.7091\n",
      "Epoch 17286: train loss: 0.1722, test loss 0.7091\n",
      "Epoch 17287: train loss: 0.1722, test loss 0.7091\n",
      "Epoch 17288: train loss: 0.1722, test loss 0.7090\n",
      "Epoch 17289: train loss: 0.1722, test loss 0.7090\n",
      "Epoch 17290: train loss: 0.1722, test loss 0.7090\n",
      "Epoch 17291: train loss: 0.1722, test loss 0.7090\n",
      "Epoch 17292: train loss: 0.1722, test loss 0.7090\n",
      "Epoch 17293: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17294: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17295: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17296: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17297: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17298: train loss: 0.1722, test loss 0.7089\n",
      "Epoch 17299: train loss: 0.1722, test loss 0.7088\n",
      "Epoch 17300: train loss: 0.1722, test loss 0.7088\n",
      "Epoch 17301: train loss: 0.1722, test loss 0.7088\n",
      "Epoch 17302: train loss: 0.1722, test loss 0.7088\n",
      "Epoch 17303: train loss: 0.1722, test loss 0.7088\n",
      "Epoch 17304: train loss: 0.1722, test loss 0.7087\n",
      "Epoch 17305: train loss: 0.1722, test loss 0.7087\n",
      "Epoch 17306: train loss: 0.1722, test loss 0.7087\n",
      "Epoch 17307: train loss: 0.1722, test loss 0.7087\n",
      "Epoch 17308: train loss: 0.1722, test loss 0.7087\n",
      "Epoch 17309: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17310: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17311: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17312: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17313: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17314: train loss: 0.1722, test loss 0.7086\n",
      "Epoch 17315: train loss: 0.1722, test loss 0.7085\n",
      "Epoch 17316: train loss: 0.1722, test loss 0.7085\n",
      "Epoch 17317: train loss: 0.1722, test loss 0.7085\n",
      "Epoch 17318: train loss: 0.1722, test loss 0.7085\n",
      "Epoch 17319: train loss: 0.1722, test loss 0.7085\n",
      "Epoch 17320: train loss: 0.1722, test loss 0.7084\n",
      "Epoch 17321: train loss: 0.1722, test loss 0.7084\n",
      "Epoch 17322: train loss: 0.1722, test loss 0.7084\n",
      "Epoch 17323: train loss: 0.1722, test loss 0.7084\n",
      "Epoch 17324: train loss: 0.1722, test loss 0.7084\n",
      "Epoch 17325: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17326: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17327: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17328: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17329: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17330: train loss: 0.1722, test loss 0.7083\n",
      "Epoch 17331: train loss: 0.1722, test loss 0.7082\n",
      "Epoch 17332: train loss: 0.1722, test loss 0.7082\n",
      "Epoch 17333: train loss: 0.1722, test loss 0.7082\n",
      "Epoch 17334: train loss: 0.1722, test loss 0.7082\n",
      "Epoch 17335: train loss: 0.1722, test loss 0.7082\n",
      "Epoch 17336: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17337: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17338: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17339: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17340: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17341: train loss: 0.1722, test loss 0.7081\n",
      "Epoch 17342: train loss: 0.1722, test loss 0.7080\n",
      "Epoch 17343: train loss: 0.1722, test loss 0.7080\n",
      "Epoch 17344: train loss: 0.1722, test loss 0.7080\n",
      "Epoch 17345: train loss: 0.1722, test loss 0.7080\n",
      "Epoch 17346: train loss: 0.1722, test loss 0.7080\n",
      "Epoch 17347: train loss: 0.1722, test loss 0.7079\n",
      "Epoch 17348: train loss: 0.1722, test loss 0.7079\n",
      "Epoch 17349: train loss: 0.1722, test loss 0.7079\n",
      "Epoch 17350: train loss: 0.1722, test loss 0.7079\n",
      "Epoch 17351: train loss: 0.1722, test loss 0.7079\n",
      "Epoch 17352: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17353: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17354: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17355: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17356: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17357: train loss: 0.1722, test loss 0.7078\n",
      "Epoch 17358: train loss: 0.1722, test loss 0.7077\n",
      "Epoch 17359: train loss: 0.1722, test loss 0.7077\n",
      "Epoch 17360: train loss: 0.1722, test loss 0.7077\n",
      "Epoch 17361: train loss: 0.1722, test loss 0.7077\n",
      "Epoch 17362: train loss: 0.1722, test loss 0.7077\n",
      "Epoch 17363: train loss: 0.1722, test loss 0.7076\n",
      "Epoch 17364: train loss: 0.1722, test loss 0.7076\n",
      "Epoch 17365: train loss: 0.1722, test loss 0.7076\n",
      "Epoch 17366: train loss: 0.1722, test loss 0.7076\n",
      "Epoch 17367: train loss: 0.1722, test loss 0.7076\n",
      "Epoch 17368: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17369: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17370: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17371: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17372: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17373: train loss: 0.1722, test loss 0.7075\n",
      "Epoch 17374: train loss: 0.1722, test loss 0.7074\n",
      "Epoch 17375: train loss: 0.1722, test loss 0.7074\n",
      "Epoch 17376: train loss: 0.1722, test loss 0.7074\n",
      "Epoch 17377: train loss: 0.1722, test loss 0.7074\n",
      "Epoch 17378: train loss: 0.1722, test loss 0.7074\n",
      "Epoch 17379: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17380: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17381: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17382: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17383: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17384: train loss: 0.1722, test loss 0.7073\n",
      "Epoch 17385: train loss: 0.1722, test loss 0.7072\n",
      "Epoch 17386: train loss: 0.1722, test loss 0.7072\n",
      "Epoch 17387: train loss: 0.1722, test loss 0.7072\n",
      "Epoch 17388: train loss: 0.1722, test loss 0.7072\n",
      "Epoch 17389: train loss: 0.1722, test loss 0.7072\n",
      "Epoch 17390: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17391: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17392: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17393: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17394: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17395: train loss: 0.1722, test loss 0.7071\n",
      "Epoch 17396: train loss: 0.1722, test loss 0.7070\n",
      "Epoch 17397: train loss: 0.1722, test loss 0.7070\n",
      "Epoch 17398: train loss: 0.1722, test loss 0.7070\n",
      "Epoch 17399: train loss: 0.1722, test loss 0.7070\n",
      "Epoch 17400: train loss: 0.1722, test loss 0.7070\n",
      "Epoch 17401: train loss: 0.1722, test loss 0.7069\n",
      "Epoch 17402: train loss: 0.1722, test loss 0.7069\n",
      "Epoch 17403: train loss: 0.1722, test loss 0.7069\n",
      "Epoch 17404: train loss: 0.1722, test loss 0.7069\n",
      "Epoch 17405: train loss: 0.1722, test loss 0.7069\n",
      "Epoch 17406: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17407: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17408: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17409: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17410: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17411: train loss: 0.1722, test loss 0.7068\n",
      "Epoch 17412: train loss: 0.1722, test loss 0.7067\n",
      "Epoch 17413: train loss: 0.1722, test loss 0.7067\n",
      "Epoch 17414: train loss: 0.1722, test loss 0.7067\n",
      "Epoch 17415: train loss: 0.1722, test loss 0.7067\n",
      "Epoch 17416: train loss: 0.1722, test loss 0.7067\n",
      "Epoch 17417: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17418: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17419: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17420: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17421: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17422: train loss: 0.1722, test loss 0.7066\n",
      "Epoch 17423: train loss: 0.1722, test loss 0.7065\n",
      "Epoch 17424: train loss: 0.1722, test loss 0.7065\n",
      "Epoch 17425: train loss: 0.1722, test loss 0.7065\n",
      "Epoch 17426: train loss: 0.1722, test loss 0.7065\n",
      "Epoch 17427: train loss: 0.1722, test loss 0.7065\n",
      "Epoch 17428: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17429: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17430: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17431: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17432: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17433: train loss: 0.1722, test loss 0.7064\n",
      "Epoch 17434: train loss: 0.1722, test loss 0.7063\n",
      "Epoch 17435: train loss: 0.1722, test loss 0.7063\n",
      "Epoch 17436: train loss: 0.1722, test loss 0.7063\n",
      "Epoch 17437: train loss: 0.1722, test loss 0.7063\n",
      "Epoch 17438: train loss: 0.1722, test loss 0.7063\n",
      "Epoch 17439: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17440: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17441: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17442: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17443: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17444: train loss: 0.1722, test loss 0.7062\n",
      "Epoch 17445: train loss: 0.1722, test loss 0.7061\n",
      "Epoch 17446: train loss: 0.1722, test loss 0.7061\n",
      "Epoch 17447: train loss: 0.1722, test loss 0.7061\n",
      "Epoch 17448: train loss: 0.1722, test loss 0.7061\n",
      "Epoch 17449: train loss: 0.1722, test loss 0.7061\n",
      "Epoch 17450: train loss: 0.1722, test loss 0.7060\n",
      "Epoch 17451: train loss: 0.1722, test loss 0.7060\n",
      "Epoch 17452: train loss: 0.1722, test loss 0.7060\n",
      "Epoch 17453: train loss: 0.1722, test loss 0.7060\n",
      "Epoch 17454: train loss: 0.1722, test loss 0.7060\n",
      "Epoch 17455: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17456: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17457: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17458: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17459: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17460: train loss: 0.1722, test loss 0.7059\n",
      "Epoch 17461: train loss: 0.1722, test loss 0.7058\n",
      "Epoch 17462: train loss: 0.1722, test loss 0.7058\n",
      "Epoch 17463: train loss: 0.1722, test loss 0.7058\n",
      "Epoch 17464: train loss: 0.1722, test loss 0.7058\n",
      "Epoch 17465: train loss: 0.1722, test loss 0.7058\n",
      "Epoch 17466: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17467: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17468: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17469: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17470: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17471: train loss: 0.1722, test loss 0.7057\n",
      "Epoch 17472: train loss: 0.1721, test loss 0.7056\n",
      "Epoch 17473: train loss: 0.1721, test loss 0.7056\n",
      "Epoch 17474: train loss: 0.1721, test loss 0.7056\n",
      "Epoch 17475: train loss: 0.1721, test loss 0.7056\n",
      "Epoch 17476: train loss: 0.1721, test loss 0.7056\n",
      "Epoch 17477: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17478: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17479: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17480: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17481: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17482: train loss: 0.1721, test loss 0.7055\n",
      "Epoch 17483: train loss: 0.1721, test loss 0.7054\n",
      "Epoch 17484: train loss: 0.1721, test loss 0.7054\n",
      "Epoch 17485: train loss: 0.1721, test loss 0.7054\n",
      "Epoch 17486: train loss: 0.1721, test loss 0.7054\n",
      "Epoch 17487: train loss: 0.1721, test loss 0.7054\n",
      "Epoch 17488: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17489: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17490: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17491: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17492: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17493: train loss: 0.1721, test loss 0.7053\n",
      "Epoch 17494: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17495: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17496: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17497: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17498: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17499: train loss: 0.1721, test loss 0.7052\n",
      "Epoch 17500: train loss: 0.1721, test loss 0.7051\n",
      "Epoch 17501: train loss: 0.1721, test loss 0.7051\n",
      "Epoch 17502: train loss: 0.1721, test loss 0.7051\n",
      "Epoch 17503: train loss: 0.1721, test loss 0.7051\n",
      "Epoch 17504: train loss: 0.1721, test loss 0.7051\n",
      "Epoch 17505: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17506: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17507: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17508: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17509: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17510: train loss: 0.1721, test loss 0.7050\n",
      "Epoch 17511: train loss: 0.1721, test loss 0.7049\n",
      "Epoch 17512: train loss: 0.1721, test loss 0.7049\n",
      "Epoch 17513: train loss: 0.1721, test loss 0.7049\n",
      "Epoch 17514: train loss: 0.1721, test loss 0.7049\n",
      "Epoch 17515: train loss: 0.1721, test loss 0.7049\n",
      "Epoch 17516: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17517: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17518: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17519: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17520: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17521: train loss: 0.1721, test loss 0.7048\n",
      "Epoch 17522: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17523: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17524: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17525: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17526: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17527: train loss: 0.1721, test loss 0.7047\n",
      "Epoch 17528: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17529: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17530: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17531: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17532: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17533: train loss: 0.1721, test loss 0.7046\n",
      "Epoch 17534: train loss: 0.1721, test loss 0.7045\n",
      "Epoch 17535: train loss: 0.1721, test loss 0.7045\n",
      "Epoch 17536: train loss: 0.1721, test loss 0.7045\n",
      "Epoch 17537: train loss: 0.1721, test loss 0.7045\n",
      "Epoch 17538: train loss: 0.1721, test loss 0.7045\n",
      "Epoch 17539: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17540: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17541: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17542: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17543: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17544: train loss: 0.1721, test loss 0.7044\n",
      "Epoch 17545: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17546: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17547: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17548: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17549: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17550: train loss: 0.1721, test loss 0.7043\n",
      "Epoch 17551: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17552: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17553: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17554: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17555: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17556: train loss: 0.1721, test loss 0.7042\n",
      "Epoch 17557: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17558: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17559: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17560: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17561: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17562: train loss: 0.1721, test loss 0.7041\n",
      "Epoch 17563: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17564: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17565: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17566: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17567: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17568: train loss: 0.1721, test loss 0.7040\n",
      "Epoch 17569: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17570: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17571: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17572: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17573: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17574: train loss: 0.1721, test loss 0.7039\n",
      "Epoch 17575: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17576: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17577: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17578: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17579: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17580: train loss: 0.1721, test loss 0.7038\n",
      "Epoch 17581: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17582: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17583: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17584: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17585: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17586: train loss: 0.1721, test loss 0.7037\n",
      "Epoch 17587: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17588: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17589: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17590: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17591: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17592: train loss: 0.1721, test loss 0.7036\n",
      "Epoch 17593: train loss: 0.1721, test loss 0.7035\n",
      "Epoch 17594: train loss: 0.1721, test loss 0.7035\n",
      "Epoch 17595: train loss: 0.1721, test loss 0.7035\n",
      "Epoch 17596: train loss: 0.1721, test loss 0.7035\n",
      "Epoch 17597: train loss: 0.1721, test loss 0.7035\n",
      "Epoch 17598: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17599: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17600: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17601: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17602: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17603: train loss: 0.1721, test loss 0.7034\n",
      "Epoch 17604: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17605: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17606: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17607: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17608: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17609: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17610: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17611: train loss: 0.1721, test loss 0.7033\n",
      "Epoch 17612: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17613: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17614: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17615: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17616: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17617: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17618: train loss: 0.1721, test loss 0.7032\n",
      "Epoch 17619: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17620: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17621: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17622: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17623: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17624: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17625: train loss: 0.1721, test loss 0.7031\n",
      "Epoch 17626: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17627: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17628: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17629: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17630: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17631: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17632: train loss: 0.1721, test loss 0.7030\n",
      "Epoch 17633: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17634: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17635: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17636: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17637: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17638: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17639: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17640: train loss: 0.1721, test loss 0.7029\n",
      "Epoch 17641: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17642: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17643: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17644: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17645: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17646: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17647: train loss: 0.1721, test loss 0.7028\n",
      "Epoch 17648: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17649: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17650: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17651: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17652: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17653: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17654: train loss: 0.1721, test loss 0.7027\n",
      "Epoch 17655: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17656: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17657: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17658: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17659: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17660: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17661: train loss: 0.1721, test loss 0.7026\n",
      "Epoch 17662: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17663: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17664: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17665: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17666: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17667: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17668: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17669: train loss: 0.1721, test loss 0.7025\n",
      "Epoch 17670: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17671: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17672: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17673: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17674: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17675: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17676: train loss: 0.1721, test loss 0.7024\n",
      "Epoch 17677: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17678: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17679: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17680: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17681: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17682: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17683: train loss: 0.1721, test loss 0.7023\n",
      "Epoch 17684: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17685: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17686: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17687: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17688: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17689: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17690: train loss: 0.1720, test loss 0.7022\n",
      "Epoch 17691: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17692: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17693: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17694: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17695: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17696: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17697: train loss: 0.1720, test loss 0.7021\n",
      "Epoch 17698: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17699: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17700: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17701: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17702: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17703: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17704: train loss: 0.1720, test loss 0.7020\n",
      "Epoch 17705: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17706: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17707: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17708: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17709: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17710: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17711: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17712: train loss: 0.1720, test loss 0.7019\n",
      "Epoch 17713: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17714: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17715: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17716: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17717: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17718: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17719: train loss: 0.1720, test loss 0.7018\n",
      "Epoch 17720: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17721: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17722: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17723: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17724: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17725: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17726: train loss: 0.1720, test loss 0.7017\n",
      "Epoch 17727: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17728: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17729: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17730: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17731: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17732: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17733: train loss: 0.1720, test loss 0.7016\n",
      "Epoch 17734: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17735: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17736: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17737: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17738: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17739: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17740: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17741: train loss: 0.1720, test loss 0.7015\n",
      "Epoch 17742: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17743: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17744: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17745: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17746: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17747: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17748: train loss: 0.1720, test loss 0.7014\n",
      "Epoch 17749: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17750: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17751: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17752: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17753: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17754: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17755: train loss: 0.1720, test loss 0.7013\n",
      "Epoch 17756: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17757: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17758: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17759: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17760: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17761: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17762: train loss: 0.1720, test loss 0.7012\n",
      "Epoch 17763: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17764: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17765: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17766: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17767: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17768: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17769: train loss: 0.1720, test loss 0.7011\n",
      "Epoch 17770: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17771: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17772: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17773: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17774: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17775: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17776: train loss: 0.1720, test loss 0.7010\n",
      "Epoch 17777: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17778: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17779: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17780: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17781: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17782: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17783: train loss: 0.1720, test loss 0.7009\n",
      "Epoch 17784: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17785: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17786: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17787: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17788: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17789: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17790: train loss: 0.1720, test loss 0.7008\n",
      "Epoch 17791: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17792: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17793: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17794: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17795: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17796: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17797: train loss: 0.1720, test loss 0.7007\n",
      "Epoch 17798: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17799: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17800: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17801: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17802: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17803: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17804: train loss: 0.1720, test loss 0.7006\n",
      "Epoch 17805: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17806: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17807: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17808: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17809: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17810: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17811: train loss: 0.1720, test loss 0.7005\n",
      "Epoch 17812: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17813: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17814: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17815: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17816: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17817: train loss: 0.1720, test loss 0.7004\n",
      "Epoch 17818: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17819: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17820: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17821: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17822: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17823: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17824: train loss: 0.1720, test loss 0.7003\n",
      "Epoch 17825: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17826: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17827: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17828: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17829: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17830: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17831: train loss: 0.1720, test loss 0.7002\n",
      "Epoch 17832: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17833: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17834: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17835: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17836: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17837: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17838: train loss: 0.1720, test loss 0.7001\n",
      "Epoch 17839: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17840: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17841: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17842: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17843: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17844: train loss: 0.1720, test loss 0.7000\n",
      "Epoch 17845: train loss: 0.1720, test loss 0.6999\n",
      "Epoch 17846: train loss: 0.1720, test loss 0.6999\n",
      "Epoch 17847: train loss: 0.1720, test loss 0.6999\n",
      "Epoch 17848: train loss: 0.1720, test loss 0.6999\n",
      "Epoch 17849: train loss: 0.1720, test loss 0.6998\n",
      "Epoch 17850: train loss: 0.1720, test loss 0.6998\n",
      "Epoch 17851: train loss: 0.1720, test loss 0.6998\n",
      "Epoch 17852: train loss: 0.1720, test loss 0.6998\n",
      "Epoch 17853: train loss: 0.1720, test loss 0.6998\n",
      "Epoch 17854: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17855: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17856: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17857: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17858: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17859: train loss: 0.1720, test loss 0.6997\n",
      "Epoch 17860: train loss: 0.1720, test loss 0.6996\n",
      "Epoch 17861: train loss: 0.1720, test loss 0.6996\n",
      "Epoch 17862: train loss: 0.1720, test loss 0.6996\n",
      "Epoch 17863: train loss: 0.1720, test loss 0.6996\n",
      "Epoch 17864: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17865: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17866: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17867: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17868: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17869: train loss: 0.1720, test loss 0.6995\n",
      "Epoch 17870: train loss: 0.1720, test loss 0.6994\n",
      "Epoch 17871: train loss: 0.1720, test loss 0.6994\n",
      "Epoch 17872: train loss: 0.1720, test loss 0.6994\n",
      "Epoch 17873: train loss: 0.1720, test loss 0.6994\n",
      "Epoch 17874: train loss: 0.1720, test loss 0.6994\n",
      "Epoch 17875: train loss: 0.1720, test loss 0.6993\n",
      "Epoch 17876: train loss: 0.1720, test loss 0.6993\n",
      "Epoch 17877: train loss: 0.1720, test loss 0.6993\n",
      "Epoch 17878: train loss: 0.1720, test loss 0.6993\n",
      "Epoch 17879: train loss: 0.1720, test loss 0.6993\n",
      "Epoch 17880: train loss: 0.1720, test loss 0.6992\n",
      "Epoch 17881: train loss: 0.1720, test loss 0.6992\n",
      "Epoch 17882: train loss: 0.1720, test loss 0.6992\n",
      "Epoch 17883: train loss: 0.1720, test loss 0.6992\n",
      "Epoch 17884: train loss: 0.1720, test loss 0.6992\n",
      "Epoch 17885: train loss: 0.1720, test loss 0.6991\n",
      "Epoch 17886: train loss: 0.1720, test loss 0.6991\n",
      "Epoch 17887: train loss: 0.1720, test loss 0.6991\n",
      "Epoch 17888: train loss: 0.1720, test loss 0.6991\n",
      "Epoch 17889: train loss: 0.1720, test loss 0.6991\n",
      "Epoch 17890: train loss: 0.1720, test loss 0.6990\n",
      "Epoch 17891: train loss: 0.1720, test loss 0.6990\n",
      "Epoch 17892: train loss: 0.1720, test loss 0.6990\n",
      "Epoch 17893: train loss: 0.1720, test loss 0.6990\n",
      "Epoch 17894: train loss: 0.1720, test loss 0.6990\n",
      "Epoch 17895: train loss: 0.1720, test loss 0.6989\n",
      "Epoch 17896: train loss: 0.1719, test loss 0.6989\n",
      "Epoch 17897: train loss: 0.1719, test loss 0.6989\n",
      "Epoch 17898: train loss: 0.1719, test loss 0.6989\n",
      "Epoch 17899: train loss: 0.1719, test loss 0.6989\n",
      "Epoch 17900: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17901: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17902: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17903: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17904: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17905: train loss: 0.1719, test loss 0.6988\n",
      "Epoch 17906: train loss: 0.1719, test loss 0.6987\n",
      "Epoch 17907: train loss: 0.1719, test loss 0.6987\n",
      "Epoch 17908: train loss: 0.1719, test loss 0.6987\n",
      "Epoch 17909: train loss: 0.1719, test loss 0.6987\n",
      "Epoch 17910: train loss: 0.1719, test loss 0.6987\n",
      "Epoch 17911: train loss: 0.1719, test loss 0.6986\n",
      "Epoch 17912: train loss: 0.1719, test loss 0.6986\n",
      "Epoch 17913: train loss: 0.1719, test loss 0.6986\n",
      "Epoch 17914: train loss: 0.1719, test loss 0.6986\n",
      "Epoch 17915: train loss: 0.1719, test loss 0.6986\n",
      "Epoch 17916: train loss: 0.1719, test loss 0.6985\n",
      "Epoch 17917: train loss: 0.1719, test loss 0.6985\n",
      "Epoch 17918: train loss: 0.1719, test loss 0.6985\n",
      "Epoch 17919: train loss: 0.1719, test loss 0.6985\n",
      "Epoch 17920: train loss: 0.1719, test loss 0.6985\n",
      "Epoch 17921: train loss: 0.1719, test loss 0.6984\n",
      "Epoch 17922: train loss: 0.1719, test loss 0.6984\n",
      "Epoch 17923: train loss: 0.1719, test loss 0.6984\n",
      "Epoch 17924: train loss: 0.1719, test loss 0.6984\n",
      "Epoch 17925: train loss: 0.1719, test loss 0.6984\n",
      "Epoch 17926: train loss: 0.1719, test loss 0.6983\n",
      "Epoch 17927: train loss: 0.1719, test loss 0.6983\n",
      "Epoch 17928: train loss: 0.1719, test loss 0.6983\n",
      "Epoch 17929: train loss: 0.1719, test loss 0.6983\n",
      "Epoch 17930: train loss: 0.1719, test loss 0.6983\n",
      "Epoch 17931: train loss: 0.1719, test loss 0.6982\n",
      "Epoch 17932: train loss: 0.1719, test loss 0.6982\n",
      "Epoch 17933: train loss: 0.1719, test loss 0.6982\n",
      "Epoch 17934: train loss: 0.1719, test loss 0.6982\n",
      "Epoch 17935: train loss: 0.1719, test loss 0.6982\n",
      "Epoch 17936: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17937: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17938: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17939: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17940: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17941: train loss: 0.1719, test loss 0.6981\n",
      "Epoch 17942: train loss: 0.1719, test loss 0.6980\n",
      "Epoch 17943: train loss: 0.1719, test loss 0.6980\n",
      "Epoch 17944: train loss: 0.1719, test loss 0.6980\n",
      "Epoch 17945: train loss: 0.1719, test loss 0.6980\n",
      "Epoch 17946: train loss: 0.1719, test loss 0.6980\n",
      "Epoch 17947: train loss: 0.1719, test loss 0.6979\n",
      "Epoch 17948: train loss: 0.1719, test loss 0.6979\n",
      "Epoch 17949: train loss: 0.1719, test loss 0.6979\n",
      "Epoch 17950: train loss: 0.1719, test loss 0.6979\n",
      "Epoch 17951: train loss: 0.1719, test loss 0.6979\n",
      "Epoch 17952: train loss: 0.1719, test loss 0.6978\n",
      "Epoch 17953: train loss: 0.1719, test loss 0.6978\n",
      "Epoch 17954: train loss: 0.1719, test loss 0.6978\n",
      "Epoch 17955: train loss: 0.1719, test loss 0.6978\n",
      "Epoch 17956: train loss: 0.1719, test loss 0.6978\n",
      "Epoch 17957: train loss: 0.1719, test loss 0.6977\n",
      "Epoch 17958: train loss: 0.1719, test loss 0.6977\n",
      "Epoch 17959: train loss: 0.1719, test loss 0.6977\n",
      "Epoch 17960: train loss: 0.1719, test loss 0.6977\n",
      "Epoch 17961: train loss: 0.1719, test loss 0.6977\n",
      "Epoch 17962: train loss: 0.1719, test loss 0.6976\n",
      "Epoch 17963: train loss: 0.1719, test loss 0.6976\n",
      "Epoch 17964: train loss: 0.1719, test loss 0.6976\n",
      "Epoch 17965: train loss: 0.1719, test loss 0.6976\n",
      "Epoch 17966: train loss: 0.1719, test loss 0.6976\n",
      "Epoch 17967: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17968: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17969: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17970: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17971: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17972: train loss: 0.1719, test loss 0.6975\n",
      "Epoch 17973: train loss: 0.1719, test loss 0.6974\n",
      "Epoch 17974: train loss: 0.1719, test loss 0.6974\n",
      "Epoch 17975: train loss: 0.1719, test loss 0.6974\n",
      "Epoch 17976: train loss: 0.1719, test loss 0.6974\n",
      "Epoch 17977: train loss: 0.1719, test loss 0.6974\n",
      "Epoch 17978: train loss: 0.1719, test loss 0.6973\n",
      "Epoch 17979: train loss: 0.1719, test loss 0.6973\n",
      "Epoch 17980: train loss: 0.1719, test loss 0.6973\n",
      "Epoch 17981: train loss: 0.1719, test loss 0.6973\n",
      "Epoch 17982: train loss: 0.1719, test loss 0.6973\n",
      "Epoch 17983: train loss: 0.1719, test loss 0.6972\n",
      "Epoch 17984: train loss: 0.1719, test loss 0.6972\n",
      "Epoch 17985: train loss: 0.1719, test loss 0.6972\n",
      "Epoch 17986: train loss: 0.1719, test loss 0.6972\n",
      "Epoch 17987: train loss: 0.1719, test loss 0.6972\n",
      "Epoch 17988: train loss: 0.1719, test loss 0.6971\n",
      "Epoch 17989: train loss: 0.1719, test loss 0.6971\n",
      "Epoch 17990: train loss: 0.1719, test loss 0.6971\n",
      "Epoch 17991: train loss: 0.1719, test loss 0.6971\n",
      "Epoch 17992: train loss: 0.1719, test loss 0.6971\n",
      "Epoch 17993: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17994: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17995: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17996: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17997: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17998: train loss: 0.1719, test loss 0.6970\n",
      "Epoch 17999: train loss: 0.1719, test loss 0.6969\n",
      "Epoch 18000: train loss: 0.1719, test loss 0.6969\n",
      "Epoch 18001: train loss: 0.1719, test loss 0.6969\n",
      "Epoch 18002: train loss: 0.1719, test loss 0.6969\n",
      "Epoch 18003: train loss: 0.1719, test loss 0.6969\n",
      "Epoch 18004: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18005: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18006: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18007: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18008: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18009: train loss: 0.1719, test loss 0.6968\n",
      "Epoch 18010: train loss: 0.1719, test loss 0.6967\n",
      "Epoch 18011: train loss: 0.1719, test loss 0.6967\n",
      "Epoch 18012: train loss: 0.1719, test loss 0.6967\n",
      "Epoch 18013: train loss: 0.1719, test loss 0.6967\n",
      "Epoch 18014: train loss: 0.1719, test loss 0.6967\n",
      "Epoch 18015: train loss: 0.1719, test loss 0.6966\n",
      "Epoch 18016: train loss: 0.1719, test loss 0.6966\n",
      "Epoch 18017: train loss: 0.1719, test loss 0.6966\n",
      "Epoch 18018: train loss: 0.1719, test loss 0.6966\n",
      "Epoch 18019: train loss: 0.1719, test loss 0.6966\n",
      "Epoch 18020: train loss: 0.1719, test loss 0.6965\n",
      "Epoch 18021: train loss: 0.1719, test loss 0.6965\n",
      "Epoch 18022: train loss: 0.1719, test loss 0.6965\n",
      "Epoch 18023: train loss: 0.1719, test loss 0.6965\n",
      "Epoch 18024: train loss: 0.1719, test loss 0.6965\n",
      "Epoch 18025: train loss: 0.1719, test loss 0.6964\n",
      "Epoch 18026: train loss: 0.1719, test loss 0.6964\n",
      "Epoch 18027: train loss: 0.1719, test loss 0.6964\n",
      "Epoch 18028: train loss: 0.1719, test loss 0.6964\n",
      "Epoch 18029: train loss: 0.1719, test loss 0.6964\n",
      "Epoch 18030: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18031: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18032: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18033: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18034: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18035: train loss: 0.1719, test loss 0.6963\n",
      "Epoch 18036: train loss: 0.1719, test loss 0.6962\n",
      "Epoch 18037: train loss: 0.1719, test loss 0.6962\n",
      "Epoch 18038: train loss: 0.1719, test loss 0.6962\n",
      "Epoch 18039: train loss: 0.1719, test loss 0.6962\n",
      "Epoch 18040: train loss: 0.1719, test loss 0.6962\n",
      "Epoch 18041: train loss: 0.1719, test loss 0.6961\n",
      "Epoch 18042: train loss: 0.1719, test loss 0.6961\n",
      "Epoch 18043: train loss: 0.1719, test loss 0.6961\n",
      "Epoch 18044: train loss: 0.1719, test loss 0.6961\n",
      "Epoch 18045: train loss: 0.1719, test loss 0.6961\n",
      "Epoch 18046: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18047: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18048: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18049: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18050: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18051: train loss: 0.1719, test loss 0.6960\n",
      "Epoch 18052: train loss: 0.1719, test loss 0.6959\n",
      "Epoch 18053: train loss: 0.1719, test loss 0.6959\n",
      "Epoch 18054: train loss: 0.1719, test loss 0.6959\n",
      "Epoch 18055: train loss: 0.1719, test loss 0.6959\n",
      "Epoch 18056: train loss: 0.1719, test loss 0.6959\n",
      "Epoch 18057: train loss: 0.1719, test loss 0.6958\n",
      "Epoch 18058: train loss: 0.1719, test loss 0.6958\n",
      "Epoch 18059: train loss: 0.1719, test loss 0.6958\n",
      "Epoch 18060: train loss: 0.1719, test loss 0.6958\n",
      "Epoch 18061: train loss: 0.1719, test loss 0.6958\n",
      "Epoch 18062: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18063: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18064: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18065: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18066: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18067: train loss: 0.1719, test loss 0.6957\n",
      "Epoch 18068: train loss: 0.1719, test loss 0.6956\n",
      "Epoch 18069: train loss: 0.1719, test loss 0.6956\n",
      "Epoch 18070: train loss: 0.1719, test loss 0.6956\n",
      "Epoch 18071: train loss: 0.1719, test loss 0.6956\n",
      "Epoch 18072: train loss: 0.1719, test loss 0.6956\n",
      "Epoch 18073: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18074: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18075: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18076: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18077: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18078: train loss: 0.1719, test loss 0.6955\n",
      "Epoch 18079: train loss: 0.1719, test loss 0.6954\n",
      "Epoch 18080: train loss: 0.1719, test loss 0.6954\n",
      "Epoch 18081: train loss: 0.1719, test loss 0.6954\n",
      "Epoch 18082: train loss: 0.1719, test loss 0.6954\n",
      "Epoch 18083: train loss: 0.1719, test loss 0.6954\n",
      "Epoch 18084: train loss: 0.1719, test loss 0.6953\n",
      "Epoch 18085: train loss: 0.1719, test loss 0.6953\n",
      "Epoch 18086: train loss: 0.1719, test loss 0.6953\n",
      "Epoch 18087: train loss: 0.1719, test loss 0.6953\n",
      "Epoch 18088: train loss: 0.1719, test loss 0.6953\n",
      "Epoch 18089: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18090: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18091: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18092: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18093: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18094: train loss: 0.1719, test loss 0.6952\n",
      "Epoch 18095: train loss: 0.1719, test loss 0.6951\n",
      "Epoch 18096: train loss: 0.1719, test loss 0.6951\n",
      "Epoch 18097: train loss: 0.1719, test loss 0.6951\n",
      "Epoch 18098: train loss: 0.1719, test loss 0.6951\n",
      "Epoch 18099: train loss: 0.1719, test loss 0.6951\n",
      "Epoch 18100: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18101: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18102: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18103: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18104: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18105: train loss: 0.1719, test loss 0.6950\n",
      "Epoch 18106: train loss: 0.1719, test loss 0.6949\n",
      "Epoch 18107: train loss: 0.1719, test loss 0.6949\n",
      "Epoch 18108: train loss: 0.1719, test loss 0.6949\n",
      "Epoch 18109: train loss: 0.1719, test loss 0.6949\n",
      "Epoch 18110: train loss: 0.1719, test loss 0.6949\n",
      "Epoch 18111: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18112: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18113: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18114: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18115: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18116: train loss: 0.1719, test loss 0.6948\n",
      "Epoch 18117: train loss: 0.1719, test loss 0.6947\n",
      "Epoch 18118: train loss: 0.1719, test loss 0.6947\n",
      "Epoch 18119: train loss: 0.1719, test loss 0.6947\n",
      "Epoch 18120: train loss: 0.1719, test loss 0.6947\n",
      "Epoch 18121: train loss: 0.1719, test loss 0.6947\n",
      "Epoch 18122: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18123: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18124: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18125: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18126: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18127: train loss: 0.1718, test loss 0.6946\n",
      "Epoch 18128: train loss: 0.1718, test loss 0.6945\n",
      "Epoch 18129: train loss: 0.1718, test loss 0.6945\n",
      "Epoch 18130: train loss: 0.1718, test loss 0.6945\n",
      "Epoch 18131: train loss: 0.1718, test loss 0.6945\n",
      "Epoch 18132: train loss: 0.1718, test loss 0.6945\n",
      "Epoch 18133: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18134: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18135: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18136: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18137: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18138: train loss: 0.1718, test loss 0.6944\n",
      "Epoch 18139: train loss: 0.1718, test loss 0.6943\n",
      "Epoch 18140: train loss: 0.1718, test loss 0.6943\n",
      "Epoch 18141: train loss: 0.1718, test loss 0.6943\n",
      "Epoch 18142: train loss: 0.1718, test loss 0.6943\n",
      "Epoch 18143: train loss: 0.1718, test loss 0.6943\n",
      "Epoch 18144: train loss: 0.1718, test loss 0.6942\n",
      "Epoch 18145: train loss: 0.1718, test loss 0.6942\n",
      "Epoch 18146: train loss: 0.1718, test loss 0.6942\n",
      "Epoch 18147: train loss: 0.1718, test loss 0.6942\n",
      "Epoch 18148: train loss: 0.1718, test loss 0.6942\n",
      "Epoch 18149: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18150: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18151: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18152: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18153: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18154: train loss: 0.1718, test loss 0.6941\n",
      "Epoch 18155: train loss: 0.1718, test loss 0.6940\n",
      "Epoch 18156: train loss: 0.1718, test loss 0.6940\n",
      "Epoch 18157: train loss: 0.1718, test loss 0.6940\n",
      "Epoch 18158: train loss: 0.1718, test loss 0.6940\n",
      "Epoch 18159: train loss: 0.1718, test loss 0.6940\n",
      "Epoch 18160: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18161: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18162: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18163: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18164: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18165: train loss: 0.1718, test loss 0.6939\n",
      "Epoch 18166: train loss: 0.1718, test loss 0.6938\n",
      "Epoch 18167: train loss: 0.1718, test loss 0.6938\n",
      "Epoch 18168: train loss: 0.1718, test loss 0.6938\n",
      "Epoch 18169: train loss: 0.1718, test loss 0.6938\n",
      "Epoch 18170: train loss: 0.1718, test loss 0.6938\n",
      "Epoch 18171: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18172: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18173: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18174: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18175: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18176: train loss: 0.1718, test loss 0.6937\n",
      "Epoch 18177: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18178: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18179: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18180: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18181: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18182: train loss: 0.1718, test loss 0.6936\n",
      "Epoch 18183: train loss: 0.1718, test loss 0.6935\n",
      "Epoch 18184: train loss: 0.1718, test loss 0.6935\n",
      "Epoch 18185: train loss: 0.1718, test loss 0.6935\n",
      "Epoch 18186: train loss: 0.1718, test loss 0.6935\n",
      "Epoch 18187: train loss: 0.1718, test loss 0.6935\n",
      "Epoch 18188: train loss: 0.1718, test loss 0.6934\n",
      "Epoch 18189: train loss: 0.1718, test loss 0.6934\n",
      "Epoch 18190: train loss: 0.1718, test loss 0.6934\n",
      "Epoch 18191: train loss: 0.1718, test loss 0.6934\n",
      "Epoch 18192: train loss: 0.1718, test loss 0.6934\n",
      "Epoch 18193: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18194: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18195: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18196: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18197: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18198: train loss: 0.1718, test loss 0.6933\n",
      "Epoch 18199: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18200: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18201: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18202: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18203: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18204: train loss: 0.1718, test loss 0.6932\n",
      "Epoch 18205: train loss: 0.1718, test loss 0.6931\n",
      "Epoch 18206: train loss: 0.1718, test loss 0.6931\n",
      "Epoch 18207: train loss: 0.1718, test loss 0.6931\n",
      "Epoch 18208: train loss: 0.1718, test loss 0.6931\n",
      "Epoch 18209: train loss: 0.1718, test loss 0.6931\n",
      "Epoch 18210: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18211: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18212: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18213: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18214: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18215: train loss: 0.1718, test loss 0.6930\n",
      "Epoch 18216: train loss: 0.1718, test loss 0.6929\n",
      "Epoch 18217: train loss: 0.1718, test loss 0.6929\n",
      "Epoch 18218: train loss: 0.1718, test loss 0.6929\n",
      "Epoch 18219: train loss: 0.1718, test loss 0.6929\n",
      "Epoch 18220: train loss: 0.1718, test loss 0.6929\n",
      "Epoch 18221: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18222: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18223: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18224: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18225: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18226: train loss: 0.1718, test loss 0.6928\n",
      "Epoch 18227: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18228: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18229: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18230: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18231: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18232: train loss: 0.1718, test loss 0.6927\n",
      "Epoch 18233: train loss: 0.1718, test loss 0.6926\n",
      "Epoch 18234: train loss: 0.1718, test loss 0.6926\n",
      "Epoch 18235: train loss: 0.1718, test loss 0.6926\n",
      "Epoch 18236: train loss: 0.1718, test loss 0.6926\n",
      "Epoch 18237: train loss: 0.1718, test loss 0.6926\n",
      "Epoch 18238: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18239: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18240: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18241: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18242: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18243: train loss: 0.1718, test loss 0.6925\n",
      "Epoch 18244: train loss: 0.1718, test loss 0.6924\n",
      "Epoch 18245: train loss: 0.1718, test loss 0.6924\n",
      "Epoch 18246: train loss: 0.1718, test loss 0.6924\n",
      "Epoch 18247: train loss: 0.1718, test loss 0.6924\n",
      "Epoch 18248: train loss: 0.1718, test loss 0.6924\n",
      "Epoch 18249: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18250: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18251: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18252: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18253: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18254: train loss: 0.1718, test loss 0.6923\n",
      "Epoch 18255: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18256: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18257: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18258: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18259: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18260: train loss: 0.1718, test loss 0.6922\n",
      "Epoch 18261: train loss: 0.1718, test loss 0.6921\n",
      "Epoch 18262: train loss: 0.1718, test loss 0.6921\n",
      "Epoch 18263: train loss: 0.1718, test loss 0.6921\n",
      "Epoch 18264: train loss: 0.1718, test loss 0.6921\n",
      "Epoch 18265: train loss: 0.1718, test loss 0.6921\n",
      "Epoch 18266: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18267: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18268: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18269: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18270: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18271: train loss: 0.1718, test loss 0.6920\n",
      "Epoch 18272: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18273: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18274: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18275: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18276: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18277: train loss: 0.1718, test loss 0.6919\n",
      "Epoch 18278: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18279: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18280: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18281: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18282: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18283: train loss: 0.1718, test loss 0.6918\n",
      "Epoch 18284: train loss: 0.1718, test loss 0.6917\n",
      "Epoch 18285: train loss: 0.1718, test loss 0.6917\n",
      "Epoch 18286: train loss: 0.1718, test loss 0.6917\n",
      "Epoch 18287: train loss: 0.1718, test loss 0.6917\n",
      "Epoch 18288: train loss: 0.1718, test loss 0.6917\n",
      "Epoch 18289: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18290: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18291: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18292: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18293: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18294: train loss: 0.1718, test loss 0.6916\n",
      "Epoch 18295: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18296: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18297: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18298: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18299: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18300: train loss: 0.1718, test loss 0.6915\n",
      "Epoch 18301: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18302: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18303: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18304: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18305: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18306: train loss: 0.1718, test loss 0.6914\n",
      "Epoch 18307: train loss: 0.1718, test loss 0.6913\n",
      "Epoch 18308: train loss: 0.1718, test loss 0.6913\n",
      "Epoch 18309: train loss: 0.1718, test loss 0.6913\n",
      "Epoch 18310: train loss: 0.1718, test loss 0.6913\n",
      "Epoch 18311: train loss: 0.1718, test loss 0.6913\n",
      "Epoch 18312: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18313: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18314: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18315: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18316: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18317: train loss: 0.1718, test loss 0.6912\n",
      "Epoch 18318: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18319: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18320: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18321: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18322: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18323: train loss: 0.1718, test loss 0.6911\n",
      "Epoch 18324: train loss: 0.1718, test loss 0.6910\n",
      "Epoch 18325: train loss: 0.1718, test loss 0.6910\n",
      "Epoch 18326: train loss: 0.1718, test loss 0.6910\n",
      "Epoch 18327: train loss: 0.1718, test loss 0.6910\n",
      "Epoch 18328: train loss: 0.1718, test loss 0.6910\n",
      "Epoch 18329: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18330: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18331: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18332: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18333: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18334: train loss: 0.1718, test loss 0.6909\n",
      "Epoch 18335: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18336: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18337: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18338: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18339: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18340: train loss: 0.1718, test loss 0.6908\n",
      "Epoch 18341: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18342: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18343: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18344: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18345: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18346: train loss: 0.1718, test loss 0.6907\n",
      "Epoch 18347: train loss: 0.1718, test loss 0.6906\n",
      "Epoch 18348: train loss: 0.1718, test loss 0.6906\n",
      "Epoch 18349: train loss: 0.1718, test loss 0.6906\n",
      "Epoch 18350: train loss: 0.1718, test loss 0.6906\n",
      "Epoch 18351: train loss: 0.1718, test loss 0.6906\n",
      "Epoch 18352: train loss: 0.1718, test loss 0.6905\n",
      "Epoch 18353: train loss: 0.1718, test loss 0.6905\n",
      "Epoch 18354: train loss: 0.1717, test loss 0.6905\n",
      "Epoch 18355: train loss: 0.1717, test loss 0.6905\n",
      "Epoch 18356: train loss: 0.1717, test loss 0.6905\n",
      "Epoch 18357: train loss: 0.1717, test loss 0.6905\n",
      "Epoch 18358: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18359: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18360: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18361: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18362: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18363: train loss: 0.1717, test loss 0.6904\n",
      "Epoch 18364: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18365: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18366: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18367: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18368: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18369: train loss: 0.1717, test loss 0.6903\n",
      "Epoch 18370: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18371: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18372: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18373: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18374: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18375: train loss: 0.1717, test loss 0.6902\n",
      "Epoch 18376: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18377: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18378: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18379: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18380: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18381: train loss: 0.1717, test loss 0.6901\n",
      "Epoch 18382: train loss: 0.1717, test loss 0.6900\n",
      "Epoch 18383: train loss: 0.1717, test loss 0.6900\n",
      "Epoch 18384: train loss: 0.1717, test loss 0.6900\n",
      "Epoch 18385: train loss: 0.1717, test loss 0.6900\n",
      "Epoch 18386: train loss: 0.1717, test loss 0.6900\n",
      "Epoch 18387: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18388: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18389: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18390: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18391: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18392: train loss: 0.1717, test loss 0.6899\n",
      "Epoch 18393: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18394: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18395: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18396: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18397: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18398: train loss: 0.1717, test loss 0.6898\n",
      "Epoch 18399: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18400: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18401: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18402: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18403: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18404: train loss: 0.1717, test loss 0.6897\n",
      "Epoch 18405: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18406: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18407: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18408: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18409: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18410: train loss: 0.1717, test loss 0.6896\n",
      "Epoch 18411: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18412: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18413: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18414: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18415: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18416: train loss: 0.1717, test loss 0.6895\n",
      "Epoch 18417: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18418: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18419: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18420: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18421: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18422: train loss: 0.1717, test loss 0.6894\n",
      "Epoch 18423: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18424: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18425: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18426: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18427: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18428: train loss: 0.1717, test loss 0.6893\n",
      "Epoch 18429: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18430: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18431: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18432: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18433: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18434: train loss: 0.1717, test loss 0.6892\n",
      "Epoch 18435: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18436: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18437: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18438: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18439: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18440: train loss: 0.1717, test loss 0.6891\n",
      "Epoch 18441: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18442: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18443: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18444: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18445: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18446: train loss: 0.1717, test loss 0.6890\n",
      "Epoch 18447: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18448: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18449: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18450: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18451: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18452: train loss: 0.1717, test loss 0.6889\n",
      "Epoch 18453: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18454: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18455: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18456: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18457: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18458: train loss: 0.1717, test loss 0.6888\n",
      "Epoch 18459: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18460: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18461: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18462: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18463: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18464: train loss: 0.1717, test loss 0.6887\n",
      "Epoch 18465: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18466: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18467: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18468: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18469: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18470: train loss: 0.1717, test loss 0.6886\n",
      "Epoch 18471: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18472: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18473: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18474: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18475: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18476: train loss: 0.1717, test loss 0.6885\n",
      "Epoch 18477: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18478: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18479: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18480: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18481: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18482: train loss: 0.1717, test loss 0.6884\n",
      "Epoch 18483: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18484: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18485: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18486: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18487: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18488: train loss: 0.1717, test loss 0.6883\n",
      "Epoch 18489: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18490: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18491: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18492: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18493: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18494: train loss: 0.1717, test loss 0.6882\n",
      "Epoch 18495: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18496: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18497: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18498: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18499: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18500: train loss: 0.1717, test loss 0.6881\n",
      "Epoch 18501: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18502: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18503: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18504: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18505: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18506: train loss: 0.1717, test loss 0.6880\n",
      "Epoch 18507: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18508: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18509: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18510: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18511: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18512: train loss: 0.1717, test loss 0.6879\n",
      "Epoch 18513: train loss: 0.1717, test loss 0.6878\n",
      "Epoch 18514: train loss: 0.1717, test loss 0.6878\n",
      "Epoch 18515: train loss: 0.1717, test loss 0.6878\n",
      "Epoch 18516: train loss: 0.1717, test loss 0.6878\n",
      "Epoch 18517: train loss: 0.1717, test loss 0.6878\n",
      "Epoch 18518: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18519: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18520: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18521: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18522: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18523: train loss: 0.1717, test loss 0.6877\n",
      "Epoch 18524: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18525: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18526: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18527: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18528: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18529: train loss: 0.1717, test loss 0.6876\n",
      "Epoch 18530: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18531: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18532: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18533: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18534: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18535: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18536: train loss: 0.1717, test loss 0.6875\n",
      "Epoch 18537: train loss: 0.1717, test loss 0.6874\n",
      "Epoch 18538: train loss: 0.1717, test loss 0.6874\n",
      "Epoch 18539: train loss: 0.1717, test loss 0.6874\n",
      "Epoch 18540: train loss: 0.1717, test loss 0.6874\n",
      "Epoch 18541: train loss: 0.1717, test loss 0.6874\n",
      "Epoch 18542: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18543: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18544: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18545: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18546: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18547: train loss: 0.1717, test loss 0.6873\n",
      "Epoch 18548: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18549: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18550: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18551: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18552: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18553: train loss: 0.1717, test loss 0.6872\n",
      "Epoch 18554: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18555: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18556: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18557: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18558: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18559: train loss: 0.1717, test loss 0.6871\n",
      "Epoch 18560: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18561: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18562: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18563: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18564: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18565: train loss: 0.1717, test loss 0.6870\n",
      "Epoch 18566: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18567: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18568: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18569: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18570: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18571: train loss: 0.1717, test loss 0.6869\n",
      "Epoch 18572: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18573: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18574: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18575: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18576: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18577: train loss: 0.1717, test loss 0.6868\n",
      "Epoch 18578: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18579: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18580: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18581: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18582: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18583: train loss: 0.1717, test loss 0.6867\n",
      "Epoch 18584: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18585: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18586: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18587: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18588: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18589: train loss: 0.1717, test loss 0.6866\n",
      "Epoch 18590: train loss: 0.1717, test loss 0.6865\n",
      "Epoch 18591: train loss: 0.1717, test loss 0.6865\n",
      "Epoch 18592: train loss: 0.1716, test loss 0.6865\n",
      "Epoch 18593: train loss: 0.1716, test loss 0.6865\n",
      "Epoch 18594: train loss: 0.1716, test loss 0.6865\n",
      "Epoch 18595: train loss: 0.1716, test loss 0.6865\n",
      "Epoch 18596: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18597: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18598: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18599: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18600: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18601: train loss: 0.1716, test loss 0.6864\n",
      "Epoch 18602: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18603: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18604: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18605: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18606: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18607: train loss: 0.1716, test loss 0.6863\n",
      "Epoch 18608: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18609: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18610: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18611: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18612: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18613: train loss: 0.1716, test loss 0.6862\n",
      "Epoch 18614: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18615: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18616: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18617: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18618: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18619: train loss: 0.1716, test loss 0.6861\n",
      "Epoch 18620: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18621: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18622: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18623: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18624: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18625: train loss: 0.1716, test loss 0.6860\n",
      "Epoch 18626: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18627: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18628: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18629: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18630: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18631: train loss: 0.1716, test loss 0.6859\n",
      "Epoch 18632: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18633: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18634: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18635: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18636: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18637: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18638: train loss: 0.1716, test loss 0.6858\n",
      "Epoch 18639: train loss: 0.1716, test loss 0.6857\n",
      "Epoch 18640: train loss: 0.1716, test loss 0.6857\n",
      "Epoch 18641: train loss: 0.1716, test loss 0.6857\n",
      "Epoch 18642: train loss: 0.1716, test loss 0.6857\n",
      "Epoch 18643: train loss: 0.1716, test loss 0.6857\n",
      "Epoch 18644: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18645: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18646: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18647: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18648: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18649: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18650: train loss: 0.1716, test loss 0.6856\n",
      "Epoch 18651: train loss: 0.1716, test loss 0.6855\n",
      "Epoch 18652: train loss: 0.1716, test loss 0.6855\n",
      "Epoch 18653: train loss: 0.1716, test loss 0.6855\n",
      "Epoch 18654: train loss: 0.1716, test loss 0.6855\n",
      "Epoch 18655: train loss: 0.1716, test loss 0.6855\n",
      "Epoch 18656: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18657: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18658: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18659: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18660: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18661: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18662: train loss: 0.1716, test loss 0.6854\n",
      "Epoch 18663: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18664: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18665: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18666: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18667: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18668: train loss: 0.1716, test loss 0.6853\n",
      "Epoch 18669: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18670: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18671: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18672: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18673: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18674: train loss: 0.1716, test loss 0.6852\n",
      "Epoch 18675: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18676: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18677: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18678: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18679: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18680: train loss: 0.1716, test loss 0.6851\n",
      "Epoch 18681: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18682: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18683: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18684: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18685: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18686: train loss: 0.1716, test loss 0.6850\n",
      "Epoch 18687: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18688: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18689: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18690: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18691: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18692: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18693: train loss: 0.1716, test loss 0.6849\n",
      "Epoch 18694: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18695: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18696: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18697: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18698: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18699: train loss: 0.1716, test loss 0.6848\n",
      "Epoch 18700: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18701: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18702: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18703: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18704: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18705: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18706: train loss: 0.1716, test loss 0.6847\n",
      "Epoch 18707: train loss: 0.1716, test loss 0.6846\n",
      "Epoch 18708: train loss: 0.1716, test loss 0.6846\n",
      "Epoch 18709: train loss: 0.1716, test loss 0.6846\n",
      "Epoch 18710: train loss: 0.1716, test loss 0.6846\n",
      "Epoch 18711: train loss: 0.1716, test loss 0.6846\n",
      "Epoch 18712: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18713: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18714: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18715: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18716: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18717: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18718: train loss: 0.1716, test loss 0.6845\n",
      "Epoch 18719: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18720: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18721: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18722: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18723: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18724: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18725: train loss: 0.1716, test loss 0.6844\n",
      "Epoch 18726: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18727: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18728: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18729: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18730: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18731: train loss: 0.1716, test loss 0.6843\n",
      "Epoch 18732: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18733: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18734: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18735: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18736: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18737: train loss: 0.1716, test loss 0.6842\n",
      "Epoch 18738: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18739: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18740: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18741: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18742: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18743: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18744: train loss: 0.1716, test loss 0.6841\n",
      "Epoch 18745: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18746: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18747: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18748: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18749: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18750: train loss: 0.1716, test loss 0.6840\n",
      "Epoch 18751: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18752: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18753: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18754: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18755: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18756: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18757: train loss: 0.1716, test loss 0.6839\n",
      "Epoch 18758: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18759: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18760: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18761: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18762: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18763: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18764: train loss: 0.1716, test loss 0.6838\n",
      "Epoch 18765: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18766: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18767: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18768: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18769: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18770: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18771: train loss: 0.1716, test loss 0.6837\n",
      "Epoch 18772: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18773: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18774: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18775: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18776: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18777: train loss: 0.1716, test loss 0.6836\n",
      "Epoch 18778: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18779: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18780: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18781: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18782: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18783: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18784: train loss: 0.1716, test loss 0.6835\n",
      "Epoch 18785: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18786: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18787: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18788: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18789: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18790: train loss: 0.1716, test loss 0.6834\n",
      "Epoch 18791: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18792: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18793: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18794: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18795: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18796: train loss: 0.1716, test loss 0.6833\n",
      "Epoch 18797: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18798: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18799: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18800: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18801: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18802: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18803: train loss: 0.1716, test loss 0.6832\n",
      "Epoch 18804: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18805: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18806: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18807: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18808: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18809: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18810: train loss: 0.1716, test loss 0.6831\n",
      "Epoch 18811: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18812: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18813: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18814: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18815: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18816: train loss: 0.1716, test loss 0.6830\n",
      "Epoch 18817: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18818: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18819: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18820: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18821: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18822: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18823: train loss: 0.1716, test loss 0.6829\n",
      "Epoch 18824: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18825: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18826: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18827: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18828: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18829: train loss: 0.1716, test loss 0.6828\n",
      "Epoch 18830: train loss: 0.1716, test loss 0.6827\n",
      "Epoch 18831: train loss: 0.1716, test loss 0.6827\n",
      "Epoch 18832: train loss: 0.1716, test loss 0.6827\n",
      "Epoch 18833: train loss: 0.1716, test loss 0.6827\n",
      "Epoch 18834: train loss: 0.1715, test loss 0.6827\n",
      "Epoch 18835: train loss: 0.1715, test loss 0.6827\n",
      "Epoch 18836: train loss: 0.1715, test loss 0.6827\n",
      "Epoch 18837: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18838: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18839: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18840: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18841: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18842: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18843: train loss: 0.1715, test loss 0.6826\n",
      "Epoch 18844: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18845: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18846: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18847: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18848: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18849: train loss: 0.1715, test loss 0.6825\n",
      "Epoch 18850: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18851: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18852: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18853: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18854: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18855: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18856: train loss: 0.1715, test loss 0.6824\n",
      "Epoch 18857: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18858: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18859: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18860: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18861: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18862: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18863: train loss: 0.1715, test loss 0.6823\n",
      "Epoch 18864: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18865: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18866: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18867: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18868: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18869: train loss: 0.1715, test loss 0.6822\n",
      "Epoch 18870: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18871: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18872: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18873: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18874: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18875: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18876: train loss: 0.1715, test loss 0.6821\n",
      "Epoch 18877: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18878: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18879: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18880: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18881: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18882: train loss: 0.1715, test loss 0.6820\n",
      "Epoch 18883: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18884: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18885: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18886: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18887: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18888: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18889: train loss: 0.1715, test loss 0.6819\n",
      "Epoch 18890: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18891: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18892: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18893: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18894: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18895: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18896: train loss: 0.1715, test loss 0.6818\n",
      "Epoch 18897: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18898: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18899: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18900: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18901: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18902: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18903: train loss: 0.1715, test loss 0.6817\n",
      "Epoch 18904: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18905: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18906: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18907: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18908: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18909: train loss: 0.1715, test loss 0.6816\n",
      "Epoch 18910: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18911: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18912: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18913: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18914: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18915: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18916: train loss: 0.1715, test loss 0.6815\n",
      "Epoch 18917: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18918: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18919: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18920: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18921: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18922: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18923: train loss: 0.1715, test loss 0.6814\n",
      "Epoch 18924: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18925: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18926: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18927: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18928: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18929: train loss: 0.1715, test loss 0.6813\n",
      "Epoch 18930: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18931: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18932: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18933: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18934: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18935: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18936: train loss: 0.1715, test loss 0.6812\n",
      "Epoch 18937: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18938: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18939: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18940: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18941: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18942: train loss: 0.1715, test loss 0.6811\n",
      "Epoch 18943: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18944: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18945: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18946: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18947: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18948: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18949: train loss: 0.1715, test loss 0.6810\n",
      "Epoch 18950: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18951: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18952: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18953: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18954: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18955: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18956: train loss: 0.1715, test loss 0.6809\n",
      "Epoch 18957: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18958: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18959: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18960: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18961: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18962: train loss: 0.1715, test loss 0.6808\n",
      "Epoch 18963: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18964: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18965: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18966: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18967: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18968: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18969: train loss: 0.1715, test loss 0.6807\n",
      "Epoch 18970: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18971: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18972: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18973: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18974: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18975: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18976: train loss: 0.1715, test loss 0.6806\n",
      "Epoch 18977: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18978: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18979: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18980: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18981: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18982: train loss: 0.1715, test loss 0.6805\n",
      "Epoch 18983: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18984: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18985: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18986: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18987: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18988: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18989: train loss: 0.1715, test loss 0.6804\n",
      "Epoch 18990: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18991: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18992: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18993: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18994: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18995: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18996: train loss: 0.1715, test loss 0.6803\n",
      "Epoch 18997: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 18998: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 18999: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 19000: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 19001: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 19002: train loss: 0.1715, test loss 0.6802\n",
      "Epoch 19003: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19004: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19005: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19006: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19007: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19008: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19009: train loss: 0.1715, test loss 0.6801\n",
      "Epoch 19010: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19011: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19012: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19013: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19014: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19015: train loss: 0.1715, test loss 0.6800\n",
      "Epoch 19016: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19017: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19018: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19019: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19020: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19021: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19022: train loss: 0.1715, test loss 0.6799\n",
      "Epoch 19023: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19024: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19025: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19026: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19027: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19028: train loss: 0.1715, test loss 0.6798\n",
      "Epoch 19029: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19030: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19031: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19032: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19033: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19034: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19035: train loss: 0.1715, test loss 0.6797\n",
      "Epoch 19036: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19037: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19038: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19039: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19040: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19041: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19042: train loss: 0.1715, test loss 0.6796\n",
      "Epoch 19043: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19044: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19045: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19046: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19047: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19048: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19049: train loss: 0.1715, test loss 0.6795\n",
      "Epoch 19050: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19051: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19052: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19053: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19054: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19055: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19056: train loss: 0.1715, test loss 0.6794\n",
      "Epoch 19057: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19058: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19059: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19060: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19061: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19062: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19063: train loss: 0.1715, test loss 0.6793\n",
      "Epoch 19064: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19065: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19066: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19067: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19068: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19069: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19070: train loss: 0.1715, test loss 0.6792\n",
      "Epoch 19071: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19072: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19073: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19074: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19075: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19076: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19077: train loss: 0.1715, test loss 0.6791\n",
      "Epoch 19078: train loss: 0.1715, test loss 0.6790\n",
      "Epoch 19079: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19080: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19081: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19082: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19083: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19084: train loss: 0.1714, test loss 0.6790\n",
      "Epoch 19085: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19086: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19087: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19088: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19089: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19090: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19091: train loss: 0.1714, test loss 0.6789\n",
      "Epoch 19092: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19093: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19094: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19095: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19096: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19097: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19098: train loss: 0.1714, test loss 0.6788\n",
      "Epoch 19099: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19100: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19101: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19102: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19103: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19104: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19105: train loss: 0.1714, test loss 0.6787\n",
      "Epoch 19106: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19107: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19108: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19109: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19110: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19111: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19112: train loss: 0.1714, test loss 0.6786\n",
      "Epoch 19113: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19114: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19115: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19116: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19117: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19118: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19119: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19120: train loss: 0.1714, test loss 0.6785\n",
      "Epoch 19121: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19122: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19123: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19124: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19125: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19126: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19127: train loss: 0.1714, test loss 0.6784\n",
      "Epoch 19128: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19129: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19130: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19131: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19132: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19133: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19134: train loss: 0.1714, test loss 0.6783\n",
      "Epoch 19135: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19136: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19137: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19138: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19139: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19140: train loss: 0.1714, test loss 0.6782\n",
      "Epoch 19141: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19142: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19143: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19144: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19145: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19146: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19147: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19148: train loss: 0.1714, test loss 0.6781\n",
      "Epoch 19149: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19150: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19151: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19152: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19153: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19154: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19155: train loss: 0.1714, test loss 0.6780\n",
      "Epoch 19156: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19157: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19158: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19159: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19160: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19161: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19162: train loss: 0.1714, test loss 0.6779\n",
      "Epoch 19163: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19164: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19165: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19166: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19167: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19168: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19169: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19170: train loss: 0.1714, test loss 0.6778\n",
      "Epoch 19171: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19172: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19173: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19174: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19175: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19176: train loss: 0.1714, test loss 0.6777\n",
      "Epoch 19177: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19178: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19179: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19180: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19181: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19182: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19183: train loss: 0.1714, test loss 0.6776\n",
      "Epoch 19184: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19185: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19186: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19187: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19188: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19189: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19190: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19191: train loss: 0.1714, test loss 0.6775\n",
      "Epoch 19192: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19193: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19194: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19195: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19196: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19197: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19198: train loss: 0.1714, test loss 0.6774\n",
      "Epoch 19199: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19200: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19201: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19202: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19203: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19204: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19205: train loss: 0.1714, test loss 0.6773\n",
      "Epoch 19206: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19207: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19208: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19209: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19210: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19211: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19212: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19213: train loss: 0.1714, test loss 0.6772\n",
      "Epoch 19214: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19215: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19216: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19217: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19218: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19219: train loss: 0.1714, test loss 0.6771\n",
      "Epoch 19220: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19221: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19222: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19223: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19224: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19225: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19226: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19227: train loss: 0.1714, test loss 0.6770\n",
      "Epoch 19228: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19229: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19230: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19231: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19232: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19233: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19234: train loss: 0.1714, test loss 0.6769\n",
      "Epoch 19235: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19236: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19237: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19238: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19239: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19240: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19241: train loss: 0.1714, test loss 0.6768\n",
      "Epoch 19242: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19243: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19244: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19245: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19246: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19247: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19248: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19249: train loss: 0.1714, test loss 0.6767\n",
      "Epoch 19250: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19251: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19252: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19253: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19254: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19255: train loss: 0.1714, test loss 0.6766\n",
      "Epoch 19256: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19257: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19258: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19259: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19260: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19261: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19262: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19263: train loss: 0.1714, test loss 0.6765\n",
      "Epoch 19264: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19265: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19266: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19267: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19268: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19269: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19270: train loss: 0.1714, test loss 0.6764\n",
      "Epoch 19271: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19272: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19273: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19274: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19275: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19276: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19277: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19278: train loss: 0.1714, test loss 0.6763\n",
      "Epoch 19279: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19280: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19281: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19282: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19283: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19284: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19285: train loss: 0.1714, test loss 0.6762\n",
      "Epoch 19286: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19287: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19288: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19289: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19290: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19291: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19292: train loss: 0.1714, test loss 0.6761\n",
      "Epoch 19293: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19294: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19295: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19296: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19297: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19298: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19299: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19300: train loss: 0.1714, test loss 0.6760\n",
      "Epoch 19301: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19302: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19303: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19304: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19305: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19306: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19307: train loss: 0.1714, test loss 0.6759\n",
      "Epoch 19308: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19309: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19310: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19311: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19312: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19313: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19314: train loss: 0.1714, test loss 0.6758\n",
      "Epoch 19315: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19316: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19317: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19318: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19319: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19320: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19321: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19322: train loss: 0.1714, test loss 0.6757\n",
      "Epoch 19323: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19324: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19325: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19326: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19327: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19328: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19329: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19330: train loss: 0.1714, test loss 0.6756\n",
      "Epoch 19331: train loss: 0.1714, test loss 0.6755\n",
      "Epoch 19332: train loss: 0.1714, test loss 0.6755\n",
      "Epoch 19333: train loss: 0.1714, test loss 0.6755\n",
      "Epoch 19334: train loss: 0.1713, test loss 0.6755\n",
      "Epoch 19335: train loss: 0.1713, test loss 0.6755\n",
      "Epoch 19336: train loss: 0.1713, test loss 0.6755\n",
      "Epoch 19337: train loss: 0.1713, test loss 0.6755\n",
      "Epoch 19338: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19339: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19340: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19341: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19342: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19343: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19344: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19345: train loss: 0.1713, test loss 0.6754\n",
      "Epoch 19346: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19347: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19348: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19349: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19350: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19351: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19352: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19353: train loss: 0.1713, test loss 0.6753\n",
      "Epoch 19354: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19355: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19356: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19357: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19358: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19359: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19360: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19361: train loss: 0.1713, test loss 0.6752\n",
      "Epoch 19362: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19363: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19364: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19365: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19366: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19367: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19368: train loss: 0.1713, test loss 0.6751\n",
      "Epoch 19369: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19370: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19371: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19372: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19373: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19374: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19375: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19376: train loss: 0.1713, test loss 0.6750\n",
      "Epoch 19377: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19378: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19379: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19380: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19381: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19382: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19383: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19384: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19385: train loss: 0.1713, test loss 0.6749\n",
      "Epoch 19386: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19387: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19388: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19389: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19390: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19391: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19392: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19393: train loss: 0.1713, test loss 0.6748\n",
      "Epoch 19394: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19395: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19396: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19397: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19398: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19399: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19400: train loss: 0.1713, test loss 0.6747\n",
      "Epoch 19401: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19402: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19403: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19404: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19405: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19406: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19407: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19408: train loss: 0.1713, test loss 0.6746\n",
      "Epoch 19409: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19410: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19411: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19412: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19413: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19414: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19415: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19416: train loss: 0.1713, test loss 0.6745\n",
      "Epoch 19417: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19418: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19419: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19420: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19421: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19422: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19423: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19424: train loss: 0.1713, test loss 0.6744\n",
      "Epoch 19425: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19426: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19427: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19428: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19429: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19430: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19431: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19432: train loss: 0.1713, test loss 0.6743\n",
      "Epoch 19433: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19434: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19435: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19436: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19437: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19438: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19439: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19440: train loss: 0.1713, test loss 0.6742\n",
      "Epoch 19441: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19442: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19443: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19444: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19445: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19446: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19447: train loss: 0.1713, test loss 0.6741\n",
      "Epoch 19448: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19449: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19450: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19451: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19452: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19453: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19454: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19455: train loss: 0.1713, test loss 0.6740\n",
      "Epoch 19456: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19457: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19458: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19459: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19460: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19461: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19462: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19463: train loss: 0.1713, test loss 0.6739\n",
      "Epoch 19464: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19465: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19466: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19467: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19468: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19469: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19470: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19471: train loss: 0.1713, test loss 0.6738\n",
      "Epoch 19472: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19473: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19474: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19475: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19476: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19477: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19478: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19479: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19480: train loss: 0.1713, test loss 0.6737\n",
      "Epoch 19481: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19482: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19483: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19484: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19485: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19486: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19487: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19488: train loss: 0.1713, test loss 0.6736\n",
      "Epoch 19489: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19490: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19491: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19492: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19493: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19494: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19495: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19496: train loss: 0.1713, test loss 0.6735\n",
      "Epoch 19497: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19498: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19499: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19500: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19501: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19502: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19503: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19504: train loss: 0.1713, test loss 0.6734\n",
      "Epoch 19505: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19506: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19507: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19508: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19509: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19510: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19511: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19512: train loss: 0.1713, test loss 0.6733\n",
      "Epoch 19513: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19514: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19515: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19516: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19517: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19518: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19519: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19520: train loss: 0.1713, test loss 0.6732\n",
      "Epoch 19521: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19522: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19523: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19524: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19525: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19526: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19527: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19528: train loss: 0.1713, test loss 0.6731\n",
      "Epoch 19529: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19530: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19531: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19532: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19533: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19534: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19535: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19536: train loss: 0.1713, test loss 0.6730\n",
      "Epoch 19537: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19538: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19539: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19540: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19541: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19542: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19543: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19544: train loss: 0.1713, test loss 0.6729\n",
      "Epoch 19545: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19546: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19547: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19548: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19549: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19550: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19551: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19552: train loss: 0.1713, test loss 0.6728\n",
      "Epoch 19553: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19554: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19555: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19556: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19557: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19558: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19559: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19560: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19561: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19562: train loss: 0.1713, test loss 0.6727\n",
      "Epoch 19563: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19564: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19565: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19566: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19567: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19568: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19569: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19570: train loss: 0.1713, test loss 0.6726\n",
      "Epoch 19571: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19572: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19573: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19574: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19575: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19576: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19577: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19578: train loss: 0.1713, test loss 0.6725\n",
      "Epoch 19579: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19580: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19581: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19582: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19583: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19584: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19585: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19586: train loss: 0.1713, test loss 0.6724\n",
      "Epoch 19587: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19588: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19589: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19590: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19591: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19592: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19593: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19594: train loss: 0.1713, test loss 0.6723\n",
      "Epoch 19595: train loss: 0.1713, test loss 0.6722\n",
      "Epoch 19596: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19597: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19598: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19599: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19600: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19601: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19602: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19603: train loss: 0.1712, test loss 0.6722\n",
      "Epoch 19604: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19605: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19606: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19607: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19608: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19609: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19610: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19611: train loss: 0.1712, test loss 0.6721\n",
      "Epoch 19612: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19613: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19614: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19615: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19616: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19617: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19618: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19619: train loss: 0.1712, test loss 0.6720\n",
      "Epoch 19620: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19621: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19622: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19623: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19624: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19625: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19626: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19627: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19628: train loss: 0.1712, test loss 0.6719\n",
      "Epoch 19629: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19630: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19631: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19632: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19633: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19634: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19635: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19636: train loss: 0.1712, test loss 0.6718\n",
      "Epoch 19637: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19638: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19639: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19640: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19641: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19642: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19643: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19644: train loss: 0.1712, test loss 0.6717\n",
      "Epoch 19645: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19646: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19647: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19648: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19649: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19650: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19651: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19652: train loss: 0.1712, test loss 0.6716\n",
      "Epoch 19653: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19654: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19655: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19656: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19657: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19658: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19659: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19660: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19661: train loss: 0.1712, test loss 0.6715\n",
      "Epoch 19662: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19663: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19664: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19665: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19666: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19667: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19668: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19669: train loss: 0.1712, test loss 0.6714\n",
      "Epoch 19670: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19671: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19672: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19673: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19674: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19675: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19676: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19677: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19678: train loss: 0.1712, test loss 0.6713\n",
      "Epoch 19679: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19680: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19681: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19682: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19683: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19684: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19685: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19686: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19687: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19688: train loss: 0.1712, test loss 0.6712\n",
      "Epoch 19689: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19690: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19691: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19692: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19693: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19694: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19695: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19696: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19697: train loss: 0.1712, test loss 0.6711\n",
      "Epoch 19698: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19699: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19700: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19701: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19702: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19703: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19704: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19705: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19706: train loss: 0.1712, test loss 0.6710\n",
      "Epoch 19707: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19708: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19709: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19710: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19711: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19712: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19713: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19714: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19715: train loss: 0.1712, test loss 0.6709\n",
      "Epoch 19716: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19717: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19718: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19719: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19720: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19721: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19722: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19723: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19724: train loss: 0.1712, test loss 0.6708\n",
      "Epoch 19725: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19726: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19727: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19728: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19729: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19730: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19731: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19732: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19733: train loss: 0.1712, test loss 0.6707\n",
      "Epoch 19734: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19735: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19736: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19737: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19738: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19739: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19740: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19741: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19742: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19743: train loss: 0.1712, test loss 0.6706\n",
      "Epoch 19744: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19745: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19746: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19747: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19748: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19749: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19750: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19751: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19752: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19753: train loss: 0.1712, test loss 0.6705\n",
      "Epoch 19754: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19755: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19756: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19757: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19758: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19759: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19760: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19761: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19762: train loss: 0.1712, test loss 0.6704\n",
      "Epoch 19763: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19764: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19765: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19766: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19767: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19768: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19769: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19770: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19771: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19772: train loss: 0.1712, test loss 0.6703\n",
      "Epoch 19773: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19774: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19775: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19776: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19777: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19778: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19779: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19780: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19781: train loss: 0.1712, test loss 0.6702\n",
      "Epoch 19782: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19783: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19784: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19785: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19786: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19787: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19788: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19789: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19790: train loss: 0.1712, test loss 0.6701\n",
      "Epoch 19791: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19792: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19793: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19794: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19795: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19796: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19797: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19798: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19799: train loss: 0.1712, test loss 0.6700\n",
      "Epoch 19800: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19801: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19802: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19803: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19804: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19805: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19806: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19807: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19808: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19809: train loss: 0.1712, test loss 0.6699\n",
      "Epoch 19810: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19811: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19812: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19813: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19814: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19815: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19816: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19817: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19818: train loss: 0.1712, test loss 0.6698\n",
      "Epoch 19819: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19820: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19821: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19822: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19823: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19824: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19825: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19826: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19827: train loss: 0.1712, test loss 0.6697\n",
      "Epoch 19828: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19829: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19830: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19831: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19832: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19833: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19834: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19835: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19836: train loss: 0.1712, test loss 0.6696\n",
      "Epoch 19837: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19838: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19839: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19840: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19841: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19842: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19843: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19844: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19845: train loss: 0.1712, test loss 0.6695\n",
      "Epoch 19846: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19847: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19848: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19849: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19850: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19851: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19852: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19853: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19854: train loss: 0.1712, test loss 0.6694\n",
      "Epoch 19855: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19856: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19857: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19858: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19859: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19860: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19861: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19862: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19863: train loss: 0.1712, test loss 0.6693\n",
      "Epoch 19864: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19865: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19866: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19867: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19868: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19869: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19870: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19871: train loss: 0.1711, test loss 0.6692\n",
      "Epoch 19872: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19873: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19874: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19875: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19876: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19877: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19878: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19879: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19880: train loss: 0.1711, test loss 0.6691\n",
      "Epoch 19881: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19882: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19883: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19884: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19885: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19886: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19887: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19888: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19889: train loss: 0.1711, test loss 0.6690\n",
      "Epoch 19890: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19891: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19892: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19893: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19894: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19895: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19896: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19897: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19898: train loss: 0.1711, test loss 0.6689\n",
      "Epoch 19899: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19900: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19901: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19902: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19903: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19904: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19905: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19906: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19907: train loss: 0.1711, test loss 0.6688\n",
      "Epoch 19908: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19909: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19910: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19911: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19912: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19913: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19914: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19915: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19916: train loss: 0.1711, test loss 0.6687\n",
      "Epoch 19917: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19918: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19919: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19920: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19921: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19922: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19923: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19924: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19925: train loss: 0.1711, test loss 0.6686\n",
      "Epoch 19926: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19927: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19928: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19929: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19930: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19931: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19932: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19933: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19934: train loss: 0.1711, test loss 0.6685\n",
      "Epoch 19935: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19936: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19937: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19938: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19939: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19940: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19941: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19942: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19943: train loss: 0.1711, test loss 0.6684\n",
      "Epoch 19944: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19945: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19946: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19947: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19948: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19949: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19950: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19951: train loss: 0.1711, test loss 0.6683\n",
      "Epoch 19952: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19953: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19954: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19955: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19956: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19957: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19958: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19959: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19960: train loss: 0.1711, test loss 0.6682\n",
      "Epoch 19961: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19962: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19963: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19964: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19965: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19966: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19967: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19968: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19969: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19970: train loss: 0.1711, test loss 0.6681\n",
      "Epoch 19971: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19972: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19973: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19974: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19975: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19976: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19977: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19978: train loss: 0.1711, test loss 0.6680\n",
      "Epoch 19979: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19980: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19981: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19982: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19983: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19984: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19985: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19986: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19987: train loss: 0.1711, test loss 0.6679\n",
      "Epoch 19988: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19989: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19990: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19991: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19992: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19993: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19994: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19995: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19996: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19997: train loss: 0.1711, test loss 0.6678\n",
      "Epoch 19998: train loss: 0.1711, test loss 0.6677\n",
      "Epoch 19999: train loss: 0.1711, test loss 0.6677\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "loss_train_array = []\n",
    "loss_test_array = []\n",
    "epoch = 20000\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss_train = loss_fn(model(X_train), Y_train)\n",
    "    loss_train_array.append(loss_train)\n",
    "    loss_test = loss_fn(model(X_test), Y_test)\n",
    "    loss_test_array.append(loss_test)\n",
    "\n",
    "   \n",
    "    print('Epoch %s: train loss: %6.4f, test loss %6.4f'%(epoch, loss_train.item(), loss_test.item()))\n",
    "    # Backward pass\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,3), dpi=120)\n",
    "plt.plot(loss_train_array)\n",
    "plt.plot(loss_test_array)\n",
    "plt.legend(['train loss', 'test loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing for each cordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_pred = model(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Pred'] = X_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_co_ord = OrderedDict()\n",
    "for co_ord, df_c in df_test.groupby(['lat','lon']):\n",
    "    loss = (df_c['Pred'] - df_c['lambda'])**2\n",
    "    loss_co_ord[str(co_ord)] = loss\n",
    "    # loss_co_ord.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,3), dpi=120)\n",
    "plt.bar(loss_co_ord.keys(), [x.mean() for x in loss_co_ord.values()])\n",
    "# notch shape box plot\n",
    "# plt.boxplot(loss_co_ord.values(),\n",
    "#                      notch=True,  # notch shape\n",
    "#                      vert=True,  # vertical box alignment\n",
    "#                      patch_artist=True,  # fill with color\n",
    "#                      labels=loss_co_ord.keys())  # will be used to label x-ticks\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model loss per co-ordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0aed7f3f2602e9ffc7bcdb3e0077e6e7eb290cd9bde9bdf0f85d0264c7b32cc9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
