{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('my_df_train.pickle')\n",
    "df_test = pd.read_pickle('my_df_test.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 1) (729, 1) (729, 60) (729, 60)\n",
      "(2513, 122) (2513,)\n",
      "(729, 122) (729,)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "inj_vol = np.stack(df_train['inj_vol'].values)\n",
    "inj_vol = inj_vol.reshape(*inj_vol.shape[:-2], -1)\n",
    "pp = np.stack(df_train['pp'].values)\n",
    "pp = pp.reshape(*pp.shape[:-2], -1)\n",
    "lat = np.stack(df_train['lat'].values)\n",
    "lat = np.reshape(lat,(-1,1))\n",
    "lon = np.stack(df_train['lon'].values)\n",
    "lon = np.reshape(lon,(-1,1))\n",
    "\n",
    "\n",
    "# (2814, 1) (2814, 1) (2814, 12, 1) (2814, 12, 1)\n",
    "# (2513, 1) (2513, 1) (2513, 12, 5) (2513, 12, 5)\n",
    "# dim: 2814 x 24\n",
    "x_train = np.concatenate([lat, lon, inj_vol, pp], axis =1)\n",
    "y_train = df_train['lambda'].values\n",
    "\n",
    "# test\n",
    "inj_vol = np.stack(df_test['inj_vol'].values)\n",
    "inj_vol = inj_vol.reshape(*inj_vol.shape[:-2], -1)\n",
    "pp = np.stack(df_test['pp'].values)\n",
    "pp = pp.reshape(*pp.shape[:-2], -1)\n",
    "lat = np.stack(df_test['lat'].values)\n",
    "lat = np.reshape(lat,(-1,1))\n",
    "lon = np.stack(df_test['lon'].values)\n",
    "lon = np.reshape(lon,(-1,1))\n",
    "\n",
    "print(lat.shape,lon.shape,inj_vol.shape,pp.shape)\n",
    "# dim: _ x 24\n",
    "x_test = np.concatenate([lat, lon, inj_vol, pp], axis =1)\n",
    "y_test = df_test['lambda'].values\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        # create layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train test data to torch tensors\n",
    "X_train = torch.from_numpy(x_train.astype('float32'))\n",
    "Y_train = torch.from_numpy(np.expand_dims(y_train, axis=-1).astype('float32'))\n",
    "X_test = torch.from_numpy(x_test.astype('float32'))\n",
    "Y_test = torch.from_numpy(np.expand_dims(y_test, axis=-1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2513, 122]) torch.Size([2513, 1]) torch.float32 torch.float32\n",
      "torch.Size([729, 122]) torch.Size([729, 1]) torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_train.dtype, Y_train.dtype)\n",
    "print(X_test.shape, Y_test.shape, X_test.dtype, Y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for initializing model weights\n",
    "torch.manual_seed(12345)\n",
    "model = Feedforward(x_train.shape[1], 32)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 8.4454, test loss 14.7466\n",
      "Epoch 1: train loss: 94.7061, test loss 107.3904\n",
      "Epoch 2: train loss: 6.5240, test loss 10.2844\n",
      "Epoch 3: train loss: 33.4334, test loss 31.9682\n",
      "Epoch 4: train loss: 27.3766, test loss 26.0774\n",
      "Epoch 5: train loss: 11.3613, test loss 11.1005\n",
      "Epoch 6: train loss: 3.2910, test loss 3.6137\n",
      "Epoch 7: train loss: 1.0911, test loss 1.5162\n",
      "Epoch 8: train loss: 0.9966, test loss 1.4797\n",
      "Epoch 9: train loss: 1.4742, test loss 1.9070\n",
      "Epoch 10: train loss: 1.7304, test loss 2.1719\n",
      "Epoch 11: train loss: 1.6956, test loss 2.2649\n",
      "Epoch 12: train loss: 1.5609, test loss 2.2132\n",
      "Epoch 13: train loss: 1.3781, test loss 2.0443\n",
      "Epoch 14: train loss: 1.1814, test loss 1.8105\n",
      "Epoch 15: train loss: 0.9963, test loss 1.5735\n",
      "Epoch 16: train loss: 0.8375, test loss 1.3520\n",
      "Epoch 17: train loss: 0.7069, test loss 1.1536\n",
      "Epoch 18: train loss: 0.6042, test loss 0.9826\n",
      "Epoch 19: train loss: 0.5251, test loss 0.8370\n",
      "Epoch 20: train loss: 0.4655, test loss 0.7152\n",
      "Epoch 21: train loss: 0.4205, test loss 0.6137\n",
      "Epoch 22: train loss: 0.3870, test loss 0.5287\n",
      "Epoch 23: train loss: 0.3625, test loss 0.4593\n",
      "Epoch 24: train loss: 0.3443, test loss 0.4032\n",
      "Epoch 25: train loss: 0.3311, test loss 0.3575\n",
      "Epoch 26: train loss: 0.3211, test loss 0.3201\n",
      "Epoch 27: train loss: 0.3130, test loss 0.2895\n",
      "Epoch 28: train loss: 0.3066, test loss 0.2641\n",
      "Epoch 29: train loss: 0.3014, test loss 0.2430\n",
      "Epoch 30: train loss: 0.2970, test loss 0.2253\n",
      "Epoch 31: train loss: 0.2932, test loss 0.2103\n",
      "Epoch 32: train loss: 0.2897, test loss 0.1975\n",
      "Epoch 33: train loss: 0.2865, test loss 0.1864\n",
      "Epoch 34: train loss: 0.2835, test loss 0.1768\n",
      "Epoch 35: train loss: 0.2807, test loss 0.1685\n",
      "Epoch 36: train loss: 0.2782, test loss 0.1611\n",
      "Epoch 37: train loss: 0.2757, test loss 0.1543\n",
      "Epoch 38: train loss: 0.2734, test loss 0.1482\n",
      "Epoch 39: train loss: 0.2712, test loss 0.1427\n",
      "Epoch 40: train loss: 0.2692, test loss 0.1378\n",
      "Epoch 41: train loss: 0.2672, test loss 0.1333\n",
      "Epoch 42: train loss: 0.2652, test loss 0.1293\n",
      "Epoch 43: train loss: 0.2634, test loss 0.1255\n",
      "Epoch 44: train loss: 0.2617, test loss 0.1220\n",
      "Epoch 45: train loss: 0.2600, test loss 0.1188\n",
      "Epoch 46: train loss: 0.2583, test loss 0.1159\n",
      "Epoch 47: train loss: 0.2568, test loss 0.1132\n",
      "Epoch 48: train loss: 0.2552, test loss 0.1107\n",
      "Epoch 49: train loss: 0.2538, test loss 0.1085\n",
      "Epoch 50: train loss: 0.2524, test loss 0.1064\n",
      "Epoch 51: train loss: 0.2510, test loss 0.1044\n",
      "Epoch 52: train loss: 0.2497, test loss 0.1026\n",
      "Epoch 53: train loss: 0.2483, test loss 0.1009\n",
      "Epoch 54: train loss: 0.2471, test loss 0.0993\n",
      "Epoch 55: train loss: 0.2459, test loss 0.0979\n",
      "Epoch 56: train loss: 0.2447, test loss 0.0965\n",
      "Epoch 57: train loss: 0.2435, test loss 0.0952\n",
      "Epoch 58: train loss: 0.2423, test loss 0.0940\n",
      "Epoch 59: train loss: 0.2412, test loss 0.0929\n",
      "Epoch 60: train loss: 0.2401, test loss 0.0918\n",
      "Epoch 61: train loss: 0.2391, test loss 0.0908\n",
      "Epoch 62: train loss: 0.2380, test loss 0.0899\n",
      "Epoch 63: train loss: 0.2370, test loss 0.0890\n",
      "Epoch 64: train loss: 0.2360, test loss 0.0882\n",
      "Epoch 65: train loss: 0.2350, test loss 0.0874\n",
      "Epoch 66: train loss: 0.2340, test loss 0.0866\n",
      "Epoch 67: train loss: 0.2330, test loss 0.0859\n",
      "Epoch 68: train loss: 0.2320, test loss 0.0852\n",
      "Epoch 69: train loss: 0.2311, test loss 0.0845\n",
      "Epoch 70: train loss: 0.2302, test loss 0.0839\n",
      "Epoch 71: train loss: 0.2292, test loss 0.0833\n",
      "Epoch 72: train loss: 0.2283, test loss 0.0827\n",
      "Epoch 73: train loss: 0.2274, test loss 0.0822\n",
      "Epoch 74: train loss: 0.2265, test loss 0.0817\n",
      "Epoch 75: train loss: 0.2256, test loss 0.0811\n",
      "Epoch 76: train loss: 0.2247, test loss 0.0807\n",
      "Epoch 77: train loss: 0.2239, test loss 0.0802\n",
      "Epoch 78: train loss: 0.2230, test loss 0.0797\n",
      "Epoch 79: train loss: 0.2221, test loss 0.0793\n",
      "Epoch 80: train loss: 0.2213, test loss 0.0789\n",
      "Epoch 81: train loss: 0.2205, test loss 0.0784\n",
      "Epoch 82: train loss: 0.2196, test loss 0.0780\n",
      "Epoch 83: train loss: 0.2188, test loss 0.0776\n",
      "Epoch 84: train loss: 0.2180, test loss 0.0773\n",
      "Epoch 85: train loss: 0.2172, test loss 0.0769\n",
      "Epoch 86: train loss: 0.2164, test loss 0.0765\n",
      "Epoch 87: train loss: 0.2156, test loss 0.0762\n",
      "Epoch 88: train loss: 0.2148, test loss 0.0759\n",
      "Epoch 89: train loss: 0.2141, test loss 0.0755\n",
      "Epoch 90: train loss: 0.2133, test loss 0.0752\n",
      "Epoch 91: train loss: 0.2126, test loss 0.0749\n",
      "Epoch 92: train loss: 0.2118, test loss 0.0746\n",
      "Epoch 93: train loss: 0.2111, test loss 0.0743\n",
      "Epoch 94: train loss: 0.2103, test loss 0.0741\n",
      "Epoch 95: train loss: 0.2096, test loss 0.0738\n",
      "Epoch 96: train loss: 0.2089, test loss 0.0735\n",
      "Epoch 97: train loss: 0.2082, test loss 0.0733\n",
      "Epoch 98: train loss: 0.2075, test loss 0.0730\n",
      "Epoch 99: train loss: 0.2068, test loss 0.0728\n",
      "Epoch 100: train loss: 0.2061, test loss 0.0726\n",
      "Epoch 101: train loss: 0.2055, test loss 0.0724\n",
      "Epoch 102: train loss: 0.2048, test loss 0.0721\n",
      "Epoch 103: train loss: 0.2041, test loss 0.0719\n",
      "Epoch 104: train loss: 0.2035, test loss 0.0717\n",
      "Epoch 105: train loss: 0.2028, test loss 0.0716\n",
      "Epoch 106: train loss: 0.2022, test loss 0.0714\n",
      "Epoch 107: train loss: 0.2016, test loss 0.0712\n",
      "Epoch 108: train loss: 0.2010, test loss 0.0710\n",
      "Epoch 109: train loss: 0.2004, test loss 0.0709\n",
      "Epoch 110: train loss: 0.1998, test loss 0.0707\n",
      "Epoch 111: train loss: 0.1992, test loss 0.0705\n",
      "Epoch 112: train loss: 0.1986, test loss 0.0704\n",
      "Epoch 113: train loss: 0.1980, test loss 0.0703\n",
      "Epoch 114: train loss: 0.1974, test loss 0.0701\n",
      "Epoch 115: train loss: 0.1969, test loss 0.0700\n",
      "Epoch 116: train loss: 0.1963, test loss 0.0699\n",
      "Epoch 117: train loss: 0.1958, test loss 0.0698\n",
      "Epoch 118: train loss: 0.1952, test loss 0.0697\n",
      "Epoch 119: train loss: 0.1947, test loss 0.0696\n",
      "Epoch 120: train loss: 0.1942, test loss 0.0695\n",
      "Epoch 121: train loss: 0.1937, test loss 0.0694\n",
      "Epoch 122: train loss: 0.1932, test loss 0.0693\n",
      "Epoch 123: train loss: 0.1927, test loss 0.0692\n",
      "Epoch 124: train loss: 0.1922, test loss 0.0691\n",
      "Epoch 125: train loss: 0.1917, test loss 0.0691\n",
      "Epoch 126: train loss: 0.1912, test loss 0.0690\n",
      "Epoch 127: train loss: 0.1907, test loss 0.0690\n",
      "Epoch 128: train loss: 0.1903, test loss 0.0689\n",
      "Epoch 129: train loss: 0.1898, test loss 0.0688\n",
      "Epoch 130: train loss: 0.1894, test loss 0.0688\n",
      "Epoch 131: train loss: 0.1889, test loss 0.0688\n",
      "Epoch 132: train loss: 0.1885, test loss 0.0687\n",
      "Epoch 133: train loss: 0.1880, test loss 0.0687\n",
      "Epoch 134: train loss: 0.1876, test loss 0.0687\n",
      "Epoch 135: train loss: 0.1872, test loss 0.0687\n",
      "Epoch 136: train loss: 0.1868, test loss 0.0686\n",
      "Epoch 137: train loss: 0.1864, test loss 0.0686\n",
      "Epoch 138: train loss: 0.1860, test loss 0.0686\n",
      "Epoch 139: train loss: 0.1856, test loss 0.0686\n",
      "Epoch 140: train loss: 0.1852, test loss 0.0686\n",
      "Epoch 141: train loss: 0.1848, test loss 0.0686\n",
      "Epoch 142: train loss: 0.1844, test loss 0.0686\n",
      "Epoch 143: train loss: 0.1841, test loss 0.0687\n",
      "Epoch 144: train loss: 0.1837, test loss 0.0687\n",
      "Epoch 145: train loss: 0.1833, test loss 0.0687\n",
      "Epoch 146: train loss: 0.1830, test loss 0.0687\n",
      "Epoch 147: train loss: 0.1826, test loss 0.0688\n",
      "Epoch 148: train loss: 0.1823, test loss 0.0688\n",
      "Epoch 149: train loss: 0.1820, test loss 0.0688\n",
      "Epoch 150: train loss: 0.1816, test loss 0.0689\n",
      "Epoch 151: train loss: 0.1813, test loss 0.0689\n",
      "Epoch 152: train loss: 0.1810, test loss 0.0689\n",
      "Epoch 153: train loss: 0.1807, test loss 0.0690\n",
      "Epoch 154: train loss: 0.1804, test loss 0.0690\n",
      "Epoch 155: train loss: 0.1801, test loss 0.0691\n",
      "Epoch 156: train loss: 0.1797, test loss 0.0692\n",
      "Epoch 157: train loss: 0.1795, test loss 0.0692\n",
      "Epoch 158: train loss: 0.1792, test loss 0.0693\n",
      "Epoch 159: train loss: 0.1789, test loss 0.0693\n",
      "Epoch 160: train loss: 0.1786, test loss 0.0694\n",
      "Epoch 161: train loss: 0.1783, test loss 0.0695\n",
      "Epoch 162: train loss: 0.1780, test loss 0.0695\n",
      "Epoch 163: train loss: 0.1778, test loss 0.0696\n",
      "Epoch 164: train loss: 0.1775, test loss 0.0697\n",
      "Epoch 165: train loss: 0.1772, test loss 0.0698\n",
      "Epoch 166: train loss: 0.1770, test loss 0.0698\n",
      "Epoch 167: train loss: 0.1767, test loss 0.0699\n",
      "Epoch 168: train loss: 0.1765, test loss 0.0700\n",
      "Epoch 169: train loss: 0.1762, test loss 0.0701\n",
      "Epoch 170: train loss: 0.1760, test loss 0.0702\n",
      "Epoch 171: train loss: 0.1758, test loss 0.0703\n",
      "Epoch 172: train loss: 0.1755, test loss 0.0704\n",
      "Epoch 173: train loss: 0.1753, test loss 0.0704\n",
      "Epoch 174: train loss: 0.1751, test loss 0.0705\n",
      "Epoch 175: train loss: 0.1748, test loss 0.0706\n",
      "Epoch 176: train loss: 0.1746, test loss 0.0707\n",
      "Epoch 177: train loss: 0.1744, test loss 0.0708\n",
      "Epoch 178: train loss: 0.1742, test loss 0.0709\n",
      "Epoch 179: train loss: 0.1740, test loss 0.0710\n",
      "Epoch 180: train loss: 0.1738, test loss 0.0711\n",
      "Epoch 181: train loss: 0.1736, test loss 0.0712\n",
      "Epoch 182: train loss: 0.1734, test loss 0.0713\n",
      "Epoch 183: train loss: 0.1732, test loss 0.0714\n",
      "Epoch 184: train loss: 0.1730, test loss 0.0715\n",
      "Epoch 185: train loss: 0.1728, test loss 0.0716\n",
      "Epoch 186: train loss: 0.1726, test loss 0.0717\n",
      "Epoch 187: train loss: 0.1724, test loss 0.0719\n",
      "Epoch 188: train loss: 0.1722, test loss 0.0720\n",
      "Epoch 189: train loss: 0.1720, test loss 0.0721\n",
      "Epoch 190: train loss: 0.1718, test loss 0.0722\n",
      "Epoch 191: train loss: 0.1717, test loss 0.0723\n",
      "Epoch 192: train loss: 0.1715, test loss 0.0724\n",
      "Epoch 193: train loss: 0.1713, test loss 0.0725\n",
      "Epoch 194: train loss: 0.1711, test loss 0.0726\n",
      "Epoch 195: train loss: 0.1710, test loss 0.0727\n",
      "Epoch 196: train loss: 0.1708, test loss 0.0728\n",
      "Epoch 197: train loss: 0.1706, test loss 0.0729\n",
      "Epoch 198: train loss: 0.1705, test loss 0.0730\n",
      "Epoch 199: train loss: 0.1703, test loss 0.0732\n",
      "Epoch 200: train loss: 0.1702, test loss 0.0733\n",
      "Epoch 201: train loss: 0.1700, test loss 0.0734\n",
      "Epoch 202: train loss: 0.1699, test loss 0.0735\n",
      "Epoch 203: train loss: 0.1697, test loss 0.0736\n",
      "Epoch 204: train loss: 0.1695, test loss 0.0737\n",
      "Epoch 205: train loss: 0.1694, test loss 0.0738\n",
      "Epoch 206: train loss: 0.1692, test loss 0.0739\n",
      "Epoch 207: train loss: 0.1691, test loss 0.0741\n",
      "Epoch 208: train loss: 0.1690, test loss 0.0742\n",
      "Epoch 209: train loss: 0.1688, test loss 0.0743\n",
      "Epoch 210: train loss: 0.1687, test loss 0.0744\n",
      "Epoch 211: train loss: 0.1685, test loss 0.0745\n",
      "Epoch 212: train loss: 0.1684, test loss 0.0746\n",
      "Epoch 213: train loss: 0.1682, test loss 0.0747\n",
      "Epoch 214: train loss: 0.1681, test loss 0.0749\n",
      "Epoch 215: train loss: 0.1679, test loss 0.0750\n",
      "Epoch 216: train loss: 0.1678, test loss 0.0751\n",
      "Epoch 217: train loss: 0.1677, test loss 0.0752\n",
      "Epoch 218: train loss: 0.1675, test loss 0.0753\n",
      "Epoch 219: train loss: 0.1674, test loss 0.0755\n",
      "Epoch 220: train loss: 0.1672, test loss 0.0756\n",
      "Epoch 221: train loss: 0.1671, test loss 0.0757\n",
      "Epoch 222: train loss: 0.1669, test loss 0.0758\n",
      "Epoch 223: train loss: 0.1668, test loss 0.0759\n",
      "Epoch 224: train loss: 0.1666, test loss 0.0760\n",
      "Epoch 225: train loss: 0.1665, test loss 0.0762\n",
      "Epoch 226: train loss: 0.1663, test loss 0.0763\n",
      "Epoch 227: train loss: 0.1662, test loss 0.0764\n",
      "Epoch 228: train loss: 0.1660, test loss 0.0765\n",
      "Epoch 229: train loss: 0.1659, test loss 0.0766\n",
      "Epoch 230: train loss: 0.1657, test loss 0.0767\n",
      "Epoch 231: train loss: 0.1656, test loss 0.0768\n",
      "Epoch 232: train loss: 0.1654, test loss 0.0769\n",
      "Epoch 233: train loss: 0.1653, test loss 0.0770\n",
      "Epoch 234: train loss: 0.1651, test loss 0.0771\n",
      "Epoch 235: train loss: 0.1650, test loss 0.0772\n",
      "Epoch 236: train loss: 0.1649, test loss 0.0773\n",
      "Epoch 237: train loss: 0.1647, test loss 0.0774\n",
      "Epoch 238: train loss: 0.1646, test loss 0.0775\n",
      "Epoch 239: train loss: 0.1644, test loss 0.0776\n",
      "Epoch 240: train loss: 0.1643, test loss 0.0777\n",
      "Epoch 241: train loss: 0.1642, test loss 0.0779\n",
      "Epoch 242: train loss: 0.1641, test loss 0.0780\n",
      "Epoch 243: train loss: 0.1639, test loss 0.0781\n",
      "Epoch 244: train loss: 0.1638, test loss 0.0782\n",
      "Epoch 245: train loss: 0.1637, test loss 0.0783\n",
      "Epoch 246: train loss: 0.1636, test loss 0.0784\n",
      "Epoch 247: train loss: 0.1634, test loss 0.0785\n",
      "Epoch 248: train loss: 0.1633, test loss 0.0786\n",
      "Epoch 249: train loss: 0.1632, test loss 0.0787\n",
      "Epoch 250: train loss: 0.1630, test loss 0.0788\n",
      "Epoch 251: train loss: 0.1629, test loss 0.0789\n",
      "Epoch 252: train loss: 0.1628, test loss 0.0790\n",
      "Epoch 253: train loss: 0.1626, test loss 0.0791\n",
      "Epoch 254: train loss: 0.1625, test loss 0.0791\n",
      "Epoch 255: train loss: 0.1624, test loss 0.0792\n",
      "Epoch 256: train loss: 0.1623, test loss 0.0793\n",
      "Epoch 257: train loss: 0.1621, test loss 0.0794\n",
      "Epoch 258: train loss: 0.1620, test loss 0.0795\n",
      "Epoch 259: train loss: 0.1619, test loss 0.0796\n",
      "Epoch 260: train loss: 0.1617, test loss 0.0797\n",
      "Epoch 261: train loss: 0.1616, test loss 0.0798\n",
      "Epoch 262: train loss: 0.1615, test loss 0.0799\n",
      "Epoch 263: train loss: 0.1613, test loss 0.0800\n",
      "Epoch 264: train loss: 0.1612, test loss 0.0801\n",
      "Epoch 265: train loss: 0.1610, test loss 0.0802\n",
      "Epoch 266: train loss: 0.1609, test loss 0.0803\n",
      "Epoch 267: train loss: 0.1607, test loss 0.0804\n",
      "Epoch 268: train loss: 0.1606, test loss 0.0806\n",
      "Epoch 269: train loss: 0.1604, test loss 0.0807\n",
      "Epoch 270: train loss: 0.1603, test loss 0.0808\n",
      "Epoch 271: train loss: 0.1601, test loss 0.0809\n",
      "Epoch 272: train loss: 0.1599, test loss 0.0810\n",
      "Epoch 273: train loss: 0.1598, test loss 0.0811\n",
      "Epoch 274: train loss: 0.1596, test loss 0.0812\n",
      "Epoch 275: train loss: 0.1594, test loss 0.0813\n",
      "Epoch 276: train loss: 0.1592, test loss 0.0814\n",
      "Epoch 277: train loss: 0.1591, test loss 0.0814\n",
      "Epoch 278: train loss: 0.1589, test loss 0.0815\n",
      "Epoch 279: train loss: 0.1587, test loss 0.0816\n",
      "Epoch 280: train loss: 0.1585, test loss 0.0817\n",
      "Epoch 281: train loss: 0.1583, test loss 0.0818\n",
      "Epoch 282: train loss: 0.1581, test loss 0.0819\n",
      "Epoch 283: train loss: 0.1579, test loss 0.0820\n",
      "Epoch 284: train loss: 0.1577, test loss 0.0820\n",
      "Epoch 285: train loss: 0.1575, test loss 0.0821\n",
      "Epoch 286: train loss: 0.1573, test loss 0.0822\n",
      "Epoch 287: train loss: 0.1571, test loss 0.0823\n",
      "Epoch 288: train loss: 0.1568, test loss 0.0823\n",
      "Epoch 289: train loss: 0.1566, test loss 0.0824\n",
      "Epoch 290: train loss: 0.1564, test loss 0.0825\n",
      "Epoch 291: train loss: 0.1562, test loss 0.0825\n",
      "Epoch 292: train loss: 0.1559, test loss 0.0825\n",
      "Epoch 293: train loss: 0.1557, test loss 0.0825\n",
      "Epoch 294: train loss: 0.1555, test loss 0.0826\n",
      "Epoch 295: train loss: 0.1552, test loss 0.0826\n",
      "Epoch 296: train loss: 0.1550, test loss 0.0826\n",
      "Epoch 297: train loss: 0.1547, test loss 0.0826\n",
      "Epoch 298: train loss: 0.1544, test loss 0.0825\n",
      "Epoch 299: train loss: 0.1541, test loss 0.0825\n",
      "Epoch 300: train loss: 0.1538, test loss 0.0825\n",
      "Epoch 301: train loss: 0.1535, test loss 0.0825\n",
      "Epoch 302: train loss: 0.1532, test loss 0.0824\n",
      "Epoch 303: train loss: 0.1529, test loss 0.0824\n",
      "Epoch 304: train loss: 0.1525, test loss 0.0823\n",
      "Epoch 305: train loss: 0.1521, test loss 0.0823\n",
      "Epoch 306: train loss: 0.1517, test loss 0.0822\n",
      "Epoch 307: train loss: 0.1513, test loss 0.0821\n",
      "Epoch 308: train loss: 0.1510, test loss 0.0820\n",
      "Epoch 309: train loss: 0.1506, test loss 0.0818\n",
      "Epoch 310: train loss: 0.1502, test loss 0.0817\n",
      "Epoch 311: train loss: 0.1498, test loss 0.0816\n",
      "Epoch 312: train loss: 0.1494, test loss 0.0814\n",
      "Epoch 313: train loss: 0.1490, test loss 0.0814\n",
      "Epoch 314: train loss: 0.1486, test loss 0.0815\n",
      "Epoch 315: train loss: 0.1482, test loss 0.0816\n",
      "Epoch 316: train loss: 0.1479, test loss 0.0818\n",
      "Epoch 317: train loss: 0.1476, test loss 0.0820\n",
      "Epoch 318: train loss: 0.1473, test loss 0.0822\n",
      "Epoch 319: train loss: 0.1471, test loss 0.0825\n",
      "Epoch 320: train loss: 0.1468, test loss 0.0827\n",
      "Epoch 321: train loss: 0.1466, test loss 0.0835\n",
      "Epoch 322: train loss: 0.1465, test loss 0.0844\n",
      "Epoch 323: train loss: 0.1463, test loss 0.0852\n",
      "Epoch 324: train loss: 0.1461, test loss 0.0859\n",
      "Epoch 325: train loss: 0.1459, test loss 0.0865\n",
      "Epoch 326: train loss: 0.1456, test loss 0.0869\n",
      "Epoch 327: train loss: 0.1454, test loss 0.0872\n",
      "Epoch 328: train loss: 0.1452, test loss 0.0875\n",
      "Epoch 329: train loss: 0.1450, test loss 0.0877\n",
      "Epoch 330: train loss: 0.1447, test loss 0.0878\n",
      "Epoch 331: train loss: 0.1445, test loss 0.0879\n",
      "Epoch 332: train loss: 0.1442, test loss 0.0880\n",
      "Epoch 333: train loss: 0.1440, test loss 0.0882\n",
      "Epoch 334: train loss: 0.1438, test loss 0.0883\n",
      "Epoch 335: train loss: 0.1436, test loss 0.0884\n",
      "Epoch 336: train loss: 0.1434, test loss 0.0886\n",
      "Epoch 337: train loss: 0.1431, test loss 0.0888\n",
      "Epoch 338: train loss: 0.1429, test loss 0.0891\n",
      "Epoch 339: train loss: 0.1428, test loss 0.0894\n",
      "Epoch 340: train loss: 0.1426, test loss 0.0897\n",
      "Epoch 341: train loss: 0.1423, test loss 0.0901\n",
      "Epoch 342: train loss: 0.1421, test loss 0.0904\n",
      "Epoch 343: train loss: 0.1419, test loss 0.0908\n",
      "Epoch 344: train loss: 0.1417, test loss 0.0911\n",
      "Epoch 345: train loss: 0.1415, test loss 0.0915\n",
      "Epoch 346: train loss: 0.1413, test loss 0.0918\n",
      "Epoch 347: train loss: 0.1411, test loss 0.0921\n",
      "Epoch 348: train loss: 0.1408, test loss 0.0924\n",
      "Epoch 349: train loss: 0.1406, test loss 0.0927\n",
      "Epoch 350: train loss: 0.1404, test loss 0.0930\n",
      "Epoch 351: train loss: 0.1403, test loss 0.0934\n",
      "Epoch 352: train loss: 0.1401, test loss 0.0937\n",
      "Epoch 353: train loss: 0.1399, test loss 0.0941\n",
      "Epoch 354: train loss: 0.1397, test loss 0.0945\n",
      "Epoch 355: train loss: 0.1395, test loss 0.0949\n",
      "Epoch 356: train loss: 0.1393, test loss 0.0953\n",
      "Epoch 357: train loss: 0.1391, test loss 0.0958\n",
      "Epoch 358: train loss: 0.1389, test loss 0.0962\n",
      "Epoch 359: train loss: 0.1387, test loss 0.0966\n",
      "Epoch 360: train loss: 0.1386, test loss 0.0970\n",
      "Epoch 361: train loss: 0.1384, test loss 0.0974\n",
      "Epoch 362: train loss: 0.1382, test loss 0.0978\n",
      "Epoch 363: train loss: 0.1380, test loss 0.0981\n",
      "Epoch 364: train loss: 0.1378, test loss 0.0985\n",
      "Epoch 365: train loss: 0.1376, test loss 0.0988\n",
      "Epoch 366: train loss: 0.1374, test loss 0.0992\n",
      "Epoch 367: train loss: 0.1372, test loss 0.0995\n",
      "Epoch 368: train loss: 0.1371, test loss 0.0999\n",
      "Epoch 369: train loss: 0.1369, test loss 0.1003\n",
      "Epoch 370: train loss: 0.1367, test loss 0.1007\n",
      "Epoch 371: train loss: 0.1365, test loss 0.1011\n",
      "Epoch 372: train loss: 0.1364, test loss 0.1015\n",
      "Epoch 373: train loss: 0.1362, test loss 0.1020\n",
      "Epoch 374: train loss: 0.1360, test loss 0.1025\n",
      "Epoch 375: train loss: 0.1358, test loss 0.1030\n",
      "Epoch 376: train loss: 0.1356, test loss 0.1036\n",
      "Epoch 377: train loss: 0.1354, test loss 0.1041\n",
      "Epoch 378: train loss: 0.1353, test loss 0.1046\n",
      "Epoch 379: train loss: 0.1351, test loss 0.1051\n",
      "Epoch 380: train loss: 0.1349, test loss 0.1056\n",
      "Epoch 381: train loss: 0.1347, test loss 0.1061\n",
      "Epoch 382: train loss: 0.1346, test loss 0.1066\n",
      "Epoch 383: train loss: 0.1344, test loss 0.1070\n",
      "Epoch 384: train loss: 0.1342, test loss 0.1074\n",
      "Epoch 385: train loss: 0.1341, test loss 0.1078\n",
      "Epoch 386: train loss: 0.1339, test loss 0.1083\n",
      "Epoch 387: train loss: 0.1338, test loss 0.1087\n",
      "Epoch 388: train loss: 0.1336, test loss 0.1092\n",
      "Epoch 389: train loss: 0.1335, test loss 0.1097\n",
      "Epoch 390: train loss: 0.1333, test loss 0.1102\n",
      "Epoch 391: train loss: 0.1332, test loss 0.1108\n",
      "Epoch 392: train loss: 0.1331, test loss 0.1113\n",
      "Epoch 393: train loss: 0.1329, test loss 0.1120\n",
      "Epoch 394: train loss: 0.1328, test loss 0.1126\n",
      "Epoch 395: train loss: 0.1327, test loss 0.1132\n",
      "Epoch 396: train loss: 0.1326, test loss 0.1139\n",
      "Epoch 397: train loss: 0.1325, test loss 0.1145\n",
      "Epoch 398: train loss: 0.1323, test loss 0.1152\n",
      "Epoch 399: train loss: 0.1322, test loss 0.1158\n",
      "Epoch 400: train loss: 0.1321, test loss 0.1165\n",
      "Epoch 401: train loss: 0.1320, test loss 0.1171\n",
      "Epoch 402: train loss: 0.1319, test loss 0.1178\n",
      "Epoch 403: train loss: 0.1318, test loss 0.1185\n",
      "Epoch 404: train loss: 0.1317, test loss 0.1193\n",
      "Epoch 405: train loss: 0.1316, test loss 0.1200\n",
      "Epoch 406: train loss: 0.1315, test loss 0.1208\n",
      "Epoch 407: train loss: 0.1314, test loss 0.1217\n",
      "Epoch 408: train loss: 0.1314, test loss 0.1225\n",
      "Epoch 409: train loss: 0.1313, test loss 0.1233\n",
      "Epoch 410: train loss: 0.1312, test loss 0.1242\n",
      "Epoch 411: train loss: 0.1311, test loss 0.1250\n",
      "Epoch 412: train loss: 0.1310, test loss 0.1257\n",
      "Epoch 413: train loss: 0.1310, test loss 0.1264\n",
      "Epoch 414: train loss: 0.1309, test loss 0.1271\n",
      "Epoch 415: train loss: 0.1308, test loss 0.1277\n",
      "Epoch 416: train loss: 0.1308, test loss 0.1284\n",
      "Epoch 417: train loss: 0.1307, test loss 0.1290\n",
      "Epoch 418: train loss: 0.1306, test loss 0.1297\n",
      "Epoch 419: train loss: 0.1306, test loss 0.1303\n",
      "Epoch 420: train loss: 0.1305, test loss 0.1309\n",
      "Epoch 421: train loss: 0.1305, test loss 0.1315\n",
      "Epoch 422: train loss: 0.1304, test loss 0.1321\n",
      "Epoch 423: train loss: 0.1303, test loss 0.1326\n",
      "Epoch 424: train loss: 0.1303, test loss 0.1331\n",
      "Epoch 425: train loss: 0.1302, test loss 0.1336\n",
      "Epoch 426: train loss: 0.1302, test loss 0.1342\n",
      "Epoch 427: train loss: 0.1301, test loss 0.1347\n",
      "Epoch 428: train loss: 0.1301, test loss 0.1352\n",
      "Epoch 429: train loss: 0.1300, test loss 0.1357\n",
      "Epoch 430: train loss: 0.1300, test loss 0.1363\n",
      "Epoch 431: train loss: 0.1299, test loss 0.1368\n",
      "Epoch 432: train loss: 0.1299, test loss 0.1374\n",
      "Epoch 433: train loss: 0.1298, test loss 0.1380\n",
      "Epoch 434: train loss: 0.1298, test loss 0.1386\n",
      "Epoch 435: train loss: 0.1297, test loss 0.1393\n",
      "Epoch 436: train loss: 0.1297, test loss 0.1400\n",
      "Epoch 437: train loss: 0.1296, test loss 0.1407\n",
      "Epoch 438: train loss: 0.1296, test loss 0.1414\n",
      "Epoch 439: train loss: 0.1296, test loss 0.1421\n",
      "Epoch 440: train loss: 0.1295, test loss 0.1428\n",
      "Epoch 441: train loss: 0.1295, test loss 0.1434\n",
      "Epoch 442: train loss: 0.1294, test loss 0.1440\n",
      "Epoch 443: train loss: 0.1294, test loss 0.1447\n",
      "Epoch 444: train loss: 0.1294, test loss 0.1453\n",
      "Epoch 445: train loss: 0.1293, test loss 0.1460\n",
      "Epoch 446: train loss: 0.1293, test loss 0.1466\n",
      "Epoch 447: train loss: 0.1292, test loss 0.1472\n",
      "Epoch 448: train loss: 0.1292, test loss 0.1477\n",
      "Epoch 449: train loss: 0.1292, test loss 0.1482\n",
      "Epoch 450: train loss: 0.1291, test loss 0.1486\n",
      "Epoch 451: train loss: 0.1291, test loss 0.1490\n",
      "Epoch 452: train loss: 0.1291, test loss 0.1494\n",
      "Epoch 453: train loss: 0.1290, test loss 0.1498\n",
      "Epoch 454: train loss: 0.1290, test loss 0.1501\n",
      "Epoch 455: train loss: 0.1290, test loss 0.1505\n",
      "Epoch 456: train loss: 0.1289, test loss 0.1510\n",
      "Epoch 457: train loss: 0.1289, test loss 0.1514\n",
      "Epoch 458: train loss: 0.1289, test loss 0.1518\n",
      "Epoch 459: train loss: 0.1289, test loss 0.1520\n",
      "Epoch 460: train loss: 0.1288, test loss 0.1522\n",
      "Epoch 461: train loss: 0.1288, test loss 0.1525\n",
      "Epoch 462: train loss: 0.1288, test loss 0.1527\n",
      "Epoch 463: train loss: 0.1287, test loss 0.1529\n",
      "Epoch 464: train loss: 0.1287, test loss 0.1532\n",
      "Epoch 465: train loss: 0.1287, test loss 0.1535\n",
      "Epoch 466: train loss: 0.1287, test loss 0.1538\n",
      "Epoch 467: train loss: 0.1286, test loss 0.1541\n",
      "Epoch 468: train loss: 0.1286, test loss 0.1543\n",
      "Epoch 469: train loss: 0.1286, test loss 0.1544\n",
      "Epoch 470: train loss: 0.1286, test loss 0.1547\n",
      "Epoch 471: train loss: 0.1285, test loss 0.1550\n",
      "Epoch 472: train loss: 0.1285, test loss 0.1552\n",
      "Epoch 473: train loss: 0.1285, test loss 0.1553\n",
      "Epoch 474: train loss: 0.1285, test loss 0.1554\n",
      "Epoch 475: train loss: 0.1284, test loss 0.1556\n",
      "Epoch 476: train loss: 0.1284, test loss 0.1558\n",
      "Epoch 477: train loss: 0.1284, test loss 0.1559\n",
      "Epoch 478: train loss: 0.1284, test loss 0.1560\n",
      "Epoch 479: train loss: 0.1283, test loss 0.1561\n",
      "Epoch 480: train loss: 0.1283, test loss 0.1563\n",
      "Epoch 481: train loss: 0.1283, test loss 0.1564\n",
      "Epoch 482: train loss: 0.1283, test loss 0.1565\n",
      "Epoch 483: train loss: 0.1283, test loss 0.1566\n",
      "Epoch 484: train loss: 0.1282, test loss 0.1567\n",
      "Epoch 485: train loss: 0.1282, test loss 0.1569\n",
      "Epoch 486: train loss: 0.1282, test loss 0.1570\n",
      "Epoch 487: train loss: 0.1282, test loss 0.1571\n",
      "Epoch 488: train loss: 0.1282, test loss 0.1572\n",
      "Epoch 489: train loss: 0.1281, test loss 0.1573\n",
      "Epoch 490: train loss: 0.1281, test loss 0.1574\n",
      "Epoch 491: train loss: 0.1281, test loss 0.1574\n",
      "Epoch 492: train loss: 0.1281, test loss 0.1575\n",
      "Epoch 493: train loss: 0.1281, test loss 0.1575\n",
      "Epoch 494: train loss: 0.1280, test loss 0.1575\n",
      "Epoch 495: train loss: 0.1280, test loss 0.1576\n",
      "Epoch 496: train loss: 0.1280, test loss 0.1576\n",
      "Epoch 497: train loss: 0.1280, test loss 0.1575\n",
      "Epoch 498: train loss: 0.1280, test loss 0.1575\n",
      "Epoch 499: train loss: 0.1279, test loss 0.1575\n",
      "Epoch 500: train loss: 0.1279, test loss 0.1575\n",
      "Epoch 501: train loss: 0.1279, test loss 0.1575\n",
      "Epoch 502: train loss: 0.1279, test loss 0.1576\n",
      "Epoch 503: train loss: 0.1279, test loss 0.1577\n",
      "Epoch 504: train loss: 0.1278, test loss 0.1579\n",
      "Epoch 505: train loss: 0.1278, test loss 0.1580\n",
      "Epoch 506: train loss: 0.1278, test loss 0.1582\n",
      "Epoch 507: train loss: 0.1278, test loss 0.1583\n",
      "Epoch 508: train loss: 0.1278, test loss 0.1583\n",
      "Epoch 509: train loss: 0.1278, test loss 0.1583\n",
      "Epoch 510: train loss: 0.1277, test loss 0.1583\n",
      "Epoch 511: train loss: 0.1277, test loss 0.1583\n",
      "Epoch 512: train loss: 0.1277, test loss 0.1583\n",
      "Epoch 513: train loss: 0.1277, test loss 0.1583\n",
      "Epoch 514: train loss: 0.1277, test loss 0.1584\n",
      "Epoch 515: train loss: 0.1277, test loss 0.1583\n",
      "Epoch 516: train loss: 0.1276, test loss 0.1582\n",
      "Epoch 517: train loss: 0.1276, test loss 0.1582\n",
      "Epoch 518: train loss: 0.1276, test loss 0.1582\n",
      "Epoch 519: train loss: 0.1276, test loss 0.1583\n",
      "Epoch 520: train loss: 0.1276, test loss 0.1584\n",
      "Epoch 521: train loss: 0.1276, test loss 0.1584\n",
      "Epoch 522: train loss: 0.1275, test loss 0.1584\n",
      "Epoch 523: train loss: 0.1275, test loss 0.1583\n",
      "Epoch 524: train loss: 0.1275, test loss 0.1583\n",
      "Epoch 525: train loss: 0.1275, test loss 0.1583\n",
      "Epoch 526: train loss: 0.1275, test loss 0.1582\n",
      "Epoch 527: train loss: 0.1275, test loss 0.1582\n",
      "Epoch 528: train loss: 0.1275, test loss 0.1582\n",
      "Epoch 529: train loss: 0.1274, test loss 0.1582\n",
      "Epoch 530: train loss: 0.1274, test loss 0.1582\n",
      "Epoch 531: train loss: 0.1274, test loss 0.1581\n",
      "Epoch 532: train loss: 0.1274, test loss 0.1582\n",
      "Epoch 533: train loss: 0.1274, test loss 0.1583\n",
      "Epoch 534: train loss: 0.1274, test loss 0.1583\n",
      "Epoch 535: train loss: 0.1274, test loss 0.1583\n",
      "Epoch 536: train loss: 0.1274, test loss 0.1582\n",
      "Epoch 537: train loss: 0.1273, test loss 0.1581\n",
      "Epoch 538: train loss: 0.1273, test loss 0.1580\n",
      "Epoch 539: train loss: 0.1273, test loss 0.1580\n",
      "Epoch 540: train loss: 0.1273, test loss 0.1581\n",
      "Epoch 541: train loss: 0.1273, test loss 0.1581\n",
      "Epoch 542: train loss: 0.1273, test loss 0.1580\n",
      "Epoch 543: train loss: 0.1273, test loss 0.1579\n",
      "Epoch 544: train loss: 0.1272, test loss 0.1578\n",
      "Epoch 545: train loss: 0.1272, test loss 0.1576\n",
      "Epoch 546: train loss: 0.1272, test loss 0.1576\n",
      "Epoch 547: train loss: 0.1272, test loss 0.1576\n",
      "Epoch 548: train loss: 0.1272, test loss 0.1575\n",
      "Epoch 549: train loss: 0.1272, test loss 0.1574\n",
      "Epoch 550: train loss: 0.1272, test loss 0.1572\n",
      "Epoch 551: train loss: 0.1272, test loss 0.1570\n",
      "Epoch 552: train loss: 0.1271, test loss 0.1570\n",
      "Epoch 553: train loss: 0.1271, test loss 0.1571\n",
      "Epoch 554: train loss: 0.1271, test loss 0.1571\n",
      "Epoch 555: train loss: 0.1271, test loss 0.1571\n",
      "Epoch 556: train loss: 0.1271, test loss 0.1570\n",
      "Epoch 557: train loss: 0.1271, test loss 0.1570\n",
      "Epoch 558: train loss: 0.1271, test loss 0.1568\n",
      "Epoch 559: train loss: 0.1270, test loss 0.1567\n",
      "Epoch 560: train loss: 0.1270, test loss 0.1567\n",
      "Epoch 561: train loss: 0.1270, test loss 0.1566\n",
      "Epoch 562: train loss: 0.1270, test loss 0.1565\n",
      "Epoch 563: train loss: 0.1270, test loss 0.1565\n",
      "Epoch 564: train loss: 0.1270, test loss 0.1565\n",
      "Epoch 565: train loss: 0.1270, test loss 0.1565\n",
      "Epoch 566: train loss: 0.1269, test loss 0.1565\n",
      "Epoch 567: train loss: 0.1269, test loss 0.1565\n",
      "Epoch 568: train loss: 0.1269, test loss 0.1564\n",
      "Epoch 569: train loss: 0.1269, test loss 0.1562\n",
      "Epoch 570: train loss: 0.1269, test loss 0.1562\n",
      "Epoch 571: train loss: 0.1269, test loss 0.1562\n",
      "Epoch 572: train loss: 0.1269, test loss 0.1561\n",
      "Epoch 573: train loss: 0.1269, test loss 0.1561\n",
      "Epoch 574: train loss: 0.1268, test loss 0.1563\n",
      "Epoch 575: train loss: 0.1268, test loss 0.1563\n",
      "Epoch 576: train loss: 0.1268, test loss 0.1563\n",
      "Epoch 577: train loss: 0.1268, test loss 0.1564\n",
      "Epoch 578: train loss: 0.1268, test loss 0.1564\n",
      "Epoch 579: train loss: 0.1268, test loss 0.1564\n",
      "Epoch 580: train loss: 0.1268, test loss 0.1564\n",
      "Epoch 581: train loss: 0.1268, test loss 0.1565\n",
      "Epoch 582: train loss: 0.1268, test loss 0.1565\n",
      "Epoch 583: train loss: 0.1267, test loss 0.1566\n",
      "Epoch 584: train loss: 0.1267, test loss 0.1566\n",
      "Epoch 585: train loss: 0.1267, test loss 0.1565\n",
      "Epoch 586: train loss: 0.1267, test loss 0.1564\n",
      "Epoch 587: train loss: 0.1267, test loss 0.1563\n",
      "Epoch 588: train loss: 0.1267, test loss 0.1563\n",
      "Epoch 589: train loss: 0.1267, test loss 0.1563\n",
      "Epoch 590: train loss: 0.1267, test loss 0.1564\n",
      "Epoch 591: train loss: 0.1267, test loss 0.1565\n",
      "Epoch 592: train loss: 0.1267, test loss 0.1566\n",
      "Epoch 593: train loss: 0.1266, test loss 0.1566\n",
      "Epoch 594: train loss: 0.1266, test loss 0.1565\n",
      "Epoch 595: train loss: 0.1266, test loss 0.1565\n",
      "Epoch 596: train loss: 0.1266, test loss 0.1563\n",
      "Epoch 597: train loss: 0.1266, test loss 0.1562\n",
      "Epoch 598: train loss: 0.1266, test loss 0.1561\n",
      "Epoch 599: train loss: 0.1266, test loss 0.1560\n",
      "Epoch 600: train loss: 0.1266, test loss 0.1559\n",
      "Epoch 601: train loss: 0.1265, test loss 0.1558\n",
      "Epoch 602: train loss: 0.1265, test loss 0.1556\n",
      "Epoch 603: train loss: 0.1265, test loss 0.1555\n",
      "Epoch 604: train loss: 0.1265, test loss 0.1554\n",
      "Epoch 605: train loss: 0.1265, test loss 0.1553\n",
      "Epoch 606: train loss: 0.1265, test loss 0.1552\n",
      "Epoch 607: train loss: 0.1265, test loss 0.1552\n",
      "Epoch 608: train loss: 0.1265, test loss 0.1551\n",
      "Epoch 609: train loss: 0.1265, test loss 0.1550\n",
      "Epoch 610: train loss: 0.1264, test loss 0.1549\n",
      "Epoch 611: train loss: 0.1264, test loss 0.1549\n",
      "Epoch 612: train loss: 0.1264, test loss 0.1549\n",
      "Epoch 613: train loss: 0.1264, test loss 0.1548\n",
      "Epoch 614: train loss: 0.1264, test loss 0.1547\n",
      "Epoch 615: train loss: 0.1264, test loss 0.1545\n",
      "Epoch 616: train loss: 0.1264, test loss 0.1544\n",
      "Epoch 617: train loss: 0.1264, test loss 0.1543\n",
      "Epoch 618: train loss: 0.1263, test loss 0.1542\n",
      "Epoch 619: train loss: 0.1263, test loss 0.1541\n",
      "Epoch 620: train loss: 0.1263, test loss 0.1540\n",
      "Epoch 621: train loss: 0.1263, test loss 0.1539\n",
      "Epoch 622: train loss: 0.1263, test loss 0.1538\n",
      "Epoch 623: train loss: 0.1263, test loss 0.1537\n",
      "Epoch 624: train loss: 0.1263, test loss 0.1535\n",
      "Epoch 625: train loss: 0.1263, test loss 0.1534\n",
      "Epoch 626: train loss: 0.1262, test loss 0.1533\n",
      "Epoch 627: train loss: 0.1262, test loss 0.1532\n",
      "Epoch 628: train loss: 0.1262, test loss 0.1530\n",
      "Epoch 629: train loss: 0.1262, test loss 0.1528\n",
      "Epoch 630: train loss: 0.1262, test loss 0.1526\n",
      "Epoch 631: train loss: 0.1262, test loss 0.1524\n",
      "Epoch 632: train loss: 0.1262, test loss 0.1521\n",
      "Epoch 633: train loss: 0.1262, test loss 0.1517\n",
      "Epoch 634: train loss: 0.1262, test loss 0.1515\n",
      "Epoch 635: train loss: 0.1261, test loss 0.1514\n",
      "Epoch 636: train loss: 0.1261, test loss 0.1513\n",
      "Epoch 637: train loss: 0.1261, test loss 0.1512\n",
      "Epoch 638: train loss: 0.1261, test loss 0.1512\n",
      "Epoch 639: train loss: 0.1261, test loss 0.1512\n",
      "Epoch 640: train loss: 0.1261, test loss 0.1511\n",
      "Epoch 641: train loss: 0.1261, test loss 0.1511\n",
      "Epoch 642: train loss: 0.1261, test loss 0.1509\n",
      "Epoch 643: train loss: 0.1261, test loss 0.1508\n",
      "Epoch 644: train loss: 0.1261, test loss 0.1505\n",
      "Epoch 645: train loss: 0.1261, test loss 0.1503\n",
      "Epoch 646: train loss: 0.1260, test loss 0.1502\n",
      "Epoch 647: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 648: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 649: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 650: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 651: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 652: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 653: train loss: 0.1260, test loss 0.1501\n",
      "Epoch 654: train loss: 0.1260, test loss 0.1500\n",
      "Epoch 655: train loss: 0.1260, test loss 0.1499\n",
      "Epoch 656: train loss: 0.1260, test loss 0.1498\n",
      "Epoch 657: train loss: 0.1260, test loss 0.1497\n",
      "Epoch 658: train loss: 0.1260, test loss 0.1497\n",
      "Epoch 659: train loss: 0.1260, test loss 0.1497\n",
      "Epoch 660: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 661: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 662: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 663: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 664: train loss: 0.1259, test loss 0.1497\n",
      "Epoch 665: train loss: 0.1259, test loss 0.1497\n",
      "Epoch 666: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 667: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 668: train loss: 0.1259, test loss 0.1496\n",
      "Epoch 669: train loss: 0.1259, test loss 0.1495\n",
      "Epoch 670: train loss: 0.1259, test loss 0.1495\n",
      "Epoch 671: train loss: 0.1259, test loss 0.1493\n",
      "Epoch 672: train loss: 0.1259, test loss 0.1492\n",
      "Epoch 673: train loss: 0.1259, test loss 0.1492\n",
      "Epoch 674: train loss: 0.1258, test loss 0.1492\n",
      "Epoch 675: train loss: 0.1258, test loss 0.1493\n",
      "Epoch 676: train loss: 0.1258, test loss 0.1493\n",
      "Epoch 677: train loss: 0.1258, test loss 0.1492\n",
      "Epoch 678: train loss: 0.1258, test loss 0.1491\n",
      "Epoch 679: train loss: 0.1258, test loss 0.1491\n",
      "Epoch 680: train loss: 0.1258, test loss 0.1491\n",
      "Epoch 681: train loss: 0.1258, test loss 0.1491\n",
      "Epoch 682: train loss: 0.1258, test loss 0.1490\n",
      "Epoch 683: train loss: 0.1258, test loss 0.1489\n",
      "Epoch 684: train loss: 0.1258, test loss 0.1488\n",
      "Epoch 685: train loss: 0.1258, test loss 0.1487\n",
      "Epoch 686: train loss: 0.1258, test loss 0.1487\n",
      "Epoch 687: train loss: 0.1258, test loss 0.1487\n",
      "Epoch 688: train loss: 0.1258, test loss 0.1487\n",
      "Epoch 689: train loss: 0.1258, test loss 0.1486\n",
      "Epoch 690: train loss: 0.1257, test loss 0.1485\n",
      "Epoch 691: train loss: 0.1257, test loss 0.1485\n",
      "Epoch 692: train loss: 0.1257, test loss 0.1485\n",
      "Epoch 693: train loss: 0.1257, test loss 0.1484\n",
      "Epoch 694: train loss: 0.1257, test loss 0.1483\n",
      "Epoch 695: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 696: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 697: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 698: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 699: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 700: train loss: 0.1257, test loss 0.1482\n",
      "Epoch 701: train loss: 0.1257, test loss 0.1481\n",
      "Epoch 702: train loss: 0.1257, test loss 0.1481\n",
      "Epoch 703: train loss: 0.1257, test loss 0.1480\n",
      "Epoch 704: train loss: 0.1257, test loss 0.1479\n",
      "Epoch 705: train loss: 0.1257, test loss 0.1478\n",
      "Epoch 706: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 707: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 708: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 709: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 710: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 711: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 712: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 713: train loss: 0.1256, test loss 0.1477\n",
      "Epoch 714: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 715: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 716: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 717: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 718: train loss: 0.1256, test loss 0.1475\n",
      "Epoch 719: train loss: 0.1256, test loss 0.1475\n",
      "Epoch 720: train loss: 0.1256, test loss 0.1475\n",
      "Epoch 721: train loss: 0.1256, test loss 0.1475\n",
      "Epoch 722: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 723: train loss: 0.1256, test loss 0.1476\n",
      "Epoch 724: train loss: 0.1255, test loss 0.1475\n",
      "Epoch 725: train loss: 0.1255, test loss 0.1474\n",
      "Epoch 726: train loss: 0.1255, test loss 0.1474\n",
      "Epoch 727: train loss: 0.1255, test loss 0.1474\n",
      "Epoch 728: train loss: 0.1255, test loss 0.1474\n",
      "Epoch 729: train loss: 0.1255, test loss 0.1473\n",
      "Epoch 730: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 731: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 732: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 733: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 734: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 735: train loss: 0.1255, test loss 0.1473\n",
      "Epoch 736: train loss: 0.1255, test loss 0.1473\n",
      "Epoch 737: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 738: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 739: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 740: train loss: 0.1255, test loss 0.1472\n",
      "Epoch 741: train loss: 0.1255, test loss 0.1473\n",
      "Epoch 742: train loss: 0.1255, test loss 0.1474\n",
      "Epoch 743: train loss: 0.1254, test loss 0.1475\n",
      "Epoch 744: train loss: 0.1254, test loss 0.1475\n",
      "Epoch 745: train loss: 0.1254, test loss 0.1474\n",
      "Epoch 746: train loss: 0.1254, test loss 0.1473\n",
      "Epoch 747: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 748: train loss: 0.1254, test loss 0.1469\n",
      "Epoch 749: train loss: 0.1254, test loss 0.1468\n",
      "Epoch 750: train loss: 0.1254, test loss 0.1469\n",
      "Epoch 751: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 752: train loss: 0.1254, test loss 0.1471\n",
      "Epoch 753: train loss: 0.1254, test loss 0.1471\n",
      "Epoch 754: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 755: train loss: 0.1254, test loss 0.1471\n",
      "Epoch 756: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 757: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 758: train loss: 0.1254, test loss 0.1470\n",
      "Epoch 759: train loss: 0.1254, test loss 0.1471\n",
      "Epoch 760: train loss: 0.1254, test loss 0.1472\n",
      "Epoch 761: train loss: 0.1254, test loss 0.1472\n",
      "Epoch 762: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 763: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 764: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 765: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 766: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 767: train loss: 0.1253, test loss 0.1471\n",
      "Epoch 768: train loss: 0.1253, test loss 0.1472\n",
      "Epoch 769: train loss: 0.1253, test loss 0.1473\n",
      "Epoch 770: train loss: 0.1253, test loss 0.1473\n",
      "Epoch 771: train loss: 0.1253, test loss 0.1473\n",
      "Epoch 772: train loss: 0.1253, test loss 0.1473\n",
      "Epoch 773: train loss: 0.1253, test loss 0.1472\n",
      "Epoch 774: train loss: 0.1253, test loss 0.1470\n",
      "Epoch 775: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 776: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 777: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 778: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 779: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 780: train loss: 0.1253, test loss 0.1469\n",
      "Epoch 781: train loss: 0.1253, test loss 0.1468\n",
      "Epoch 782: train loss: 0.1252, test loss 0.1468\n",
      "Epoch 783: train loss: 0.1252, test loss 0.1468\n",
      "Epoch 784: train loss: 0.1252, test loss 0.1467\n",
      "Epoch 785: train loss: 0.1252, test loss 0.1466\n",
      "Epoch 786: train loss: 0.1252, test loss 0.1464\n",
      "Epoch 787: train loss: 0.1252, test loss 0.1464\n",
      "Epoch 788: train loss: 0.1252, test loss 0.1464\n",
      "Epoch 789: train loss: 0.1252, test loss 0.1465\n",
      "Epoch 790: train loss: 0.1252, test loss 0.1465\n",
      "Epoch 791: train loss: 0.1252, test loss 0.1466\n",
      "Epoch 792: train loss: 0.1252, test loss 0.1466\n",
      "Epoch 793: train loss: 0.1252, test loss 0.1465\n",
      "Epoch 794: train loss: 0.1252, test loss 0.1464\n",
      "Epoch 795: train loss: 0.1252, test loss 0.1462\n",
      "Epoch 796: train loss: 0.1252, test loss 0.1460\n",
      "Epoch 797: train loss: 0.1252, test loss 0.1459\n",
      "Epoch 798: train loss: 0.1252, test loss 0.1459\n",
      "Epoch 799: train loss: 0.1252, test loss 0.1460\n",
      "Epoch 800: train loss: 0.1252, test loss 0.1461\n",
      "Epoch 801: train loss: 0.1251, test loss 0.1461\n",
      "Epoch 802: train loss: 0.1251, test loss 0.1460\n",
      "Epoch 803: train loss: 0.1251, test loss 0.1459\n",
      "Epoch 804: train loss: 0.1251, test loss 0.1457\n",
      "Epoch 805: train loss: 0.1251, test loss 0.1455\n",
      "Epoch 806: train loss: 0.1251, test loss 0.1454\n",
      "Epoch 807: train loss: 0.1251, test loss 0.1453\n",
      "Epoch 808: train loss: 0.1251, test loss 0.1452\n",
      "Epoch 809: train loss: 0.1251, test loss 0.1452\n",
      "Epoch 810: train loss: 0.1251, test loss 0.1452\n",
      "Epoch 811: train loss: 0.1251, test loss 0.1452\n",
      "Epoch 812: train loss: 0.1251, test loss 0.1451\n",
      "Epoch 813: train loss: 0.1251, test loss 0.1450\n",
      "Epoch 814: train loss: 0.1251, test loss 0.1449\n",
      "Epoch 815: train loss: 0.1251, test loss 0.1449\n",
      "Epoch 816: train loss: 0.1251, test loss 0.1448\n",
      "Epoch 817: train loss: 0.1251, test loss 0.1447\n",
      "Epoch 818: train loss: 0.1251, test loss 0.1445\n",
      "Epoch 819: train loss: 0.1250, test loss 0.1444\n",
      "Epoch 820: train loss: 0.1250, test loss 0.1443\n",
      "Epoch 821: train loss: 0.1250, test loss 0.1443\n",
      "Epoch 822: train loss: 0.1250, test loss 0.1442\n",
      "Epoch 823: train loss: 0.1250, test loss 0.1441\n",
      "Epoch 824: train loss: 0.1250, test loss 0.1440\n",
      "Epoch 825: train loss: 0.1250, test loss 0.1439\n",
      "Epoch 826: train loss: 0.1250, test loss 0.1438\n",
      "Epoch 827: train loss: 0.1250, test loss 0.1438\n",
      "Epoch 828: train loss: 0.1250, test loss 0.1437\n",
      "Epoch 829: train loss: 0.1250, test loss 0.1436\n",
      "Epoch 830: train loss: 0.1250, test loss 0.1436\n",
      "Epoch 831: train loss: 0.1250, test loss 0.1436\n",
      "Epoch 832: train loss: 0.1250, test loss 0.1437\n",
      "Epoch 833: train loss: 0.1250, test loss 0.1438\n",
      "Epoch 834: train loss: 0.1250, test loss 0.1438\n",
      "Epoch 835: train loss: 0.1250, test loss 0.1436\n",
      "Epoch 836: train loss: 0.1250, test loss 0.1435\n",
      "Epoch 837: train loss: 0.1250, test loss 0.1435\n",
      "Epoch 838: train loss: 0.1249, test loss 0.1435\n",
      "Epoch 839: train loss: 0.1249, test loss 0.1435\n",
      "Epoch 840: train loss: 0.1249, test loss 0.1435\n",
      "Epoch 841: train loss: 0.1249, test loss 0.1435\n",
      "Epoch 842: train loss: 0.1249, test loss 0.1435\n",
      "Epoch 843: train loss: 0.1249, test loss 0.1434\n",
      "Epoch 844: train loss: 0.1249, test loss 0.1432\n",
      "Epoch 845: train loss: 0.1249, test loss 0.1431\n",
      "Epoch 846: train loss: 0.1249, test loss 0.1430\n",
      "Epoch 847: train loss: 0.1249, test loss 0.1430\n",
      "Epoch 848: train loss: 0.1249, test loss 0.1431\n",
      "Epoch 849: train loss: 0.1249, test loss 0.1430\n",
      "Epoch 850: train loss: 0.1249, test loss 0.1429\n",
      "Epoch 851: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 852: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 853: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 854: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 855: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 856: train loss: 0.1249, test loss 0.1428\n",
      "Epoch 857: train loss: 0.1248, test loss 0.1427\n",
      "Epoch 858: train loss: 0.1248, test loss 0.1427\n",
      "Epoch 859: train loss: 0.1248, test loss 0.1426\n",
      "Epoch 860: train loss: 0.1248, test loss 0.1425\n",
      "Epoch 861: train loss: 0.1248, test loss 0.1425\n",
      "Epoch 862: train loss: 0.1248, test loss 0.1424\n",
      "Epoch 863: train loss: 0.1248, test loss 0.1423\n",
      "Epoch 864: train loss: 0.1248, test loss 0.1423\n",
      "Epoch 865: train loss: 0.1248, test loss 0.1421\n",
      "Epoch 866: train loss: 0.1248, test loss 0.1419\n",
      "Epoch 867: train loss: 0.1248, test loss 0.1418\n",
      "Epoch 868: train loss: 0.1248, test loss 0.1418\n",
      "Epoch 869: train loss: 0.1248, test loss 0.1419\n",
      "Epoch 870: train loss: 0.1248, test loss 0.1420\n",
      "Epoch 871: train loss: 0.1248, test loss 0.1421\n",
      "Epoch 872: train loss: 0.1248, test loss 0.1421\n",
      "Epoch 873: train loss: 0.1248, test loss 0.1421\n",
      "Epoch 874: train loss: 0.1248, test loss 0.1421\n",
      "Epoch 875: train loss: 0.1248, test loss 0.1420\n",
      "Epoch 876: train loss: 0.1247, test loss 0.1418\n",
      "Epoch 877: train loss: 0.1247, test loss 0.1416\n",
      "Epoch 878: train loss: 0.1247, test loss 0.1416\n",
      "Epoch 879: train loss: 0.1247, test loss 0.1415\n",
      "Epoch 880: train loss: 0.1247, test loss 0.1415\n",
      "Epoch 881: train loss: 0.1247, test loss 0.1415\n",
      "Epoch 882: train loss: 0.1247, test loss 0.1414\n",
      "Epoch 883: train loss: 0.1247, test loss 0.1413\n",
      "Epoch 884: train loss: 0.1247, test loss 0.1412\n",
      "Epoch 885: train loss: 0.1247, test loss 0.1412\n",
      "Epoch 886: train loss: 0.1247, test loss 0.1412\n",
      "Epoch 887: train loss: 0.1247, test loss 0.1413\n",
      "Epoch 888: train loss: 0.1247, test loss 0.1413\n",
      "Epoch 889: train loss: 0.1247, test loss 0.1413\n",
      "Epoch 890: train loss: 0.1247, test loss 0.1412\n",
      "Epoch 891: train loss: 0.1247, test loss 0.1411\n",
      "Epoch 892: train loss: 0.1246, test loss 0.1410\n",
      "Epoch 893: train loss: 0.1246, test loss 0.1409\n",
      "Epoch 894: train loss: 0.1246, test loss 0.1409\n",
      "Epoch 895: train loss: 0.1246, test loss 0.1409\n",
      "Epoch 896: train loss: 0.1246, test loss 0.1408\n",
      "Epoch 897: train loss: 0.1246, test loss 0.1408\n",
      "Epoch 898: train loss: 0.1246, test loss 0.1408\n",
      "Epoch 899: train loss: 0.1246, test loss 0.1407\n",
      "Epoch 900: train loss: 0.1246, test loss 0.1406\n",
      "Epoch 901: train loss: 0.1246, test loss 0.1405\n",
      "Epoch 902: train loss: 0.1246, test loss 0.1405\n",
      "Epoch 903: train loss: 0.1246, test loss 0.1403\n",
      "Epoch 904: train loss: 0.1246, test loss 0.1402\n",
      "Epoch 905: train loss: 0.1245, test loss 0.1401\n",
      "Epoch 906: train loss: 0.1245, test loss 0.1400\n",
      "Epoch 907: train loss: 0.1245, test loss 0.1399\n",
      "Epoch 908: train loss: 0.1245, test loss 0.1399\n",
      "Epoch 909: train loss: 0.1245, test loss 0.1399\n",
      "Epoch 910: train loss: 0.1245, test loss 0.1399\n",
      "Epoch 911: train loss: 0.1245, test loss 0.1398\n",
      "Epoch 912: train loss: 0.1245, test loss 0.1397\n",
      "Epoch 913: train loss: 0.1245, test loss 0.1396\n",
      "Epoch 914: train loss: 0.1245, test loss 0.1395\n",
      "Epoch 915: train loss: 0.1245, test loss 0.1394\n",
      "Epoch 916: train loss: 0.1245, test loss 0.1392\n",
      "Epoch 917: train loss: 0.1245, test loss 0.1390\n",
      "Epoch 918: train loss: 0.1245, test loss 0.1389\n",
      "Epoch 919: train loss: 0.1244, test loss 0.1389\n",
      "Epoch 920: train loss: 0.1244, test loss 0.1389\n",
      "Epoch 921: train loss: 0.1244, test loss 0.1389\n",
      "Epoch 922: train loss: 0.1244, test loss 0.1388\n",
      "Epoch 923: train loss: 0.1244, test loss 0.1387\n",
      "Epoch 924: train loss: 0.1244, test loss 0.1386\n",
      "Epoch 925: train loss: 0.1244, test loss 0.1384\n",
      "Epoch 926: train loss: 0.1244, test loss 0.1382\n",
      "Epoch 927: train loss: 0.1244, test loss 0.1381\n",
      "Epoch 928: train loss: 0.1244, test loss 0.1380\n",
      "Epoch 929: train loss: 0.1244, test loss 0.1380\n",
      "Epoch 930: train loss: 0.1244, test loss 0.1380\n",
      "Epoch 931: train loss: 0.1243, test loss 0.1380\n",
      "Epoch 932: train loss: 0.1243, test loss 0.1377\n",
      "Epoch 933: train loss: 0.1243, test loss 0.1374\n",
      "Epoch 934: train loss: 0.1243, test loss 0.1372\n",
      "Epoch 935: train loss: 0.1243, test loss 0.1372\n",
      "Epoch 936: train loss: 0.1243, test loss 0.1371\n",
      "Epoch 937: train loss: 0.1243, test loss 0.1371\n",
      "Epoch 938: train loss: 0.1243, test loss 0.1370\n",
      "Epoch 939: train loss: 0.1243, test loss 0.1368\n",
      "Epoch 940: train loss: 0.1243, test loss 0.1364\n",
      "Epoch 941: train loss: 0.1243, test loss 0.1363\n",
      "Epoch 942: train loss: 0.1242, test loss 0.1362\n",
      "Epoch 943: train loss: 0.1242, test loss 0.1361\n",
      "Epoch 944: train loss: 0.1242, test loss 0.1360\n",
      "Epoch 945: train loss: 0.1242, test loss 0.1359\n",
      "Epoch 946: train loss: 0.1242, test loss 0.1358\n",
      "Epoch 947: train loss: 0.1242, test loss 0.1357\n",
      "Epoch 948: train loss: 0.1242, test loss 0.1356\n",
      "Epoch 949: train loss: 0.1242, test loss 0.1357\n",
      "Epoch 950: train loss: 0.1242, test loss 0.1358\n",
      "Epoch 951: train loss: 0.1242, test loss 0.1358\n",
      "Epoch 952: train loss: 0.1242, test loss 0.1357\n",
      "Epoch 953: train loss: 0.1241, test loss 0.1357\n",
      "Epoch 954: train loss: 0.1241, test loss 0.1356\n",
      "Epoch 955: train loss: 0.1241, test loss 0.1354\n",
      "Epoch 956: train loss: 0.1241, test loss 0.1354\n",
      "Epoch 957: train loss: 0.1241, test loss 0.1355\n",
      "Epoch 958: train loss: 0.1241, test loss 0.1357\n",
      "Epoch 959: train loss: 0.1241, test loss 0.1357\n",
      "Epoch 960: train loss: 0.1241, test loss 0.1358\n",
      "Epoch 961: train loss: 0.1241, test loss 0.1357\n",
      "Epoch 962: train loss: 0.1241, test loss 0.1355\n",
      "Epoch 963: train loss: 0.1241, test loss 0.1354\n",
      "Epoch 964: train loss: 0.1240, test loss 0.1354\n",
      "Epoch 965: train loss: 0.1240, test loss 0.1354\n",
      "Epoch 966: train loss: 0.1240, test loss 0.1354\n",
      "Epoch 967: train loss: 0.1240, test loss 0.1355\n",
      "Epoch 968: train loss: 0.1240, test loss 0.1354\n",
      "Epoch 969: train loss: 0.1240, test loss 0.1353\n",
      "Epoch 970: train loss: 0.1240, test loss 0.1352\n",
      "Epoch 971: train loss: 0.1240, test loss 0.1351\n",
      "Epoch 972: train loss: 0.1240, test loss 0.1352\n",
      "Epoch 973: train loss: 0.1240, test loss 0.1351\n",
      "Epoch 974: train loss: 0.1239, test loss 0.1352\n",
      "Epoch 975: train loss: 0.1239, test loss 0.1352\n",
      "Epoch 976: train loss: 0.1239, test loss 0.1350\n",
      "Epoch 977: train loss: 0.1239, test loss 0.1348\n",
      "Epoch 978: train loss: 0.1239, test loss 0.1348\n",
      "Epoch 979: train loss: 0.1239, test loss 0.1348\n",
      "Epoch 980: train loss: 0.1239, test loss 0.1347\n",
      "Epoch 981: train loss: 0.1239, test loss 0.1346\n",
      "Epoch 982: train loss: 0.1239, test loss 0.1345\n",
      "Epoch 983: train loss: 0.1239, test loss 0.1342\n",
      "Epoch 984: train loss: 0.1238, test loss 0.1341\n",
      "Epoch 985: train loss: 0.1238, test loss 0.1340\n",
      "Epoch 986: train loss: 0.1238, test loss 0.1339\n",
      "Epoch 987: train loss: 0.1238, test loss 0.1338\n",
      "Epoch 988: train loss: 0.1238, test loss 0.1336\n",
      "Epoch 989: train loss: 0.1238, test loss 0.1334\n",
      "Epoch 990: train loss: 0.1238, test loss 0.1332\n",
      "Epoch 991: train loss: 0.1238, test loss 0.1330\n",
      "Epoch 992: train loss: 0.1238, test loss 0.1328\n",
      "Epoch 993: train loss: 0.1237, test loss 0.1328\n",
      "Epoch 994: train loss: 0.1237, test loss 0.1327\n",
      "Epoch 995: train loss: 0.1237, test loss 0.1326\n",
      "Epoch 996: train loss: 0.1237, test loss 0.1325\n",
      "Epoch 997: train loss: 0.1237, test loss 0.1325\n",
      "Epoch 998: train loss: 0.1237, test loss 0.1325\n",
      "Epoch 999: train loss: 0.1237, test loss 0.1326\n",
      "Epoch 1000: train loss: 0.1237, test loss 0.1323\n",
      "Epoch 1001: train loss: 0.1237, test loss 0.1320\n",
      "Epoch 1002: train loss: 0.1236, test loss 0.1318\n",
      "Epoch 1003: train loss: 0.1236, test loss 0.1317\n",
      "Epoch 1004: train loss: 0.1236, test loss 0.1317\n",
      "Epoch 1005: train loss: 0.1236, test loss 0.1316\n",
      "Epoch 1006: train loss: 0.1236, test loss 0.1315\n",
      "Epoch 1007: train loss: 0.1236, test loss 0.1315\n",
      "Epoch 1008: train loss: 0.1236, test loss 0.1314\n",
      "Epoch 1009: train loss: 0.1236, test loss 0.1313\n",
      "Epoch 1010: train loss: 0.1236, test loss 0.1310\n",
      "Epoch 1011: train loss: 0.1235, test loss 0.1308\n",
      "Epoch 1012: train loss: 0.1235, test loss 0.1308\n",
      "Epoch 1013: train loss: 0.1235, test loss 0.1308\n",
      "Epoch 1014: train loss: 0.1235, test loss 0.1310\n",
      "Epoch 1015: train loss: 0.1235, test loss 0.1308\n",
      "Epoch 1016: train loss: 0.1235, test loss 0.1307\n",
      "Epoch 1017: train loss: 0.1235, test loss 0.1307\n",
      "Epoch 1018: train loss: 0.1235, test loss 0.1307\n",
      "Epoch 1019: train loss: 0.1235, test loss 0.1307\n",
      "Epoch 1020: train loss: 0.1234, test loss 0.1304\n",
      "Epoch 1021: train loss: 0.1234, test loss 0.1302\n",
      "Epoch 1022: train loss: 0.1234, test loss 0.1302\n",
      "Epoch 1023: train loss: 0.1234, test loss 0.1303\n",
      "Epoch 1024: train loss: 0.1234, test loss 0.1304\n",
      "Epoch 1025: train loss: 0.1234, test loss 0.1303\n",
      "Epoch 1026: train loss: 0.1234, test loss 0.1301\n",
      "Epoch 1027: train loss: 0.1234, test loss 0.1301\n",
      "Epoch 1028: train loss: 0.1233, test loss 0.1302\n",
      "Epoch 1029: train loss: 0.1233, test loss 0.1303\n",
      "Epoch 1030: train loss: 0.1233, test loss 0.1299\n",
      "Epoch 1031: train loss: 0.1233, test loss 0.1295\n",
      "Epoch 1032: train loss: 0.1233, test loss 0.1292\n",
      "Epoch 1033: train loss: 0.1233, test loss 0.1290\n",
      "Epoch 1034: train loss: 0.1232, test loss 0.1289\n",
      "Epoch 1035: train loss: 0.1232, test loss 0.1288\n",
      "Epoch 1036: train loss: 0.1232, test loss 0.1286\n",
      "Epoch 1037: train loss: 0.1232, test loss 0.1285\n",
      "Epoch 1038: train loss: 0.1232, test loss 0.1283\n",
      "Epoch 1039: train loss: 0.1231, test loss 0.1282\n",
      "Epoch 1040: train loss: 0.1231, test loss 0.1281\n",
      "Epoch 1041: train loss: 0.1231, test loss 0.1278\n",
      "Epoch 1042: train loss: 0.1231, test loss 0.1274\n",
      "Epoch 1043: train loss: 0.1231, test loss 0.1269\n",
      "Epoch 1044: train loss: 0.1230, test loss 0.1264\n",
      "Epoch 1045: train loss: 0.1230, test loss 0.1260\n",
      "Epoch 1046: train loss: 0.1230, test loss 0.1255\n",
      "Epoch 1047: train loss: 0.1229, test loss 0.1249\n",
      "Epoch 1048: train loss: 0.1229, test loss 0.1243\n",
      "Epoch 1049: train loss: 0.1229, test loss 0.1237\n",
      "Epoch 1050: train loss: 0.1228, test loss 0.1231\n",
      "Epoch 1051: train loss: 0.1228, test loss 0.1224\n",
      "Epoch 1052: train loss: 0.1227, test loss 0.1216\n",
      "Epoch 1053: train loss: 0.1227, test loss 0.1209\n",
      "Epoch 1054: train loss: 0.1226, test loss 0.1203\n",
      "Epoch 1055: train loss: 0.1225, test loss 0.1194\n",
      "Epoch 1056: train loss: 0.1225, test loss 0.1185\n",
      "Epoch 1057: train loss: 0.1224, test loss 0.1175\n",
      "Epoch 1058: train loss: 0.1224, test loss 0.1164\n",
      "Epoch 1059: train loss: 0.1223, test loss 0.1154\n",
      "Epoch 1060: train loss: 0.1222, test loss 0.1141\n",
      "Epoch 1061: train loss: 0.1221, test loss 0.1127\n",
      "Epoch 1062: train loss: 0.1221, test loss 0.1111\n",
      "Epoch 1063: train loss: 0.1220, test loss 0.1098\n",
      "Epoch 1064: train loss: 0.1219, test loss 0.1085\n",
      "Epoch 1065: train loss: 0.1218, test loss 0.1071\n",
      "Epoch 1066: train loss: 0.1217, test loss 0.1057\n",
      "Epoch 1067: train loss: 0.1216, test loss 0.1042\n",
      "Epoch 1068: train loss: 0.1215, test loss 0.1028\n",
      "Epoch 1069: train loss: 0.1214, test loss 0.1014\n",
      "Epoch 1070: train loss: 0.1214, test loss 0.1001\n",
      "Epoch 1071: train loss: 0.1213, test loss 0.0990\n",
      "Epoch 1072: train loss: 0.1212, test loss 0.0982\n",
      "Epoch 1073: train loss: 0.1211, test loss 0.0973\n",
      "Epoch 1074: train loss: 0.1210, test loss 0.0963\n",
      "Epoch 1075: train loss: 0.1209, test loss 0.0954\n",
      "Epoch 1076: train loss: 0.1208, test loss 0.0947\n",
      "Epoch 1077: train loss: 0.1207, test loss 0.0940\n",
      "Epoch 1078: train loss: 0.1206, test loss 0.0930\n",
      "Epoch 1079: train loss: 0.1205, test loss 0.0925\n",
      "Epoch 1080: train loss: 0.1205, test loss 0.0922\n",
      "Epoch 1081: train loss: 0.1204, test loss 0.0915\n",
      "Epoch 1082: train loss: 0.1203, test loss 0.0907\n",
      "Epoch 1083: train loss: 0.1202, test loss 0.0904\n",
      "Epoch 1084: train loss: 0.1202, test loss 0.0906\n",
      "Epoch 1085: train loss: 0.1201, test loss 0.0905\n",
      "Epoch 1086: train loss: 0.1200, test loss 0.0899\n",
      "Epoch 1087: train loss: 0.1200, test loss 0.0895\n",
      "Epoch 1088: train loss: 0.1199, test loss 0.0897\n",
      "Epoch 1089: train loss: 0.1198, test loss 0.0900\n",
      "Epoch 1090: train loss: 0.1197, test loss 0.0902\n",
      "Epoch 1091: train loss: 0.1197, test loss 0.0902\n",
      "Epoch 1092: train loss: 0.1196, test loss 0.0902\n",
      "Epoch 1093: train loss: 0.1195, test loss 0.0902\n",
      "Epoch 1094: train loss: 0.1195, test loss 0.0902\n",
      "Epoch 1095: train loss: 0.1194, test loss 0.0900\n",
      "Epoch 1096: train loss: 0.1193, test loss 0.0900\n",
      "Epoch 1097: train loss: 0.1193, test loss 0.0899\n",
      "Epoch 1098: train loss: 0.1192, test loss 0.0901\n",
      "Epoch 1099: train loss: 0.1191, test loss 0.0905\n",
      "Epoch 1100: train loss: 0.1191, test loss 0.0906\n",
      "Epoch 1101: train loss: 0.1190, test loss 0.0904\n",
      "Epoch 1102: train loss: 0.1189, test loss 0.0901\n",
      "Epoch 1103: train loss: 0.1189, test loss 0.0901\n",
      "Epoch 1104: train loss: 0.1188, test loss 0.0901\n",
      "Epoch 1105: train loss: 0.1187, test loss 0.0897\n",
      "Epoch 1106: train loss: 0.1187, test loss 0.0896\n",
      "Epoch 1107: train loss: 0.1186, test loss 0.0899\n",
      "Epoch 1108: train loss: 0.1185, test loss 0.0899\n",
      "Epoch 1109: train loss: 0.1184, test loss 0.0897\n",
      "Epoch 1110: train loss: 0.1184, test loss 0.0897\n",
      "Epoch 1111: train loss: 0.1183, test loss 0.0896\n",
      "Epoch 1112: train loss: 0.1183, test loss 0.0893\n",
      "Epoch 1113: train loss: 0.1182, test loss 0.0896\n",
      "Epoch 1114: train loss: 0.1181, test loss 0.0899\n",
      "Epoch 1115: train loss: 0.1180, test loss 0.0897\n",
      "Epoch 1116: train loss: 0.1180, test loss 0.0897\n",
      "Epoch 1117: train loss: 0.1179, test loss 0.0900\n",
      "Epoch 1118: train loss: 0.1179, test loss 0.0899\n",
      "Epoch 1119: train loss: 0.1178, test loss 0.0898\n",
      "Epoch 1120: train loss: 0.1177, test loss 0.0901\n",
      "Epoch 1121: train loss: 0.1177, test loss 0.0908\n",
      "Epoch 1122: train loss: 0.1176, test loss 0.0907\n",
      "Epoch 1123: train loss: 0.1176, test loss 0.0901\n",
      "Epoch 1124: train loss: 0.1175, test loss 0.0904\n",
      "Epoch 1125: train loss: 0.1175, test loss 0.0912\n",
      "Epoch 1126: train loss: 0.1174, test loss 0.0910\n",
      "Epoch 1127: train loss: 0.1174, test loss 0.0904\n",
      "Epoch 1128: train loss: 0.1173, test loss 0.0908\n",
      "Epoch 1129: train loss: 0.1173, test loss 0.0917\n",
      "Epoch 1130: train loss: 0.1172, test loss 0.0915\n",
      "Epoch 1131: train loss: 0.1172, test loss 0.0909\n",
      "Epoch 1132: train loss: 0.1171, test loss 0.0912\n",
      "Epoch 1133: train loss: 0.1171, test loss 0.0922\n",
      "Epoch 1134: train loss: 0.1170, test loss 0.0921\n",
      "Epoch 1135: train loss: 0.1170, test loss 0.0914\n",
      "Epoch 1136: train loss: 0.1169, test loss 0.0916\n",
      "Epoch 1137: train loss: 0.1168, test loss 0.0925\n",
      "Epoch 1138: train loss: 0.1168, test loss 0.0928\n",
      "Epoch 1139: train loss: 0.1167, test loss 0.0924\n",
      "Epoch 1140: train loss: 0.1167, test loss 0.0925\n",
      "Epoch 1141: train loss: 0.1166, test loss 0.0931\n",
      "Epoch 1142: train loss: 0.1165, test loss 0.0931\n",
      "Epoch 1143: train loss: 0.1165, test loss 0.0924\n",
      "Epoch 1144: train loss: 0.1164, test loss 0.0923\n",
      "Epoch 1145: train loss: 0.1164, test loss 0.0927\n",
      "Epoch 1146: train loss: 0.1163, test loss 0.0925\n",
      "Epoch 1147: train loss: 0.1163, test loss 0.0925\n",
      "Epoch 1148: train loss: 0.1162, test loss 0.0930\n",
      "Epoch 1149: train loss: 0.1162, test loss 0.0930\n",
      "Epoch 1150: train loss: 0.1161, test loss 0.0927\n",
      "Epoch 1151: train loss: 0.1161, test loss 0.0928\n",
      "Epoch 1152: train loss: 0.1160, test loss 0.0928\n",
      "Epoch 1153: train loss: 0.1160, test loss 0.0924\n",
      "Epoch 1154: train loss: 0.1159, test loss 0.0928\n",
      "Epoch 1155: train loss: 0.1160, test loss 0.0932\n",
      "Epoch 1156: train loss: 0.1159, test loss 0.0929\n",
      "Epoch 1157: train loss: 0.1158, test loss 0.0930\n",
      "Epoch 1158: train loss: 0.1158, test loss 0.0936\n",
      "Epoch 1159: train loss: 0.1157, test loss 0.0933\n",
      "Epoch 1160: train loss: 0.1157, test loss 0.0932\n",
      "Epoch 1161: train loss: 0.1157, test loss 0.0937\n",
      "Epoch 1162: train loss: 0.1156, test loss 0.0936\n",
      "Epoch 1163: train loss: 0.1156, test loss 0.0934\n",
      "Epoch 1164: train loss: 0.1155, test loss 0.0937\n",
      "Epoch 1165: train loss: 0.1155, test loss 0.0935\n",
      "Epoch 1166: train loss: 0.1155, test loss 0.0932\n",
      "Epoch 1167: train loss: 0.1154, test loss 0.0935\n",
      "Epoch 1168: train loss: 0.1154, test loss 0.0936\n",
      "Epoch 1169: train loss: 0.1153, test loss 0.0937\n",
      "Epoch 1170: train loss: 0.1153, test loss 0.0938\n",
      "Epoch 1171: train loss: 0.1153, test loss 0.0935\n",
      "Epoch 1172: train loss: 0.1152, test loss 0.0934\n",
      "Epoch 1173: train loss: 0.1152, test loss 0.0934\n",
      "Epoch 1174: train loss: 0.1152, test loss 0.0932\n",
      "Epoch 1175: train loss: 0.1151, test loss 0.0938\n",
      "Epoch 1176: train loss: 0.1151, test loss 0.0940\n",
      "Epoch 1177: train loss: 0.1151, test loss 0.0938\n",
      "Epoch 1178: train loss: 0.1150, test loss 0.0941\n",
      "Epoch 1179: train loss: 0.1150, test loss 0.0938\n",
      "Epoch 1180: train loss: 0.1149, test loss 0.0939\n",
      "Epoch 1181: train loss: 0.1149, test loss 0.0943\n",
      "Epoch 1182: train loss: 0.1149, test loss 0.0943\n",
      "Epoch 1183: train loss: 0.1148, test loss 0.0949\n",
      "Epoch 1184: train loss: 0.1148, test loss 0.0949\n",
      "Epoch 1185: train loss: 0.1148, test loss 0.0946\n",
      "Epoch 1186: train loss: 0.1147, test loss 0.0947\n",
      "Epoch 1187: train loss: 0.1147, test loss 0.0945\n",
      "Epoch 1188: train loss: 0.1147, test loss 0.0951\n",
      "Epoch 1189: train loss: 0.1146, test loss 0.0952\n",
      "Epoch 1190: train loss: 0.1146, test loss 0.0956\n",
      "Epoch 1191: train loss: 0.1146, test loss 0.0958\n",
      "Epoch 1192: train loss: 0.1145, test loss 0.0956\n",
      "Epoch 1193: train loss: 0.1145, test loss 0.0954\n",
      "Epoch 1194: train loss: 0.1145, test loss 0.0957\n",
      "Epoch 1195: train loss: 0.1144, test loss 0.0958\n",
      "Epoch 1196: train loss: 0.1144, test loss 0.0966\n",
      "Epoch 1197: train loss: 0.1144, test loss 0.0968\n",
      "Epoch 1198: train loss: 0.1143, test loss 0.0967\n",
      "Epoch 1199: train loss: 0.1143, test loss 0.0966\n",
      "Epoch 1200: train loss: 0.1142, test loss 0.0963\n",
      "Epoch 1201: train loss: 0.1142, test loss 0.0961\n",
      "Epoch 1202: train loss: 0.1142, test loss 0.0964\n",
      "Epoch 1203: train loss: 0.1141, test loss 0.0967\n",
      "Epoch 1204: train loss: 0.1141, test loss 0.0967\n",
      "Epoch 1205: train loss: 0.1141, test loss 0.0969\n",
      "Epoch 1206: train loss: 0.1140, test loss 0.0969\n",
      "Epoch 1207: train loss: 0.1140, test loss 0.0966\n",
      "Epoch 1208: train loss: 0.1140, test loss 0.0970\n",
      "Epoch 1209: train loss: 0.1140, test loss 0.0970\n",
      "Epoch 1210: train loss: 0.1139, test loss 0.0974\n",
      "Epoch 1211: train loss: 0.1139, test loss 0.0975\n",
      "Epoch 1212: train loss: 0.1140, test loss 0.0972\n",
      "Epoch 1213: train loss: 0.1138, test loss 0.0976\n",
      "Epoch 1214: train loss: 0.1138, test loss 0.0976\n",
      "Epoch 1215: train loss: 0.1138, test loss 0.0975\n",
      "Epoch 1216: train loss: 0.1137, test loss 0.0978\n",
      "Epoch 1217: train loss: 0.1137, test loss 0.0977\n",
      "Epoch 1218: train loss: 0.1136, test loss 0.0979\n",
      "Epoch 1219: train loss: 0.1136, test loss 0.0979\n",
      "Epoch 1220: train loss: 0.1136, test loss 0.0978\n",
      "Epoch 1221: train loss: 0.1135, test loss 0.0979\n",
      "Epoch 1222: train loss: 0.1135, test loss 0.0982\n",
      "Epoch 1223: train loss: 0.1135, test loss 0.0983\n",
      "Epoch 1224: train loss: 0.1135, test loss 0.0986\n",
      "Epoch 1225: train loss: 0.1134, test loss 0.0983\n",
      "Epoch 1226: train loss: 0.1134, test loss 0.0985\n",
      "Epoch 1227: train loss: 0.1133, test loss 0.0983\n",
      "Epoch 1228: train loss: 0.1133, test loss 0.0985\n",
      "Epoch 1229: train loss: 0.1133, test loss 0.0985\n",
      "Epoch 1230: train loss: 0.1132, test loss 0.0989\n",
      "Epoch 1231: train loss: 0.1132, test loss 0.0989\n",
      "Epoch 1232: train loss: 0.1131, test loss 0.0986\n",
      "Epoch 1233: train loss: 0.1131, test loss 0.0985\n",
      "Epoch 1234: train loss: 0.1131, test loss 0.0982\n",
      "Epoch 1235: train loss: 0.1130, test loss 0.0988\n",
      "Epoch 1236: train loss: 0.1130, test loss 0.0989\n",
      "Epoch 1237: train loss: 0.1129, test loss 0.0992\n",
      "Epoch 1238: train loss: 0.1129, test loss 0.0990\n",
      "Epoch 1239: train loss: 0.1129, test loss 0.0985\n",
      "Epoch 1240: train loss: 0.1128, test loss 0.0987\n",
      "Epoch 1241: train loss: 0.1127, test loss 0.0986\n",
      "Epoch 1242: train loss: 0.1126, test loss 0.0990\n",
      "Epoch 1243: train loss: 0.1126, test loss 0.0995\n",
      "Epoch 1244: train loss: 0.1125, test loss 0.0994\n",
      "Epoch 1245: train loss: 0.1124, test loss 0.0992\n",
      "Epoch 1246: train loss: 0.1123, test loss 0.0986\n",
      "Epoch 1247: train loss: 0.1123, test loss 0.0989\n",
      "Epoch 1248: train loss: 0.1122, test loss 0.0987\n",
      "Epoch 1249: train loss: 0.1121, test loss 0.0994\n",
      "Epoch 1250: train loss: 0.1120, test loss 0.0996\n",
      "Epoch 1251: train loss: 0.1119, test loss 0.0995\n",
      "Epoch 1252: train loss: 0.1118, test loss 0.0993\n",
      "Epoch 1253: train loss: 0.1118, test loss 0.0987\n",
      "Epoch 1254: train loss: 0.1117, test loss 0.0990\n",
      "Epoch 1255: train loss: 0.1116, test loss 0.0993\n",
      "Epoch 1256: train loss: 0.1115, test loss 0.0999\n",
      "Epoch 1257: train loss: 0.1114, test loss 0.0999\n",
      "Epoch 1258: train loss: 0.1114, test loss 0.0994\n",
      "Epoch 1259: train loss: 0.1113, test loss 0.0997\n",
      "Epoch 1260: train loss: 0.1112, test loss 0.0996\n",
      "Epoch 1261: train loss: 0.1111, test loss 0.1001\n",
      "Epoch 1262: train loss: 0.1111, test loss 0.1006\n",
      "Epoch 1263: train loss: 0.1110, test loss 0.1005\n",
      "Epoch 1264: train loss: 0.1109, test loss 0.1007\n",
      "Epoch 1265: train loss: 0.1109, test loss 0.1008\n",
      "Epoch 1266: train loss: 0.1109, test loss 0.1002\n",
      "Epoch 1267: train loss: 0.1108, test loss 0.1004\n",
      "Epoch 1268: train loss: 0.1108, test loss 0.1012\n",
      "Epoch 1269: train loss: 0.1107, test loss 0.1011\n",
      "Epoch 1270: train loss: 0.1106, test loss 0.1016\n",
      "Epoch 1271: train loss: 0.1106, test loss 0.1017\n",
      "Epoch 1272: train loss: 0.1106, test loss 0.1011\n",
      "Epoch 1273: train loss: 0.1105, test loss 0.1013\n",
      "Epoch 1274: train loss: 0.1104, test loss 0.1015\n",
      "Epoch 1275: train loss: 0.1105, test loss 0.1014\n",
      "Epoch 1276: train loss: 0.1104, test loss 0.1021\n",
      "Epoch 1277: train loss: 0.1103, test loss 0.1018\n",
      "Epoch 1278: train loss: 0.1103, test loss 0.1018\n",
      "Epoch 1279: train loss: 0.1102, test loss 0.1020\n",
      "Epoch 1280: train loss: 0.1102, test loss 0.1015\n",
      "Epoch 1281: train loss: 0.1101, test loss 0.1018\n",
      "Epoch 1282: train loss: 0.1101, test loss 0.1020\n",
      "Epoch 1283: train loss: 0.1101, test loss 0.1018\n",
      "Epoch 1284: train loss: 0.1100, test loss 0.1022\n",
      "Epoch 1285: train loss: 0.1099, test loss 0.1016\n",
      "Epoch 1286: train loss: 0.1099, test loss 0.1017\n",
      "Epoch 1287: train loss: 0.1098, test loss 0.1016\n",
      "Epoch 1288: train loss: 0.1098, test loss 0.1019\n",
      "Epoch 1289: train loss: 0.1097, test loss 0.1017\n",
      "Epoch 1290: train loss: 0.1097, test loss 0.1017\n",
      "Epoch 1291: train loss: 0.1097, test loss 0.1012\n",
      "Epoch 1292: train loss: 0.1096, test loss 0.1019\n",
      "Epoch 1293: train loss: 0.1096, test loss 0.1015\n",
      "Epoch 1294: train loss: 0.1095, test loss 0.1020\n",
      "Epoch 1295: train loss: 0.1094, test loss 0.1022\n",
      "Epoch 1296: train loss: 0.1094, test loss 0.1017\n",
      "Epoch 1297: train loss: 0.1094, test loss 0.1023\n",
      "Epoch 1298: train loss: 0.1093, test loss 0.1016\n",
      "Epoch 1299: train loss: 0.1092, test loss 0.1021\n",
      "Epoch 1300: train loss: 0.1091, test loss 0.1025\n",
      "Epoch 1301: train loss: 0.1092, test loss 0.1022\n",
      "Epoch 1302: train loss: 0.1091, test loss 0.1032\n",
      "Epoch 1303: train loss: 0.1089, test loss 0.1026\n",
      "Epoch 1304: train loss: 0.1089, test loss 0.1024\n",
      "Epoch 1305: train loss: 0.1089, test loss 0.1029\n",
      "Epoch 1306: train loss: 0.1088, test loss 0.1022\n",
      "Epoch 1307: train loss: 0.1086, test loss 0.1027\n",
      "Epoch 1308: train loss: 0.1086, test loss 0.1034\n",
      "Epoch 1309: train loss: 0.1086, test loss 0.1030\n",
      "Epoch 1310: train loss: 0.1084, test loss 0.1036\n",
      "Epoch 1311: train loss: 0.1083, test loss 0.1032\n",
      "Epoch 1312: train loss: 0.1082, test loss 0.1028\n",
      "Epoch 1313: train loss: 0.1081, test loss 0.1031\n",
      "Epoch 1314: train loss: 0.1081, test loss 0.1029\n",
      "Epoch 1315: train loss: 0.1080, test loss 0.1036\n",
      "Epoch 1316: train loss: 0.1079, test loss 0.1033\n",
      "Epoch 1317: train loss: 0.1078, test loss 0.1033\n",
      "Epoch 1318: train loss: 0.1077, test loss 0.1031\n",
      "Epoch 1319: train loss: 0.1076, test loss 0.1025\n",
      "Epoch 1320: train loss: 0.1075, test loss 0.1032\n",
      "Epoch 1321: train loss: 0.1074, test loss 0.1029\n",
      "Epoch 1322: train loss: 0.1073, test loss 0.1036\n",
      "Epoch 1323: train loss: 0.1072, test loss 0.1037\n",
      "Epoch 1324: train loss: 0.1072, test loss 0.1031\n",
      "Epoch 1325: train loss: 0.1071, test loss 0.1036\n",
      "Epoch 1326: train loss: 0.1070, test loss 0.1031\n",
      "Epoch 1327: train loss: 0.1068, test loss 0.1037\n",
      "Epoch 1328: train loss: 0.1067, test loss 0.1040\n",
      "Epoch 1329: train loss: 0.1067, test loss 0.1039\n",
      "Epoch 1330: train loss: 0.1066, test loss 0.1045\n",
      "Epoch 1331: train loss: 0.1065, test loss 0.1040\n",
      "Epoch 1332: train loss: 0.1064, test loss 0.1041\n",
      "Epoch 1333: train loss: 0.1063, test loss 0.1046\n",
      "Epoch 1334: train loss: 0.1063, test loss 0.1043\n",
      "Epoch 1335: train loss: 0.1061, test loss 0.1046\n",
      "Epoch 1336: train loss: 0.1060, test loss 0.1045\n",
      "Epoch 1337: train loss: 0.1060, test loss 0.1042\n",
      "Epoch 1338: train loss: 0.1059, test loss 0.1044\n",
      "Epoch 1339: train loss: 0.1058, test loss 0.1047\n",
      "Epoch 1340: train loss: 0.1057, test loss 0.1054\n",
      "Epoch 1341: train loss: 0.1056, test loss 0.1053\n",
      "Epoch 1342: train loss: 0.1056, test loss 0.1053\n",
      "Epoch 1343: train loss: 0.1055, test loss 0.1050\n",
      "Epoch 1344: train loss: 0.1054, test loss 0.1050\n",
      "Epoch 1345: train loss: 0.1054, test loss 0.1059\n",
      "Epoch 1346: train loss: 0.1052, test loss 0.1060\n",
      "Epoch 1347: train loss: 0.1052, test loss 0.1062\n",
      "Epoch 1348: train loss: 0.1052, test loss 0.1071\n",
      "Epoch 1349: train loss: 0.1051, test loss 0.1062\n",
      "Epoch 1350: train loss: 0.1050, test loss 0.1063\n",
      "Epoch 1351: train loss: 0.1049, test loss 0.1069\n",
      "Epoch 1352: train loss: 0.1049, test loss 0.1068\n",
      "Epoch 1353: train loss: 0.1047, test loss 0.1074\n",
      "Epoch 1354: train loss: 0.1047, test loss 0.1077\n",
      "Epoch 1355: train loss: 0.1046, test loss 0.1074\n",
      "Epoch 1356: train loss: 0.1046, test loss 0.1079\n",
      "Epoch 1357: train loss: 0.1045, test loss 0.1076\n",
      "Epoch 1358: train loss: 0.1044, test loss 0.1082\n",
      "Epoch 1359: train loss: 0.1044, test loss 0.1083\n",
      "Epoch 1360: train loss: 0.1044, test loss 0.1088\n",
      "Epoch 1361: train loss: 0.1043, test loss 0.1086\n",
      "Epoch 1362: train loss: 0.1042, test loss 0.1092\n",
      "Epoch 1363: train loss: 0.1041, test loss 0.1093\n",
      "Epoch 1364: train loss: 0.1041, test loss 0.1094\n",
      "Epoch 1365: train loss: 0.1042, test loss 0.1099\n",
      "Epoch 1366: train loss: 0.1040, test loss 0.1092\n",
      "Epoch 1367: train loss: 0.1039, test loss 0.1093\n",
      "Epoch 1368: train loss: 0.1038, test loss 0.1099\n",
      "Epoch 1369: train loss: 0.1040, test loss 0.1104\n",
      "Epoch 1370: train loss: 0.1040, test loss 0.1111\n",
      "Epoch 1371: train loss: 0.1037, test loss 0.1108\n",
      "Epoch 1372: train loss: 0.1038, test loss 0.1104\n",
      "Epoch 1373: train loss: 0.1039, test loss 0.1105\n",
      "Epoch 1374: train loss: 0.1035, test loss 0.1096\n",
      "Epoch 1375: train loss: 0.1037, test loss 0.1102\n",
      "Epoch 1376: train loss: 0.1038, test loss 0.1112\n",
      "Epoch 1377: train loss: 0.1034, test loss 0.1114\n",
      "Epoch 1378: train loss: 0.1038, test loss 0.1119\n",
      "Epoch 1379: train loss: 0.1038, test loss 0.1118\n",
      "Epoch 1380: train loss: 0.1033, test loss 0.1112\n",
      "Epoch 1381: train loss: 0.1042, test loss 0.1121\n",
      "Epoch 1382: train loss: 0.1035, test loss 0.1108\n",
      "Epoch 1383: train loss: 0.1034, test loss 0.1108\n",
      "Epoch 1384: train loss: 0.1039, test loss 0.1119\n",
      "Epoch 1385: train loss: 0.1030, test loss 0.1111\n",
      "Epoch 1386: train loss: 0.1035, test loss 0.1121\n",
      "Epoch 1387: train loss: 0.1034, test loss 0.1130\n",
      "Epoch 1388: train loss: 0.1028, test loss 0.1125\n",
      "Epoch 1389: train loss: 0.1031, test loss 0.1126\n",
      "Epoch 1390: train loss: 0.1030, test loss 0.1127\n",
      "Epoch 1391: train loss: 0.1027, test loss 0.1121\n",
      "Epoch 1392: train loss: 0.1029, test loss 0.1127\n",
      "Epoch 1393: train loss: 0.1029, test loss 0.1135\n",
      "Epoch 1394: train loss: 0.1025, test loss 0.1134\n",
      "Epoch 1395: train loss: 0.1027, test loss 0.1137\n",
      "Epoch 1396: train loss: 0.1027, test loss 0.1143\n",
      "Epoch 1397: train loss: 0.1024, test loss 0.1138\n",
      "Epoch 1398: train loss: 0.1025, test loss 0.1137\n",
      "Epoch 1399: train loss: 0.1025, test loss 0.1141\n",
      "Epoch 1400: train loss: 0.1023, test loss 0.1139\n",
      "Epoch 1401: train loss: 0.1023, test loss 0.1143\n",
      "Epoch 1402: train loss: 0.1023, test loss 0.1149\n",
      "Epoch 1403: train loss: 0.1022, test loss 0.1145\n",
      "Epoch 1404: train loss: 0.1021, test loss 0.1146\n",
      "Epoch 1405: train loss: 0.1022, test loss 0.1152\n",
      "Epoch 1406: train loss: 0.1021, test loss 0.1149\n",
      "Epoch 1407: train loss: 0.1020, test loss 0.1152\n",
      "Epoch 1408: train loss: 0.1020, test loss 0.1156\n",
      "Epoch 1409: train loss: 0.1019, test loss 0.1157\n",
      "Epoch 1410: train loss: 0.1018, test loss 0.1160\n",
      "Epoch 1411: train loss: 0.1018, test loss 0.1162\n",
      "Epoch 1412: train loss: 0.1018, test loss 0.1162\n",
      "Epoch 1413: train loss: 0.1017, test loss 0.1162\n",
      "Epoch 1414: train loss: 0.1017, test loss 0.1162\n",
      "Epoch 1415: train loss: 0.1016, test loss 0.1162\n",
      "Epoch 1416: train loss: 0.1016, test loss 0.1163\n",
      "Epoch 1417: train loss: 0.1016, test loss 0.1163\n",
      "Epoch 1418: train loss: 0.1015, test loss 0.1164\n",
      "Epoch 1419: train loss: 0.1015, test loss 0.1167\n",
      "Epoch 1420: train loss: 0.1015, test loss 0.1169\n",
      "Epoch 1421: train loss: 0.1015, test loss 0.1176\n",
      "Epoch 1422: train loss: 0.1015, test loss 0.1171\n",
      "Epoch 1423: train loss: 0.1013, test loss 0.1171\n",
      "Epoch 1424: train loss: 0.1013, test loss 0.1170\n",
      "Epoch 1425: train loss: 0.1014, test loss 0.1164\n",
      "Epoch 1426: train loss: 0.1012, test loss 0.1168\n",
      "Epoch 1427: train loss: 0.1011, test loss 0.1169\n",
      "Epoch 1428: train loss: 0.1011, test loss 0.1170\n",
      "Epoch 1429: train loss: 0.1011, test loss 0.1178\n",
      "Epoch 1430: train loss: 0.1011, test loss 0.1173\n",
      "Epoch 1431: train loss: 0.1010, test loss 0.1175\n",
      "Epoch 1432: train loss: 0.1009, test loss 0.1174\n",
      "Epoch 1433: train loss: 0.1010, test loss 0.1171\n",
      "Epoch 1434: train loss: 0.1009, test loss 0.1178\n",
      "Epoch 1435: train loss: 0.1009, test loss 0.1173\n",
      "Epoch 1436: train loss: 0.1008, test loss 0.1178\n",
      "Epoch 1437: train loss: 0.1007, test loss 0.1179\n",
      "Epoch 1438: train loss: 0.1008, test loss 0.1176\n",
      "Epoch 1439: train loss: 0.1007, test loss 0.1189\n",
      "Epoch 1440: train loss: 0.1006, test loss 0.1187\n",
      "Epoch 1441: train loss: 0.1005, test loss 0.1187\n",
      "Epoch 1442: train loss: 0.1006, test loss 0.1187\n",
      "Epoch 1443: train loss: 0.1007, test loss 0.1174\n",
      "Epoch 1444: train loss: 0.1005, test loss 0.1184\n",
      "Epoch 1445: train loss: 0.1004, test loss 0.1181\n",
      "Epoch 1446: train loss: 0.1003, test loss 0.1188\n",
      "Epoch 1447: train loss: 0.1003, test loss 0.1198\n",
      "Epoch 1448: train loss: 0.1005, test loss 0.1191\n",
      "Epoch 1449: train loss: 0.1004, test loss 0.1205\n",
      "Epoch 1450: train loss: 0.1002, test loss 0.1193\n",
      "Epoch 1451: train loss: 0.1001, test loss 0.1193\n",
      "Epoch 1452: train loss: 0.1002, test loss 0.1203\n",
      "Epoch 1453: train loss: 0.1003, test loss 0.1194\n",
      "Epoch 1454: train loss: 0.1001, test loss 0.1207\n",
      "Epoch 1455: train loss: 0.0999, test loss 0.1197\n",
      "Epoch 1456: train loss: 0.0999, test loss 0.1200\n",
      "Epoch 1457: train loss: 0.0999, test loss 0.1207\n",
      "Epoch 1458: train loss: 0.1000, test loss 0.1197\n",
      "Epoch 1459: train loss: 0.0999, test loss 0.1209\n",
      "Epoch 1460: train loss: 0.0997, test loss 0.1197\n",
      "Epoch 1461: train loss: 0.0996, test loss 0.1200\n",
      "Epoch 1462: train loss: 0.0998, test loss 0.1213\n",
      "Epoch 1463: train loss: 0.0998, test loss 0.1201\n",
      "Epoch 1464: train loss: 0.0995, test loss 0.1209\n",
      "Epoch 1465: train loss: 0.0994, test loss 0.1200\n",
      "Epoch 1466: train loss: 0.0994, test loss 0.1202\n",
      "Epoch 1467: train loss: 0.0993, test loss 0.1204\n",
      "Epoch 1468: train loss: 0.0993, test loss 0.1203\n",
      "Epoch 1469: train loss: 0.0992, test loss 0.1207\n",
      "Epoch 1470: train loss: 0.0994, test loss 0.1199\n",
      "Epoch 1471: train loss: 0.0993, test loss 0.1208\n",
      "Epoch 1472: train loss: 0.0992, test loss 0.1193\n",
      "Epoch 1473: train loss: 0.0990, test loss 0.1199\n",
      "Epoch 1474: train loss: 0.0990, test loss 0.1198\n",
      "Epoch 1475: train loss: 0.0989, test loss 0.1207\n",
      "Epoch 1476: train loss: 0.0990, test loss 0.1203\n",
      "Epoch 1477: train loss: 0.0990, test loss 0.1212\n",
      "Epoch 1478: train loss: 0.0989, test loss 0.1194\n",
      "Epoch 1479: train loss: 0.0987, test loss 0.1199\n",
      "Epoch 1480: train loss: 0.0986, test loss 0.1197\n",
      "Epoch 1481: train loss: 0.0986, test loss 0.1200\n",
      "Epoch 1482: train loss: 0.0988, test loss 0.1218\n",
      "Epoch 1483: train loss: 0.0989, test loss 0.1203\n",
      "Epoch 1484: train loss: 0.0985, test loss 0.1208\n",
      "Epoch 1485: train loss: 0.0984, test loss 0.1194\n",
      "Epoch 1486: train loss: 0.0983, test loss 0.1196\n",
      "Epoch 1487: train loss: 0.0983, test loss 0.1206\n",
      "Epoch 1488: train loss: 0.0983, test loss 0.1205\n",
      "Epoch 1489: train loss: 0.0985, test loss 0.1216\n",
      "Epoch 1490: train loss: 0.0985, test loss 0.1193\n",
      "Epoch 1491: train loss: 0.0981, test loss 0.1194\n",
      "Epoch 1492: train loss: 0.0980, test loss 0.1183\n",
      "Epoch 1493: train loss: 0.0980, test loss 0.1186\n",
      "Epoch 1494: train loss: 0.0980, test loss 0.1204\n",
      "Epoch 1495: train loss: 0.0982, test loss 0.1199\n",
      "Epoch 1496: train loss: 0.0980, test loss 0.1209\n",
      "Epoch 1497: train loss: 0.0978, test loss 0.1192\n",
      "Epoch 1498: train loss: 0.0977, test loss 0.1190\n",
      "Epoch 1499: train loss: 0.0976, test loss 0.1181\n",
      "Epoch 1500: train loss: 0.0975, test loss 0.1189\n",
      "Epoch 1501: train loss: 0.0975, test loss 0.1194\n",
      "Epoch 1502: train loss: 0.0975, test loss 0.1205\n",
      "Epoch 1503: train loss: 0.0975, test loss 0.1199\n",
      "Epoch 1504: train loss: 0.0975, test loss 0.1203\n",
      "Epoch 1505: train loss: 0.0974, test loss 0.1182\n",
      "Epoch 1506: train loss: 0.0972, test loss 0.1183\n",
      "Epoch 1507: train loss: 0.0973, test loss 0.1191\n",
      "Epoch 1508: train loss: 0.0974, test loss 0.1186\n",
      "Epoch 1509: train loss: 0.0972, test loss 0.1201\n",
      "Epoch 1510: train loss: 0.0970, test loss 0.1198\n",
      "Epoch 1511: train loss: 0.0970, test loss 0.1200\n",
      "Epoch 1512: train loss: 0.0970, test loss 0.1208\n",
      "Epoch 1513: train loss: 0.0971, test loss 0.1199\n",
      "Epoch 1514: train loss: 0.0971, test loss 0.1210\n",
      "Epoch 1515: train loss: 0.0969, test loss 0.1194\n",
      "Epoch 1516: train loss: 0.0968, test loss 0.1194\n",
      "Epoch 1517: train loss: 0.0969, test loss 0.1209\n",
      "Epoch 1518: train loss: 0.0968, test loss 0.1202\n",
      "Epoch 1519: train loss: 0.0968, test loss 0.1213\n",
      "Epoch 1520: train loss: 0.0967, test loss 0.1204\n",
      "Epoch 1521: train loss: 0.0966, test loss 0.1204\n",
      "Epoch 1522: train loss: 0.0965, test loss 0.1205\n",
      "Epoch 1523: train loss: 0.0966, test loss 0.1203\n",
      "Epoch 1524: train loss: 0.0966, test loss 0.1215\n",
      "Epoch 1525: train loss: 0.0965, test loss 0.1201\n",
      "Epoch 1526: train loss: 0.0964, test loss 0.1201\n",
      "Epoch 1527: train loss: 0.0964, test loss 0.1202\n",
      "Epoch 1528: train loss: 0.0964, test loss 0.1196\n",
      "Epoch 1529: train loss: 0.0963, test loss 0.1207\n",
      "Epoch 1530: train loss: 0.0963, test loss 0.1202\n",
      "Epoch 1531: train loss: 0.0962, test loss 0.1205\n",
      "Epoch 1532: train loss: 0.0962, test loss 0.1203\n",
      "Epoch 1533: train loss: 0.0961, test loss 0.1199\n",
      "Epoch 1534: train loss: 0.0962, test loss 0.1202\n",
      "Epoch 1535: train loss: 0.0961, test loss 0.1193\n",
      "Epoch 1536: train loss: 0.0961, test loss 0.1199\n",
      "Epoch 1537: train loss: 0.0960, test loss 0.1193\n",
      "Epoch 1538: train loss: 0.0960, test loss 0.1198\n",
      "Epoch 1539: train loss: 0.0959, test loss 0.1201\n",
      "Epoch 1540: train loss: 0.0959, test loss 0.1200\n",
      "Epoch 1541: train loss: 0.0960, test loss 0.1207\n",
      "Epoch 1542: train loss: 0.0960, test loss 0.1193\n",
      "Epoch 1543: train loss: 0.0959, test loss 0.1197\n",
      "Epoch 1544: train loss: 0.0958, test loss 0.1190\n",
      "Epoch 1545: train loss: 0.0958, test loss 0.1190\n",
      "Epoch 1546: train loss: 0.0960, test loss 0.1207\n",
      "Epoch 1547: train loss: 0.0960, test loss 0.1195\n",
      "Epoch 1548: train loss: 0.0957, test loss 0.1204\n",
      "Epoch 1549: train loss: 0.0956, test loss 0.1199\n",
      "Epoch 1550: train loss: 0.0958, test loss 0.1192\n",
      "Epoch 1551: train loss: 0.0960, test loss 0.1208\n",
      "Epoch 1552: train loss: 0.0956, test loss 0.1188\n",
      "Epoch 1553: train loss: 0.0955, test loss 0.1191\n",
      "Epoch 1554: train loss: 0.0955, test loss 0.1200\n",
      "Epoch 1555: train loss: 0.0955, test loss 0.1200\n",
      "Epoch 1556: train loss: 0.0955, test loss 0.1212\n",
      "Epoch 1557: train loss: 0.0954, test loss 0.1210\n",
      "Epoch 1558: train loss: 0.0954, test loss 0.1206\n",
      "Epoch 1559: train loss: 0.0955, test loss 0.1211\n",
      "Epoch 1560: train loss: 0.0954, test loss 0.1195\n",
      "Epoch 1561: train loss: 0.0954, test loss 0.1200\n",
      "Epoch 1562: train loss: 0.0953, test loss 0.1195\n",
      "Epoch 1563: train loss: 0.0953, test loss 0.1194\n",
      "Epoch 1564: train loss: 0.0955, test loss 0.1213\n",
      "Epoch 1565: train loss: 0.0952, test loss 0.1203\n",
      "Epoch 1566: train loss: 0.0952, test loss 0.1205\n",
      "Epoch 1567: train loss: 0.0952, test loss 0.1212\n",
      "Epoch 1568: train loss: 0.0951, test loss 0.1203\n",
      "Epoch 1569: train loss: 0.0951, test loss 0.1204\n",
      "Epoch 1570: train loss: 0.0950, test loss 0.1198\n",
      "Epoch 1571: train loss: 0.0951, test loss 0.1194\n",
      "Epoch 1572: train loss: 0.0951, test loss 0.1204\n",
      "Epoch 1573: train loss: 0.0950, test loss 0.1200\n",
      "Epoch 1574: train loss: 0.0949, test loss 0.1203\n",
      "Epoch 1575: train loss: 0.0950, test loss 0.1211\n",
      "Epoch 1576: train loss: 0.0949, test loss 0.1206\n",
      "Epoch 1577: train loss: 0.0949, test loss 0.1206\n",
      "Epoch 1578: train loss: 0.0949, test loss 0.1205\n",
      "Epoch 1579: train loss: 0.0948, test loss 0.1197\n",
      "Epoch 1580: train loss: 0.0948, test loss 0.1192\n",
      "Epoch 1581: train loss: 0.0948, test loss 0.1197\n",
      "Epoch 1582: train loss: 0.0948, test loss 0.1195\n",
      "Epoch 1583: train loss: 0.0947, test loss 0.1206\n",
      "Epoch 1584: train loss: 0.0947, test loss 0.1211\n",
      "Epoch 1585: train loss: 0.0947, test loss 0.1211\n",
      "Epoch 1586: train loss: 0.0948, test loss 0.1221\n",
      "Epoch 1587: train loss: 0.0947, test loss 0.1203\n",
      "Epoch 1588: train loss: 0.0946, test loss 0.1206\n",
      "Epoch 1589: train loss: 0.0945, test loss 0.1201\n",
      "Epoch 1590: train loss: 0.0946, test loss 0.1200\n",
      "Epoch 1591: train loss: 0.0947, test loss 0.1217\n",
      "Epoch 1592: train loss: 0.0945, test loss 0.1217\n",
      "Epoch 1593: train loss: 0.0944, test loss 0.1226\n",
      "Epoch 1594: train loss: 0.0945, test loss 0.1236\n",
      "Epoch 1595: train loss: 0.0944, test loss 0.1227\n",
      "Epoch 1596: train loss: 0.0944, test loss 0.1228\n",
      "Epoch 1597: train loss: 0.0943, test loss 0.1218\n",
      "Epoch 1598: train loss: 0.0943, test loss 0.1217\n",
      "Epoch 1599: train loss: 0.0943, test loss 0.1218\n",
      "Epoch 1600: train loss: 0.0942, test loss 0.1221\n",
      "Epoch 1601: train loss: 0.0942, test loss 0.1229\n",
      "Epoch 1602: train loss: 0.0942, test loss 0.1232\n",
      "Epoch 1603: train loss: 0.0941, test loss 0.1233\n",
      "Epoch 1604: train loss: 0.0941, test loss 0.1231\n",
      "Epoch 1605: train loss: 0.0941, test loss 0.1231\n",
      "Epoch 1606: train loss: 0.0941, test loss 0.1229\n",
      "Epoch 1607: train loss: 0.0941, test loss 0.1238\n",
      "Epoch 1608: train loss: 0.0941, test loss 0.1237\n",
      "Epoch 1609: train loss: 0.0940, test loss 0.1252\n",
      "Epoch 1610: train loss: 0.0940, test loss 0.1252\n",
      "Epoch 1611: train loss: 0.0939, test loss 0.1258\n",
      "Epoch 1612: train loss: 0.0939, test loss 0.1256\n",
      "Epoch 1613: train loss: 0.0939, test loss 0.1254\n",
      "Epoch 1614: train loss: 0.0938, test loss 0.1254\n",
      "Epoch 1615: train loss: 0.0938, test loss 0.1254\n",
      "Epoch 1616: train loss: 0.0938, test loss 0.1267\n",
      "Epoch 1617: train loss: 0.0939, test loss 0.1264\n",
      "Epoch 1618: train loss: 0.0940, test loss 0.1284\n",
      "Epoch 1619: train loss: 0.0940, test loss 0.1263\n",
      "Epoch 1620: train loss: 0.0938, test loss 0.1275\n",
      "Epoch 1621: train loss: 0.0936, test loss 0.1263\n",
      "Epoch 1622: train loss: 0.0937, test loss 0.1262\n",
      "Epoch 1623: train loss: 0.0939, test loss 0.1282\n",
      "Epoch 1624: train loss: 0.0941, test loss 0.1266\n",
      "Epoch 1625: train loss: 0.0938, test loss 0.1290\n",
      "Epoch 1626: train loss: 0.0936, test loss 0.1275\n",
      "Epoch 1627: train loss: 0.0935, test loss 0.1283\n",
      "Epoch 1628: train loss: 0.0934, test loss 0.1282\n",
      "Epoch 1629: train loss: 0.0935, test loss 0.1274\n",
      "Epoch 1630: train loss: 0.0936, test loss 0.1288\n",
      "Epoch 1631: train loss: 0.0936, test loss 0.1273\n",
      "Epoch 1632: train loss: 0.0935, test loss 0.1288\n",
      "Epoch 1633: train loss: 0.0933, test loss 0.1279\n",
      "Epoch 1634: train loss: 0.0933, test loss 0.1283\n",
      "Epoch 1635: train loss: 0.0932, test loss 0.1286\n",
      "Epoch 1636: train loss: 0.0932, test loss 0.1282\n",
      "Epoch 1637: train loss: 0.0933, test loss 0.1293\n",
      "Epoch 1638: train loss: 0.0933, test loss 0.1282\n",
      "Epoch 1639: train loss: 0.0932, test loss 0.1294\n",
      "Epoch 1640: train loss: 0.0931, test loss 0.1285\n",
      "Epoch 1641: train loss: 0.0930, test loss 0.1293\n",
      "Epoch 1642: train loss: 0.0930, test loss 0.1291\n",
      "Epoch 1643: train loss: 0.0930, test loss 0.1297\n",
      "Epoch 1644: train loss: 0.0930, test loss 0.1292\n",
      "Epoch 1645: train loss: 0.0930, test loss 0.1304\n",
      "Epoch 1646: train loss: 0.0930, test loss 0.1291\n",
      "Epoch 1647: train loss: 0.0930, test loss 0.1312\n",
      "Epoch 1648: train loss: 0.0929, test loss 0.1299\n",
      "Epoch 1649: train loss: 0.0927, test loss 0.1311\n",
      "Epoch 1650: train loss: 0.0926, test loss 0.1309\n",
      "Epoch 1651: train loss: 0.0925, test loss 0.1316\n",
      "Epoch 1652: train loss: 0.0925, test loss 0.1320\n",
      "Epoch 1653: train loss: 0.0924, test loss 0.1321\n",
      "Epoch 1654: train loss: 0.0924, test loss 0.1327\n",
      "Epoch 1655: train loss: 0.0924, test loss 0.1320\n",
      "Epoch 1656: train loss: 0.0925, test loss 0.1334\n",
      "Epoch 1657: train loss: 0.0925, test loss 0.1317\n",
      "Epoch 1658: train loss: 0.0925, test loss 0.1341\n",
      "Epoch 1659: train loss: 0.0925, test loss 0.1322\n",
      "Epoch 1660: train loss: 0.0923, test loss 0.1347\n",
      "Epoch 1661: train loss: 0.0921, test loss 0.1332\n",
      "Epoch 1662: train loss: 0.0920, test loss 0.1344\n",
      "Epoch 1663: train loss: 0.0919, test loss 0.1337\n",
      "Epoch 1664: train loss: 0.0919, test loss 0.1342\n",
      "Epoch 1665: train loss: 0.0918, test loss 0.1335\n",
      "Epoch 1666: train loss: 0.0918, test loss 0.1342\n",
      "Epoch 1667: train loss: 0.0919, test loss 0.1331\n",
      "Epoch 1668: train loss: 0.0920, test loss 0.1352\n",
      "Epoch 1669: train loss: 0.0921, test loss 0.1329\n",
      "Epoch 1670: train loss: 0.0919, test loss 0.1352\n",
      "Epoch 1671: train loss: 0.0917, test loss 0.1336\n",
      "Epoch 1672: train loss: 0.0915, test loss 0.1347\n",
      "Epoch 1673: train loss: 0.0915, test loss 0.1344\n",
      "Epoch 1674: train loss: 0.0914, test loss 0.1349\n",
      "Epoch 1675: train loss: 0.0914, test loss 0.1353\n",
      "Epoch 1676: train loss: 0.0913, test loss 0.1353\n",
      "Epoch 1677: train loss: 0.0913, test loss 0.1359\n",
      "Epoch 1678: train loss: 0.0913, test loss 0.1354\n",
      "Epoch 1679: train loss: 0.0913, test loss 0.1362\n",
      "Epoch 1680: train loss: 0.0913, test loss 0.1352\n",
      "Epoch 1681: train loss: 0.0915, test loss 0.1375\n",
      "Epoch 1682: train loss: 0.0915, test loss 0.1352\n",
      "Epoch 1683: train loss: 0.0915, test loss 0.1374\n",
      "Epoch 1684: train loss: 0.0913, test loss 0.1352\n",
      "Epoch 1685: train loss: 0.0912, test loss 0.1372\n",
      "Epoch 1686: train loss: 0.0910, test loss 0.1364\n",
      "Epoch 1687: train loss: 0.0909, test loss 0.1375\n",
      "Epoch 1688: train loss: 0.0909, test loss 0.1383\n",
      "Epoch 1689: train loss: 0.0911, test loss 0.1374\n",
      "Epoch 1690: train loss: 0.0916, test loss 0.1406\n",
      "Epoch 1691: train loss: 0.0920, test loss 0.1373\n",
      "Epoch 1692: train loss: 0.0914, test loss 0.1403\n",
      "Epoch 1693: train loss: 0.0909, test loss 0.1375\n",
      "Epoch 1694: train loss: 0.0906, test loss 0.1387\n",
      "Epoch 1695: train loss: 0.0906, test loss 0.1388\n",
      "Epoch 1696: train loss: 0.0907, test loss 0.1380\n",
      "Epoch 1697: train loss: 0.0911, test loss 0.1407\n",
      "Epoch 1698: train loss: 0.0914, test loss 0.1382\n",
      "Epoch 1699: train loss: 0.0910, test loss 0.1411\n",
      "Epoch 1700: train loss: 0.0906, test loss 0.1390\n",
      "Epoch 1701: train loss: 0.0904, test loss 0.1403\n",
      "Epoch 1702: train loss: 0.0903, test loss 0.1401\n",
      "Epoch 1703: train loss: 0.0904, test loss 0.1397\n",
      "Epoch 1704: train loss: 0.0904, test loss 0.1406\n",
      "Epoch 1705: train loss: 0.0904, test loss 0.1392\n",
      "Epoch 1706: train loss: 0.0905, test loss 0.1409\n",
      "Epoch 1707: train loss: 0.0903, test loss 0.1393\n",
      "Epoch 1708: train loss: 0.0903, test loss 0.1407\n",
      "Epoch 1709: train loss: 0.0902, test loss 0.1397\n",
      "Epoch 1710: train loss: 0.0901, test loss 0.1401\n",
      "Epoch 1711: train loss: 0.0901, test loss 0.1406\n",
      "Epoch 1712: train loss: 0.0902, test loss 0.1397\n",
      "Epoch 1713: train loss: 0.0905, test loss 0.1420\n",
      "Epoch 1714: train loss: 0.0905, test loss 0.1400\n",
      "Epoch 1715: train loss: 0.0904, test loss 0.1426\n",
      "Epoch 1716: train loss: 0.0903, test loss 0.1404\n",
      "Epoch 1717: train loss: 0.0901, test loss 0.1421\n",
      "Epoch 1718: train loss: 0.0899, test loss 0.1408\n",
      "Epoch 1719: train loss: 0.0898, test loss 0.1411\n",
      "Epoch 1720: train loss: 0.0899, test loss 0.1425\n",
      "Epoch 1721: train loss: 0.0900, test loss 0.1413\n",
      "Epoch 1722: train loss: 0.0902, test loss 0.1437\n",
      "Epoch 1723: train loss: 0.0903, test loss 0.1418\n",
      "Epoch 1724: train loss: 0.0902, test loss 0.1447\n",
      "Epoch 1725: train loss: 0.0900, test loss 0.1429\n",
      "Epoch 1726: train loss: 0.0898, test loss 0.1449\n",
      "Epoch 1727: train loss: 0.0896, test loss 0.1437\n",
      "Epoch 1728: train loss: 0.0896, test loss 0.1436\n",
      "Epoch 1729: train loss: 0.0898, test loss 0.1450\n",
      "Epoch 1730: train loss: 0.0898, test loss 0.1434\n",
      "Epoch 1731: train loss: 0.0899, test loss 0.1456\n",
      "Epoch 1732: train loss: 0.0898, test loss 0.1439\n",
      "Epoch 1733: train loss: 0.0896, test loss 0.1458\n",
      "Epoch 1734: train loss: 0.0894, test loss 0.1446\n",
      "Epoch 1735: train loss: 0.0894, test loss 0.1446\n",
      "Epoch 1736: train loss: 0.0895, test loss 0.1456\n",
      "Epoch 1737: train loss: 0.0895, test loss 0.1440\n",
      "Epoch 1738: train loss: 0.0895, test loss 0.1456\n",
      "Epoch 1739: train loss: 0.0894, test loss 0.1442\n",
      "Epoch 1740: train loss: 0.0894, test loss 0.1457\n",
      "Epoch 1741: train loss: 0.0893, test loss 0.1446\n",
      "Epoch 1742: train loss: 0.0892, test loss 0.1451\n",
      "Epoch 1743: train loss: 0.0892, test loss 0.1449\n",
      "Epoch 1744: train loss: 0.0892, test loss 0.1446\n",
      "Epoch 1745: train loss: 0.0892, test loss 0.1455\n",
      "Epoch 1746: train loss: 0.0892, test loss 0.1449\n",
      "Epoch 1747: train loss: 0.0893, test loss 0.1465\n",
      "Epoch 1748: train loss: 0.0892, test loss 0.1454\n",
      "Epoch 1749: train loss: 0.0893, test loss 0.1471\n",
      "Epoch 1750: train loss: 0.0892, test loss 0.1455\n",
      "Epoch 1751: train loss: 0.0892, test loss 0.1471\n",
      "Epoch 1752: train loss: 0.0891, test loss 0.1457\n",
      "Epoch 1753: train loss: 0.0890, test loss 0.1469\n",
      "Epoch 1754: train loss: 0.0889, test loss 0.1463\n",
      "Epoch 1755: train loss: 0.0889, test loss 0.1465\n",
      "Epoch 1756: train loss: 0.0889, test loss 0.1473\n",
      "Epoch 1757: train loss: 0.0890, test loss 0.1465\n",
      "Epoch 1758: train loss: 0.0893, test loss 0.1487\n",
      "Epoch 1759: train loss: 0.0895, test loss 0.1468\n",
      "Epoch 1760: train loss: 0.0895, test loss 0.1499\n",
      "Epoch 1761: train loss: 0.0892, test loss 0.1475\n",
      "Epoch 1762: train loss: 0.0890, test loss 0.1492\n",
      "Epoch 1763: train loss: 0.0888, test loss 0.1478\n",
      "Epoch 1764: train loss: 0.0887, test loss 0.1481\n",
      "Epoch 1765: train loss: 0.0887, test loss 0.1486\n",
      "Epoch 1766: train loss: 0.0889, test loss 0.1476\n",
      "Epoch 1767: train loss: 0.0893, test loss 0.1506\n",
      "Epoch 1768: train loss: 0.0896, test loss 0.1483\n",
      "Epoch 1769: train loss: 0.0893, test loss 0.1515\n",
      "Epoch 1770: train loss: 0.0888, test loss 0.1495\n",
      "Epoch 1771: train loss: 0.0886, test loss 0.1502\n",
      "Epoch 1772: train loss: 0.0885, test loss 0.1495\n",
      "Epoch 1773: train loss: 0.0886, test loss 0.1485\n",
      "Epoch 1774: train loss: 0.0886, test loss 0.1491\n",
      "Epoch 1775: train loss: 0.0886, test loss 0.1485\n",
      "Epoch 1776: train loss: 0.0886, test loss 0.1503\n",
      "Epoch 1777: train loss: 0.0886, test loss 0.1499\n",
      "Epoch 1778: train loss: 0.0886, test loss 0.1515\n",
      "Epoch 1779: train loss: 0.0885, test loss 0.1501\n",
      "Epoch 1780: train loss: 0.0885, test loss 0.1509\n",
      "Epoch 1781: train loss: 0.0885, test loss 0.1494\n",
      "Epoch 1782: train loss: 0.0884, test loss 0.1503\n",
      "Epoch 1783: train loss: 0.0884, test loss 0.1497\n",
      "Epoch 1784: train loss: 0.0883, test loss 0.1507\n",
      "Epoch 1785: train loss: 0.0883, test loss 0.1511\n",
      "Epoch 1786: train loss: 0.0883, test loss 0.1510\n",
      "Epoch 1787: train loss: 0.0883, test loss 0.1521\n",
      "Epoch 1788: train loss: 0.0883, test loss 0.1512\n",
      "Epoch 1789: train loss: 0.0883, test loss 0.1522\n",
      "Epoch 1790: train loss: 0.0883, test loss 0.1512\n",
      "Epoch 1791: train loss: 0.0883, test loss 0.1522\n",
      "Epoch 1792: train loss: 0.0883, test loss 0.1512\n",
      "Epoch 1793: train loss: 0.0883, test loss 0.1524\n",
      "Epoch 1794: train loss: 0.0882, test loss 0.1515\n",
      "Epoch 1795: train loss: 0.0882, test loss 0.1529\n",
      "Epoch 1796: train loss: 0.0881, test loss 0.1521\n",
      "Epoch 1797: train loss: 0.0881, test loss 0.1531\n",
      "Epoch 1798: train loss: 0.0880, test loss 0.1528\n",
      "Epoch 1799: train loss: 0.0880, test loss 0.1534\n",
      "Epoch 1800: train loss: 0.0880, test loss 0.1533\n",
      "Epoch 1801: train loss: 0.0880, test loss 0.1531\n",
      "Epoch 1802: train loss: 0.0880, test loss 0.1538\n",
      "Epoch 1803: train loss: 0.0880, test loss 0.1531\n",
      "Epoch 1804: train loss: 0.0881, test loss 0.1545\n",
      "Epoch 1805: train loss: 0.0881, test loss 0.1533\n",
      "Epoch 1806: train loss: 0.0880, test loss 0.1548\n",
      "Epoch 1807: train loss: 0.0879, test loss 0.1537\n",
      "Epoch 1808: train loss: 0.0878, test loss 0.1544\n",
      "Epoch 1809: train loss: 0.0878, test loss 0.1539\n",
      "Epoch 1810: train loss: 0.0878, test loss 0.1549\n",
      "Epoch 1811: train loss: 0.0879, test loss 0.1542\n",
      "Epoch 1812: train loss: 0.0879, test loss 0.1559\n",
      "Epoch 1813: train loss: 0.0879, test loss 0.1547\n",
      "Epoch 1814: train loss: 0.0878, test loss 0.1561\n",
      "Epoch 1815: train loss: 0.0877, test loss 0.1553\n",
      "Epoch 1816: train loss: 0.0877, test loss 0.1560\n",
      "Epoch 1817: train loss: 0.0876, test loss 0.1555\n",
      "Epoch 1818: train loss: 0.0876, test loss 0.1557\n",
      "Epoch 1819: train loss: 0.0876, test loss 0.1561\n",
      "Epoch 1820: train loss: 0.0876, test loss 0.1560\n",
      "Epoch 1821: train loss: 0.0876, test loss 0.1572\n",
      "Epoch 1822: train loss: 0.0877, test loss 0.1565\n",
      "Epoch 1823: train loss: 0.0877, test loss 0.1583\n",
      "Epoch 1824: train loss: 0.0877, test loss 0.1568\n",
      "Epoch 1825: train loss: 0.0876, test loss 0.1582\n",
      "Epoch 1826: train loss: 0.0875, test loss 0.1569\n",
      "Epoch 1827: train loss: 0.0875, test loss 0.1576\n",
      "Epoch 1828: train loss: 0.0875, test loss 0.1568\n",
      "Epoch 1829: train loss: 0.0876, test loss 0.1582\n",
      "Epoch 1830: train loss: 0.0876, test loss 0.1570\n",
      "Epoch 1831: train loss: 0.0875, test loss 0.1588\n",
      "Epoch 1832: train loss: 0.0875, test loss 0.1575\n",
      "Epoch 1833: train loss: 0.0874, test loss 0.1586\n",
      "Epoch 1834: train loss: 0.0874, test loss 0.1576\n",
      "Epoch 1835: train loss: 0.0873, test loss 0.1581\n",
      "Epoch 1836: train loss: 0.0873, test loss 0.1583\n",
      "Epoch 1837: train loss: 0.0873, test loss 0.1586\n",
      "Epoch 1838: train loss: 0.0874, test loss 0.1602\n",
      "Epoch 1839: train loss: 0.0875, test loss 0.1590\n",
      "Epoch 1840: train loss: 0.0877, test loss 0.1616\n",
      "Epoch 1841: train loss: 0.0876, test loss 0.1590\n",
      "Epoch 1842: train loss: 0.0874, test loss 0.1609\n",
      "Epoch 1843: train loss: 0.0873, test loss 0.1594\n",
      "Epoch 1844: train loss: 0.0872, test loss 0.1601\n",
      "Epoch 1845: train loss: 0.0872, test loss 0.1598\n",
      "Epoch 1846: train loss: 0.0872, test loss 0.1604\n",
      "Epoch 1847: train loss: 0.0871, test loss 0.1609\n",
      "Epoch 1848: train loss: 0.0871, test loss 0.1613\n",
      "Epoch 1849: train loss: 0.0871, test loss 0.1617\n",
      "Epoch 1850: train loss: 0.0871, test loss 0.1619\n",
      "Epoch 1851: train loss: 0.0871, test loss 0.1627\n",
      "Epoch 1852: train loss: 0.0873, test loss 0.1614\n",
      "Epoch 1853: train loss: 0.0874, test loss 0.1631\n",
      "Epoch 1854: train loss: 0.0874, test loss 0.1604\n",
      "Epoch 1855: train loss: 0.0873, test loss 0.1624\n",
      "Epoch 1856: train loss: 0.0871, test loss 0.1606\n",
      "Epoch 1857: train loss: 0.0870, test loss 0.1618\n",
      "Epoch 1858: train loss: 0.0870, test loss 0.1617\n",
      "Epoch 1859: train loss: 0.0870, test loss 0.1627\n",
      "Epoch 1860: train loss: 0.0870, test loss 0.1625\n",
      "Epoch 1861: train loss: 0.0871, test loss 0.1640\n",
      "Epoch 1862: train loss: 0.0872, test loss 0.1618\n",
      "Epoch 1863: train loss: 0.0872, test loss 0.1636\n",
      "Epoch 1864: train loss: 0.0871, test loss 0.1613\n",
      "Epoch 1865: train loss: 0.0870, test loss 0.1630\n",
      "Epoch 1866: train loss: 0.0869, test loss 0.1618\n",
      "Epoch 1867: train loss: 0.0869, test loss 0.1628\n",
      "Epoch 1868: train loss: 0.0869, test loss 0.1625\n",
      "Epoch 1869: train loss: 0.0869, test loss 0.1636\n",
      "Epoch 1870: train loss: 0.0869, test loss 0.1631\n",
      "Epoch 1871: train loss: 0.0870, test loss 0.1648\n",
      "Epoch 1872: train loss: 0.0870, test loss 0.1630\n",
      "Epoch 1873: train loss: 0.0869, test loss 0.1647\n",
      "Epoch 1874: train loss: 0.0868, test loss 0.1631\n",
      "Epoch 1875: train loss: 0.0868, test loss 0.1639\n",
      "Epoch 1876: train loss: 0.0868, test loss 0.1628\n",
      "Epoch 1877: train loss: 0.0868, test loss 0.1633\n",
      "Epoch 1878: train loss: 0.0867, test loss 0.1631\n",
      "Epoch 1879: train loss: 0.0867, test loss 0.1639\n",
      "Epoch 1880: train loss: 0.0867, test loss 0.1653\n",
      "Epoch 1881: train loss: 0.0867, test loss 0.1659\n",
      "Epoch 1882: train loss: 0.0868, test loss 0.1674\n",
      "Epoch 1883: train loss: 0.0870, test loss 0.1655\n",
      "Epoch 1884: train loss: 0.0870, test loss 0.1678\n",
      "Epoch 1885: train loss: 0.0871, test loss 0.1643\n",
      "Epoch 1886: train loss: 0.0869, test loss 0.1665\n",
      "Epoch 1887: train loss: 0.0868, test loss 0.1645\n",
      "Epoch 1888: train loss: 0.0867, test loss 0.1663\n",
      "Epoch 1889: train loss: 0.0866, test loss 0.1659\n",
      "Epoch 1890: train loss: 0.0866, test loss 0.1668\n",
      "Epoch 1891: train loss: 0.0866, test loss 0.1670\n",
      "Epoch 1892: train loss: 0.0866, test loss 0.1673\n",
      "Epoch 1893: train loss: 0.0867, test loss 0.1684\n",
      "Epoch 1894: train loss: 0.0869, test loss 0.1662\n",
      "Epoch 1895: train loss: 0.0871, test loss 0.1689\n",
      "Epoch 1896: train loss: 0.0872, test loss 0.1654\n",
      "Epoch 1897: train loss: 0.0870, test loss 0.1684\n",
      "Epoch 1898: train loss: 0.0868, test loss 0.1657\n",
      "Epoch 1899: train loss: 0.0866, test loss 0.1678\n",
      "Epoch 1900: train loss: 0.0865, test loss 0.1671\n",
      "Epoch 1901: train loss: 0.0865, test loss 0.1673\n",
      "Epoch 1902: train loss: 0.0868, test loss 0.1696\n",
      "Epoch 1903: train loss: 0.0873, test loss 0.1665\n",
      "Epoch 1904: train loss: 0.0874, test loss 0.1705\n",
      "Epoch 1905: train loss: 0.0871, test loss 0.1662\n",
      "Epoch 1906: train loss: 0.0866, test loss 0.1686\n",
      "Epoch 1907: train loss: 0.0864, test loss 0.1677\n",
      "Epoch 1908: train loss: 0.0866, test loss 0.1676\n",
      "Epoch 1909: train loss: 0.0869, test loss 0.1708\n",
      "Epoch 1910: train loss: 0.0872, test loss 0.1673\n",
      "Epoch 1911: train loss: 0.0871, test loss 0.1711\n",
      "Epoch 1912: train loss: 0.0867, test loss 0.1672\n",
      "Epoch 1913: train loss: 0.0864, test loss 0.1687\n",
      "Epoch 1914: train loss: 0.0864, test loss 0.1692\n",
      "Epoch 1915: train loss: 0.0866, test loss 0.1684\n",
      "Epoch 1916: train loss: 0.0868, test loss 0.1719\n",
      "Epoch 1917: train loss: 0.0868, test loss 0.1689\n",
      "Epoch 1918: train loss: 0.0865, test loss 0.1710\n",
      "Epoch 1919: train loss: 0.0863, test loss 0.1692\n",
      "Epoch 1920: train loss: 0.0864, test loss 0.1685\n",
      "Epoch 1921: train loss: 0.0867, test loss 0.1710\n",
      "Epoch 1922: train loss: 0.0870, test loss 0.1679\n",
      "Epoch 1923: train loss: 0.0869, test loss 0.1720\n",
      "Epoch 1924: train loss: 0.0866, test loss 0.1690\n",
      "Epoch 1925: train loss: 0.0863, test loss 0.1708\n",
      "Epoch 1926: train loss: 0.0862, test loss 0.1709\n",
      "Epoch 1927: train loss: 0.0863, test loss 0.1704\n",
      "Epoch 1928: train loss: 0.0864, test loss 0.1722\n",
      "Epoch 1929: train loss: 0.0863, test loss 0.1702\n",
      "Epoch 1930: train loss: 0.0863, test loss 0.1714\n",
      "Epoch 1931: train loss: 0.0862, test loss 0.1698\n",
      "Epoch 1932: train loss: 0.0862, test loss 0.1699\n",
      "Epoch 1933: train loss: 0.0862, test loss 0.1706\n",
      "Epoch 1934: train loss: 0.0862, test loss 0.1701\n",
      "Epoch 1935: train loss: 0.0863, test loss 0.1725\n",
      "Epoch 1936: train loss: 0.0863, test loss 0.1709\n",
      "Epoch 1937: train loss: 0.0862, test loss 0.1728\n",
      "Epoch 1938: train loss: 0.0862, test loss 0.1713\n",
      "Epoch 1939: train loss: 0.0861, test loss 0.1718\n",
      "Epoch 1940: train loss: 0.0861, test loss 0.1707\n",
      "Epoch 1941: train loss: 0.0861, test loss 0.1715\n",
      "Epoch 1942: train loss: 0.0861, test loss 0.1708\n",
      "Epoch 1943: train loss: 0.0861, test loss 0.1722\n",
      "Epoch 1944: train loss: 0.0861, test loss 0.1713\n",
      "Epoch 1945: train loss: 0.0862, test loss 0.1729\n",
      "Epoch 1946: train loss: 0.0861, test loss 0.1713\n",
      "Epoch 1947: train loss: 0.0862, test loss 0.1733\n",
      "Epoch 1948: train loss: 0.0862, test loss 0.1714\n",
      "Epoch 1949: train loss: 0.0862, test loss 0.1738\n",
      "Epoch 1950: train loss: 0.0862, test loss 0.1718\n",
      "Epoch 1951: train loss: 0.0862, test loss 0.1738\n",
      "Epoch 1952: train loss: 0.0861, test loss 0.1719\n",
      "Epoch 1953: train loss: 0.0860, test loss 0.1728\n",
      "Epoch 1954: train loss: 0.0860, test loss 0.1722\n",
      "Epoch 1955: train loss: 0.0860, test loss 0.1728\n",
      "Epoch 1956: train loss: 0.0860, test loss 0.1739\n",
      "Epoch 1957: train loss: 0.0860, test loss 0.1734\n",
      "Epoch 1958: train loss: 0.0862, test loss 0.1755\n",
      "Epoch 1959: train loss: 0.0862, test loss 0.1728\n",
      "Epoch 1960: train loss: 0.0862, test loss 0.1749\n",
      "Epoch 1961: train loss: 0.0861, test loss 0.1725\n",
      "Epoch 1962: train loss: 0.0861, test loss 0.1745\n",
      "Epoch 1963: train loss: 0.0860, test loss 0.1727\n",
      "Epoch 1964: train loss: 0.0860, test loss 0.1747\n",
      "Epoch 1965: train loss: 0.0859, test loss 0.1731\n",
      "Epoch 1966: train loss: 0.0859, test loss 0.1744\n",
      "Epoch 1967: train loss: 0.0859, test loss 0.1738\n",
      "Epoch 1968: train loss: 0.0858, test loss 0.1747\n",
      "Epoch 1969: train loss: 0.0858, test loss 0.1749\n",
      "Epoch 1970: train loss: 0.0858, test loss 0.1750\n",
      "Epoch 1971: train loss: 0.0859, test loss 0.1763\n",
      "Epoch 1972: train loss: 0.0860, test loss 0.1745\n",
      "Epoch 1973: train loss: 0.0862, test loss 0.1772\n",
      "Epoch 1974: train loss: 0.0865, test loss 0.1736\n",
      "Epoch 1975: train loss: 0.0865, test loss 0.1779\n",
      "Epoch 1976: train loss: 0.0864, test loss 0.1741\n",
      "Epoch 1977: train loss: 0.0860, test loss 0.1775\n",
      "Epoch 1978: train loss: 0.0858, test loss 0.1759\n",
      "Epoch 1979: train loss: 0.0857, test loss 0.1764\n",
      "Epoch 1980: train loss: 0.0859, test loss 0.1778\n",
      "Epoch 1981: train loss: 0.0860, test loss 0.1754\n",
      "Epoch 1982: train loss: 0.0862, test loss 0.1783\n",
      "Epoch 1983: train loss: 0.0864, test loss 0.1745\n",
      "Epoch 1984: train loss: 0.0862, test loss 0.1777\n",
      "Epoch 1985: train loss: 0.0860, test loss 0.1746\n",
      "Epoch 1986: train loss: 0.0858, test loss 0.1769\n",
      "Epoch 1987: train loss: 0.0857, test loss 0.1757\n",
      "Epoch 1988: train loss: 0.0857, test loss 0.1767\n",
      "Epoch 1989: train loss: 0.0857, test loss 0.1769\n",
      "Epoch 1990: train loss: 0.0857, test loss 0.1765\n",
      "Epoch 1991: train loss: 0.0858, test loss 0.1777\n",
      "Epoch 1992: train loss: 0.0858, test loss 0.1754\n",
      "Epoch 1993: train loss: 0.0860, test loss 0.1778\n",
      "Epoch 1994: train loss: 0.0862, test loss 0.1747\n",
      "Epoch 1995: train loss: 0.0861, test loss 0.1788\n",
      "Epoch 1996: train loss: 0.0860, test loss 0.1759\n",
      "Epoch 1997: train loss: 0.0859, test loss 0.1792\n",
      "Epoch 1998: train loss: 0.0857, test loss 0.1771\n",
      "Epoch 1999: train loss: 0.0856, test loss 0.1783\n",
      "Epoch 2000: train loss: 0.0856, test loss 0.1770\n",
      "Epoch 2001: train loss: 0.0856, test loss 0.1775\n",
      "Epoch 2002: train loss: 0.0856, test loss 0.1765\n",
      "Epoch 2003: train loss: 0.0856, test loss 0.1778\n",
      "Epoch 2004: train loss: 0.0856, test loss 0.1766\n",
      "Epoch 2005: train loss: 0.0857, test loss 0.1785\n",
      "Epoch 2006: train loss: 0.0856, test loss 0.1770\n",
      "Epoch 2007: train loss: 0.0856, test loss 0.1788\n",
      "Epoch 2008: train loss: 0.0855, test loss 0.1775\n",
      "Epoch 2009: train loss: 0.0855, test loss 0.1785\n",
      "Epoch 2010: train loss: 0.0855, test loss 0.1779\n",
      "Epoch 2011: train loss: 0.0855, test loss 0.1792\n",
      "Epoch 2012: train loss: 0.0856, test loss 0.1780\n",
      "Epoch 2013: train loss: 0.0856, test loss 0.1797\n",
      "Epoch 2014: train loss: 0.0857, test loss 0.1773\n",
      "Epoch 2015: train loss: 0.0859, test loss 0.1806\n",
      "Epoch 2016: train loss: 0.0862, test loss 0.1771\n",
      "Epoch 2017: train loss: 0.0861, test loss 0.1816\n",
      "Epoch 2018: train loss: 0.0861, test loss 0.1780\n",
      "Epoch 2019: train loss: 0.0859, test loss 0.1818\n",
      "Epoch 2020: train loss: 0.0856, test loss 0.1788\n",
      "Epoch 2021: train loss: 0.0856, test loss 0.1806\n",
      "Epoch 2022: train loss: 0.0855, test loss 0.1784\n",
      "Epoch 2023: train loss: 0.0854, test loss 0.1792\n",
      "Epoch 2024: train loss: 0.0854, test loss 0.1784\n",
      "Epoch 2025: train loss: 0.0854, test loss 0.1788\n",
      "Epoch 2026: train loss: 0.0854, test loss 0.1799\n",
      "Epoch 2027: train loss: 0.0854, test loss 0.1795\n",
      "Epoch 2028: train loss: 0.0855, test loss 0.1819\n",
      "Epoch 2029: train loss: 0.0856, test loss 0.1798\n",
      "Epoch 2030: train loss: 0.0857, test loss 0.1825\n",
      "Epoch 2031: train loss: 0.0858, test loss 0.1790\n",
      "Epoch 2032: train loss: 0.0858, test loss 0.1820\n",
      "Epoch 2033: train loss: 0.0858, test loss 0.1784\n",
      "Epoch 2034: train loss: 0.0857, test loss 0.1815\n",
      "Epoch 2035: train loss: 0.0856, test loss 0.1788\n",
      "Epoch 2036: train loss: 0.0855, test loss 0.1817\n",
      "Epoch 2037: train loss: 0.0854, test loss 0.1799\n",
      "Epoch 2038: train loss: 0.0853, test loss 0.1814\n",
      "Epoch 2039: train loss: 0.0853, test loss 0.1801\n",
      "Epoch 2040: train loss: 0.0853, test loss 0.1806\n",
      "Epoch 2041: train loss: 0.0853, test loss 0.1795\n",
      "Epoch 2042: train loss: 0.0853, test loss 0.1807\n",
      "Epoch 2043: train loss: 0.0853, test loss 0.1792\n",
      "Epoch 2044: train loss: 0.0854, test loss 0.1811\n",
      "Epoch 2045: train loss: 0.0853, test loss 0.1797\n",
      "Epoch 2046: train loss: 0.0853, test loss 0.1813\n",
      "Epoch 2047: train loss: 0.0852, test loss 0.1804\n",
      "Epoch 2048: train loss: 0.0852, test loss 0.1817\n",
      "Epoch 2049: train loss: 0.0852, test loss 0.1809\n",
      "Epoch 2050: train loss: 0.0853, test loss 0.1824\n",
      "Epoch 2051: train loss: 0.0853, test loss 0.1806\n",
      "Epoch 2052: train loss: 0.0854, test loss 0.1826\n",
      "Epoch 2053: train loss: 0.0855, test loss 0.1800\n",
      "Epoch 2054: train loss: 0.0856, test loss 0.1831\n",
      "Epoch 2055: train loss: 0.0858, test loss 0.1799\n",
      "Epoch 2056: train loss: 0.0859, test loss 0.1841\n",
      "Epoch 2057: train loss: 0.0862, test loss 0.1803\n",
      "Epoch 2058: train loss: 0.0865, test loss 0.1858\n",
      "Epoch 2059: train loss: 0.0867, test loss 0.1805\n",
      "Epoch 2060: train loss: 0.0863, test loss 0.1852\n",
      "Epoch 2061: train loss: 0.0860, test loss 0.1801\n",
      "Epoch 2062: train loss: 0.0856, test loss 0.1828\n",
      "Epoch 2063: train loss: 0.0853, test loss 0.1799\n",
      "Epoch 2064: train loss: 0.0851, test loss 0.1814\n",
      "Epoch 2065: train loss: 0.0851, test loss 0.1812\n",
      "Epoch 2066: train loss: 0.0851, test loss 0.1814\n",
      "Epoch 2067: train loss: 0.0853, test loss 0.1840\n",
      "Epoch 2068: train loss: 0.0855, test loss 0.1821\n",
      "Epoch 2069: train loss: 0.0856, test loss 0.1855\n",
      "Epoch 2070: train loss: 0.0857, test loss 0.1819\n",
      "Epoch 2071: train loss: 0.0855, test loss 0.1844\n",
      "Epoch 2072: train loss: 0.0852, test loss 0.1817\n",
      "Epoch 2073: train loss: 0.0850, test loss 0.1827\n",
      "Epoch 2074: train loss: 0.0851, test loss 0.1835\n",
      "Epoch 2075: train loss: 0.0852, test loss 0.1829\n",
      "Epoch 2076: train loss: 0.0855, test loss 0.1861\n",
      "Epoch 2077: train loss: 0.0859, test loss 0.1831\n",
      "Epoch 2078: train loss: 0.0862, test loss 0.1877\n",
      "Epoch 2079: train loss: 0.0864, test loss 0.1825\n",
      "Epoch 2080: train loss: 0.0860, test loss 0.1867\n",
      "Epoch 2081: train loss: 0.0856, test loss 0.1822\n",
      "Epoch 2082: train loss: 0.0852, test loss 0.1845\n",
      "Epoch 2083: train loss: 0.0850, test loss 0.1833\n",
      "Epoch 2084: train loss: 0.0851, test loss 0.1832\n",
      "Epoch 2085: train loss: 0.0855, test loss 0.1861\n",
      "Epoch 2086: train loss: 0.0857, test loss 0.1828\n",
      "Epoch 2087: train loss: 0.0856, test loss 0.1863\n",
      "Epoch 2088: train loss: 0.0854, test loss 0.1831\n",
      "Epoch 2089: train loss: 0.0851, test loss 0.1852\n",
      "Epoch 2090: train loss: 0.0849, test loss 0.1840\n",
      "Epoch 2091: train loss: 0.0850, test loss 0.1839\n",
      "Epoch 2092: train loss: 0.0853, test loss 0.1859\n",
      "Epoch 2093: train loss: 0.0855, test loss 0.1829\n",
      "Epoch 2094: train loss: 0.0855, test loss 0.1859\n",
      "Epoch 2095: train loss: 0.0854, test loss 0.1830\n",
      "Epoch 2096: train loss: 0.0852, test loss 0.1859\n",
      "Epoch 2097: train loss: 0.0850, test loss 0.1842\n",
      "Epoch 2098: train loss: 0.0849, test loss 0.1851\n",
      "Epoch 2099: train loss: 0.0849, test loss 0.1858\n",
      "Epoch 2100: train loss: 0.0850, test loss 0.1844\n",
      "Epoch 2101: train loss: 0.0850, test loss 0.1859\n",
      "Epoch 2102: train loss: 0.0850, test loss 0.1838\n",
      "Epoch 2103: train loss: 0.0849, test loss 0.1852\n",
      "Epoch 2104: train loss: 0.0848, test loss 0.1846\n",
      "Epoch 2105: train loss: 0.0848, test loss 0.1858\n",
      "Epoch 2106: train loss: 0.0848, test loss 0.1865\n",
      "Epoch 2107: train loss: 0.0848, test loss 0.1868\n",
      "Epoch 2108: train loss: 0.0848, test loss 0.1876\n",
      "Epoch 2109: train loss: 0.0849, test loss 0.1863\n",
      "Epoch 2110: train loss: 0.0849, test loss 0.1876\n",
      "Epoch 2111: train loss: 0.0850, test loss 0.1859\n",
      "Epoch 2112: train loss: 0.0849, test loss 0.1880\n",
      "Epoch 2113: train loss: 0.0849, test loss 0.1864\n",
      "Epoch 2114: train loss: 0.0848, test loss 0.1877\n",
      "Epoch 2115: train loss: 0.0847, test loss 0.1868\n",
      "Epoch 2116: train loss: 0.0847, test loss 0.1871\n",
      "Epoch 2117: train loss: 0.0847, test loss 0.1867\n",
      "Epoch 2118: train loss: 0.0847, test loss 0.1874\n",
      "Epoch 2119: train loss: 0.0847, test loss 0.1872\n",
      "Epoch 2120: train loss: 0.0848, test loss 0.1883\n",
      "Epoch 2121: train loss: 0.0849, test loss 0.1870\n",
      "Epoch 2122: train loss: 0.0850, test loss 0.1888\n",
      "Epoch 2123: train loss: 0.0851, test loss 0.1861\n",
      "Epoch 2124: train loss: 0.0852, test loss 0.1890\n",
      "Epoch 2125: train loss: 0.0855, test loss 0.1863\n",
      "Epoch 2126: train loss: 0.0857, test loss 0.1906\n",
      "Epoch 2127: train loss: 0.0860, test loss 0.1867\n",
      "Epoch 2128: train loss: 0.0859, test loss 0.1911\n",
      "Epoch 2129: train loss: 0.0860, test loss 0.1865\n",
      "Epoch 2130: train loss: 0.0856, test loss 0.1901\n",
      "Epoch 2131: train loss: 0.0854, test loss 0.1860\n",
      "Epoch 2132: train loss: 0.0849, test loss 0.1883\n",
      "Epoch 2133: train loss: 0.0846, test loss 0.1870\n",
      "Epoch 2134: train loss: 0.0847, test loss 0.1874\n",
      "Epoch 2135: train loss: 0.0849, test loss 0.1899\n",
      "Epoch 2136: train loss: 0.0851, test loss 0.1880\n",
      "Epoch 2137: train loss: 0.0850, test loss 0.1909\n",
      "Epoch 2138: train loss: 0.0848, test loss 0.1885\n",
      "Epoch 2139: train loss: 0.0847, test loss 0.1898\n",
      "Epoch 2140: train loss: 0.0846, test loss 0.1891\n",
      "Epoch 2141: train loss: 0.0846, test loss 0.1893\n",
      "Epoch 2142: train loss: 0.0846, test loss 0.1903\n",
      "Epoch 2143: train loss: 0.0847, test loss 0.1893\n",
      "Epoch 2144: train loss: 0.0847, test loss 0.1909\n",
      "Epoch 2145: train loss: 0.0846, test loss 0.1893\n",
      "Epoch 2146: train loss: 0.0846, test loss 0.1903\n",
      "Epoch 2147: train loss: 0.0845, test loss 0.1895\n",
      "Epoch 2148: train loss: 0.0845, test loss 0.1901\n",
      "Epoch 2149: train loss: 0.0845, test loss 0.1900\n",
      "Epoch 2150: train loss: 0.0845, test loss 0.1906\n",
      "Epoch 2151: train loss: 0.0846, test loss 0.1897\n",
      "Epoch 2152: train loss: 0.0846, test loss 0.1910\n",
      "Epoch 2153: train loss: 0.0847, test loss 0.1892\n",
      "Epoch 2154: train loss: 0.0847, test loss 0.1911\n",
      "Epoch 2155: train loss: 0.0847, test loss 0.1890\n",
      "Epoch 2156: train loss: 0.0846, test loss 0.1907\n",
      "Epoch 2157: train loss: 0.0846, test loss 0.1894\n",
      "Epoch 2158: train loss: 0.0845, test loss 0.1907\n",
      "Epoch 2159: train loss: 0.0844, test loss 0.1904\n",
      "Epoch 2160: train loss: 0.0844, test loss 0.1911\n",
      "Epoch 2161: train loss: 0.0844, test loss 0.1907\n",
      "Epoch 2162: train loss: 0.0845, test loss 0.1916\n",
      "Epoch 2163: train loss: 0.0845, test loss 0.1904\n",
      "Epoch 2164: train loss: 0.0845, test loss 0.1917\n",
      "Epoch 2165: train loss: 0.0845, test loss 0.1902\n",
      "Epoch 2166: train loss: 0.0845, test loss 0.1915\n",
      "Epoch 2167: train loss: 0.0844, test loss 0.1908\n",
      "Epoch 2168: train loss: 0.0844, test loss 0.1919\n",
      "Epoch 2169: train loss: 0.0844, test loss 0.1918\n",
      "Epoch 2170: train loss: 0.0844, test loss 0.1931\n",
      "Epoch 2171: train loss: 0.0845, test loss 0.1923\n",
      "Epoch 2172: train loss: 0.0845, test loss 0.1937\n",
      "Epoch 2173: train loss: 0.0845, test loss 0.1920\n",
      "Epoch 2174: train loss: 0.0845, test loss 0.1937\n",
      "Epoch 2175: train loss: 0.0845, test loss 0.1921\n",
      "Epoch 2176: train loss: 0.0845, test loss 0.1940\n",
      "Epoch 2177: train loss: 0.0845, test loss 0.1928\n",
      "Epoch 2178: train loss: 0.0845, test loss 0.1948\n",
      "Epoch 2179: train loss: 0.0845, test loss 0.1935\n",
      "Epoch 2180: train loss: 0.0845, test loss 0.1953\n",
      "Epoch 2181: train loss: 0.0846, test loss 0.1934\n",
      "Epoch 2182: train loss: 0.0846, test loss 0.1955\n",
      "Epoch 2183: train loss: 0.0847, test loss 0.1935\n",
      "Epoch 2184: train loss: 0.0850, test loss 0.1966\n",
      "Epoch 2185: train loss: 0.0855, test loss 0.1937\n",
      "Epoch 2186: train loss: 0.0859, test loss 0.1984\n",
      "Epoch 2187: train loss: 0.0869, test loss 0.1938\n",
      "Epoch 2188: train loss: 0.0860, test loss 0.1975\n",
      "Epoch 2189: train loss: 0.0854, test loss 0.1924\n",
      "Epoch 2190: train loss: 0.0849, test loss 0.1949\n",
      "Epoch 2191: train loss: 0.0844, test loss 0.1931\n",
      "Epoch 2192: train loss: 0.0843, test loss 0.1949\n",
      "Epoch 2193: train loss: 0.0843, test loss 0.1963\n",
      "Epoch 2194: train loss: 0.0845, test loss 0.1954\n",
      "Epoch 2195: train loss: 0.0847, test loss 0.1976\n",
      "Epoch 2196: train loss: 0.0847, test loss 0.1946\n",
      "Epoch 2197: train loss: 0.0845, test loss 0.1964\n",
      "Epoch 2198: train loss: 0.0844, test loss 0.1950\n",
      "Epoch 2199: train loss: 0.0842, test loss 0.1968\n",
      "Epoch 2200: train loss: 0.0842, test loss 0.1974\n",
      "Epoch 2201: train loss: 0.0842, test loss 0.1979\n",
      "Epoch 2202: train loss: 0.0843, test loss 0.1989\n",
      "Epoch 2203: train loss: 0.0844, test loss 0.1975\n",
      "Epoch 2204: train loss: 0.0844, test loss 0.1986\n",
      "Epoch 2205: train loss: 0.0844, test loss 0.1963\n",
      "Epoch 2206: train loss: 0.0843, test loss 0.1976\n",
      "Epoch 2207: train loss: 0.0842, test loss 0.1970\n",
      "Epoch 2208: train loss: 0.0842, test loss 0.1980\n",
      "Epoch 2209: train loss: 0.0842, test loss 0.1981\n",
      "Epoch 2210: train loss: 0.0841, test loss 0.1984\n",
      "Epoch 2211: train loss: 0.0841, test loss 0.1983\n",
      "Epoch 2212: train loss: 0.0842, test loss 0.1975\n",
      "Epoch 2213: train loss: 0.0842, test loss 0.1980\n",
      "Epoch 2214: train loss: 0.0844, test loss 0.1963\n",
      "Epoch 2215: train loss: 0.0845, test loss 0.1981\n",
      "Epoch 2216: train loss: 0.0846, test loss 0.1962\n",
      "Epoch 2217: train loss: 0.0848, test loss 0.1992\n",
      "Epoch 2218: train loss: 0.0853, test loss 0.1967\n",
      "Epoch 2219: train loss: 0.0856, test loss 0.2009\n",
      "Epoch 2220: train loss: 0.0862, test loss 0.1969\n",
      "Epoch 2221: train loss: 0.0854, test loss 0.2003\n",
      "Epoch 2222: train loss: 0.0850, test loss 0.1960\n",
      "Epoch 2223: train loss: 0.0845, test loss 0.1976\n",
      "Epoch 2224: train loss: 0.0841, test loss 0.1958\n",
      "Epoch 2225: train loss: 0.0841, test loss 0.1965\n",
      "Epoch 2226: train loss: 0.0843, test loss 0.1985\n",
      "Epoch 2227: train loss: 0.0845, test loss 0.1976\n",
      "Epoch 2228: train loss: 0.0847, test loss 0.2006\n",
      "Epoch 2229: train loss: 0.0848, test loss 0.1981\n",
      "Epoch 2230: train loss: 0.0847, test loss 0.2005\n",
      "Epoch 2231: train loss: 0.0844, test loss 0.1980\n",
      "Epoch 2232: train loss: 0.0841, test loss 0.1991\n",
      "Epoch 2233: train loss: 0.0840, test loss 0.1988\n",
      "Epoch 2234: train loss: 0.0841, test loss 0.1987\n",
      "Epoch 2235: train loss: 0.0842, test loss 0.2001\n",
      "Epoch 2236: train loss: 0.0843, test loss 0.1986\n",
      "Epoch 2237: train loss: 0.0842, test loss 0.2000\n",
      "Epoch 2238: train loss: 0.0841, test loss 0.1984\n",
      "Epoch 2239: train loss: 0.0840, test loss 0.1991\n",
      "Epoch 2240: train loss: 0.0840, test loss 0.1991\n",
      "Epoch 2241: train loss: 0.0840, test loss 0.1989\n",
      "Epoch 2242: train loss: 0.0841, test loss 0.2000\n",
      "Epoch 2243: train loss: 0.0842, test loss 0.1986\n",
      "Epoch 2244: train loss: 0.0842, test loss 0.1996\n",
      "Epoch 2245: train loss: 0.0841, test loss 0.1980\n",
      "Epoch 2246: train loss: 0.0840, test loss 0.1988\n",
      "Epoch 2247: train loss: 0.0840, test loss 0.1988\n",
      "Epoch 2248: train loss: 0.0840, test loss 0.1991\n",
      "Epoch 2249: train loss: 0.0841, test loss 0.2006\n",
      "Epoch 2250: train loss: 0.0842, test loss 0.1996\n",
      "Epoch 2251: train loss: 0.0842, test loss 0.2010\n",
      "Epoch 2252: train loss: 0.0842, test loss 0.1991\n",
      "Epoch 2253: train loss: 0.0840, test loss 0.1999\n",
      "Epoch 2254: train loss: 0.0840, test loss 0.1991\n",
      "Epoch 2255: train loss: 0.0839, test loss 0.2000\n",
      "Epoch 2256: train loss: 0.0839, test loss 0.2003\n",
      "Epoch 2257: train loss: 0.0839, test loss 0.2008\n",
      "Epoch 2258: train loss: 0.0839, test loss 0.2014\n",
      "Epoch 2259: train loss: 0.0840, test loss 0.2006\n",
      "Epoch 2260: train loss: 0.0841, test loss 0.2015\n",
      "Epoch 2261: train loss: 0.0841, test loss 0.2000\n",
      "Epoch 2262: train loss: 0.0842, test loss 0.2016\n",
      "Epoch 2263: train loss: 0.0843, test loss 0.2003\n",
      "Epoch 2264: train loss: 0.0844, test loss 0.2024\n",
      "Epoch 2265: train loss: 0.0844, test loss 0.2004\n",
      "Epoch 2266: train loss: 0.0845, test loss 0.2025\n",
      "Epoch 2267: train loss: 0.0847, test loss 0.2004\n",
      "Epoch 2268: train loss: 0.0849, test loss 0.2033\n",
      "Epoch 2269: train loss: 0.0855, test loss 0.2009\n",
      "Epoch 2270: train loss: 0.0852, test loss 0.2044\n",
      "Epoch 2271: train loss: 0.0855, test loss 0.2014\n",
      "Epoch 2272: train loss: 0.0848, test loss 0.2043\n",
      "Epoch 2273: train loss: 0.0842, test loss 0.2016\n",
      "Epoch 2274: train loss: 0.0839, test loss 0.2025\n",
      "Epoch 2275: train loss: 0.0838, test loss 0.2023\n",
      "Epoch 2276: train loss: 0.0840, test loss 0.2018\n",
      "Epoch 2277: train loss: 0.0843, test loss 0.2038\n",
      "Epoch 2278: train loss: 0.0845, test loss 0.2023\n",
      "Epoch 2279: train loss: 0.0844, test loss 0.2051\n",
      "Epoch 2280: train loss: 0.0842, test loss 0.2039\n",
      "Epoch 2281: train loss: 0.0840, test loss 0.2057\n",
      "Epoch 2282: train loss: 0.0838, test loss 0.2048\n",
      "Epoch 2283: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2284: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2285: train loss: 0.0839, test loss 0.2035\n",
      "Epoch 2286: train loss: 0.0839, test loss 0.2044\n",
      "Epoch 2287: train loss: 0.0839, test loss 0.2034\n",
      "Epoch 2288: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2289: train loss: 0.0837, test loss 0.2048\n",
      "Epoch 2290: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2291: train loss: 0.0839, test loss 0.2056\n",
      "Epoch 2292: train loss: 0.0840, test loss 0.2038\n",
      "Epoch 2293: train loss: 0.0838, test loss 0.2040\n",
      "Epoch 2294: train loss: 0.0838, test loss 0.2027\n",
      "Epoch 2295: train loss: 0.0837, test loss 0.2031\n",
      "Epoch 2296: train loss: 0.0837, test loss 0.2039\n",
      "Epoch 2297: train loss: 0.0837, test loss 0.2045\n",
      "Epoch 2298: train loss: 0.0837, test loss 0.2059\n",
      "Epoch 2299: train loss: 0.0838, test loss 0.2053\n",
      "Epoch 2300: train loss: 0.0838, test loss 0.2059\n",
      "Epoch 2301: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2302: train loss: 0.0838, test loss 0.2055\n",
      "Epoch 2303: train loss: 0.0838, test loss 0.2046\n",
      "Epoch 2304: train loss: 0.0837, test loss 0.2061\n",
      "Epoch 2305: train loss: 0.0837, test loss 0.2058\n",
      "Epoch 2306: train loss: 0.0837, test loss 0.2070\n",
      "Epoch 2307: train loss: 0.0836, test loss 0.2070\n",
      "Epoch 2308: train loss: 0.0837, test loss 0.2068\n",
      "Epoch 2309: train loss: 0.0838, test loss 0.2075\n",
      "Epoch 2310: train loss: 0.0839, test loss 0.2062\n",
      "Epoch 2311: train loss: 0.0841, test loss 0.2080\n",
      "Epoch 2312: train loss: 0.0843, test loss 0.2065\n",
      "Epoch 2313: train loss: 0.0843, test loss 0.2090\n",
      "Epoch 2314: train loss: 0.0844, test loss 0.2071\n",
      "Epoch 2315: train loss: 0.0843, test loss 0.2094\n",
      "Epoch 2316: train loss: 0.0841, test loss 0.2072\n",
      "Epoch 2317: train loss: 0.0838, test loss 0.2084\n",
      "Epoch 2318: train loss: 0.0836, test loss 0.2072\n",
      "Epoch 2319: train loss: 0.0836, test loss 0.2073\n",
      "Epoch 2320: train loss: 0.0836, test loss 0.2077\n",
      "Epoch 2321: train loss: 0.0836, test loss 0.2073\n",
      "Epoch 2322: train loss: 0.0836, test loss 0.2084\n",
      "Epoch 2323: train loss: 0.0837, test loss 0.2080\n",
      "Epoch 2324: train loss: 0.0836, test loss 0.2090\n",
      "Epoch 2325: train loss: 0.0836, test loss 0.2083\n",
      "Epoch 2326: train loss: 0.0836, test loss 0.2093\n",
      "Epoch 2327: train loss: 0.0836, test loss 0.2085\n",
      "Epoch 2328: train loss: 0.0836, test loss 0.2094\n",
      "Epoch 2329: train loss: 0.0837, test loss 0.2087\n",
      "Epoch 2330: train loss: 0.0836, test loss 0.2099\n",
      "Epoch 2331: train loss: 0.0836, test loss 0.2094\n",
      "Epoch 2332: train loss: 0.0836, test loss 0.2103\n",
      "Epoch 2333: train loss: 0.0836, test loss 0.2094\n",
      "Epoch 2334: train loss: 0.0836, test loss 0.2104\n",
      "Epoch 2335: train loss: 0.0837, test loss 0.2095\n",
      "Epoch 2336: train loss: 0.0838, test loss 0.2112\n",
      "Epoch 2337: train loss: 0.0841, test loss 0.2104\n",
      "Epoch 2338: train loss: 0.0841, test loss 0.2128\n",
      "Epoch 2339: train loss: 0.0841, test loss 0.2112\n",
      "Epoch 2340: train loss: 0.0839, test loss 0.2129\n",
      "Epoch 2341: train loss: 0.0837, test loss 0.2114\n",
      "Epoch 2342: train loss: 0.0836, test loss 0.2122\n",
      "Epoch 2343: train loss: 0.0836, test loss 0.2111\n",
      "Epoch 2344: train loss: 0.0835, test loss 0.2117\n",
      "Epoch 2345: train loss: 0.0834, test loss 0.2111\n",
      "Epoch 2346: train loss: 0.0834, test loss 0.2118\n",
      "Epoch 2347: train loss: 0.0834, test loss 0.2121\n",
      "Epoch 2348: train loss: 0.0834, test loss 0.2124\n",
      "Epoch 2349: train loss: 0.0835, test loss 0.2130\n",
      "Epoch 2350: train loss: 0.0837, test loss 0.2121\n",
      "Epoch 2351: train loss: 0.0840, test loss 0.2133\n",
      "Epoch 2352: train loss: 0.0844, test loss 0.2116\n",
      "Epoch 2353: train loss: 0.0846, test loss 0.2132\n",
      "Epoch 2354: train loss: 0.0852, test loss 0.2116\n",
      "Epoch 2355: train loss: 0.0845, test loss 0.2128\n",
      "Epoch 2356: train loss: 0.0841, test loss 0.2111\n",
      "Epoch 2357: train loss: 0.0837, test loss 0.2124\n",
      "Epoch 2358: train loss: 0.0834, test loss 0.2126\n",
      "Epoch 2359: train loss: 0.0833, test loss 0.2141\n",
      "Epoch 2360: train loss: 0.0833, test loss 0.2149\n",
      "Epoch 2361: train loss: 0.0833, test loss 0.2148\n",
      "Epoch 2362: train loss: 0.0834, test loss 0.2151\n",
      "Epoch 2363: train loss: 0.0835, test loss 0.2140\n",
      "Epoch 2364: train loss: 0.0835, test loss 0.2150\n",
      "Epoch 2365: train loss: 0.0835, test loss 0.2151\n",
      "Epoch 2366: train loss: 0.0835, test loss 0.2171\n",
      "Epoch 2367: train loss: 0.0836, test loss 0.2172\n",
      "Epoch 2368: train loss: 0.0836, test loss 0.2183\n",
      "Epoch 2369: train loss: 0.0837, test loss 0.2167\n",
      "Epoch 2370: train loss: 0.0837, test loss 0.2163\n",
      "Epoch 2371: train loss: 0.0836, test loss 0.2143\n",
      "Epoch 2372: train loss: 0.0834, test loss 0.2141\n",
      "Epoch 2373: train loss: 0.0833, test loss 0.2134\n",
      "Epoch 2374: train loss: 0.0832, test loss 0.2140\n",
      "Epoch 2375: train loss: 0.0831, test loss 0.2137\n",
      "Epoch 2376: train loss: 0.0830, test loss 0.2138\n",
      "Epoch 2377: train loss: 0.0830, test loss 0.2138\n",
      "Epoch 2378: train loss: 0.0831, test loss 0.2136\n",
      "Epoch 2379: train loss: 0.0831, test loss 0.2142\n",
      "Epoch 2380: train loss: 0.0831, test loss 0.2143\n",
      "Epoch 2381: train loss: 0.0831, test loss 0.2154\n",
      "Epoch 2382: train loss: 0.0831, test loss 0.2154\n",
      "Epoch 2383: train loss: 0.0830, test loss 0.2157\n",
      "Epoch 2384: train loss: 0.0829, test loss 0.2153\n",
      "Epoch 2385: train loss: 0.0829, test loss 0.2156\n",
      "Epoch 2386: train loss: 0.0829, test loss 0.2161\n",
      "Epoch 2387: train loss: 0.0829, test loss 0.2166\n",
      "Epoch 2388: train loss: 0.0830, test loss 0.2176\n",
      "Epoch 2389: train loss: 0.0831, test loss 0.2177\n",
      "Epoch 2390: train loss: 0.0833, test loss 0.2184\n",
      "Epoch 2391: train loss: 0.0835, test loss 0.2178\n",
      "Epoch 2392: train loss: 0.0836, test loss 0.2183\n",
      "Epoch 2393: train loss: 0.0840, test loss 0.2172\n",
      "Epoch 2394: train loss: 0.0837, test loss 0.2178\n",
      "Epoch 2395: train loss: 0.0835, test loss 0.2167\n",
      "Epoch 2396: train loss: 0.0830, test loss 0.2172\n",
      "Epoch 2397: train loss: 0.0828, test loss 0.2176\n",
      "Epoch 2398: train loss: 0.0828, test loss 0.2183\n",
      "Epoch 2399: train loss: 0.0830, test loss 0.2192\n",
      "Epoch 2400: train loss: 0.0832, test loss 0.2186\n",
      "Epoch 2401: train loss: 0.0833, test loss 0.2190\n",
      "Epoch 2402: train loss: 0.0833, test loss 0.2175\n",
      "Epoch 2403: train loss: 0.0830, test loss 0.2177\n",
      "Epoch 2404: train loss: 0.0828, test loss 0.2174\n",
      "Epoch 2405: train loss: 0.0827, test loss 0.2182\n",
      "Epoch 2406: train loss: 0.0828, test loss 0.2196\n",
      "Epoch 2407: train loss: 0.0831, test loss 0.2199\n",
      "Epoch 2408: train loss: 0.0833, test loss 0.2210\n",
      "Epoch 2409: train loss: 0.0834, test loss 0.2197\n",
      "Epoch 2410: train loss: 0.0830, test loss 0.2194\n",
      "Epoch 2411: train loss: 0.0828, test loss 0.2178\n",
      "Epoch 2412: train loss: 0.0826, test loss 0.2179\n",
      "Epoch 2413: train loss: 0.0826, test loss 0.2185\n",
      "Epoch 2414: train loss: 0.0828, test loss 0.2189\n",
      "Epoch 2415: train loss: 0.0829, test loss 0.2202\n",
      "Epoch 2416: train loss: 0.0830, test loss 0.2195\n",
      "Epoch 2417: train loss: 0.0830, test loss 0.2200\n",
      "Epoch 2418: train loss: 0.0829, test loss 0.2183\n",
      "Epoch 2419: train loss: 0.0827, test loss 0.2183\n",
      "Epoch 2420: train loss: 0.0826, test loss 0.2176\n",
      "Epoch 2421: train loss: 0.0825, test loss 0.2180\n",
      "Epoch 2422: train loss: 0.0825, test loss 0.2184\n",
      "Epoch 2423: train loss: 0.0825, test loss 0.2187\n",
      "Epoch 2424: train loss: 0.0825, test loss 0.2192\n",
      "Epoch 2425: train loss: 0.0825, test loss 0.2190\n",
      "Epoch 2426: train loss: 0.0825, test loss 0.2193\n",
      "Epoch 2427: train loss: 0.0825, test loss 0.2188\n",
      "Epoch 2428: train loss: 0.0825, test loss 0.2190\n",
      "Epoch 2429: train loss: 0.0824, test loss 0.2188\n",
      "Epoch 2430: train loss: 0.0824, test loss 0.2191\n",
      "Epoch 2431: train loss: 0.0824, test loss 0.2194\n",
      "Epoch 2432: train loss: 0.0824, test loss 0.2195\n",
      "Epoch 2433: train loss: 0.0824, test loss 0.2199\n",
      "Epoch 2434: train loss: 0.0825, test loss 0.2198\n",
      "Epoch 2435: train loss: 0.0825, test loss 0.2202\n",
      "Epoch 2436: train loss: 0.0827, test loss 0.2194\n",
      "Epoch 2437: train loss: 0.0828, test loss 0.2198\n",
      "Epoch 2438: train loss: 0.0830, test loss 0.2185\n",
      "Epoch 2439: train loss: 0.0829, test loss 0.2190\n",
      "Epoch 2440: train loss: 0.0828, test loss 0.2175\n",
      "Epoch 2441: train loss: 0.0825, test loss 0.2180\n",
      "Epoch 2442: train loss: 0.0823, test loss 0.2179\n",
      "Epoch 2443: train loss: 0.0822, test loss 0.2188\n",
      "Epoch 2444: train loss: 0.0823, test loss 0.2199\n",
      "Epoch 2445: train loss: 0.0823, test loss 0.2199\n",
      "Epoch 2446: train loss: 0.0824, test loss 0.2204\n",
      "Epoch 2447: train loss: 0.0824, test loss 0.2191\n",
      "Epoch 2448: train loss: 0.0823, test loss 0.2193\n",
      "Epoch 2449: train loss: 0.0823, test loss 0.2188\n",
      "Epoch 2450: train loss: 0.0822, test loss 0.2198\n",
      "Epoch 2451: train loss: 0.0821, test loss 0.2197\n",
      "Epoch 2452: train loss: 0.0821, test loss 0.2203\n",
      "Epoch 2453: train loss: 0.0821, test loss 0.2207\n",
      "Epoch 2454: train loss: 0.0821, test loss 0.2203\n",
      "Epoch 2455: train loss: 0.0822, test loss 0.2206\n",
      "Epoch 2456: train loss: 0.0823, test loss 0.2195\n",
      "Epoch 2457: train loss: 0.0825, test loss 0.2202\n",
      "Epoch 2458: train loss: 0.0827, test loss 0.2183\n",
      "Epoch 2459: train loss: 0.0824, test loss 0.2193\n",
      "Epoch 2460: train loss: 0.0821, test loss 0.2177\n",
      "Epoch 2461: train loss: 0.0820, test loss 0.2183\n",
      "Epoch 2462: train loss: 0.0820, test loss 0.2186\n",
      "Epoch 2463: train loss: 0.0820, test loss 0.2187\n",
      "Epoch 2464: train loss: 0.0820, test loss 0.2197\n",
      "Epoch 2465: train loss: 0.0820, test loss 0.2192\n",
      "Epoch 2466: train loss: 0.0820, test loss 0.2199\n",
      "Epoch 2467: train loss: 0.0820, test loss 0.2193\n",
      "Epoch 2468: train loss: 0.0819, test loss 0.2199\n",
      "Epoch 2469: train loss: 0.0819, test loss 0.2196\n",
      "Epoch 2470: train loss: 0.0819, test loss 0.2201\n",
      "Epoch 2471: train loss: 0.0819, test loss 0.2206\n",
      "Epoch 2472: train loss: 0.0820, test loss 0.2202\n",
      "Epoch 2473: train loss: 0.0822, test loss 0.2215\n",
      "Epoch 2474: train loss: 0.0823, test loss 0.2197\n",
      "Epoch 2475: train loss: 0.0821, test loss 0.2206\n",
      "Epoch 2476: train loss: 0.0820, test loss 0.2186\n",
      "Epoch 2477: train loss: 0.0818, test loss 0.2186\n",
      "Epoch 2478: train loss: 0.0818, test loss 0.2180\n",
      "Epoch 2479: train loss: 0.0819, test loss 0.2168\n",
      "Epoch 2480: train loss: 0.0819, test loss 0.2176\n",
      "Epoch 2481: train loss: 0.0818, test loss 0.2171\n",
      "Epoch 2482: train loss: 0.0817, test loss 0.2181\n",
      "Epoch 2483: train loss: 0.0817, test loss 0.2190\n",
      "Epoch 2484: train loss: 0.0818, test loss 0.2183\n",
      "Epoch 2485: train loss: 0.0817, test loss 0.2193\n",
      "Epoch 2486: train loss: 0.0817, test loss 0.2187\n",
      "Epoch 2487: train loss: 0.0816, test loss 0.2190\n",
      "Epoch 2488: train loss: 0.0816, test loss 0.2188\n",
      "Epoch 2489: train loss: 0.0816, test loss 0.2182\n",
      "Epoch 2490: train loss: 0.0816, test loss 0.2186\n",
      "Epoch 2491: train loss: 0.0816, test loss 0.2179\n",
      "Epoch 2492: train loss: 0.0816, test loss 0.2187\n",
      "Epoch 2493: train loss: 0.0815, test loss 0.2183\n",
      "Epoch 2494: train loss: 0.0815, test loss 0.2187\n",
      "Epoch 2495: train loss: 0.0814, test loss 0.2183\n",
      "Epoch 2496: train loss: 0.0815, test loss 0.2178\n",
      "Epoch 2497: train loss: 0.0815, test loss 0.2182\n",
      "Epoch 2498: train loss: 0.0815, test loss 0.2175\n",
      "Epoch 2499: train loss: 0.0815, test loss 0.2184\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "loss_train_array = []\n",
    "loss_test_array = []\n",
    "epoch = 2500\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss_train = loss_fn(model(X_train), Y_train)\n",
    "    loss_train_array.append(loss_train)\n",
    "    loss_test = loss_fn(model(X_test), Y_test)\n",
    "    loss_test_array.append(loss_test)\n",
    "\n",
    "   \n",
    "    print('Epoch %s: train loss: %6.4f, test loss %6.4f'%(epoch, loss_train.item(), loss_test.item()))\n",
    "    # Backward pass\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,3), dpi=120)\n",
    "plt.plot(loss_train_array)\n",
    "plt.plot(loss_test_array)\n",
    "plt.legend(['train loss', 'test loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mse loss')\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing for each cordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_pred = model(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Pred'] = X_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_co_ord = OrderedDict()\n",
    "for co_ord, df_c in df_test.groupby(['lat','lon']):\n",
    "    loss = (df_c['Pred'] - df_c['lambda'])**2\n",
    "    loss_co_ord[str(co_ord)] = loss\n",
    "    # loss_co_ord.append(loss.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,3), dpi=120)\n",
    "plt.bar(loss_co_ord.keys(), [x.mean() for x in loss_co_ord.values()])\n",
    "# notch shape box plot\n",
    "# plt.boxplot(loss_co_ord.values(),\n",
    "#                      notch=True,  # notch shape\n",
    "#                      vert=True,  # vertical box alignment\n",
    "#                      patch_artist=True,  # fill with color\n",
    "#                      labels=loss_co_ord.keys())  # will be used to label x-ticks\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Model loss per co-ordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0aed7f3f2602e9ffc7bcdb3e0077e6e7eb290cd9bde9bdf0f85d0264c7b32cc9"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
